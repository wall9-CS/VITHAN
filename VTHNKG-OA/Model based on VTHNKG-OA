{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyP8En6c172zzpBAGcgbkSJK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749780187829,"user_tz":-540,"elapsed":20613,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"1c277d85-7d73-4d5f-ee0b-37f599c7ee8f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1749780191486,"user_tz":-540,"elapsed":3663,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1749780193327,"user_tz":-540,"elapsed":15,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0: # numeric triplet 처리\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","        else:\n","            self.proj_ent_vis = nn.Identity()\n","            self.proj_rel_vis = nn.Identity()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1749780195839,"user_tz":-540,"elapsed":43,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1749780203053,"user_tz":-540,"elapsed":64,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-OA/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, \"/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, \"/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load('/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt')\n","        elif os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load('/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt')\n","        elif os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load('/content/drive/MyDrive/code/VTKG-I/textual_features_ent.pt')\n","        self.rel2txt = torch.load('/content/drive/MyDrive/code/VTKG-I/textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-OA/\"\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1749780210413,"user_tz":-540,"elapsed":859,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"e480c3e3-27fa-47ab-d702-fa2187cf91e7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-OA\n"," checkpoint\t\t\t      relation2textlong.txt\n"," entities.txt\t\t\t      relation2text.txt\n"," entity2id.txt\t\t\t      relations.txt\n"," entity2textlong.txt\t\t      result\n"," entity2text.txt\t\t      test.txt\n"," legacy\t\t\t\t      train.txt\n","'Model based on VTHNKG-OA'\t      triplets_simple_qualifier_4o.txt\n","'Model based on VTHNKG-OA Weighted'   triplets.txt\n","'process VTHNKG-OA'\t\t      valid.txt\n"," relation2id.txt\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-OA_seed42_dim512\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=512, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749781281266,"user_tz":-540,"elapsed":1058079,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"28f08c6e-afda-47e4-ee0d-5eafe7166eda"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 26.407080 \t 13.839594 \t12.567486 \t 0.000000 \t 2.137819 s\n","1 \t 22.360462 \t 11.257538 \t11.102923 \t 0.000000 \t 2.884039 s\n","2 \t 22.080784 \t 11.079175 \t11.001609 \t 0.000000 \t 3.619625 s\n","3 \t 20.477900 \t 10.122077 \t10.355822 \t 0.000000 \t 4.384036 s\n","4 \t 20.433732 \t 10.224971 \t10.208761 \t 0.000000 \t 5.020422 s\n","5 \t 20.345304 \t 9.928816 \t10.416488 \t 0.000000 \t 5.660532 s\n","6 \t 20.426236 \t 9.966522 \t10.459713 \t 0.000000 \t 6.287108 s\n","7 \t 20.088509 \t 10.183090 \t9.905418 \t 0.000000 \t 6.935321 s\n","8 \t 20.041358 \t 9.784497 \t10.256861 \t 0.000000 \t 7.565285 s\n","9 \t 20.217115 \t 9.968080 \t10.249036 \t 0.000000 \t 8.212631 s\n","10 \t 20.314517 \t 10.063756 \t10.250762 \t 0.000000 \t 8.853057 s\n","11 \t 19.814704 \t 9.635498 \t10.179207 \t 0.000000 \t 9.481872 s\n","12 \t 20.323835 \t 10.027647 \t10.296188 \t 0.000000 \t 10.105575 s\n","13 \t 19.856042 \t 9.738558 \t10.117484 \t 0.000000 \t 10.882666 s\n","14 \t 19.977561 \t 9.875747 \t10.101814 \t 0.000000 \t 11.515338 s\n","15 \t 20.205336 \t 10.200547 \t10.004788 \t 0.000000 \t 12.155560 s\n","16 \t 20.118254 \t 10.008806 \t10.109447 \t 0.000000 \t 12.799628 s\n","17 \t 20.074327 \t 9.870895 \t10.203431 \t 0.000000 \t 13.494614 s\n","18 \t 19.915504 \t 9.615294 \t10.300210 \t 0.000000 \t 14.227278 s\n","19 \t 20.211155 \t 9.929620 \t10.281535 \t 0.000000 \t 14.937097 s\n","20 \t 19.948060 \t 9.846092 \t10.101968 \t 0.000000 \t 15.704597 s\n","21 \t 20.165510 \t 9.960464 \t10.205045 \t 0.000000 \t 16.443387 s\n","22 \t 19.764278 \t 9.573777 \t10.190501 \t 0.000000 \t 17.074266 s\n","23 \t 19.939634 \t 9.593026 \t10.346608 \t 0.000000 \t 17.718757 s\n","24 \t 19.315668 \t 9.351789 \t9.963880 \t 0.000000 \t 18.356544 s\n","25 \t 20.033411 \t 10.024007 \t10.009405 \t 0.000000 \t 19.112567 s\n","26 \t 19.951887 \t 9.803032 \t10.148855 \t 0.000000 \t 19.761644 s\n","27 \t 19.615864 \t 9.803046 \t9.812818 \t 0.000000 \t 20.416808 s\n","28 \t 19.815914 \t 9.884673 \t9.931242 \t 0.000000 \t 21.054332 s\n","29 \t 19.859847 \t 9.870065 \t9.989782 \t 0.000000 \t 21.699271 s\n","30 \t 19.584790 \t 9.642400 \t9.942390 \t 0.000000 \t 22.331049 s\n","31 \t 19.556528 \t 9.660158 \t9.896370 \t 0.000000 \t 22.973841 s\n","32 \t 19.883761 \t 10.048330 \t9.835431 \t 0.000000 \t 23.618232 s\n","33 \t 20.169783 \t 10.114019 \t10.055763 \t 0.000000 \t 24.254026 s\n","34 \t 19.829388 \t 9.800998 \t10.028390 \t 0.000000 \t 24.894809 s\n","35 \t 19.953201 \t 9.845324 \t10.107877 \t 0.000000 \t 25.531788 s\n","36 \t 19.911860 \t 9.777132 \t10.134728 \t 0.000000 \t 26.171737 s\n","37 \t 19.353220 \t 9.605994 \t9.747226 \t 0.000000 \t 27.119404 s\n","38 \t 19.878535 \t 9.816991 \t10.061544 \t 0.000000 \t 27.837580 s\n","39 \t 20.162647 \t 10.003327 \t10.159320 \t 0.000000 \t 28.593288 s\n","40 \t 20.013860 \t 10.009376 \t10.004484 \t 0.000000 \t 29.341538 s\n","41 \t 19.570243 \t 9.747263 \t9.822979 \t 0.000000 \t 29.974489 s\n","42 \t 19.660996 \t 9.901774 \t9.759222 \t 0.000000 \t 30.617526 s\n","43 \t 19.689236 \t 9.593247 \t10.095988 \t 0.000000 \t 31.264389 s\n","44 \t 20.087778 \t 9.998071 \t10.089707 \t 0.000000 \t 31.919421 s\n","45 \t 19.909897 \t 9.969897 \t9.940001 \t 0.000000 \t 32.683072 s\n","46 \t 19.457050 \t 9.512389 \t9.944661 \t 0.000000 \t 33.332029 s\n","47 \t 19.537336 \t 9.547023 \t9.990313 \t 0.000000 \t 33.969504 s\n","48 \t 19.694361 \t 9.801194 \t9.893167 \t 0.000000 \t 34.611279 s\n","49 \t 19.564711 \t 9.657272 \t9.907438 \t 0.000000 \t 35.256848 s\n","50 \t 20.352883 \t 10.126018 \t10.226865 \t 0.000000 \t 35.912899 s\n","51 \t 19.189598 \t 9.140211 \t10.049387 \t 0.000000 \t 36.556141 s\n","52 \t 20.240334 \t 9.902328 \t10.338006 \t 0.000000 \t 37.200631 s\n","53 \t 20.329590 \t 10.237038 \t10.092553 \t 0.000000 \t 37.833835 s\n","54 \t 19.523012 \t 9.544988 \t9.978024 \t 0.000000 \t 38.601960 s\n","55 \t 19.996530 \t 10.083962 \t9.912567 \t 0.000000 \t 39.277906 s\n","56 \t 19.646604 \t 9.895764 \t9.750839 \t 0.000000 \t 40.026398 s\n","57 \t 19.673996 \t 9.669837 \t10.004158 \t 0.000000 \t 40.744855 s\n","58 \t 19.864614 \t 9.817169 \t10.047445 \t 0.000000 \t 41.527285 s\n","59 \t 19.628810 \t 9.710105 \t9.918705 \t 0.000000 \t 42.285998 s\n","60 \t 19.807578 \t 9.844611 \t9.962967 \t 0.000000 \t 42.932727 s\n","61 \t 19.294691 \t 9.654068 \t9.640623 \t 0.000000 \t 43.575859 s\n","62 \t 19.594122 \t 9.661799 \t9.932322 \t 0.000000 \t 44.224695 s\n","63 \t 20.446263 \t 10.008549 \t10.437714 \t 0.000000 \t 44.998288 s\n","64 \t 20.012037 \t 9.772647 \t10.239389 \t 0.000000 \t 45.645295 s\n","65 \t 19.732926 \t 9.893170 \t9.839756 \t 0.000000 \t 46.301132 s\n","66 \t 20.133753 \t 9.981544 \t10.152209 \t 0.000000 \t 46.961068 s\n","67 \t 19.769960 \t 9.809114 \t9.960846 \t 0.000000 \t 47.609421 s\n","68 \t 20.053128 \t 9.949192 \t10.103936 \t 0.000000 \t 48.256641 s\n","69 \t 19.825685 \t 9.978841 \t9.846844 \t 0.000000 \t 48.919469 s\n","70 \t 19.853939 \t 10.109186 \t9.744753 \t 0.000000 \t 49.570518 s\n","71 \t 19.450268 \t 9.856024 \t9.594244 \t 0.000000 \t 50.222810 s\n","72 \t 19.472131 \t 9.719025 \t9.753106 \t 0.000000 \t 50.890637 s\n","73 \t 19.749680 \t 9.733129 \t10.016552 \t 0.000000 \t 51.538317 s\n","74 \t 20.034312 \t 9.918840 \t10.115471 \t 0.000000 \t 52.341587 s\n","75 \t 19.320506 \t 9.587066 \t9.733440 \t 0.000000 \t 53.106976 s\n","76 \t 19.345408 \t 9.564872 \t9.780537 \t 0.000000 \t 53.837333 s\n","77 \t 19.634609 \t 9.721567 \t9.913042 \t 0.000000 \t 54.626592 s\n","78 \t 19.763235 \t 9.752465 \t10.010770 \t 0.000000 \t 55.373176 s\n","79 \t 19.765549 \t 9.910596 \t9.854953 \t 0.000000 \t 56.029343 s\n","80 \t 19.223557 \t 9.528667 \t9.694890 \t 0.000000 \t 56.692009 s\n","81 \t 19.155395 \t 9.731121 \t9.424274 \t 0.000000 \t 57.347232 s\n","82 \t 19.111784 \t 9.581269 \t9.530515 \t 0.000000 \t 57.998896 s\n","83 \t 18.919944 \t 9.080992 \t9.838952 \t 0.000000 \t 58.661599 s\n","84 \t 19.286037 \t 9.642312 \t9.643726 \t 0.000000 \t 59.333856 s\n","85 \t 19.495158 \t 9.935019 \t9.560140 \t 0.000000 \t 60.113942 s\n","86 \t 19.044125 \t 9.597937 \t9.446187 \t 0.000000 \t 60.767142 s\n","87 \t 19.361114 \t 9.615303 \t9.745811 \t 0.000000 \t 61.430454 s\n","88 \t 19.346244 \t 9.706357 \t9.639887 \t 0.000000 \t 62.083096 s\n","89 \t 19.466015 \t 9.330976 \t10.135038 \t 0.000000 \t 62.746917 s\n","90 \t 19.264496 \t 9.624928 \t9.639568 \t 0.000000 \t 63.418993 s\n","91 \t 19.996672 \t 10.092069 \t9.904603 \t 0.000000 \t 64.081066 s\n","92 \t 19.285342 \t 9.684243 \t9.601099 \t 0.000000 \t 64.731624 s\n","93 \t 19.264243 \t 9.658799 \t9.605444 \t 0.000000 \t 65.567436 s\n","94 \t 19.744759 \t 9.519114 \t10.225645 \t 0.000000 \t 66.316523 s\n","95 \t 19.501817 \t 9.823444 \t9.678373 \t 0.000000 \t 67.067019 s\n","96 \t 18.784401 \t 9.099515 \t9.684886 \t 0.000000 \t 67.827749 s\n","97 \t 19.267589 \t 9.582717 \t9.684871 \t 0.000000 \t 68.593861 s\n","98 \t 18.995737 \t 9.826318 \t9.169419 \t 0.000000 \t 69.250216 s\n","99 \t 18.998576 \t 9.505369 \t9.493207 \t 0.000000 \t 69.911515 s\n","100 \t 18.799123 \t 9.654907 \t9.144216 \t 0.000000 \t 70.576334 s\n","101 \t 19.273391 \t 9.588322 \t9.685069 \t 0.000000 \t 71.234831 s\n","102 \t 18.873153 \t 9.507497 \t9.365655 \t 0.000000 \t 72.032134 s\n","103 \t 19.060200 \t 9.566625 \t9.493575 \t 0.000000 \t 72.697878 s\n","104 \t 19.380712 \t 9.659492 \t9.721220 \t 0.000000 \t 73.360301 s\n","105 \t 18.841187 \t 9.417065 \t9.424122 \t 0.000000 \t 74.018811 s\n","106 \t 19.518893 \t 9.648489 \t9.870404 \t 0.000000 \t 74.684960 s\n","107 \t 19.315404 \t 9.645867 \t9.669537 \t 0.000000 \t 75.354474 s\n","108 \t 18.623209 \t 9.276667 \t9.346542 \t 0.000000 \t 76.017420 s\n","109 \t 19.043031 \t 9.459181 \t9.583850 \t 0.000000 \t 76.690752 s\n","110 \t 18.697292 \t 9.473308 \t9.223985 \t 0.000000 \t 77.354895 s\n","111 \t 19.183705 \t 9.576209 \t9.607497 \t 0.000000 \t 78.162952 s\n","112 \t 18.493067 \t 9.304511 \t9.188555 \t 0.000000 \t 78.958049 s\n","113 \t 18.948798 \t 9.450067 \t9.498732 \t 0.000000 \t 79.725001 s\n","114 \t 18.559395 \t 9.335648 \t9.223747 \t 0.000000 \t 80.497482 s\n","115 \t 18.474110 \t 9.583497 \t8.890613 \t 0.000000 \t 81.297610 s\n","116 \t 18.839808 \t 9.516286 \t9.323521 \t 0.000000 \t 81.967070 s\n","117 \t 18.644810 \t 9.417548 \t9.227262 \t 0.000000 \t 82.645011 s\n","118 \t 18.936852 \t 9.501113 \t9.435739 \t 0.000000 \t 83.308167 s\n","119 \t 18.603984 \t 9.039742 \t9.564243 \t 0.000000 \t 83.978137 s\n","120 \t 19.125518 \t 9.642900 \t9.482617 \t 0.000000 \t 84.651366 s\n","121 \t 18.637600 \t 9.704140 \t8.933460 \t 0.000000 \t 85.338943 s\n","122 \t 18.773359 \t 9.560465 \t9.212895 \t 0.000000 \t 86.133175 s\n","123 \t 18.909865 \t 9.408327 \t9.501538 \t 0.000000 \t 86.804404 s\n","124 \t 19.285419 \t 9.562570 \t9.722849 \t 0.000000 \t 87.479303 s\n","125 \t 18.992708 \t 9.778246 \t9.214462 \t 0.000000 \t 88.145419 s\n","126 \t 19.254868 \t 9.863494 \t9.391374 \t 0.000000 \t 88.816456 s\n","127 \t 18.927519 \t 9.688749 \t9.238769 \t 0.000000 \t 89.487818 s\n","128 \t 19.615615 \t 9.867190 \t9.748425 \t 0.000000 \t 90.158308 s\n","129 \t 19.142860 \t 9.709949 \t9.432911 \t 0.000000 \t 90.833917 s\n","130 \t 18.998662 \t 9.519461 \t9.479201 \t 0.000000 \t 91.573207 s\n","131 \t 19.281197 \t 9.699035 \t9.582162 \t 0.000000 \t 92.343015 s\n","132 \t 19.792089 \t 9.700306 \t10.091783 \t 0.000000 \t 93.122386 s\n","133 \t 18.955010 \t 9.480814 \t9.474196 \t 0.000000 \t 94.128874 s\n","134 \t 19.108593 \t 9.510925 \t9.597669 \t 0.000000 \t 94.833632 s\n","135 \t 19.252684 \t 9.700194 \t9.552490 \t 0.000000 \t 95.510918 s\n","136 \t 19.048553 \t 9.525389 \t9.523165 \t 0.000000 \t 96.193789 s\n","137 \t 18.680189 \t 9.615713 \t9.064476 \t 0.000000 \t 96.872266 s\n","138 \t 19.322326 \t 9.273556 \t10.048770 \t 0.000000 \t 97.550350 s\n","139 \t 18.704345 \t 9.454276 \t9.250069 \t 0.000000 \t 98.232867 s\n","140 \t 18.838498 \t 9.518098 \t9.320399 \t 0.000000 \t 98.910573 s\n","141 \t 18.924063 \t 9.802275 \t9.121788 \t 0.000000 \t 99.582212 s\n","142 \t 19.285942 \t 9.830224 \t9.455718 \t 0.000000 \t 100.257761 s\n","143 \t 18.885293 \t 9.512508 \t9.372785 \t 0.000000 \t 100.938227 s\n","144 \t 19.046131 \t 9.763511 \t9.282620 \t 0.000000 \t 101.623099 s\n","145 \t 18.853174 \t 9.539030 \t9.314145 \t 0.000000 \t 102.540857 s\n","146 \t 19.271823 \t 9.667324 \t9.604499 \t 0.000000 \t 103.223733 s\n","147 \t 18.979906 \t 9.632717 \t9.347189 \t 0.000000 \t 103.912881 s\n","148 \t 19.286469 \t 9.669347 \t9.617121 \t 0.000000 \t 104.630032 s\n","149 \t 19.065966 \t 9.777489 \t9.288477 \t 0.000000 \t 105.387432 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/130 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 130/130 [00:34<00:00,  3.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3754\n","Hit@10: 0.4615\n","Hit@3: 0.3769\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2528\n","Hit@10: 0.3850\n","Hit@3: 0.2439\n","Hit@1: 0.1864\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3018\n","Hit@10: 0.5154\n","Hit@3: 0.3154\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3407\n","Hit@10: 0.5698\n","Hit@3: 0.3626\n","Hit@1: 0.2342\n","150 \t 18.927608 \t 9.609056 \t9.318552 \t 0.000000 \t 143.407747 s\n","151 \t 19.552440 \t 9.604945 \t9.947494 \t 0.000000 \t 144.130471 s\n","152 \t 19.171940 \t 9.479202 \t9.692738 \t 0.000000 \t 144.867495 s\n","153 \t 19.283146 \t 9.704091 \t9.579054 \t 0.000000 \t 145.638348 s\n","154 \t 19.357813 \t 9.662639 \t9.695174 \t 0.000000 \t 146.607896 s\n","155 \t 19.290420 \t 9.465385 \t9.825035 \t 0.000000 \t 147.267124 s\n","156 \t 19.008492 \t 9.509881 \t9.498610 \t 0.000000 \t 147.924059 s\n","157 \t 19.953032 \t 9.680894 \t10.272138 \t 0.000000 \t 148.580425 s\n","158 \t 19.397304 \t 9.612325 \t9.784978 \t 0.000000 \t 149.245937 s\n","159 \t 18.463007 \t 9.270305 \t9.192702 \t 0.000000 \t 149.913991 s\n","160 \t 19.161110 \t 9.419314 \t9.741796 \t 0.000000 \t 150.574561 s\n","161 \t 18.619714 \t 9.189180 \t9.430533 \t 0.000000 \t 151.269666 s\n","162 \t 19.643893 \t 9.666298 \t9.977594 \t 0.000000 \t 152.018068 s\n","163 \t 18.817481 \t 9.337824 \t9.479656 \t 0.000000 \t 152.740345 s\n","164 \t 18.999309 \t 9.513044 \t9.486264 \t 0.000000 \t 153.486017 s\n","165 \t 19.530235 \t 9.966247 \t9.563989 \t 0.000000 \t 154.372967 s\n","166 \t 19.839995 \t 9.687012 \t10.152983 \t 0.000000 \t 155.123206 s\n","167 \t 19.199367 \t 9.332120 \t9.867247 \t 0.000000 \t 155.845716 s\n","168 \t 19.526269 \t 9.567275 \t9.958993 \t 0.000000 \t 156.626726 s\n","169 \t 18.731520 \t 9.413949 \t9.317571 \t 0.000000 \t 157.512940 s\n","170 \t 18.727478 \t 9.671943 \t9.055534 \t 0.000000 \t 158.356388 s\n","171 \t 19.378210 \t 9.820704 \t9.557506 \t 0.000000 \t 159.268074 s\n","172 \t 19.028808 \t 9.588060 \t9.440747 \t 0.000000 \t 160.142337 s\n","173 \t 18.929226 \t 9.119348 \t9.809878 \t 0.000000 \t 160.891142 s\n","174 \t 19.183012 \t 9.777219 \t9.405793 \t 0.000000 \t 161.626114 s\n","175 \t 19.062208 \t 9.352456 \t9.709752 \t 0.000000 \t 162.289088 s\n","176 \t 18.486536 \t 9.651054 \t8.835482 \t 0.000000 \t 163.099036 s\n","177 \t 18.528206 \t 9.256875 \t9.271332 \t 0.000000 \t 163.766193 s\n","178 \t 18.832734 \t 9.586787 \t9.245947 \t 0.000000 \t 164.440240 s\n","179 \t 18.674136 \t 9.319827 \t9.354310 \t 0.000000 \t 165.105252 s\n","180 \t 19.206145 \t 9.825101 \t9.381044 \t 0.000000 \t 165.774508 s\n","181 \t 18.699609 \t 9.347578 \t9.352032 \t 0.000000 \t 166.444898 s\n","182 \t 18.665606 \t 9.424720 \t9.240886 \t 0.000000 \t 167.112282 s\n","183 \t 18.468209 \t 9.421838 \t9.046371 \t 0.000000 \t 167.771892 s\n","184 \t 18.660025 \t 9.413066 \t9.246959 \t 0.000000 \t 168.446963 s\n","185 \t 19.086938 \t 9.667981 \t9.418957 \t 0.000000 \t 169.116846 s\n","186 \t 18.885980 \t 9.515203 \t9.370776 \t 0.000000 \t 169.788434 s\n","187 \t 18.665307 \t 9.448772 \t9.216534 \t 0.000000 \t 170.570966 s\n","188 \t 18.755666 \t 9.421743 \t9.333923 \t 0.000000 \t 171.505445 s\n","189 \t 18.250711 \t 9.366164 \t8.884548 \t 0.000000 \t 172.300359 s\n","190 \t 19.230055 \t 9.408221 \t9.821834 \t 0.000000 \t 173.061365 s\n","191 \t 17.921595 \t 9.195318 \t8.726276 \t 0.000000 \t 173.729409 s\n","192 \t 18.575538 \t 9.382106 \t9.193432 \t 0.000000 \t 174.403821 s\n","193 \t 18.448714 \t 9.189125 \t9.259589 \t 0.000000 \t 175.079522 s\n","194 \t 18.737289 \t 9.524832 \t9.212457 \t 0.000000 \t 175.756344 s\n","195 \t 19.090262 \t 9.822904 \t9.267358 \t 0.000000 \t 176.436832 s\n","196 \t 19.189081 \t 9.808238 \t9.380844 \t 0.000000 \t 177.110897 s\n","197 \t 18.666339 \t 9.458642 \t9.207697 \t 0.000000 \t 177.921134 s\n","198 \t 19.406722 \t 9.933538 \t9.473183 \t 0.000000 \t 178.595227 s\n","199 \t 18.781069 \t 9.465218 \t9.315851 \t 0.000000 \t 179.265490 s\n","200 \t 18.758631 \t 9.866810 \t8.891820 \t 0.000000 \t 179.937593 s\n","201 \t 18.955425 \t 9.461767 \t9.493658 \t 0.000000 \t 180.622789 s\n","202 \t 18.814027 \t 9.675966 \t9.138062 \t 0.000000 \t 181.292809 s\n","203 \t 18.196516 \t 8.988807 \t9.207709 \t 0.000000 \t 181.970420 s\n","204 \t 19.125820 \t 9.671148 \t9.454672 \t 0.000000 \t 182.643917 s\n","205 \t 18.923637 \t 9.607863 \t9.315774 \t 0.000000 \t 183.519913 s\n","206 \t 19.015863 \t 9.840210 \t9.175653 \t 0.000000 \t 184.297301 s\n","207 \t 18.552083 \t 9.435187 \t9.116896 \t 0.000000 \t 185.074616 s\n","208 \t 18.919888 \t 9.614505 \t9.305383 \t 0.000000 \t 185.889149 s\n","209 \t 18.807172 \t 9.347255 \t9.459917 \t 0.000000 \t 186.560173 s\n","210 \t 19.417658 \t 9.883906 \t9.533751 \t 0.000000 \t 187.226788 s\n","211 \t 18.410791 \t 9.185908 \t9.224883 \t 0.000000 \t 187.898788 s\n","212 \t 19.013071 \t 9.709332 \t9.303740 \t 0.000000 \t 188.571581 s\n","213 \t 18.682949 \t 9.682691 \t9.000257 \t 0.000000 \t 189.239100 s\n","214 \t 19.290539 \t 9.700996 \t9.589542 \t 0.000000 \t 190.050341 s\n","215 \t 18.551695 \t 9.217306 \t9.334389 \t 0.000000 \t 190.714178 s\n","216 \t 19.108742 \t 9.689258 \t9.419483 \t 0.000000 \t 191.376556 s\n","217 \t 18.829218 \t 9.216847 \t9.612370 \t 0.000000 \t 192.075849 s\n","218 \t 18.693584 \t 9.588215 \t9.105370 \t 0.000000 \t 192.740369 s\n","219 \t 18.868332 \t 9.763202 \t9.105130 \t 0.000000 \t 193.409726 s\n","220 \t 19.499225 \t 9.690846 \t9.808378 \t 0.000000 \t 194.076607 s\n","221 \t 19.226699 \t 9.842209 \t9.384489 \t 0.000000 \t 194.768416 s\n","222 \t 18.411444 \t 9.456685 \t8.954758 \t 0.000000 \t 195.439336 s\n","223 \t 18.921785 \t 9.626777 \t9.295008 \t 0.000000 \t 196.158653 s\n","224 \t 18.515613 \t 8.947114 \t9.568498 \t 0.000000 \t 197.088367 s\n","225 \t 19.035314 \t 9.599443 \t9.435871 \t 0.000000 \t 197.867446 s\n","226 \t 18.549625 \t 9.380149 \t9.169477 \t 0.000000 \t 198.694920 s\n","227 \t 19.396195 \t 9.485112 \t9.911084 \t 0.000000 \t 199.371893 s\n","228 \t 19.206482 \t 9.518272 \t9.688210 \t 0.000000 \t 200.051304 s\n","229 \t 19.707504 \t 10.111746 \t9.595758 \t 0.000000 \t 200.728002 s\n","230 \t 18.964425 \t 9.893177 \t9.071248 \t 0.000000 \t 201.388696 s\n","231 \t 19.297831 \t 9.469856 \t9.827974 \t 0.000000 \t 202.072817 s\n","232 \t 19.217868 \t 9.616819 \t9.601049 \t 0.000000 \t 202.745770 s\n","233 \t 18.528648 \t 9.484903 \t9.043746 \t 0.000000 \t 203.413440 s\n","234 \t 18.434738 \t 9.544107 \t8.890631 \t 0.000000 \t 204.092608 s\n","235 \t 19.022541 \t 9.827142 \t9.195399 \t 0.000000 \t 204.882643 s\n","236 \t 18.826934 \t 9.632070 \t9.194864 \t 0.000000 \t 205.564085 s\n","237 \t 18.576572 \t 9.173180 \t9.403391 \t 0.000000 \t 206.251523 s\n","238 \t 18.839517 \t 9.491456 \t9.348062 \t 0.000000 \t 206.925906 s\n","239 \t 18.770149 \t 9.414934 \t9.355215 \t 0.000000 \t 207.598402 s\n","240 \t 18.860367 \t 9.357349 \t9.503017 \t 0.000000 \t 208.279707 s\n","241 \t 19.249878 \t 9.453748 \t9.796130 \t 0.000000 \t 208.981958 s\n","242 \t 19.875504 \t 9.768572 \t10.106932 \t 0.000000 \t 209.790193 s\n","243 \t 18.827085 \t 9.369072 \t9.458013 \t 0.000000 \t 210.540065 s\n","244 \t 18.908176 \t 9.369133 \t9.539044 \t 0.000000 \t 211.303391 s\n","245 \t 19.069067 \t 9.545588 \t9.523479 \t 0.000000 \t 212.066608 s\n","246 \t 18.259546 \t 9.023628 \t9.235918 \t 0.000000 \t 212.865259 s\n","247 \t 18.755297 \t 9.528445 \t9.226852 \t 0.000000 \t 213.541104 s\n","248 \t 18.769722 \t 9.567861 \t9.201862 \t 0.000000 \t 214.211871 s\n","249 \t 18.475616 \t 9.316273 \t9.159344 \t 0.000000 \t 214.870830 s\n","250 \t 18.624030 \t 9.408055 \t9.215975 \t 0.000000 \t 215.553726 s\n","251 \t 18.860107 \t 9.566283 \t9.293825 \t 0.000000 \t 216.228205 s\n","252 \t 19.104139 \t 9.727243 \t9.376896 \t 0.000000 \t 216.890356 s\n","253 \t 19.139654 \t 9.849854 \t9.289800 \t 0.000000 \t 217.561752 s\n","254 \t 19.476542 \t 9.634880 \t9.841663 \t 0.000000 \t 218.360697 s\n","255 \t 18.467692 \t 8.845848 \t9.621845 \t 0.000000 \t 219.023447 s\n","256 \t 18.886490 \t 9.633377 \t9.253112 \t 0.000000 \t 219.700298 s\n","257 \t 19.079951 \t 9.661581 \t9.418370 \t 0.000000 \t 220.362306 s\n","258 \t 18.982845 \t 9.619424 \t9.363421 \t 0.000000 \t 221.032590 s\n","259 \t 18.811463 \t 9.439157 \t9.372306 \t 0.000000 \t 221.706821 s\n","260 \t 18.970173 \t 9.686498 \t9.283675 \t 0.000000 \t 222.493364 s\n","261 \t 18.566134 \t 9.184383 \t9.381752 \t 0.000000 \t 223.260276 s\n","262 \t 19.690295 \t 10.069055 \t9.621241 \t 0.000000 \t 224.039947 s\n","263 \t 19.098203 \t 9.701205 \t9.396998 \t 0.000000 \t 224.995438 s\n","264 \t 19.104989 \t 9.737281 \t9.367708 \t 0.000000 \t 225.656386 s\n","265 \t 18.859593 \t 9.589960 \t9.269634 \t 0.000000 \t 226.313022 s\n","266 \t 19.383694 \t 9.390853 \t9.992841 \t 0.000000 \t 227.003494 s\n","267 \t 18.760592 \t 9.637961 \t9.122631 \t 0.000000 \t 227.674075 s\n","268 \t 18.725163 \t 9.438249 \t9.286913 \t 0.000000 \t 228.339063 s\n","269 \t 18.952741 \t 9.435647 \t9.517095 \t 0.000000 \t 229.008842 s\n","270 \t 18.936918 \t 9.598117 \t9.338802 \t 0.000000 \t 229.681290 s\n","271 \t 18.670601 \t 9.733336 \t8.937264 \t 0.000000 \t 230.345389 s\n","272 \t 19.202124 \t 9.497887 \t9.704237 \t 0.000000 \t 231.009377 s\n","273 \t 19.494081 \t 9.616342 \t9.877740 \t 0.000000 \t 231.802336 s\n","274 \t 18.259928 \t 9.170708 \t9.089220 \t 0.000000 \t 232.467430 s\n","275 \t 18.924994 \t 9.305396 \t9.619598 \t 0.000000 \t 233.129469 s\n","276 \t 18.721830 \t 9.119331 \t9.602499 \t 0.000000 \t 233.797760 s\n","277 \t 18.837168 \t 9.495571 \t9.341597 \t 0.000000 \t 234.463042 s\n","278 \t 18.957931 \t 9.631777 \t9.326154 \t 0.000000 \t 235.168761 s\n","279 \t 19.162231 \t 9.390604 \t9.771628 \t 0.000000 \t 235.921921 s\n","280 \t 18.549800 \t 9.383109 \t9.166691 \t 0.000000 \t 236.677827 s\n","281 \t 19.407417 \t 9.777374 \t9.630043 \t 0.000000 \t 237.486649 s\n","282 \t 18.654669 \t 9.317042 \t9.337626 \t 0.000000 \t 238.261733 s\n","283 \t 19.296738 \t 9.782715 \t9.514022 \t 0.000000 \t 238.936509 s\n","284 \t 18.747974 \t 9.510353 \t9.237621 \t 0.000000 \t 239.722833 s\n","285 \t 18.881856 \t 9.438635 \t9.443221 \t 0.000000 \t 240.387953 s\n","286 \t 18.768263 \t 9.552315 \t9.215948 \t 0.000000 \t 241.067422 s\n","287 \t 18.634218 \t 9.546467 \t9.087752 \t 0.000000 \t 241.725079 s\n","288 \t 18.691363 \t 9.596265 \t9.095098 \t 0.000000 \t 242.388949 s\n","289 \t 18.900351 \t 9.383823 \t9.516527 \t 0.000000 \t 243.056411 s\n","290 \t 19.209576 \t 9.591592 \t9.617984 \t 0.000000 \t 243.722362 s\n","291 \t 18.650044 \t 9.121028 \t9.529016 \t 0.000000 \t 244.392264 s\n","292 \t 19.435627 \t 9.824371 \t9.611256 \t 0.000000 \t 245.047265 s\n","293 \t 19.226869 \t 10.019342 \t9.207527 \t 0.000000 \t 245.727011 s\n","294 \t 18.518625 \t 9.113029 \t9.405597 \t 0.000000 \t 246.512367 s\n","295 \t 18.712373 \t 9.192477 \t9.519896 \t 0.000000 \t 247.182125 s\n","296 \t 19.092954 \t 9.622299 \t9.470654 \t 0.000000 \t 247.878628 s\n","297 \t 19.168242 \t 9.648479 \t9.519763 \t 0.000000 \t 248.644378 s\n","298 \t 19.169596 \t 9.438152 \t9.731443 \t 0.000000 \t 249.395904 s\n","299 \t 18.692640 \t 9.496962 \t9.195679 \t 0.000000 \t 250.177317 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3782\n","Hit@10: 0.4769\n","Hit@3: 0.3731\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2732\n","Hit@10: 0.4373\n","Hit@3: 0.2666\n","Hit@1: 0.1969\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2943\n","Hit@10: 0.5308\n","Hit@3: 0.3385\n","Hit@1: 0.1923\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3025\n","Hit@10: 0.5923\n","Hit@3: 0.3581\n","Hit@1: 0.1689\n","300 \t 18.757315 \t 9.245573 \t9.511741 \t 0.000000 \t 287.532079 s\n","301 \t 18.669751 \t 9.577712 \t9.092039 \t 0.000000 \t 288.285183 s\n","302 \t 18.632565 \t 9.434084 \t9.198482 \t 0.000000 \t 289.042657 s\n","303 \t 18.435761 \t 9.177288 \t9.258473 \t 0.000000 \t 289.842763 s\n","304 \t 18.330293 \t 9.806499 \t8.523794 \t 0.000000 \t 290.622140 s\n","305 \t 18.187318 \t 9.221003 \t8.966315 \t 0.000000 \t 291.420357 s\n","306 \t 18.896289 \t 9.465065 \t9.431224 \t 0.000000 \t 292.084518 s\n","307 \t 18.931349 \t 9.268055 \t9.663294 \t 0.000000 \t 292.767166 s\n","308 \t 18.838652 \t 9.270061 \t9.568590 \t 0.000000 \t 293.438053 s\n","309 \t 18.315273 \t 9.085842 \t9.229431 \t 0.000000 \t 294.096261 s\n","310 \t 18.751964 \t 9.341891 \t9.410073 \t 0.000000 \t 294.771966 s\n","311 \t 19.259729 \t 9.753340 \t9.506390 \t 0.000000 \t 295.488862 s\n","312 \t 18.988632 \t 9.358683 \t9.629949 \t 0.000000 \t 296.251938 s\n","313 \t 18.793720 \t 9.563588 \t9.230133 \t 0.000000 \t 297.018320 s\n","314 \t 19.173335 \t 9.666245 \t9.507091 \t 0.000000 \t 297.803283 s\n","315 \t 18.601117 \t 9.277884 \t9.323234 \t 0.000000 \t 298.571639 s\n","316 \t 19.137548 \t 9.543645 \t9.593904 \t 0.000000 \t 299.488518 s\n","317 \t 18.377048 \t 9.194890 \t9.182158 \t 0.000000 \t 300.289013 s\n","318 \t 18.522587 \t 9.266428 \t9.256159 \t 0.000000 \t 301.204887 s\n","319 \t 18.327224 \t 9.016390 \t9.310834 \t 0.000000 \t 302.075795 s\n","320 \t 18.439790 \t 9.230328 \t9.209462 \t 0.000000 \t 302.975057 s\n","321 \t 18.804015 \t 9.577658 \t9.226357 \t 0.000000 \t 303.844158 s\n","322 \t 18.465827 \t 9.130425 \t9.335402 \t 0.000000 \t 304.564948 s\n","323 \t 18.777487 \t 9.559964 \t9.217524 \t 0.000000 \t 305.234614 s\n","324 \t 19.278059 \t 9.716872 \t9.561188 \t 0.000000 \t 305.903652 s\n","325 \t 18.937080 \t 9.422066 \t9.515015 \t 0.000000 \t 306.559230 s\n","326 \t 18.884027 \t 9.604086 \t9.279941 \t 0.000000 \t 307.224397 s\n","327 \t 18.844449 \t 9.532709 \t9.311739 \t 0.000000 \t 307.895259 s\n","328 \t 18.660481 \t 9.471065 \t9.189417 \t 0.000000 \t 308.692055 s\n","329 \t 18.811320 \t 9.294645 \t9.516675 \t 0.000000 \t 309.368424 s\n","330 \t 17.778474 \t 8.870176 \t8.908297 \t 0.000000 \t 310.039684 s\n","331 \t 18.989819 \t 9.514039 \t9.475780 \t 0.000000 \t 310.706336 s\n","332 \t 19.425999 \t 9.733654 \t9.692346 \t 0.000000 \t 311.371217 s\n","333 \t 18.460773 \t 9.161091 \t9.299682 \t 0.000000 \t 312.037119 s\n","334 \t 18.667198 \t 9.447447 \t9.219751 \t 0.000000 \t 312.717244 s\n","335 \t 19.054011 \t 9.294779 \t9.759233 \t 0.000000 \t 313.378423 s\n","336 \t 18.492210 \t 9.496971 \t8.995240 \t 0.000000 \t 314.105997 s\n","337 \t 18.593172 \t 9.494531 \t9.098642 \t 0.000000 \t 314.848333 s\n","338 \t 18.538891 \t 9.215427 \t9.323463 \t 0.000000 \t 315.611005 s\n","339 \t 18.354063 \t 9.199061 \t9.155002 \t 0.000000 \t 316.402282 s\n","340 \t 18.658751 \t 9.414991 \t9.243759 \t 0.000000 \t 317.268790 s\n","341 \t 18.712976 \t 9.470534 \t9.242442 \t 0.000000 \t 317.931705 s\n","342 \t 18.511997 \t 9.568092 \t8.943906 \t 0.000000 \t 318.595825 s\n","343 \t 18.978461 \t 9.491141 \t9.487321 \t 0.000000 \t 319.265455 s\n","344 \t 19.309831 \t 9.548894 \t9.760936 \t 0.000000 \t 319.933044 s\n","345 \t 18.491343 \t 9.053310 \t9.438033 \t 0.000000 \t 320.609668 s\n","346 \t 18.999991 \t 9.727975 \t9.272016 \t 0.000000 \t 321.272685 s\n","347 \t 18.463328 \t 9.532590 \t8.930738 \t 0.000000 \t 321.953185 s\n","348 \t 18.821996 \t 9.479633 \t9.342362 \t 0.000000 \t 322.620519 s\n","349 \t 18.957184 \t 9.153845 \t9.803339 \t 0.000000 \t 323.279651 s\n","350 \t 18.896855 \t 9.447502 \t9.449354 \t 0.000000 \t 323.962472 s\n","351 \t 18.418014 \t 9.155481 \t9.262532 \t 0.000000 \t 324.629317 s\n","352 \t 18.961827 \t 9.442721 \t9.519106 \t 0.000000 \t 325.432930 s\n","353 \t 18.123426 \t 9.192073 \t8.931354 \t 0.000000 \t 326.097238 s\n","354 \t 18.803572 \t 9.469539 \t9.334033 \t 0.000000 \t 326.807655 s\n","355 \t 18.417513 \t 9.300878 \t9.116635 \t 0.000000 \t 327.580477 s\n","356 \t 18.993417 \t 9.580548 \t9.412868 \t 0.000000 \t 328.332611 s\n","357 \t 18.984990 \t 9.462945 \t9.522045 \t 0.000000 \t 329.104270 s\n","358 \t 18.489454 \t 9.487022 \t9.002431 \t 0.000000 \t 329.868973 s\n","359 \t 18.844190 \t 9.245881 \t9.598308 \t 0.000000 \t 330.541706 s\n","360 \t 18.666204 \t 9.556124 \t9.110081 \t 0.000000 \t 331.219649 s\n","361 \t 19.230914 \t 9.488101 \t9.742814 \t 0.000000 \t 332.014060 s\n","362 \t 18.134720 \t 9.021718 \t9.113002 \t 0.000000 \t 332.684594 s\n","363 \t 18.539076 \t 9.461251 \t9.077824 \t 0.000000 \t 333.368453 s\n","364 \t 18.938794 \t 9.583411 \t9.355383 \t 0.000000 \t 334.040795 s\n","365 \t 18.708861 \t 9.595750 \t9.113111 \t 0.000000 \t 334.704942 s\n","366 \t 18.201332 \t 9.375788 \t8.825544 \t 0.000000 \t 335.390254 s\n","367 \t 18.922558 \t 9.509003 \t9.413555 \t 0.000000 \t 336.064426 s\n","368 \t 19.150784 \t 9.714052 \t9.436732 \t 0.000000 \t 336.732442 s\n","369 \t 18.331618 \t 8.994370 \t9.337248 \t 0.000000 \t 337.557339 s\n","370 \t 19.005733 \t 9.591692 \t9.414042 \t 0.000000 \t 338.226869 s\n","371 \t 18.716460 \t 9.460251 \t9.256210 \t 0.000000 \t 338.891631 s\n","372 \t 18.795054 \t 9.479568 \t9.315487 \t 0.000000 \t 339.574500 s\n","373 \t 19.252548 \t 9.637984 \t9.614564 \t 0.000000 \t 340.352354 s\n","374 \t 18.896381 \t 9.535723 \t9.360659 \t 0.000000 \t 341.120106 s\n","375 \t 18.298166 \t 9.062632 \t9.235535 \t 0.000000 \t 341.900076 s\n","376 \t 19.445177 \t 9.644750 \t9.800427 \t 0.000000 \t 342.703237 s\n","377 \t 18.783333 \t 9.378509 \t9.404824 \t 0.000000 \t 343.374971 s\n","378 \t 18.465407 \t 9.361842 \t9.103566 \t 0.000000 \t 344.041776 s\n","379 \t 19.014943 \t 9.460181 \t9.554763 \t 0.000000 \t 344.863056 s\n","380 \t 19.167215 \t 9.619856 \t9.547360 \t 0.000000 \t 345.532948 s\n","381 \t 18.089430 \t 9.117703 \t8.971728 \t 0.000000 \t 346.203493 s\n","382 \t 18.794529 \t 9.343467 \t9.451063 \t 0.000000 \t 346.878569 s\n","383 \t 18.936072 \t 9.543428 \t9.392644 \t 0.000000 \t 347.546818 s\n","384 \t 18.783844 \t 9.446010 \t9.337834 \t 0.000000 \t 348.219894 s\n","385 \t 18.795943 \t 9.743464 \t9.052480 \t 0.000000 \t 348.909548 s\n","386 \t 18.815283 \t 9.640162 \t9.175120 \t 0.000000 \t 349.573609 s\n","387 \t 18.148869 \t 9.162298 \t8.986571 \t 0.000000 \t 350.242587 s\n","388 \t 18.338380 \t 9.119559 \t9.218821 \t 0.000000 \t 350.936430 s\n","389 \t 18.182313 \t 9.046047 \t9.136266 \t 0.000000 \t 351.601833 s\n","390 \t 19.067352 \t 9.509656 \t9.557696 \t 0.000000 \t 352.391643 s\n","391 \t 18.758314 \t 9.581318 \t9.176996 \t 0.000000 \t 353.115784 s\n","392 \t 18.775589 \t 9.660098 \t9.115490 \t 0.000000 \t 353.864172 s\n","393 \t 18.648667 \t 9.356665 \t9.292002 \t 0.000000 \t 354.632528 s\n","394 \t 19.231434 \t 9.430659 \t9.800775 \t 0.000000 \t 355.455343 s\n","395 \t 18.983699 \t 9.557002 \t9.426697 \t 0.000000 \t 356.154297 s\n","396 \t 18.325522 \t 9.112479 \t9.213043 \t 0.000000 \t 356.823615 s\n","397 \t 18.772401 \t 9.332559 \t9.439842 \t 0.000000 \t 357.493307 s\n","398 \t 18.698903 \t 9.432852 \t9.266051 \t 0.000000 \t 358.153536 s\n","399 \t 18.948812 \t 9.226175 \t9.722638 \t 0.000000 \t 358.826316 s\n","400 \t 18.885395 \t 9.690350 \t9.195045 \t 0.000000 \t 359.490096 s\n","401 \t 18.926984 \t 9.559132 \t9.367852 \t 0.000000 \t 360.285681 s\n","402 \t 19.130383 \t 9.423222 \t9.707162 \t 0.000000 \t 360.960935 s\n","403 \t 18.754646 \t 9.266471 \t9.488174 \t 0.000000 \t 361.623673 s\n","404 \t 18.245985 \t 9.343915 \t8.902069 \t 0.000000 \t 362.282543 s\n","405 \t 17.686196 \t 8.971511 \t8.714685 \t 0.000000 \t 362.959778 s\n","406 \t 18.768204 \t 9.169128 \t9.599076 \t 0.000000 \t 363.620542 s\n","407 \t 18.906303 \t 9.359752 \t9.546552 \t 0.000000 \t 364.289894 s\n","408 \t 18.586006 \t 9.316768 \t9.269238 \t 0.000000 \t 364.969542 s\n","409 \t 18.831711 \t 9.403875 \t9.427835 \t 0.000000 \t 365.666507 s\n","410 \t 18.903785 \t 9.521126 \t9.382659 \t 0.000000 \t 366.434954 s\n","411 \t 18.573597 \t 9.398529 \t9.175068 \t 0.000000 \t 367.191296 s\n","412 \t 18.788853 \t 9.650684 \t9.138169 \t 0.000000 \t 367.973319 s\n","413 \t 18.611589 \t 9.362036 \t9.249553 \t 0.000000 \t 368.908007 s\n","414 \t 18.960728 \t 9.469400 \t9.491328 \t 0.000000 \t 369.569648 s\n","415 \t 18.856285 \t 9.780930 \t9.075356 \t 0.000000 \t 370.250429 s\n","416 \t 18.637458 \t 9.275359 \t9.362098 \t 0.000000 \t 370.919948 s\n","417 \t 17.844870 \t 8.912367 \t8.932502 \t 0.000000 \t 371.595434 s\n","418 \t 18.594452 \t 9.707609 \t8.886844 \t 0.000000 \t 372.268893 s\n","419 \t 19.317097 \t 9.737684 \t9.579412 \t 0.000000 \t 372.943882 s\n","420 \t 18.665476 \t 9.180293 \t9.485182 \t 0.000000 \t 373.601136 s\n","421 \t 18.919044 \t 9.555630 \t9.363414 \t 0.000000 \t 374.270264 s\n","422 \t 19.334767 \t 9.813868 \t9.520900 \t 0.000000 \t 374.943520 s\n","423 \t 18.901196 \t 9.623101 \t9.278095 \t 0.000000 \t 375.603069 s\n","424 \t 18.060773 \t 9.156178 \t8.904594 \t 0.000000 \t 376.257702 s\n","425 \t 18.252687 \t 9.032743 \t9.219943 \t 0.000000 \t 377.084323 s\n","426 \t 18.757156 \t 9.480308 \t9.276848 \t 0.000000 \t 377.752274 s\n","427 \t 18.599907 \t 9.391589 \t9.208318 \t 0.000000 \t 378.425863 s\n","428 \t 18.443770 \t 9.510411 \t8.933360 \t 0.000000 \t 379.166368 s\n","429 \t 18.450418 \t 9.383383 \t9.067035 \t 0.000000 \t 379.933912 s\n","430 \t 18.791292 \t 9.407482 \t9.383809 \t 0.000000 \t 380.716421 s\n","431 \t 19.112433 \t 9.671642 \t9.440791 \t 0.000000 \t 381.529744 s\n","432 \t 18.933068 \t 9.661184 \t9.271884 \t 0.000000 \t 382.210147 s\n","433 \t 19.249834 \t 9.579943 \t9.669891 \t 0.000000 \t 382.881485 s\n","434 \t 19.151208 \t 9.715084 \t9.436123 \t 0.000000 \t 383.688938 s\n","435 \t 18.451782 \t 9.329152 \t9.122632 \t 0.000000 \t 384.349145 s\n","436 \t 18.195651 \t 9.430587 \t8.765064 \t 0.000000 \t 385.016451 s\n","437 \t 18.305923 \t 9.345352 \t8.960571 \t 0.000000 \t 385.680412 s\n","438 \t 19.056579 \t 9.479295 \t9.577284 \t 0.000000 \t 386.378844 s\n","439 \t 18.525647 \t 9.298427 \t9.227221 \t 0.000000 \t 387.044600 s\n","440 \t 18.909137 \t 9.599431 \t9.309705 \t 0.000000 \t 387.711773 s\n","441 \t 18.839731 \t 9.322234 \t9.517497 \t 0.000000 \t 388.390148 s\n","442 \t 18.641853 \t 9.105620 \t9.536233 \t 0.000000 \t 389.197882 s\n","443 \t 18.045477 \t 8.960978 \t9.084499 \t 0.000000 \t 389.876115 s\n","444 \t 19.116338 \t 9.800168 \t9.316170 \t 0.000000 \t 390.542624 s\n","445 \t 18.908627 \t 9.543868 \t9.364758 \t 0.000000 \t 391.212185 s\n","446 \t 18.721119 \t 9.254611 \t9.466508 \t 0.000000 \t 391.935133 s\n","447 \t 18.669428 \t 9.584413 \t9.085016 \t 0.000000 \t 392.691169 s\n","448 \t 18.459516 \t 9.520790 \t8.938725 \t 0.000000 \t 393.441610 s\n","449 \t 18.540414 \t 9.272544 \t9.267869 \t 0.000000 \t 394.267401 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3832\n","Hit@10: 0.4885\n","Hit@3: 0.3923\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2776\n","Hit@10: 0.4477\n","Hit@3: 0.2857\n","Hit@1: 0.1969\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3014\n","Hit@10: 0.5077\n","Hit@3: 0.3385\n","Hit@1: 0.1923\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3131\n","Hit@10: 0.5878\n","Hit@3: 0.3694\n","Hit@1: 0.1689\n","450 \t 18.474219 \t 9.375265 \t9.098954 \t 0.000000 \t 431.347804 s\n","451 \t 19.228498 \t 9.744254 \t9.484244 \t 0.000000 \t 432.272722 s\n","452 \t 19.233674 \t 9.500545 \t9.733130 \t 0.000000 \t 433.048954 s\n","453 \t 18.750340 \t 9.530663 \t9.219677 \t 0.000000 \t 433.882135 s\n","454 \t 18.646690 \t 9.435153 \t9.211537 \t 0.000000 \t 434.542646 s\n","455 \t 19.335246 \t 9.769543 \t9.565703 \t 0.000000 \t 435.214326 s\n","456 \t 19.529248 \t 9.982566 \t9.546682 \t 0.000000 \t 435.894012 s\n","457 \t 18.942883 \t 9.422908 \t9.519975 \t 0.000000 \t 436.552377 s\n","458 \t 18.663649 \t 9.280076 \t9.383572 \t 0.000000 \t 437.218201 s\n","459 \t 19.197154 \t 9.851478 \t9.345676 \t 0.000000 \t 437.887165 s\n","460 \t 18.669580 \t 9.273701 \t9.395878 \t 0.000000 \t 438.679198 s\n","461 \t 19.146996 \t 9.430237 \t9.716759 \t 0.000000 \t 439.402828 s\n","462 \t 18.680424 \t 9.330016 \t9.350408 \t 0.000000 \t 440.121902 s\n","463 \t 19.252284 \t 9.963172 \t9.289113 \t 0.000000 \t 440.852535 s\n","464 \t 18.987286 \t 9.280945 \t9.706341 \t 0.000000 \t 441.603794 s\n","465 \t 19.167691 \t 9.355710 \t9.811981 \t 0.000000 \t 442.348118 s\n","466 \t 19.302744 \t 9.831338 \t9.471406 \t 0.000000 \t 443.086725 s\n","467 \t 18.589575 \t 9.293383 \t9.296192 \t 0.000000 \t 443.808515 s\n","468 \t 18.935909 \t 9.513727 \t9.422182 \t 0.000000 \t 444.907234 s\n","469 \t 19.086499 \t 9.382919 \t9.703580 \t 0.000000 \t 445.830956 s\n","470 \t 18.448119 \t 9.447453 \t9.000665 \t 0.000000 \t 446.756273 s\n","471 \t 18.573195 \t 9.202847 \t9.370347 \t 0.000000 \t 447.614676 s\n","472 \t 18.928297 \t 9.314692 \t9.613605 \t 0.000000 \t 448.362917 s\n","473 \t 19.089598 \t 9.453408 \t9.636190 \t 0.000000 \t 449.108305 s\n","474 \t 18.955454 \t 9.249194 \t9.706259 \t 0.000000 \t 449.834742 s\n","475 \t 18.706736 \t 9.358221 \t9.348515 \t 0.000000 \t 450.509763 s\n","476 \t 18.628371 \t 9.619554 \t9.008818 \t 0.000000 \t 451.174530 s\n","477 \t 19.299429 \t 9.745188 \t9.554241 \t 0.000000 \t 451.990777 s\n","478 \t 18.953443 \t 9.402800 \t9.550643 \t 0.000000 \t 452.659381 s\n","479 \t 18.992697 \t 9.292400 \t9.700297 \t 0.000000 \t 453.318568 s\n","480 \t 19.167510 \t 9.644183 \t9.523327 \t 0.000000 \t 453.987917 s\n","481 \t 18.671767 \t 9.383407 \t9.288361 \t 0.000000 \t 454.659804 s\n","482 \t 18.609927 \t 9.439605 \t9.170321 \t 0.000000 \t 455.321521 s\n","483 \t 18.771109 \t 9.055114 \t9.715995 \t 0.000000 \t 455.992575 s\n","484 \t 18.573087 \t 8.986468 \t9.586618 \t 0.000000 \t 456.667940 s\n","485 \t 19.547688 \t 9.817526 \t9.730162 \t 0.000000 \t 457.391719 s\n","486 \t 19.328279 \t 9.845110 \t9.483168 \t 0.000000 \t 458.161112 s\n","487 \t 18.908930 \t 9.436274 \t9.472657 \t 0.000000 \t 459.088430 s\n","488 \t 19.716446 \t 9.822574 \t9.893872 \t 0.000000 \t 459.877919 s\n","489 \t 19.055558 \t 9.536117 \t9.519442 \t 0.000000 \t 460.640315 s\n","490 \t 18.818920 \t 9.438684 \t9.380237 \t 0.000000 \t 461.307413 s\n","491 \t 18.858719 \t 9.463360 \t9.395359 \t 0.000000 \t 461.981220 s\n","492 \t 18.812917 \t 9.212885 \t9.600032 \t 0.000000 \t 462.648603 s\n","493 \t 18.586835 \t 9.155332 \t9.431503 \t 0.000000 \t 463.320424 s\n","494 \t 19.475996 \t 9.682744 \t9.793252 \t 0.000000 \t 463.982419 s\n","495 \t 18.920893 \t 9.483830 \t9.437062 \t 0.000000 \t 464.648303 s\n","496 \t 19.034299 \t 9.476934 \t9.557365 \t 0.000000 \t 465.312162 s\n","497 \t 19.469686 \t 9.987910 \t9.481776 \t 0.000000 \t 465.983068 s\n","498 \t 19.093969 \t 9.428498 \t9.665472 \t 0.000000 \t 466.790892 s\n","499 \t 18.664344 \t 9.515668 \t9.148676 \t 0.000000 \t 467.462011 s\n","500 \t 19.823141 \t 9.801311 \t10.021830 \t 0.000000 \t 468.134226 s\n","501 \t 19.482130 \t 9.826847 \t9.655284 \t 0.000000 \t 468.791233 s\n","502 \t 19.418318 \t 9.822612 \t9.595705 \t 0.000000 \t 469.464038 s\n","503 \t 19.044934 \t 9.630342 \t9.414593 \t 0.000000 \t 470.137729 s\n","504 \t 18.919711 \t 9.464641 \t9.455070 \t 0.000000 \t 470.924300 s\n","505 \t 18.524650 \t 9.406995 \t9.117655 \t 0.000000 \t 471.687205 s\n","506 \t 19.104038 \t 9.636579 \t9.467460 \t 0.000000 \t 472.573636 s\n","507 \t 18.616662 \t 9.183362 \t9.433300 \t 0.000000 \t 473.348686 s\n","508 \t 18.928903 \t 9.272250 \t9.656653 \t 0.000000 \t 474.026163 s\n","509 \t 18.715502 \t 9.446638 \t9.268864 \t 0.000000 \t 474.821440 s\n","510 \t 18.895144 \t 9.595600 \t9.299544 \t 0.000000 \t 475.491073 s\n","511 \t 18.889066 \t 9.571888 \t9.317178 \t 0.000000 \t 476.152071 s\n","512 \t 19.002306 \t 9.495075 \t9.507231 \t 0.000000 \t 476.817976 s\n","513 \t 18.826213 \t 9.408759 \t9.417454 \t 0.000000 \t 477.492774 s\n","514 \t 18.720996 \t 9.494180 \t9.226816 \t 0.000000 \t 478.164461 s\n","515 \t 19.145157 \t 9.678802 \t9.466355 \t 0.000000 \t 478.836425 s\n","516 \t 18.945294 \t 9.606339 \t9.338954 \t 0.000000 \t 479.503907 s\n","517 \t 19.275806 \t 9.688279 \t9.587527 \t 0.000000 \t 480.320554 s\n","518 \t 18.503076 \t 9.176482 \t9.326594 \t 0.000000 \t 480.992558 s\n","519 \t 18.975849 \t 9.351350 \t9.624499 \t 0.000000 \t 481.671086 s\n","520 \t 19.096784 \t 9.646580 \t9.450203 \t 0.000000 \t 482.332656 s\n","521 \t 18.798699 \t 9.284466 \t9.514234 \t 0.000000 \t 483.006780 s\n","522 \t 19.361924 \t 9.782171 \t9.579753 \t 0.000000 \t 483.725668 s\n","523 \t 19.133992 \t 9.341311 \t9.792682 \t 0.000000 \t 484.479332 s\n","524 \t 18.796736 \t 9.542938 \t9.253797 \t 0.000000 \t 485.251326 s\n","525 \t 19.217812 \t 9.749897 \t9.467916 \t 0.000000 \t 486.074581 s\n","526 \t 18.805351 \t 9.461870 \t9.343481 \t 0.000000 \t 486.912331 s\n","527 \t 18.863057 \t 9.210047 \t9.653011 \t 0.000000 \t 487.574649 s\n","528 \t 19.061478 \t 9.398456 \t9.663021 \t 0.000000 \t 488.247126 s\n","529 \t 18.595709 \t 9.310874 \t9.284835 \t 0.000000 \t 488.905714 s\n","530 \t 18.764324 \t 9.513434 \t9.250890 \t 0.000000 \t 489.586826 s\n","531 \t 18.413438 \t 9.092919 \t9.320518 \t 0.000000 \t 490.251507 s\n","532 \t 18.992313 \t 9.508361 \t9.483953 \t 0.000000 \t 490.915542 s\n","533 \t 18.852295 \t 9.224418 \t9.627876 \t 0.000000 \t 491.603409 s\n","534 \t 19.250528 \t 9.416862 \t9.833667 \t 0.000000 \t 492.272227 s\n","535 \t 18.782155 \t 9.481153 \t9.301002 \t 0.000000 \t 492.936661 s\n","536 \t 18.674275 \t 9.244764 \t9.429511 \t 0.000000 \t 493.763199 s\n","537 \t 18.878515 \t 9.155785 \t9.722731 \t 0.000000 \t 494.431475 s\n","538 \t 18.371041 \t 8.952232 \t9.418809 \t 0.000000 \t 495.097670 s\n","539 \t 18.957273 \t 9.650908 \t9.306365 \t 0.000000 \t 495.779967 s\n","540 \t 18.411211 \t 9.567595 \t8.843616 \t 0.000000 \t 496.496517 s\n","541 \t 19.178109 \t 9.557604 \t9.620505 \t 0.000000 \t 497.255932 s\n","542 \t 19.555200 \t 9.904072 \t9.651128 \t 0.000000 \t 498.014890 s\n","543 \t 18.991992 \t 9.267029 \t9.724964 \t 0.000000 \t 498.787828 s\n","544 \t 19.752633 \t 9.671350 \t10.081283 \t 0.000000 \t 499.545359 s\n","545 \t 18.971279 \t 9.401422 \t9.569856 \t 0.000000 \t 500.218061 s\n","546 \t 19.367172 \t 9.671458 \t9.695714 \t 0.000000 \t 501.043413 s\n","547 \t 19.614598 \t 9.368486 \t10.246111 \t 0.000000 \t 501.718689 s\n","548 \t 19.251606 \t 9.742122 \t9.509484 \t 0.000000 \t 502.394381 s\n","549 \t 19.964279 \t 9.877745 \t10.086535 \t 0.000000 \t 503.076748 s\n","550 \t 18.970693 \t 9.335737 \t9.634955 \t 0.000000 \t 503.755045 s\n","551 \t 19.167019 \t 9.432000 \t9.735019 \t 0.000000 \t 504.432314 s\n","552 \t 19.199857 \t 9.450221 \t9.749637 \t 0.000000 \t 505.116296 s\n","553 \t 19.249689 \t 9.645739 \t9.603950 \t 0.000000 \t 505.786658 s\n","554 \t 19.038389 \t 9.510885 \t9.527503 \t 0.000000 \t 506.455292 s\n","555 \t 18.979034 \t 9.254113 \t9.724922 \t 0.000000 \t 507.113432 s\n","556 \t 19.166437 \t 9.615693 \t9.550744 \t 0.000000 \t 507.776782 s\n","557 \t 19.085772 \t 9.163410 \t9.922361 \t 0.000000 \t 508.581793 s\n","558 \t 19.128537 \t 9.491443 \t9.637094 \t 0.000000 \t 509.292703 s\n","559 \t 18.913801 \t 9.377664 \t9.536138 \t 0.000000 \t 510.070107 s\n","560 \t 19.099205 \t 9.716496 \t9.382709 \t 0.000000 \t 510.821703 s\n","561 \t 20.078112 \t 9.775541 \t10.302571 \t 0.000000 \t 511.619937 s\n","562 \t 18.991103 \t 9.588313 \t9.402790 \t 0.000000 \t 512.383552 s\n","563 \t 19.468170 \t 9.832512 \t9.635658 \t 0.000000 \t 513.058058 s\n","564 \t 19.320041 \t 9.648854 \t9.671187 \t 0.000000 \t 513.725019 s\n","565 \t 19.163656 \t 9.601074 \t9.562582 \t 0.000000 \t 514.392332 s\n","566 \t 19.218618 \t 9.204372 \t10.014245 \t 0.000000 \t 515.062743 s\n","567 \t 19.112560 \t 9.595727 \t9.516832 \t 0.000000 \t 515.740062 s\n","568 \t 19.654329 \t 9.730395 \t9.923934 \t 0.000000 \t 516.416134 s\n","569 \t 19.508770 \t 9.693433 \t9.815337 \t 0.000000 \t 517.215768 s\n","570 \t 19.275506 \t 9.570629 \t9.704877 \t 0.000000 \t 517.882728 s\n","571 \t 19.149031 \t 9.625138 \t9.523892 \t 0.000000 \t 518.544073 s\n","572 \t 19.100247 \t 9.332635 \t9.767612 \t 0.000000 \t 519.216603 s\n","573 \t 19.486698 \t 9.753531 \t9.733167 \t 0.000000 \t 519.883286 s\n","574 \t 19.054516 \t 9.418922 \t9.635593 \t 0.000000 \t 520.552460 s\n","575 \t 18.993862 \t 9.207261 \t9.786601 \t 0.000000 \t 521.222506 s\n","576 \t 19.364380 \t 9.371100 \t9.993280 \t 0.000000 \t 521.911758 s\n","577 \t 19.598437 \t 9.528593 \t10.069844 \t 0.000000 \t 522.622431 s\n","578 \t 19.105680 \t 9.452925 \t9.652755 \t 0.000000 \t 523.560399 s\n","579 \t 18.983047 \t 9.505854 \t9.477193 \t 0.000000 \t 524.318070 s\n","580 \t 19.056429 \t 9.492741 \t9.563688 \t 0.000000 \t 525.140921 s\n","581 \t 19.511344 \t 9.693490 \t9.817854 \t 0.000000 \t 525.809633 s\n","582 \t 19.084215 \t 9.364400 \t9.719816 \t 0.000000 \t 526.477741 s\n","583 \t 18.791910 \t 9.048660 \t9.743250 \t 0.000000 \t 527.162467 s\n","584 \t 19.054718 \t 9.339365 \t9.715353 \t 0.000000 \t 527.829220 s\n","585 \t 18.806307 \t 9.486004 \t9.320302 \t 0.000000 \t 528.500483 s\n","586 \t 18.943196 \t 9.174802 \t9.768394 \t 0.000000 \t 529.302696 s\n","587 \t 19.425419 \t 9.619811 \t9.805608 \t 0.000000 \t 529.972420 s\n","588 \t 18.323668 \t 8.877433 \t9.446234 \t 0.000000 \t 530.640106 s\n","589 \t 19.057396 \t 9.438766 \t9.618630 \t 0.000000 \t 531.313908 s\n","590 \t 19.338602 \t 9.748860 \t9.589743 \t 0.000000 \t 531.983870 s\n","591 \t 19.795860 \t 9.659382 \t10.136477 \t 0.000000 \t 532.652176 s\n","592 \t 18.935812 \t 9.519732 \t9.416081 \t 0.000000 \t 533.334359 s\n","593 \t 19.336454 \t 9.641875 \t9.694579 \t 0.000000 \t 533.999798 s\n","594 \t 18.954069 \t 9.381118 \t9.572951 \t 0.000000 \t 534.659337 s\n","595 \t 18.737196 \t 9.662675 \t9.074521 \t 0.000000 \t 535.517584 s\n","596 \t 18.853932 \t 9.457304 \t9.396628 \t 0.000000 \t 536.274960 s\n","597 \t 18.893196 \t 9.410661 \t9.482535 \t 0.000000 \t 537.028828 s\n","598 \t 18.756577 \t 9.605909 \t9.150668 \t 0.000000 \t 537.808773 s\n","599 \t 18.603140 \t 9.113075 \t9.490065 \t 0.000000 \t 538.577059 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:33<00:00,  3.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3650\n","Hit@10: 0.4500\n","Hit@3: 0.3462\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2391\n","Hit@10: 0.3693\n","Hit@3: 0.2247\n","Hit@1: 0.1742\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2896\n","Hit@10: 0.4769\n","Hit@3: 0.3000\n","Hit@1: 0.2000\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2742\n","Hit@10: 0.5090\n","Hit@3: 0.3198\n","Hit@1: 0.1532\n","600 \t 19.105889 \t 9.708889 \t9.397000 \t 0.000000 \t 575.484093 s\n","601 \t 18.958760 \t 9.507144 \t9.451616 \t 0.000000 \t 576.256625 s\n","602 \t 19.031031 \t 9.425293 \t9.605738 \t 0.000000 \t 577.028647 s\n","603 \t 18.759511 \t 9.258973 \t9.500538 \t 0.000000 \t 577.795643 s\n","604 \t 18.633416 \t 9.193122 \t9.440294 \t 0.000000 \t 578.591435 s\n","605 \t 19.191982 \t 9.580940 \t9.611042 \t 0.000000 \t 579.269587 s\n","606 \t 19.121446 \t 9.455248 \t9.666198 \t 0.000000 \t 579.929747 s\n","607 \t 19.347849 \t 9.681567 \t9.666282 \t 0.000000 \t 580.596546 s\n","608 \t 19.277513 \t 9.835476 \t9.442037 \t 0.000000 \t 581.270881 s\n","609 \t 18.576495 \t 9.066823 \t9.509672 \t 0.000000 \t 581.937310 s\n","610 \t 19.004179 \t 9.399519 \t9.604659 \t 0.000000 \t 582.604238 s\n","611 \t 18.857482 \t 9.328963 \t9.528519 \t 0.000000 \t 583.257216 s\n","612 \t 19.337368 \t 9.572825 \t9.764543 \t 0.000000 \t 583.969434 s\n","613 \t 19.295027 \t 9.465217 \t9.829809 \t 0.000000 \t 584.893966 s\n","614 \t 19.192477 \t 9.553534 \t9.638944 \t 0.000000 \t 585.761430 s\n","615 \t 19.276777 \t 9.474113 \t9.802664 \t 0.000000 \t 586.553609 s\n","616 \t 19.287682 \t 9.568448 \t9.719234 \t 0.000000 \t 587.309109 s\n","617 \t 19.394999 \t 9.941701 \t9.453298 \t 0.000000 \t 588.331407 s\n","618 \t 19.223586 \t 9.390548 \t9.833039 \t 0.000000 \t 589.315227 s\n","619 \t 18.861078 \t 9.638430 \t9.222649 \t 0.000000 \t 590.218224 s\n","620 \t 19.131807 \t 9.683212 \t9.448595 \t 0.000000 \t 591.041536 s\n","621 \t 19.208871 \t 9.593752 \t9.615119 \t 0.000000 \t 591.834064 s\n","622 \t 18.891614 \t 9.379607 \t9.512007 \t 0.000000 \t 592.557706 s\n","623 \t 18.673723 \t 9.291668 \t9.382055 \t 0.000000 \t 593.365872 s\n","624 \t 18.719801 \t 9.191259 \t9.528542 \t 0.000000 \t 594.033530 s\n","625 \t 18.555556 \t 8.845546 \t9.710010 \t 0.000000 \t 594.707368 s\n","626 \t 19.398326 \t 9.581889 \t9.816436 \t 0.000000 \t 595.376535 s\n","627 \t 18.778968 \t 9.448220 \t9.330747 \t 0.000000 \t 596.044430 s\n","628 \t 19.027574 \t 9.589392 \t9.438181 \t 0.000000 \t 596.713764 s\n","629 \t 18.376066 \t 9.188972 \t9.187093 \t 0.000000 \t 597.395546 s\n","630 \t 19.695795 \t 9.752536 \t9.943259 \t 0.000000 \t 598.066302 s\n","631 \t 19.060094 \t 9.472859 \t9.587234 \t 0.000000 \t 598.743449 s\n","632 \t 19.096031 \t 9.720184 \t9.375847 \t 0.000000 \t 599.418586 s\n","633 \t 19.171592 \t 9.171483 \t10.000109 \t 0.000000 \t 600.089694 s\n","634 \t 19.244719 \t 9.421452 \t9.823266 \t 0.000000 \t 600.931100 s\n","635 \t 18.776427 \t 9.492951 \t9.283477 \t 0.000000 \t 601.718179 s\n","636 \t 19.200751 \t 9.715973 \t9.484778 \t 0.000000 \t 602.465762 s\n","637 \t 19.099791 \t 9.664162 \t9.435628 \t 0.000000 \t 603.264904 s\n","638 \t 18.986126 \t 9.433560 \t9.552565 \t 0.000000 \t 604.031571 s\n","639 \t 19.051962 \t 9.637328 \t9.414634 \t 0.000000 \t 604.706354 s\n","640 \t 19.118598 \t 9.323832 \t9.794766 \t 0.000000 \t 605.364208 s\n","641 \t 18.562756 \t 9.330397 \t9.232359 \t 0.000000 \t 606.028591 s\n","642 \t 19.268779 \t 9.753644 \t9.515135 \t 0.000000 \t 606.700943 s\n","643 \t 18.391308 \t 9.444685 \t8.946623 \t 0.000000 \t 607.370442 s\n","644 \t 19.238834 \t 9.613217 \t9.625618 \t 0.000000 \t 608.042230 s\n","645 \t 18.557413 \t 9.307662 \t9.249751 \t 0.000000 \t 608.847776 s\n","646 \t 18.566304 \t 9.129020 \t9.437284 \t 0.000000 \t 609.513476 s\n","647 \t 19.112354 \t 9.423094 \t9.689260 \t 0.000000 \t 610.177437 s\n","648 \t 19.074780 \t 9.383124 \t9.691655 \t 0.000000 \t 610.854625 s\n","649 \t 19.191926 \t 9.374851 \t9.817076 \t 0.000000 \t 611.523001 s\n","650 \t 19.157940 \t 9.696757 \t9.461183 \t 0.000000 \t 612.188920 s\n","651 \t 18.727098 \t 9.407735 \t9.319364 \t 0.000000 \t 612.856171 s\n","652 \t 19.284344 \t 9.569960 \t9.714384 \t 0.000000 \t 613.541827 s\n","653 \t 18.869464 \t 9.555590 \t9.313875 \t 0.000000 \t 614.280347 s\n","654 \t 18.790759 \t 9.521738 \t9.269021 \t 0.000000 \t 615.041243 s\n","655 \t 18.987624 \t 9.574895 \t9.412730 \t 0.000000 \t 615.800078 s\n","656 \t 19.128357 \t 9.559309 \t9.569048 \t 0.000000 \t 616.631624 s\n","657 \t 18.599883 \t 9.231237 \t9.368645 \t 0.000000 \t 617.450062 s\n","658 \t 19.005181 \t 9.447987 \t9.557194 \t 0.000000 \t 618.112424 s\n","659 \t 18.790180 \t 9.515368 \t9.274812 \t 0.000000 \t 618.802338 s\n","660 \t 18.566794 \t 9.221235 \t9.345560 \t 0.000000 \t 619.474886 s\n","661 \t 19.324152 \t 9.848146 \t9.476006 \t 0.000000 \t 620.136250 s\n","662 \t 20.579136 \t 10.120750 \t10.458386 \t 0.000000 \t 620.820549 s\n","663 \t 20.380582 \t 9.808173 \t10.572409 \t 0.000000 \t 621.492894 s\n","664 \t 19.505011 \t 9.656695 \t9.848316 \t 0.000000 \t 622.156367 s\n","665 \t 19.290766 \t 9.540375 \t9.750391 \t 0.000000 \t 622.965642 s\n","666 \t 19.546611 \t 9.832177 \t9.714434 \t 0.000000 \t 623.634907 s\n","667 \t 19.531613 \t 9.383114 \t10.148499 \t 0.000000 \t 624.301150 s\n","668 \t 19.356137 \t 9.713041 \t9.643096 \t 0.000000 \t 624.984760 s\n","669 \t 19.042340 \t 9.477172 \t9.565167 \t 0.000000 \t 625.651510 s\n","670 \t 19.515451 \t 9.683845 \t9.831607 \t 0.000000 \t 626.320230 s\n","671 \t 19.497537 \t 9.725995 \t9.771541 \t 0.000000 \t 627.046760 s\n","672 \t 19.348685 \t 9.742313 \t9.606372 \t 0.000000 \t 627.809410 s\n","673 \t 19.265002 \t 9.676250 \t9.588753 \t 0.000000 \t 628.579664 s\n","674 \t 19.665172 \t 9.658575 \t10.006597 \t 0.000000 \t 629.570573 s\n","675 \t 18.896680 \t 9.773956 \t9.122724 \t 0.000000 \t 630.271058 s\n","676 \t 19.400565 \t 9.442465 \t9.958101 \t 0.000000 \t 630.937067 s\n","677 \t 18.775414 \t 9.475825 \t9.299589 \t 0.000000 \t 631.605098 s\n","678 \t 19.130210 \t 9.477100 \t9.653110 \t 0.000000 \t 632.285554 s\n","679 \t 18.728959 \t 9.125744 \t9.603216 \t 0.000000 \t 632.952346 s\n","680 \t 19.509271 \t 10.009337 \t9.499934 \t 0.000000 \t 633.609178 s\n","681 \t 19.130318 \t 9.350607 \t9.779711 \t 0.000000 \t 634.292462 s\n","682 \t 19.298542 \t 9.519746 \t9.778796 \t 0.000000 \t 634.965535 s\n","683 \t 19.040385 \t 9.329053 \t9.711332 \t 0.000000 \t 635.630333 s\n","684 \t 19.889610 \t 9.706865 \t10.182745 \t 0.000000 \t 636.434614 s\n","685 \t 19.477959 \t 9.570209 \t9.907750 \t 0.000000 \t 637.104006 s\n","686 \t 19.739441 \t 9.771005 \t9.968436 \t 0.000000 \t 637.758604 s\n","687 \t 19.314270 \t 9.724167 \t9.590103 \t 0.000000 \t 638.428684 s\n","688 \t 19.565914 \t 9.573164 \t9.992751 \t 0.000000 \t 639.100554 s\n","689 \t 19.652952 \t 9.910704 \t9.742249 \t 0.000000 \t 639.797836 s\n","690 \t 19.605236 \t 9.451672 \t10.153565 \t 0.000000 \t 640.569575 s\n","691 \t 19.071746 \t 9.701996 \t9.369750 \t 0.000000 \t 641.316872 s\n","692 \t 19.307121 \t 9.723044 \t9.584077 \t 0.000000 \t 642.105903 s\n","693 \t 19.582284 \t 9.797061 \t9.785223 \t 0.000000 \t 642.885067 s\n","694 \t 19.146359 \t 9.682364 \t9.463996 \t 0.000000 \t 643.548277 s\n","695 \t 19.036281 \t 9.564226 \t9.472055 \t 0.000000 \t 644.358751 s\n","696 \t 18.629836 \t 9.375749 \t9.254087 \t 0.000000 \t 645.024075 s\n","697 \t 18.926844 \t 9.482944 \t9.443899 \t 0.000000 \t 645.692258 s\n","698 \t 19.606422 \t 9.834218 \t9.772204 \t 0.000000 \t 646.351723 s\n","699 \t 18.999612 \t 9.220574 \t9.779037 \t 0.000000 \t 647.011354 s\n","700 \t 19.588228 \t 9.760907 \t9.827321 \t 0.000000 \t 647.679224 s\n","701 \t 19.311243 \t 9.611787 \t9.699456 \t 0.000000 \t 648.344974 s\n","702 \t 19.230334 \t 9.491861 \t9.738473 \t 0.000000 \t 649.005794 s\n","703 \t 19.126957 \t 9.498624 \t9.628333 \t 0.000000 \t 649.664045 s\n","704 \t 19.255095 \t 9.509255 \t9.745840 \t 0.000000 \t 650.319381 s\n","705 \t 18.756098 \t 9.427445 \t9.328653 \t 0.000000 \t 651.001500 s\n","706 \t 18.961935 \t 9.564630 \t9.397305 \t 0.000000 \t 651.786805 s\n","707 \t 18.970804 \t 9.474575 \t9.496229 \t 0.000000 \t 652.451956 s\n","708 \t 18.946747 \t 9.288081 \t9.658666 \t 0.000000 \t 653.171045 s\n","709 \t 18.909733 \t 9.496154 \t9.413578 \t 0.000000 \t 653.917221 s\n","710 \t 19.393614 \t 9.539721 \t9.853893 \t 0.000000 \t 654.689086 s\n","711 \t 18.460746 \t 9.124589 \t9.336156 \t 0.000000 \t 655.502943 s\n","712 \t 19.023899 \t 9.298931 \t9.724968 \t 0.000000 \t 656.226740 s\n","713 \t 19.194777 \t 9.403579 \t9.791198 \t 0.000000 \t 656.891305 s\n","714 \t 19.722032 \t 9.358669 \t10.363363 \t 0.000000 \t 657.693729 s\n","715 \t 19.340195 \t 9.655359 \t9.684835 \t 0.000000 \t 658.350405 s\n","716 \t 19.285967 \t 9.492505 \t9.793462 \t 0.000000 \t 659.012255 s\n","717 \t 18.591217 \t 9.057167 \t9.534051 \t 0.000000 \t 659.682871 s\n","718 \t 19.761724 \t 9.824381 \t9.937343 \t 0.000000 \t 660.371243 s\n","719 \t 19.301704 \t 9.631320 \t9.670384 \t 0.000000 \t 661.039945 s\n","720 \t 19.054041 \t 9.539550 \t9.514492 \t 0.000000 \t 661.701195 s\n","721 \t 19.050992 \t 9.526866 \t9.524126 \t 0.000000 \t 662.380866 s\n","722 \t 18.515893 \t 9.008908 \t9.506985 \t 0.000000 \t 663.049502 s\n","723 \t 18.642241 \t 9.088657 \t9.553583 \t 0.000000 \t 663.848257 s\n","724 \t 19.084645 \t 9.519110 \t9.565536 \t 0.000000 \t 664.507906 s\n","725 \t 19.057854 \t 9.483582 \t9.574271 \t 0.000000 \t 665.167740 s\n","726 \t 19.139523 \t 9.572217 \t9.567306 \t 0.000000 \t 665.874506 s\n","727 \t 19.076831 \t 9.512790 \t9.564040 \t 0.000000 \t 666.653704 s\n","728 \t 19.141574 \t 9.467540 \t9.674034 \t 0.000000 \t 667.401093 s\n","729 \t 18.918240 \t 9.359667 \t9.558573 \t 0.000000 \t 668.196831 s\n","730 \t 19.176554 \t 9.592996 \t9.583559 \t 0.000000 \t 668.960171 s\n","731 \t 19.137261 \t 9.685721 \t9.451540 \t 0.000000 \t 669.638458 s\n","732 \t 18.864621 \t 9.299016 \t9.565605 \t 0.000000 \t 670.316365 s\n","733 \t 19.155685 \t 9.676147 \t9.479539 \t 0.000000 \t 671.115658 s\n","734 \t 18.872786 \t 9.372615 \t9.500171 \t 0.000000 \t 671.799309 s\n","735 \t 18.974184 \t 9.435816 \t9.538367 \t 0.000000 \t 672.468418 s\n","736 \t 19.024776 \t 9.618968 \t9.405808 \t 0.000000 \t 673.130363 s\n","737 \t 19.250614 \t 9.203249 \t10.047365 \t 0.000000 \t 673.819035 s\n","738 \t 19.380154 \t 9.689768 \t9.690386 \t 0.000000 \t 674.486341 s\n","739 \t 19.572979 \t 9.637642 \t9.935337 \t 0.000000 \t 675.150266 s\n","740 \t 19.019622 \t 9.419363 \t9.600259 \t 0.000000 \t 675.821859 s\n","741 \t 18.786399 \t 9.190016 \t9.596382 \t 0.000000 \t 676.488276 s\n","742 \t 18.971091 \t 9.517020 \t9.454072 \t 0.000000 \t 677.155379 s\n","743 \t 18.644871 \t 9.406416 \t9.238455 \t 0.000000 \t 677.819505 s\n","744 \t 18.911339 \t 9.194441 \t9.716897 \t 0.000000 \t 678.619547 s\n","745 \t 18.410418 \t 9.058696 \t9.351722 \t 0.000000 \t 679.391320 s\n","746 \t 18.918630 \t 9.273720 \t9.644909 \t 0.000000 \t 680.170490 s\n","747 \t 17.856962 \t 9.109329 \t8.747633 \t 0.000000 \t 680.954979 s\n","748 \t 18.308925 \t 8.911773 \t9.397151 \t 0.000000 \t 681.756461 s\n","749 \t 18.904081 \t 9.219535 \t9.684546 \t 0.000000 \t 682.426758 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3665\n","Hit@10: 0.4885\n","Hit@3: 0.3962\n","Hit@1: 0.2885\n","Link Prediction on Validation Set (All)\n","MRR: 0.2405\n","Hit@10: 0.3868\n","Hit@3: 0.2578\n","Hit@1: 0.1551\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2854\n","Hit@10: 0.5154\n","Hit@3: 0.2846\n","Hit@1: 0.1846\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2705\n","Hit@10: 0.5405\n","Hit@3: 0.3153\n","Hit@1: 0.1419\n","750 \t 19.506029 \t 9.444051 \t10.061978 \t 0.000000 \t 719.799582 s\n","751 \t 18.586470 \t 9.336038 \t9.250431 \t 0.000000 \t 720.586246 s\n","752 \t 18.950021 \t 9.568384 \t9.381637 \t 0.000000 \t 721.385590 s\n","753 \t 18.808152 \t 9.457282 \t9.350870 \t 0.000000 \t 722.047634 s\n","754 \t 19.029208 \t 9.309755 \t9.719453 \t 0.000000 \t 722.716550 s\n","755 \t 19.634963 \t 9.900269 \t9.734694 \t 0.000000 \t 723.508131 s\n","756 \t 18.582536 \t 9.068331 \t9.514204 \t 0.000000 \t 724.168525 s\n","757 \t 19.124107 \t 9.546286 \t9.577820 \t 0.000000 \t 724.845300 s\n","758 \t 19.224784 \t 9.685096 \t9.539687 \t 0.000000 \t 725.510439 s\n","759 \t 18.972532 \t 9.328401 \t9.644131 \t 0.000000 \t 726.168685 s\n","760 \t 19.258989 \t 9.678661 \t9.580329 \t 0.000000 \t 726.892077 s\n","761 \t 18.648378 \t 9.327219 \t9.321160 \t 0.000000 \t 727.681213 s\n","762 \t 19.339812 \t 9.647229 \t9.692583 \t 0.000000 \t 728.511069 s\n","763 \t 18.971156 \t 9.468843 \t9.502313 \t 0.000000 \t 729.250058 s\n","764 \t 19.160234 \t 9.741458 \t9.418777 \t 0.000000 \t 730.071415 s\n","765 \t 19.113437 \t 9.394801 \t9.718635 \t 0.000000 \t 730.855574 s\n","766 \t 18.824176 \t 9.237621 \t9.586555 \t 0.000000 \t 731.940065 s\n","767 \t 19.222762 \t 9.559711 \t9.663050 \t 0.000000 \t 732.852425 s\n","768 \t 18.540727 \t 9.097055 \t9.443671 \t 0.000000 \t 733.816438 s\n","769 \t 18.012989 \t 8.909763 \t9.103227 \t 0.000000 \t 734.754506 s\n","770 \t 19.057428 \t 9.457958 \t9.599471 \t 0.000000 \t 735.509392 s\n","771 \t 19.146099 \t 9.617064 \t9.529035 \t 0.000000 \t 736.174173 s\n","772 \t 18.745726 \t 9.387430 \t9.358295 \t 0.000000 \t 736.846899 s\n","773 \t 19.354685 \t 9.746851 \t9.607834 \t 0.000000 \t 737.516349 s\n","774 \t 18.953457 \t 9.409251 \t9.544205 \t 0.000000 \t 738.181559 s\n","775 \t 19.054605 \t 9.512423 \t9.542182 \t 0.000000 \t 738.831519 s\n","776 \t 19.165195 \t 9.528209 \t9.636985 \t 0.000000 \t 739.516781 s\n","777 \t 19.051051 \t 9.581534 \t9.469517 \t 0.000000 \t 740.182286 s\n","778 \t 19.463847 \t 9.434608 \t10.029240 \t 0.000000 \t 740.977785 s\n","779 \t 18.605289 \t 9.317721 \t9.287568 \t 0.000000 \t 741.655044 s\n","780 \t 19.044846 \t 9.574825 \t9.470020 \t 0.000000 \t 742.316168 s\n","781 \t 19.014281 \t 9.532407 \t9.481874 \t 0.000000 \t 743.003465 s\n","782 \t 18.674148 \t 9.536828 \t9.137319 \t 0.000000 \t 743.689133 s\n","783 \t 19.140491 \t 9.468252 \t9.672239 \t 0.000000 \t 744.366946 s\n","784 \t 19.441429 \t 9.582236 \t9.859193 \t 0.000000 \t 745.069886 s\n","785 \t 18.455379 \t 8.926415 \t9.528963 \t 0.000000 \t 745.862823 s\n","786 \t 18.812883 \t 9.494773 \t9.318111 \t 0.000000 \t 746.627131 s\n","787 \t 19.101299 \t 9.542586 \t9.558712 \t 0.000000 \t 747.575807 s\n","788 \t 18.793999 \t 9.483809 \t9.310190 \t 0.000000 \t 748.337231 s\n","789 \t 19.011582 \t 9.466539 \t9.545043 \t 0.000000 \t 749.005174 s\n","790 \t 18.866434 \t 9.381453 \t9.484982 \t 0.000000 \t 749.676733 s\n","791 \t 18.766971 \t 9.353822 \t9.413149 \t 0.000000 \t 750.341442 s\n","792 \t 19.232714 \t 9.555898 \t9.676815 \t 0.000000 \t 751.027094 s\n","793 \t 19.173256 \t 9.555932 \t9.617323 \t 0.000000 \t 751.697159 s\n","794 \t 18.962568 \t 9.447280 \t9.515288 \t 0.000000 \t 752.349378 s\n","795 \t 19.394053 \t 9.710282 \t9.683770 \t 0.000000 \t 753.157095 s\n","796 \t 18.786787 \t 9.267832 \t9.518955 \t 0.000000 \t 753.829484 s\n","797 \t 18.864086 \t 9.310669 \t9.553418 \t 0.000000 \t 754.503676 s\n","798 \t 19.050489 \t 9.536275 \t9.514214 \t 0.000000 \t 755.185431 s\n","799 \t 19.467200 \t 9.255875 \t10.211325 \t 0.000000 \t 755.859258 s\n","800 \t 19.255017 \t 9.602235 \t9.652782 \t 0.000000 \t 756.524430 s\n","801 \t 18.806952 \t 9.540836 \t9.266115 \t 0.000000 \t 757.203418 s\n","802 \t 18.542456 \t 9.117537 \t9.424919 \t 0.000000 \t 757.872601 s\n","803 \t 18.896208 \t 9.444321 \t9.451887 \t 0.000000 \t 758.638042 s\n","804 \t 19.185581 \t 9.729696 \t9.455884 \t 0.000000 \t 759.448348 s\n","805 \t 19.403618 \t 9.751440 \t9.652178 \t 0.000000 \t 760.405560 s\n","806 \t 18.899117 \t 9.077552 \t9.821566 \t 0.000000 \t 761.174540 s\n","807 \t 18.595219 \t 9.086884 \t9.508335 \t 0.000000 \t 761.835851 s\n","808 \t 19.300505 \t 9.442071 \t9.858433 \t 0.000000 \t 762.526200 s\n","809 \t 18.792804 \t 9.113318 \t9.679485 \t 0.000000 \t 763.202373 s\n","810 \t 18.873620 \t 9.347951 \t9.525668 \t 0.000000 \t 763.861454 s\n","811 \t 18.959848 \t 9.662816 \t9.297032 \t 0.000000 \t 764.546445 s\n","812 \t 18.740305 \t 9.443641 \t9.296664 \t 0.000000 \t 765.222573 s\n","813 \t 18.843449 \t 9.085993 \t9.757456 \t 0.000000 \t 765.905713 s\n","814 \t 18.861380 \t 9.349033 \t9.512346 \t 0.000000 \t 766.592206 s\n","815 \t 18.772270 \t 9.332105 \t9.440166 \t 0.000000 \t 767.401238 s\n","816 \t 18.761755 \t 9.110629 \t9.651126 \t 0.000000 \t 768.073052 s\n","817 \t 19.125720 \t 9.347833 \t9.777886 \t 0.000000 \t 768.758172 s\n","818 \t 18.413322 \t 9.227771 \t9.185551 \t 0.000000 \t 769.424077 s\n","819 \t 18.689181 \t 9.181909 \t9.507273 \t 0.000000 \t 770.096900 s\n","820 \t 19.023867 \t 9.399336 \t9.624531 \t 0.000000 \t 770.782854 s\n","821 \t 18.951512 \t 9.542140 \t9.409373 \t 0.000000 \t 771.503143 s\n","822 \t 19.324450 \t 9.636874 \t9.687576 \t 0.000000 \t 772.251321 s\n","823 \t 18.693550 \t 9.290369 \t9.403180 \t 0.000000 \t 773.026852 s\n","824 \t 19.217688 \t 9.510312 \t9.707376 \t 0.000000 \t 773.846210 s\n","825 \t 19.116874 \t 9.514320 \t9.602553 \t 0.000000 \t 774.578373 s\n","826 \t 19.098553 \t 9.525817 \t9.572735 \t 0.000000 \t 775.373802 s\n","827 \t 19.363639 \t 9.603319 \t9.760320 \t 0.000000 \t 776.074536 s\n","828 \t 19.270772 \t 9.537371 \t9.733401 \t 0.000000 \t 776.740710 s\n","829 \t 18.228894 \t 9.078121 \t9.150773 \t 0.000000 \t 777.416321 s\n","830 \t 18.872382 \t 9.032326 \t9.840056 \t 0.000000 \t 778.098309 s\n","831 \t 18.728477 \t 9.262746 \t9.465731 \t 0.000000 \t 778.768660 s\n","832 \t 19.085269 \t 9.435448 \t9.649821 \t 0.000000 \t 779.430525 s\n","833 \t 19.477647 \t 9.656422 \t9.821225 \t 0.000000 \t 780.101155 s\n","834 \t 18.881297 \t 9.232972 \t9.648325 \t 0.000000 \t 780.775624 s\n","835 \t 18.794811 \t 9.532081 \t9.262731 \t 0.000000 \t 781.438490 s\n","836 \t 18.789532 \t 9.463996 \t9.325535 \t 0.000000 \t 782.103309 s\n","837 \t 19.297442 \t 9.439402 \t9.858041 \t 0.000000 \t 782.905761 s\n","838 \t 19.243845 \t 9.396376 \t9.847470 \t 0.000000 \t 783.567473 s\n","839 \t 18.898104 \t 9.401506 \t9.496598 \t 0.000000 \t 784.301461 s\n","840 \t 18.835382 \t 9.393805 \t9.441576 \t 0.000000 \t 785.065747 s\n","841 \t 18.621357 \t 9.302275 \t9.319082 \t 0.000000 \t 785.820331 s\n","842 \t 18.329988 \t 8.845199 \t9.484788 \t 0.000000 \t 786.598266 s\n","843 \t 18.595132 \t 9.123974 \t9.471158 \t 0.000000 \t 787.365172 s\n","844 \t 18.812210 \t 9.125686 \t9.686525 \t 0.000000 \t 788.045702 s\n","845 \t 19.110028 \t 9.589720 \t9.520308 \t 0.000000 \t 788.709228 s\n","846 \t 18.713219 \t 9.263378 \t9.449841 \t 0.000000 \t 789.386136 s\n","847 \t 19.384247 \t 9.562891 \t9.821356 \t 0.000000 \t 790.064531 s\n","848 \t 19.387201 \t 9.525439 \t9.861763 \t 0.000000 \t 790.737566 s\n","849 \t 18.970797 \t 9.471674 \t9.499123 \t 0.000000 \t 791.534854 s\n","850 \t 18.727356 \t 9.655566 \t9.071791 \t 0.000000 \t 792.199832 s\n","851 \t 19.027455 \t 9.553081 \t9.474375 \t 0.000000 \t 792.871914 s\n","852 \t 18.698872 \t 9.503122 \t9.195750 \t 0.000000 \t 793.542993 s\n","853 \t 18.995794 \t 9.496965 \t9.498828 \t 0.000000 \t 794.224908 s\n","854 \t 18.781138 \t 9.379839 \t9.401299 \t 0.000000 \t 794.923476 s\n","855 \t 19.027878 \t 9.454941 \t9.572937 \t 0.000000 \t 795.594767 s\n","856 \t 18.982412 \t 9.512483 \t9.469929 \t 0.000000 \t 796.274991 s\n","857 \t 19.330587 \t 9.709178 \t9.621409 \t 0.000000 \t 796.941881 s\n","858 \t 18.507808 \t 8.851313 \t9.656494 \t 0.000000 \t 797.721613 s\n","859 \t 18.612365 \t 9.128873 \t9.483492 \t 0.000000 \t 798.488122 s\n","860 \t 18.747116 \t 9.531713 \t9.215404 \t 0.000000 \t 799.260513 s\n","861 \t 18.873013 \t 9.376970 \t9.496044 \t 0.000000 \t 800.224681 s\n","862 \t 18.420612 \t 9.236028 \t9.184583 \t 0.000000 \t 800.888192 s\n","863 \t 18.865199 \t 9.469405 \t9.395793 \t 0.000000 \t 801.559840 s\n","864 \t 18.572504 \t 9.383504 \t9.188999 \t 0.000000 \t 802.223090 s\n","865 \t 19.259590 \t 9.497242 \t9.762348 \t 0.000000 \t 802.890728 s\n","866 \t 18.521524 \t 9.067835 \t9.453690 \t 0.000000 \t 803.556800 s\n","867 \t 18.989048 \t 9.443645 \t9.545403 \t 0.000000 \t 804.224820 s\n","868 \t 18.671733 \t 9.203618 \t9.468115 \t 0.000000 \t 804.893273 s\n","869 \t 19.467913 \t 9.729356 \t9.738557 \t 0.000000 \t 805.558275 s\n","870 \t 18.725080 \t 9.504424 \t9.220656 \t 0.000000 \t 806.369396 s\n","871 \t 18.945309 \t 9.272901 \t9.672409 \t 0.000000 \t 807.044063 s\n","872 \t 19.105214 \t 9.465653 \t9.639561 \t 0.000000 \t 807.720345 s\n","873 \t 19.173250 \t 9.286605 \t9.886645 \t 0.000000 \t 808.381932 s\n","874 \t 18.330530 \t 9.138389 \t9.192142 \t 0.000000 \t 809.045694 s\n","875 \t 18.807083 \t 9.423983 \t9.383101 \t 0.000000 \t 809.718057 s\n","876 \t 18.371186 \t 8.827567 \t9.543619 \t 0.000000 \t 810.435214 s\n","877 \t 18.721464 \t 9.107359 \t9.614104 \t 0.000000 \t 811.193069 s\n","878 \t 18.985808 \t 9.497274 \t9.488534 \t 0.000000 \t 812.117109 s\n","879 \t 18.673477 \t 9.525424 \t9.148053 \t 0.000000 \t 812.949697 s\n","880 \t 18.764333 \t 9.329638 \t9.434695 \t 0.000000 \t 813.648362 s\n","881 \t 18.451293 \t 9.016776 \t9.434517 \t 0.000000 \t 814.317899 s\n","882 \t 18.579867 \t 9.209541 \t9.370327 \t 0.000000 \t 814.993622 s\n","883 \t 19.003484 \t 9.353761 \t9.649723 \t 0.000000 \t 815.666752 s\n","884 \t 18.915912 \t 9.212731 \t9.703181 \t 0.000000 \t 816.342135 s\n","885 \t 18.633590 \t 9.276809 \t9.356780 \t 0.000000 \t 817.013553 s\n","886 \t 19.663507 \t 9.652106 \t10.011401 \t 0.000000 \t 817.690635 s\n","887 \t 19.302496 \t 9.598899 \t9.703597 \t 0.000000 \t 818.486654 s\n","888 \t 19.213247 \t 9.261137 \t9.952111 \t 0.000000 \t 819.159220 s\n","889 \t 18.721031 \t 9.660048 \t9.060984 \t 0.000000 \t 819.828784 s\n","890 \t 19.047031 \t 9.644145 \t9.402887 \t 0.000000 \t 820.502605 s\n","891 \t 18.822346 \t 9.663341 \t9.159006 \t 0.000000 \t 821.164419 s\n","892 \t 18.854344 \t 9.494681 \t9.359663 \t 0.000000 \t 821.848242 s\n","893 \t 18.849905 \t 9.364441 \t9.485464 \t 0.000000 \t 822.511265 s\n","894 \t 18.983031 \t 9.535627 \t9.447405 \t 0.000000 \t 823.217037 s\n","895 \t 19.612851 \t 9.540630 \t10.072221 \t 0.000000 \t 824.002962 s\n","896 \t 18.946919 \t 9.558697 \t9.388223 \t 0.000000 \t 824.764670 s\n","897 \t 18.678315 \t 9.342319 \t9.335996 \t 0.000000 \t 825.712527 s\n","898 \t 18.781645 \t 9.435568 \t9.346077 \t 0.000000 \t 826.486967 s\n","899 \t 18.651197 \t 9.295688 \t9.355509 \t 0.000000 \t 827.151175 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3783\n","Hit@10: 0.4731\n","Hit@3: 0.3769\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2478\n","Hit@10: 0.3990\n","Hit@3: 0.2491\n","Hit@1: 0.1742\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3067\n","Hit@10: 0.5154\n","Hit@3: 0.3385\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2920\n","Hit@10: 0.5428\n","Hit@3: 0.3423\n","Hit@1: 0.1599\n","900 \t 18.568361 \t 9.281232 \t9.287129 \t 0.000000 \t 864.819599 s\n","901 \t 18.913506 \t 9.301269 \t9.612237 \t 0.000000 \t 865.594835 s\n","902 \t 18.777688 \t 9.126208 \t9.651480 \t 0.000000 \t 866.355253 s\n","903 \t 18.721895 \t 9.401036 \t9.320859 \t 0.000000 \t 867.031246 s\n","904 \t 19.293354 \t 9.401672 \t9.891683 \t 0.000000 \t 867.707073 s\n","905 \t 19.268517 \t 9.643629 \t9.624888 \t 0.000000 \t 868.386250 s\n","906 \t 19.399142 \t 9.504338 \t9.894804 \t 0.000000 \t 869.199292 s\n","907 \t 19.303231 \t 9.720572 \t9.582660 \t 0.000000 \t 869.882195 s\n","908 \t 19.051078 \t 9.265279 \t9.785799 \t 0.000000 \t 870.554730 s\n","909 \t 18.993670 \t 9.586157 \t9.407513 \t 0.000000 \t 871.228545 s\n","910 \t 18.269009 \t 8.979805 \t9.289204 \t 0.000000 \t 871.918417 s\n","911 \t 19.385540 \t 9.488230 \t9.897310 \t 0.000000 \t 872.675131 s\n","912 \t 19.156100 \t 9.301907 \t9.854193 \t 0.000000 \t 873.446933 s\n","913 \t 18.755997 \t 9.371356 \t9.384641 \t 0.000000 \t 874.220934 s\n","914 \t 18.374601 \t 9.157248 \t9.217354 \t 0.000000 \t 874.995368 s\n","915 \t 18.962769 \t 9.629941 \t9.332829 \t 0.000000 \t 875.995548 s\n","916 \t 18.474686 \t 9.254999 \t9.219686 \t 0.000000 \t 877.032448 s\n","917 \t 18.981099 \t 9.392428 \t9.588671 \t 0.000000 \t 878.077669 s\n","918 \t 18.567341 \t 9.111932 \t9.455410 \t 0.000000 \t 878.969497 s\n","919 \t 19.102577 \t 9.915812 \t9.186766 \t 0.000000 \t 880.108415 s\n","920 \t 19.040365 \t 9.523099 \t9.517266 \t 0.000000 \t 880.929116 s\n","921 \t 18.006655 \t 8.811680 \t9.194975 \t 0.000000 \t 881.592520 s\n","922 \t 18.768651 \t 9.331857 \t9.436794 \t 0.000000 \t 882.255937 s\n","923 \t 18.789470 \t 9.267241 \t9.522228 \t 0.000000 \t 882.929229 s\n","924 \t 18.478877 \t 9.080288 \t9.398589 \t 0.000000 \t 883.602047 s\n","925 \t 18.697482 \t 9.207805 \t9.489677 \t 0.000000 \t 884.400518 s\n","926 \t 18.971347 \t 9.440482 \t9.530865 \t 0.000000 \t 885.084210 s\n","927 \t 18.500869 \t 9.143440 \t9.357429 \t 0.000000 \t 885.748993 s\n","928 \t 19.265354 \t 9.457037 \t9.808317 \t 0.000000 \t 886.424985 s\n","929 \t 19.318941 \t 9.815748 \t9.503194 \t 0.000000 \t 887.094069 s\n","930 \t 18.921232 \t 9.658323 \t9.262909 \t 0.000000 \t 887.763103 s\n","931 \t 18.730834 \t 9.476648 \t9.254186 \t 0.000000 \t 888.439729 s\n","932 \t 19.363556 \t 9.666835 \t9.696722 \t 0.000000 \t 889.100791 s\n","933 \t 19.717976 \t 9.812908 \t9.905067 \t 0.000000 \t 889.785139 s\n","934 \t 18.794291 \t 9.179326 \t9.614965 \t 0.000000 \t 890.511739 s\n","935 \t 19.492867 \t 9.716728 \t9.776138 \t 0.000000 \t 891.277536 s\n","936 \t 18.933927 \t 9.649562 \t9.284365 \t 0.000000 \t 892.194239 s\n","937 \t 18.766898 \t 9.586853 \t9.180045 \t 0.000000 \t 893.031805 s\n","938 \t 18.686759 \t 9.249927 \t9.436832 \t 0.000000 \t 893.766571 s\n","939 \t 19.413712 \t 9.643979 \t9.769732 \t 0.000000 \t 894.447676 s\n","940 \t 19.007996 \t 9.266796 \t9.741200 \t 0.000000 \t 895.107743 s\n","941 \t 18.733365 \t 9.599480 \t9.133885 \t 0.000000 \t 895.770483 s\n","942 \t 18.798688 \t 9.469089 \t9.329599 \t 0.000000 \t 896.449708 s\n","943 \t 19.168795 \t 9.530002 \t9.638793 \t 0.000000 \t 897.118383 s\n","944 \t 19.620666 \t 9.817090 \t9.803576 \t 0.000000 \t 897.794901 s\n","945 \t 18.345432 \t 9.087207 \t9.258225 \t 0.000000 \t 898.461779 s\n","946 \t 19.041559 \t 9.469221 \t9.572338 \t 0.000000 \t 899.146343 s\n","947 \t 18.721910 \t 9.468272 \t9.253638 \t 0.000000 \t 899.940638 s\n","948 \t 18.872137 \t 9.321723 \t9.550415 \t 0.000000 \t 900.614388 s\n","949 \t 18.490407 \t 8.917597 \t9.572810 \t 0.000000 \t 901.272247 s\n","950 \t 18.891755 \t 9.374492 \t9.517263 \t 0.000000 \t 901.945103 s\n","951 \t 18.753090 \t 9.543974 \t9.209115 \t 0.000000 \t 902.623763 s\n","952 \t 19.237110 \t 9.616362 \t9.620748 \t 0.000000 \t 903.313747 s\n","953 \t 18.857547 \t 9.361867 \t9.495680 \t 0.000000 \t 904.089007 s\n","954 \t 18.358362 \t 9.246088 \t9.112275 \t 0.000000 \t 904.848777 s\n","955 \t 18.796101 \t 9.309911 \t9.486189 \t 0.000000 \t 905.780389 s\n","956 \t 18.603064 \t 9.126248 \t9.476816 \t 0.000000 \t 906.560704 s\n","957 \t 18.871320 \t 9.533332 \t9.337988 \t 0.000000 \t 907.223629 s\n","958 \t 19.206175 \t 9.421114 \t9.785061 \t 0.000000 \t 907.892495 s\n","959 \t 18.986347 \t 9.555021 \t9.431326 \t 0.000000 \t 908.557593 s\n","960 \t 18.867027 \t 9.419779 \t9.447248 \t 0.000000 \t 909.224586 s\n","961 \t 18.835073 \t 9.508996 \t9.326076 \t 0.000000 \t 909.880666 s\n","962 \t 18.665998 \t 9.236272 \t9.429727 \t 0.000000 \t 910.569036 s\n","963 \t 18.995609 \t 9.750980 \t9.244629 \t 0.000000 \t 911.229940 s\n","964 \t 19.089924 \t 9.565252 \t9.524673 \t 0.000000 \t 912.032592 s\n","965 \t 18.641219 \t 9.441226 \t9.199994 \t 0.000000 \t 912.699767 s\n","966 \t 19.196063 \t 9.608035 \t9.588028 \t 0.000000 \t 913.373472 s\n","967 \t 19.435247 \t 9.578050 \t9.857198 \t 0.000000 \t 914.044644 s\n","968 \t 18.741444 \t 9.207197 \t9.534246 \t 0.000000 \t 914.728630 s\n","969 \t 18.884394 \t 9.380171 \t9.504223 \t 0.000000 \t 915.406770 s\n","970 \t 19.255401 \t 9.749293 \t9.506107 \t 0.000000 \t 916.071343 s\n","971 \t 18.630251 \t 9.249003 \t9.381248 \t 0.000000 \t 916.805696 s\n","972 \t 18.909286 \t 9.303790 \t9.605496 \t 0.000000 \t 917.658020 s\n","973 \t 18.450732 \t 9.099266 \t9.351467 \t 0.000000 \t 918.490575 s\n","974 \t 18.820319 \t 9.517879 \t9.302441 \t 0.000000 \t 919.465144 s\n","975 \t 18.700499 \t 9.598891 \t9.101607 \t 0.000000 \t 920.129546 s\n","976 \t 19.026155 \t 9.056829 \t9.969326 \t 0.000000 \t 920.798720 s\n","977 \t 18.584622 \t 9.205024 \t9.379599 \t 0.000000 \t 921.485729 s\n","978 \t 18.985090 \t 9.628781 \t9.356308 \t 0.000000 \t 922.148434 s\n","979 \t 19.524222 \t 9.472316 \t10.051907 \t 0.000000 \t 922.808513 s\n","980 \t 19.375812 \t 9.643357 \t9.732454 \t 0.000000 \t 923.487911 s\n","981 \t 18.620479 \t 9.362648 \t9.257831 \t 0.000000 \t 924.157190 s\n","982 \t 19.006211 \t 9.349634 \t9.656578 \t 0.000000 \t 924.832769 s\n","983 \t 19.615826 \t 9.948171 \t9.667655 \t 0.000000 \t 925.512099 s\n","984 \t 18.334295 \t 9.198322 \t9.135973 \t 0.000000 \t 926.193447 s\n","985 \t 18.676632 \t 9.353392 \t9.323240 \t 0.000000 \t 927.003903 s\n","986 \t 18.418468 \t 9.460459 \t8.958009 \t 0.000000 \t 927.701794 s\n","987 \t 19.000806 \t 9.604458 \t9.396348 \t 0.000000 \t 928.379867 s\n","988 \t 19.498920 \t 9.582345 \t9.916576 \t 0.000000 \t 929.066280 s\n","989 \t 18.344336 \t 9.282062 \t9.062274 \t 0.000000 \t 929.794009 s\n","990 \t 19.088266 \t 9.514319 \t9.573947 \t 0.000000 \t 930.565710 s\n","991 \t 19.379076 \t 9.655024 \t9.724053 \t 0.000000 \t 931.330309 s\n","992 \t 18.166677 \t 9.058952 \t9.107724 \t 0.000000 \t 932.111231 s\n","993 \t 18.990563 \t 9.476851 \t9.513713 \t 0.000000 \t 932.872029 s\n","994 \t 19.017429 \t 9.419915 \t9.597515 \t 0.000000 \t 933.553746 s\n","995 \t 19.585419 \t 9.721772 \t9.863647 \t 0.000000 \t 934.223936 s\n","996 \t 19.095124 \t 9.642690 \t9.452434 \t 0.000000 \t 935.038661 s\n","997 \t 19.298607 \t 9.712056 \t9.586551 \t 0.000000 \t 935.710288 s\n","998 \t 18.856663 \t 9.143991 \t9.712671 \t 0.000000 \t 936.374615 s\n","999 \t 19.051067 \t 9.575062 \t9.476005 \t 0.000000 \t 937.048223 s\n","1000 \t 18.812222 \t 9.559987 \t9.252235 \t 0.000000 \t 937.722248 s\n","1001 \t 19.274997 \t 9.539974 \t9.735022 \t 0.000000 \t 938.391372 s\n","1002 \t 18.890354 \t 9.308249 \t9.582106 \t 0.000000 \t 939.063486 s\n","1003 \t 19.051535 \t 9.420125 \t9.631408 \t 0.000000 \t 939.731645 s\n","1004 \t 18.944433 \t 9.531933 \t9.412500 \t 0.000000 \t 940.524516 s\n","1005 \t 18.688448 \t 9.378004 \t9.310444 \t 0.000000 \t 941.208291 s\n","1006 \t 18.766353 \t 9.496603 \t9.269749 \t 0.000000 \t 941.875309 s\n","1007 \t 18.142159 \t 9.071319 \t9.070840 \t 0.000000 \t 942.568040 s\n","1008 \t 18.757441 \t 9.108146 \t9.649294 \t 0.000000 \t 943.364492 s\n","1009 \t 18.912075 \t 9.374449 \t9.537625 \t 0.000000 \t 944.110389 s\n","1010 \t 19.068024 \t 9.539289 \t9.528735 \t 0.000000 \t 944.913418 s\n","1011 \t 18.631371 \t 9.314564 \t9.316808 \t 0.000000 \t 945.691800 s\n","1012 \t 18.692390 \t 9.126967 \t9.565423 \t 0.000000 \t 946.349725 s\n","1013 \t 19.303382 \t 9.684029 \t9.619353 \t 0.000000 \t 947.172518 s\n","1014 \t 19.076286 \t 9.701014 \t9.375272 \t 0.000000 \t 947.842874 s\n","1015 \t 19.230195 \t 9.416596 \t9.813599 \t 0.000000 \t 948.522085 s\n","1016 \t 18.941105 \t 9.602932 \t9.338172 \t 0.000000 \t 949.179870 s\n","1017 \t 18.692331 \t 9.490036 \t9.202296 \t 0.000000 \t 949.852180 s\n","1018 \t 19.029724 \t 9.774942 \t9.254782 \t 0.000000 \t 950.521601 s\n","1019 \t 18.729861 \t 9.245570 \t9.484292 \t 0.000000 \t 951.206782 s\n","1020 \t 19.312181 \t 9.548537 \t9.763644 \t 0.000000 \t 951.872599 s\n","1021 \t 18.984807 \t 9.715423 \t9.269384 \t 0.000000 \t 952.538915 s\n","1022 \t 18.871252 \t 9.364424 \t9.506828 \t 0.000000 \t 953.222941 s\n","1023 \t 18.444199 \t 9.021846 \t9.422353 \t 0.000000 \t 954.032238 s\n","1024 \t 19.393943 \t 9.464809 \t9.929133 \t 0.000000 \t 954.707719 s\n","1025 \t 18.582643 \t 9.253714 \t9.328929 \t 0.000000 \t 955.371023 s\n","1026 \t 18.873529 \t 9.446027 \t9.427503 \t 0.000000 \t 956.087606 s\n","1027 \t 18.715492 \t 9.049277 \t9.666215 \t 0.000000 \t 956.839982 s\n","1028 \t 19.114850 \t 9.406508 \t9.708342 \t 0.000000 \t 957.614756 s\n","1029 \t 18.485713 \t 9.215255 \t9.270457 \t 0.000000 \t 958.442032 s\n","1030 \t 18.598584 \t 9.081855 \t9.516728 \t 0.000000 \t 959.207810 s\n","1031 \t 18.981137 \t 9.168376 \t9.812761 \t 0.000000 \t 959.884357 s\n","1032 \t 18.551182 \t 9.451818 \t9.099364 \t 0.000000 \t 960.570240 s\n","1033 \t 19.120607 \t 9.544696 \t9.575912 \t 0.000000 \t 961.243082 s\n","1034 \t 18.972637 \t 9.386840 \t9.585797 \t 0.000000 \t 962.053595 s\n","1035 \t 19.198673 \t 9.622870 \t9.575802 \t 0.000000 \t 962.716669 s\n","1036 \t 18.649811 \t 9.390045 \t9.259767 \t 0.000000 \t 963.379428 s\n","1037 \t 19.107687 \t 9.469136 \t9.638551 \t 0.000000 \t 964.058608 s\n","1038 \t 18.972367 \t 9.298183 \t9.674185 \t 0.000000 \t 964.735407 s\n","1039 \t 18.800094 \t 8.969768 \t9.830326 \t 0.000000 \t 965.399929 s\n","1040 \t 18.648270 \t 9.408181 \t9.240088 \t 0.000000 \t 966.066677 s\n","1041 \t 18.528507 \t 9.134270 \t9.394238 \t 0.000000 \t 966.744918 s\n","1042 \t 19.236821 \t 9.680120 \t9.556702 \t 0.000000 \t 967.417614 s\n","1043 \t 19.049063 \t 9.559756 \t9.489306 \t 0.000000 \t 968.083461 s\n","1044 \t 19.540694 \t 9.700841 \t9.839854 \t 0.000000 \t 968.793942 s\n","1045 \t 18.392302 \t 9.236034 \t9.156268 \t 0.000000 \t 969.717756 s\n","1046 \t 18.371508 \t 9.091605 \t9.279903 \t 0.000000 \t 970.496087 s\n","1047 \t 18.854250 \t 9.355293 \t9.498957 \t 0.000000 \t 971.277601 s\n","1048 \t 19.834908 \t 10.097005 \t9.737904 \t 0.000000 \t 972.055186 s\n","1049 \t 18.685930 \t 9.253918 \t9.432013 \t 0.000000 \t 972.717256 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3808\n","Hit@10: 0.4731\n","Hit@3: 0.3962\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2483\n","Hit@10: 0.3955\n","Hit@3: 0.2578\n","Hit@1: 0.1725\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3040\n","Hit@10: 0.4692\n","Hit@3: 0.3231\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2917\n","Hit@10: 0.5248\n","Hit@3: 0.3378\n","Hit@1: 0.1599\n"]}]},{"cell_type":"markdown","source":["# Test.py\n"],"metadata":{"id":"5n3-cArhGOo2"}},{"cell_type":"code","source":["KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTHNKG-OA/checkpoint/Reproduce/VTHNKG-OA_seed42_dim512/lr_0.0004_dim_512__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                else: # qualifier\n","                    sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChVIC_5BHELi","executionInfo":{"status":"ok","timestamp":1749781530568,"user_tz":-540,"elapsed":40499,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"28a46e00-af8a-4fbc-cf69-3b46678f1829"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 132/132 [00:36<00:00,  3.61it/s]"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3848\n","Hit@10: 0.4735\n","Hit@3: 0.3750\n","Hit@1: 0.3371\n","Link Prediction on Validation Set (All)\n","MRR: 0.2573\n","Hit@10: 0.3946\n","Hit@3: 0.2580\n","Hit@1: 0.1804\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2618\n","Hit@10: 0.4394\n","Hit@3: 0.2576\n","Hit@1: 0.1894\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2855\n","Hit@10: 0.5358\n","Hit@3: 0.3406\n","Hit@1: 0.1540\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}