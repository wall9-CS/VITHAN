{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyPJcrSmOqxLnhns1vVsHRSe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749781078018,"user_tz":-540,"elapsed":22584,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"f06fc7ad-576e-4915-b513-353da88ac28c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","from collections import Counter\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1749781081597,"user_tz":-540,"elapsed":3563,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1749781081601,"user_tz":-540,"elapsed":2,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0: # numeric triplet 처리\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","        else:\n","            self.proj_ent_vis = nn.Identity()\n","            self.proj_rel_vis = nn.Identity()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1749781081603,"user_tz":-540,"elapsed":1,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1749781081790,"user_tz":-540,"elapsed":186,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-OA/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, \"/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, \"/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load('/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt')\n","        elif os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load('/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt')\n","        elif os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load('/content/drive/MyDrive/code/VTKG-I/textual_features_ent.pt')\n","        self.rel2txt = torch.load('/content/drive/MyDrive/code/VTKG-I/textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-OA/\"\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1749781083852,"user_tz":-540,"elapsed":1836,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"4eb8091b-3737-4333-dc8a-063d898f8121"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-OA\n"," checkpoint\t\t\t      relation2textlong.txt\n"," entities.txt\t\t\t      relation2text.txt\n"," entity2id.txt\t\t\t      relations.txt\n"," entity2textlong.txt\t\t      result\n"," entity2text.txt\t\t      test.txt\n"," legacy\t\t\t\t      train.txt\n","'Model based on VTHNKG-OA'\t      triplets_simple_qualifier_4o.txt\n","'Model based on VTHNKG-OA Weighted'   triplets.txt\n","'process VTHNKG-OA'\t\t      valid.txt\n"," relation2id.txt\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-OA_seed0_Weighted_512dim\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=512, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.1, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# relation 등장 횟수 세기 (Relation 용 Weighted Cross Entropy를 위해)\n","rel_counts = Counter()\n","for tri in KG.train:\n","    for idx in range(1, len(tri), 2):\n","        rel_id = tri[idx]\n","        if rel_id < KG.num_rel:\n","            rel_counts[rel_id] += 1\n","\n","nSamples = np.zeros(KG.num_rel)\n","for rel_id, cnt in rel_counts.items():\n","    nSamples[rel_id] = cnt\n","\n","# Soft Weighting (Focal style)\n","sum_samples = np.sum(nSamples)\n","gamma = 0.5  # 0.5~1.0 사이로 실험해볼 것\n","softWeights = [(1 - (x / sum_samples)) ** gamma if sum_samples > 0 else 0.0 for x in nSamples]\n","\n","# Normalize to mean 1 (optional but recommended)\n","softWeights = torch.FloatTensor(softWeights)\n","softWeights = softWeights * (len(softWeights) / softWeights.sum())\n","softWeights = softWeights.cuda()\n","\n","# Loss 정의 (label smoothing 값도 확인)\n","rel_criterion = nn.CrossEntropyLoss(weight=softWeights, label_smoothing=args.smoothing)\n","# entity는 기존처럼 smoothing만 사용\n","ent_criterion = nn.CrossEntropyLoss(label_smoothing=args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = ent_criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = rel_criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749782154157,"user_tz":-540,"elapsed":1045404,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"14ab052e-29df-459f-e5c9-38bfb577b296"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 26.333256 \t 13.444819 \t12.888437 \t 0.000000 \t 2.024004 s\n","1 \t 19.825073 \t 9.139139 \t10.685935 \t 0.000000 \t 2.657159 s\n","2 \t 22.453230 \t 11.518245 \t10.934985 \t 0.000000 \t 3.414726 s\n","3 \t 19.676437 \t 9.994816 \t9.681622 \t 0.000000 \t 4.044433 s\n","4 \t 19.680267 \t 10.139911 \t9.540358 \t 0.000000 \t 4.676985 s\n","5 \t 19.172261 \t 9.460688 \t9.711574 \t 0.000000 \t 5.451601 s\n","6 \t 19.217890 \t 9.357161 \t9.860730 \t 0.000000 \t 6.184836 s\n","7 \t 18.491348 \t 9.623793 \t8.867554 \t 0.000000 \t 6.916153 s\n","8 \t 18.689244 \t 9.086258 \t9.602985 \t 0.000000 \t 7.671384 s\n","9 \t 18.839777 \t 9.356727 \t9.483051 \t 0.000000 \t 8.304982 s\n","10 \t 18.842052 \t 9.518726 \t9.323328 \t 0.000000 \t 8.942747 s\n","11 \t 18.107083 \t 8.859507 \t9.247576 \t 0.000000 \t 9.562308 s\n","12 \t 19.132113 \t 9.538279 \t9.593833 \t 0.000000 \t 10.329064 s\n","13 \t 18.406223 \t 9.045358 \t9.360866 \t 0.000000 \t 10.961519 s\n","14 \t 18.349233 \t 9.188922 \t9.160310 \t 0.000000 \t 11.631784 s\n","15 \t 18.717009 \t 9.745315 \t8.971694 \t 0.000000 \t 12.260720 s\n","16 \t 18.708293 \t 9.436579 \t9.271715 \t 0.000000 \t 12.897233 s\n","17 \t 18.721333 \t 9.255382 \t9.465951 \t 0.000000 \t 13.769190 s\n","18 \t 18.407156 \t 8.684382 \t9.722775 \t 0.000000 \t 14.662308 s\n","19 \t 18.962065 \t 9.440448 \t9.521616 \t 0.000000 \t 15.498872 s\n","20 \t 18.410398 \t 9.218596 \t9.191803 \t 0.000000 \t 16.135203 s\n","21 \t 18.914531 \t 9.478251 \t9.436280 \t 0.000000 \t 16.785355 s\n","22 \t 18.048151 \t 8.712535 \t9.335616 \t 0.000000 \t 17.560744 s\n","23 \t 18.364672 \t 8.704754 \t9.659916 \t 0.000000 \t 18.279694 s\n","24 \t 17.254057 \t 8.230098 \t9.023959 \t 0.000000 \t 19.016910 s\n","25 \t 18.488690 \t 9.544165 \t8.944526 \t 0.000000 \t 19.767539 s\n","26 \t 18.500824 \t 9.146023 \t9.354801 \t 0.000000 \t 20.518361 s\n","27 \t 17.750633 \t 9.133251 \t8.617382 \t 0.000000 \t 21.231529 s\n","28 \t 18.049059 \t 9.184091 \t8.864968 \t 0.000000 \t 21.857342 s\n","29 \t 18.169097 \t 9.174893 \t8.994204 \t 0.000000 \t 22.491287 s\n","30 \t 17.712014 \t 8.692856 \t9.019157 \t 0.000000 \t 23.131648 s\n","31 \t 17.773538 \t 8.813879 \t8.959659 \t 0.000000 \t 23.768615 s\n","32 \t 18.169472 \t 9.542548 \t8.626923 \t 0.000000 \t 24.402632 s\n","33 \t 18.831160 \t 9.645164 \t9.185995 \t 0.000000 \t 25.044335 s\n","34 \t 18.251659 \t 9.096897 \t9.154763 \t 0.000000 \t 25.819694 s\n","35 \t 18.394808 \t 9.114794 \t9.280015 \t 0.000000 \t 26.462190 s\n","36 \t 18.312701 \t 9.008504 \t9.304197 \t 0.000000 \t 27.093769 s\n","37 \t 17.226219 \t 8.680766 \t8.545454 \t 0.000000 \t 27.726273 s\n","38 \t 18.348310 \t 9.097685 \t9.250625 \t 0.000000 \t 28.364498 s\n","39 \t 18.658751 \t 9.364853 \t9.293898 \t 0.000000 \t 28.998016 s\n","40 \t 18.602740 \t 9.512960 \t9.089780 \t 0.000000 \t 29.624222 s\n","41 \t 17.625373 \t 8.917818 \t8.707555 \t 0.000000 \t 30.272812 s\n","42 \t 17.796412 \t 9.175190 \t8.621222 \t 0.000000 \t 30.969253 s\n","43 \t 18.096441 \t 8.765716 \t9.330726 \t 0.000000 \t 31.691794 s\n","44 \t 18.576306 \t 9.327976 \t9.248331 \t 0.000000 \t 32.455225 s\n","45 \t 18.247192 \t 9.307745 \t8.939447 \t 0.000000 \t 33.232090 s\n","46 \t 17.414952 \t 8.532477 \t8.882475 \t 0.000000 \t 34.137304 s\n","47 \t 17.705376 \t 8.673628 \t9.031747 \t 0.000000 \t 34.851066 s\n","48 \t 17.925661 \t 9.073225 \t8.852436 \t 0.000000 \t 35.572217 s\n","49 \t 17.871334 \t 8.924660 \t8.946673 \t 0.000000 \t 36.319235 s\n","50 \t 19.304953 \t 9.627587 \t9.677366 \t 0.000000 \t 36.962857 s\n","51 \t 17.180573 \t 7.964641 \t9.215932 \t 0.000000 \t 37.606033 s\n","52 \t 19.180898 \t 9.368725 \t9.812172 \t 0.000000 \t 38.247025 s\n","53 \t 19.137324 \t 9.963319 \t9.174006 \t 0.000000 \t 38.880691 s\n","54 \t 17.669085 \t 8.644240 \t9.024845 \t 0.000000 \t 39.521627 s\n","55 \t 18.431273 \t 9.559776 \t8.871497 \t 0.000000 \t 40.157649 s\n","56 \t 17.796476 \t 9.202866 \t8.593610 \t 0.000000 \t 40.802652 s\n","57 \t 17.931318 \t 8.846380 \t9.084938 \t 0.000000 \t 41.442200 s\n","58 \t 18.269650 \t 9.121747 \t9.147902 \t 0.000000 \t 42.080260 s\n","59 \t 17.875051 \t 8.928973 \t8.946079 \t 0.000000 \t 42.847966 s\n","60 \t 18.269915 \t 9.179701 \t9.090213 \t 0.000000 \t 43.489175 s\n","61 \t 17.421914 \t 8.963090 \t8.458824 \t 0.000000 \t 44.166694 s\n","62 \t 18.029358 \t 8.919078 \t9.110281 \t 0.000000 \t 44.923913 s\n","63 \t 19.644211 \t 9.643518 \t10.000693 \t 0.000000 \t 45.698748 s\n","64 \t 18.710943 \t 9.047400 \t9.663544 \t 0.000000 \t 46.442502 s\n","65 \t 17.924585 \t 9.156943 \t8.767642 \t 0.000000 \t 47.191471 s\n","66 \t 18.680742 \t 9.369433 \t9.311309 \t 0.000000 \t 47.823551 s\n","67 \t 18.051432 \t 9.095248 \t8.956184 \t 0.000000 \t 48.594855 s\n","68 \t 18.690706 \t 9.421055 \t9.269651 \t 0.000000 \t 49.242805 s\n","69 \t 18.576412 \t 9.557987 \t9.018425 \t 0.000000 \t 49.896084 s\n","70 \t 18.154179 \t 9.564787 \t8.589392 \t 0.000000 \t 50.538118 s\n","71 \t 17.755177 \t 9.306578 \t8.448600 \t 0.000000 \t 51.200814 s\n","72 \t 17.780741 \t 9.052129 \t8.728611 \t 0.000000 \t 51.836416 s\n","73 \t 18.186099 \t 9.071845 \t9.114254 \t 0.000000 \t 52.477726 s\n","74 \t 18.999128 \t 9.500145 \t9.498983 \t 0.000000 \t 53.107016 s\n","75 \t 17.815950 \t 8.906630 \t8.909321 \t 0.000000 \t 53.752944 s\n","76 \t 17.892569 \t 8.937938 \t8.954631 \t 0.000000 \t 54.541278 s\n","77 \t 18.136259 \t 9.039195 \t9.097064 \t 0.000000 \t 55.181602 s\n","78 \t 18.360146 \t 9.179908 \t9.180237 \t 0.000000 \t 55.811932 s\n","79 \t 18.810205 \t 9.654429 \t9.155775 \t 0.000000 \t 56.460140 s\n","80 \t 17.464872 \t 8.787055 \t8.677817 \t 0.000000 \t 57.160948 s\n","81 \t 18.762272 \t 9.848137 \t8.914135 \t 0.000000 \t 57.894688 s\n","82 \t 18.125540 \t 9.101408 \t9.024131 \t 0.000000 \t 58.653004 s\n","83 \t 17.472147 \t 8.222744 \t9.249403 \t 0.000000 \t 59.396609 s\n","84 \t 17.729206 \t 8.939691 \t8.789515 \t 0.000000 \t 60.140722 s\n","85 \t 18.270867 \t 9.589690 \t8.681178 \t 0.000000 \t 60.903002 s\n","86 \t 17.755878 \t 9.027121 \t8.728758 \t 0.000000 \t 61.552200 s\n","87 \t 18.143963 \t 9.034421 \t9.109541 \t 0.000000 \t 62.197267 s\n","88 \t 18.204826 \t 9.178810 \t9.026016 \t 0.000000 \t 62.822583 s\n","89 \t 18.705769 \t 8.715562 \t9.990207 \t 0.000000 \t 63.452522 s\n","90 \t 18.105529 \t 9.111375 \t8.994153 \t 0.000000 \t 64.076211 s\n","91 \t 18.911987 \t 9.674210 \t9.237777 \t 0.000000 \t 64.726485 s\n","92 \t 17.706854 \t 8.963922 \t8.742931 \t 0.000000 \t 65.362811 s\n","93 \t 17.813163 \t 8.963121 \t8.850042 \t 0.000000 \t 66.004772 s\n","94 \t 18.833778 \t 8.731263 \t10.102516 \t 0.000000 \t 66.781825 s\n","95 \t 18.113524 \t 9.280285 \t8.833239 \t 0.000000 \t 67.422232 s\n","96 \t 17.325764 \t 8.173516 \t9.152249 \t 0.000000 \t 68.062783 s\n","97 \t 17.904896 \t 8.976783 \t8.928113 \t 0.000000 \t 68.690217 s\n","98 \t 17.335280 \t 9.109094 \t8.226186 \t 0.000000 \t 69.320657 s\n","99 \t 17.342893 \t 8.676869 \t8.666023 \t 0.000000 \t 70.030519 s\n","100 \t 16.997266 \t 8.886460 \t8.110806 \t 0.000000 \t 70.773432 s\n","101 \t 17.712037 \t 8.773844 \t8.938193 \t 0.000000 \t 71.560736 s\n","102 \t 17.228785 \t 8.685580 \t8.543206 \t 0.000000 \t 72.295704 s\n","103 \t 17.085155 \t 8.615847 \t8.469309 \t 0.000000 \t 73.190577 s\n","104 \t 17.800823 \t 8.894864 \t8.905959 \t 0.000000 \t 73.840345 s\n","105 \t 17.093286 \t 8.542728 \t8.550558 \t 0.000000 \t 74.493943 s\n","106 \t 18.241414 \t 8.897048 \t9.344365 \t 0.000000 \t 75.138413 s\n","107 \t 17.969340 \t 8.902453 \t9.066887 \t 0.000000 \t 75.792596 s\n","108 \t 16.841052 \t 8.298352 \t8.542700 \t 0.000000 \t 76.428139 s\n","109 \t 17.267894 \t 8.562332 \t8.705562 \t 0.000000 \t 77.058041 s\n","110 \t 16.614804 \t 8.553191 \t8.061612 \t 0.000000 \t 77.686395 s\n","111 \t 17.782957 \t 8.730934 \t9.052023 \t 0.000000 \t 78.334753 s\n","112 \t 16.397616 \t 8.226763 \t8.170853 \t 0.000000 \t 78.988380 s\n","113 \t 17.028364 \t 8.484845 \t8.543519 \t 0.000000 \t 79.751016 s\n","114 \t 16.677193 \t 8.371979 \t8.305213 \t 0.000000 \t 80.394801 s\n","115 \t 17.152759 \t 8.730649 \t8.422110 \t 0.000000 \t 81.062443 s\n","116 \t 17.027840 \t 8.591608 \t8.436232 \t 0.000000 \t 81.691174 s\n","117 \t 16.925489 \t 8.484852 \t8.440637 \t 0.000000 \t 82.338718 s\n","118 \t 17.267333 \t 8.597644 \t8.669690 \t 0.000000 \t 83.037762 s\n","119 \t 16.881950 \t 7.800143 \t9.081807 \t 0.000000 \t 83.772985 s\n","120 \t 17.618544 \t 8.855762 \t8.762781 \t 0.000000 \t 84.545589 s\n","121 \t 17.061098 \t 8.957427 \t8.103672 \t 0.000000 \t 85.289583 s\n","122 \t 17.321942 \t 8.821768 \t8.500174 \t 0.000000 \t 86.025429 s\n","123 \t 17.370380 \t 8.418771 \t8.951610 \t 0.000000 \t 86.810409 s\n","124 \t 18.188644 \t 8.795687 \t9.392959 \t 0.000000 \t 87.444275 s\n","125 \t 17.620986 \t 9.155378 \t8.465608 \t 0.000000 \t 88.090178 s\n","126 \t 18.069730 \t 9.337019 \t8.732711 \t 0.000000 \t 88.736777 s\n","127 \t 17.321452 \t 9.088487 \t8.232966 \t 0.000000 \t 89.369788 s\n","128 \t 18.112985 \t 9.219234 \t8.893750 \t 0.000000 \t 90.014655 s\n","129 \t 18.090198 \t 9.047543 \t9.042655 \t 0.000000 \t 90.662762 s\n","130 \t 17.438728 \t 8.653437 \t8.785292 \t 0.000000 \t 91.300117 s\n","131 \t 18.245912 \t 9.117725 \t9.128186 \t 0.000000 \t 91.937718 s\n","132 \t 18.539250 \t 8.981710 \t9.557540 \t 0.000000 \t 92.576635 s\n","133 \t 17.270857 \t 8.524975 \t8.745882 \t 0.000000 \t 93.218979 s\n","134 \t 17.892424 \t 8.698778 \t9.193646 \t 0.000000 \t 93.879760 s\n","135 \t 18.032650 \t 8.963492 \t9.069159 \t 0.000000 \t 94.650705 s\n","136 \t 17.612802 \t 8.654430 \t8.958371 \t 0.000000 \t 95.291300 s\n","137 \t 17.460373 \t 8.864987 \t8.595385 \t 0.000000 \t 95.992981 s\n","138 \t 18.013205 \t 8.284217 \t9.728988 \t 0.000000 \t 96.727154 s\n","139 \t 17.047078 \t 8.542943 \t8.504135 \t 0.000000 \t 97.491179 s\n","140 \t 17.224649 \t 8.605494 \t8.619156 \t 0.000000 \t 98.233421 s\n","141 \t 17.693615 \t 9.289216 \t8.404399 \t 0.000000 \t 98.991986 s\n","142 \t 18.002836 \t 9.237090 \t8.765746 \t 0.000000 \t 99.633951 s\n","143 \t 17.409935 \t 8.657010 \t8.752924 \t 0.000000 \t 100.270899 s\n","144 \t 17.821611 \t 9.176681 \t8.644931 \t 0.000000 \t 100.914873 s\n","145 \t 17.453090 \t 8.618648 \t8.834442 \t 0.000000 \t 101.561417 s\n","146 \t 18.007480 \t 8.929189 \t9.078291 \t 0.000000 \t 102.209936 s\n","147 \t 17.548778 \t 8.896841 \t8.651937 \t 0.000000 \t 102.983670 s\n","148 \t 17.863466 \t 8.962810 \t8.900657 \t 0.000000 \t 103.629601 s\n","149 \t 17.586445 \t 9.151842 \t8.434604 \t 0.000000 \t 104.280222 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/130 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 130/130 [00:33<00:00,  3.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3740\n","Hit@10: 0.4615\n","Hit@3: 0.3654\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2454\n","Hit@10: 0.3850\n","Hit@3: 0.2439\n","Hit@1: 0.1725\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2982\n","Hit@10: 0.5154\n","Hit@3: 0.3077\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2544\n","Hit@10: 0.5158\n","Hit@3: 0.2770\n","Hit@1: 0.1351\n","150 \t 17.322073 \t 8.897160 \t8.424912 \t 0.000000 \t 140.974763 s\n","151 \t 17.781649 \t 8.785785 \t8.995864 \t 0.000000 \t 141.642904 s\n","152 \t 17.098856 \t 8.358873 \t8.739982 \t 0.000000 \t 142.295324 s\n","153 \t 17.376472 \t 8.837197 \t8.539275 \t 0.000000 \t 142.952573 s\n","154 \t 17.773829 \t 8.908959 \t8.864870 \t 0.000000 \t 143.591408 s\n","155 \t 17.717452 \t 8.611031 \t9.106422 \t 0.000000 \t 144.232407 s\n","156 \t 17.549160 \t 8.786053 \t8.763107 \t 0.000000 \t 145.009885 s\n","157 \t 19.068921 \t 9.163582 \t9.905338 \t 0.000000 \t 145.656469 s\n","158 \t 18.856668 \t 9.083533 \t9.773135 \t 0.000000 \t 146.304285 s\n","159 \t 18.203148 \t 8.860238 \t9.342910 \t 0.000000 \t 146.952975 s\n","160 \t 18.746052 \t 9.073725 \t9.672327 \t 0.000000 \t 147.587435 s\n","161 \t 17.556167 \t 8.329976 \t9.226191 \t 0.000000 \t 148.238585 s\n","162 \t 18.568045 \t 9.070234 \t9.497810 \t 0.000000 \t 149.052402 s\n","163 \t 18.173759 \t 8.881607 \t9.292154 \t 0.000000 \t 149.865252 s\n","164 \t 17.921977 \t 9.188181 \t8.733795 \t 0.000000 \t 150.638749 s\n","165 \t 18.624180 \t 9.543489 \t9.080691 \t 0.000000 \t 151.489579 s\n","166 \t 19.129509 \t 9.033138 \t10.096371 \t 0.000000 \t 152.354859 s\n","167 \t 18.420437 \t 8.705926 \t9.714511 \t 0.000000 \t 153.022865 s\n","168 \t 18.719534 \t 8.883567 \t9.835967 \t 0.000000 \t 153.738779 s\n","169 \t 17.627024 \t 8.672041 \t8.954983 \t 0.000000 \t 154.460701 s\n","170 \t 17.849537 \t 9.295824 \t8.553714 \t 0.000000 \t 155.123138 s\n","171 \t 19.123527 \t 9.520812 \t9.602715 \t 0.000000 \t 155.795646 s\n","172 \t 18.358822 \t 9.300466 \t9.058357 \t 0.000000 \t 156.488025 s\n","173 \t 18.481264 \t 8.620390 \t9.860874 \t 0.000000 \t 157.212337 s\n","174 \t 18.979381 \t 9.718226 \t9.261155 \t 0.000000 \t 157.942559 s\n","175 \t 18.297960 \t 9.090435 \t9.207526 \t 0.000000 \t 158.620062 s\n","176 \t 17.789957 \t 9.280191 \t8.509766 \t 0.000000 \t 159.293984 s\n","177 \t 18.118201 \t 8.903490 \t9.214711 \t 0.000000 \t 160.138019 s\n","178 \t 17.780313 \t 8.845872 \t8.934442 \t 0.000000 \t 160.879104 s\n","179 \t 18.199902 \t 8.771303 \t9.428599 \t 0.000000 \t 161.570450 s\n","180 \t 18.564273 \t 9.701649 \t8.862624 \t 0.000000 \t 162.405945 s\n","181 \t 17.988475 \t 8.947095 \t9.041379 \t 0.000000 \t 163.358119 s\n","182 \t 17.994434 \t 8.853570 \t9.140864 \t 0.000000 \t 164.150882 s\n","183 \t 17.915211 \t 8.979199 \t8.936011 \t 0.000000 \t 164.968592 s\n","184 \t 18.081167 \t 8.784558 \t9.296609 \t 0.000000 \t 165.726852 s\n","185 \t 18.514855 \t 9.245918 \t9.268937 \t 0.000000 \t 166.434062 s\n","186 \t 18.477297 \t 9.243020 \t9.234277 \t 0.000000 \t 167.090777 s\n","187 \t 18.141090 \t 9.011619 \t9.129472 \t 0.000000 \t 167.728839 s\n","188 \t 18.202135 \t 8.981903 \t9.220233 \t 0.000000 \t 168.517612 s\n","189 \t 17.774295 \t 9.017167 \t8.757128 \t 0.000000 \t 169.156865 s\n","190 \t 18.260867 \t 9.025757 \t9.235110 \t 0.000000 \t 169.806571 s\n","191 \t 16.722553 \t 8.387975 \t8.334579 \t 0.000000 \t 170.455156 s\n","192 \t 17.593417 \t 8.814352 \t8.779065 \t 0.000000 \t 171.118104 s\n","193 \t 18.060949 \t 8.699468 \t9.361481 \t 0.000000 \t 171.773395 s\n","194 \t 18.119629 \t 9.073202 \t9.046427 \t 0.000000 \t 172.452200 s\n","195 \t 18.875741 \t 9.619236 \t9.256505 \t 0.000000 \t 173.104728 s\n","196 \t 18.151127 \t 9.276252 \t8.874876 \t 0.000000 \t 173.764816 s\n","197 \t 17.993043 \t 9.087128 \t8.905915 \t 0.000000 \t 174.405503 s\n","198 \t 18.951977 \t 9.388913 \t9.563065 \t 0.000000 \t 175.099450 s\n","199 \t 18.421683 \t 9.114786 \t9.306897 \t 0.000000 \t 175.849814 s\n","200 \t 18.424895 \t 9.501902 \t8.922994 \t 0.000000 \t 176.767447 s\n","201 \t 18.514352 \t 9.340366 \t9.173985 \t 0.000000 \t 177.546681 s\n","202 \t 18.445333 \t 9.489073 \t8.956260 \t 0.000000 \t 178.283736 s\n","203 \t 17.818072 \t 8.512475 \t9.305597 \t 0.000000 \t 178.934530 s\n","204 \t 18.566902 \t 9.440421 \t9.126482 \t 0.000000 \t 179.577288 s\n","205 \t 18.154490 \t 9.256913 \t8.897577 \t 0.000000 \t 180.225490 s\n","206 \t 18.307912 \t 9.440228 \t8.867684 \t 0.000000 \t 180.871378 s\n","207 \t 17.454922 \t 8.916739 \t8.538183 \t 0.000000 \t 181.538600 s\n","208 \t 17.812837 \t 8.939979 \t8.872857 \t 0.000000 \t 182.191912 s\n","209 \t 18.395240 \t 8.945061 \t9.450180 \t 0.000000 \t 182.957914 s\n","210 \t 18.558907 \t 9.502754 \t9.056152 \t 0.000000 \t 183.623408 s\n","211 \t 17.603548 \t 8.529991 \t9.073557 \t 0.000000 \t 184.275980 s\n","212 \t 18.332865 \t 9.306248 \t9.026616 \t 0.000000 \t 184.913649 s\n","213 \t 17.760622 \t 9.242738 \t8.517884 \t 0.000000 \t 185.562388 s\n","214 \t 18.793348 \t 9.562044 \t9.231304 \t 0.000000 \t 186.215865 s\n","215 \t 17.653043 \t 8.691951 \t8.961091 \t 0.000000 \t 186.857253 s\n","216 \t 18.704842 \t 9.655329 \t9.049513 \t 0.000000 \t 187.504884 s\n","217 \t 18.132729 \t 8.781799 \t9.350929 \t 0.000000 \t 188.335199 s\n","218 \t 18.381992 \t 9.414705 \t8.967288 \t 0.000000 \t 189.092148 s\n","219 \t 17.874521 \t 9.383362 \t8.491159 \t 0.000000 \t 189.830381 s\n","220 \t 18.887103 \t 9.175302 \t9.711802 \t 0.000000 \t 190.603340 s\n","221 \t 18.708715 \t 9.716296 \t8.992419 \t 0.000000 \t 191.306281 s\n","222 \t 17.496824 \t 8.954623 \t8.542201 \t 0.000000 \t 191.958716 s\n","223 \t 18.730424 \t 9.246869 \t9.483556 \t 0.000000 \t 192.608047 s\n","224 \t 18.021564 \t 8.501685 \t9.519879 \t 0.000000 \t 193.256056 s\n","225 \t 18.511442 \t 9.016148 \t9.495295 \t 0.000000 \t 193.911896 s\n","226 \t 17.960388 \t 8.790439 \t9.169949 \t 0.000000 \t 194.706931 s\n","227 \t 17.805493 \t 8.466376 \t9.339118 \t 0.000000 \t 195.353177 s\n","228 \t 18.287418 \t 8.960826 \t9.326591 \t 0.000000 \t 196.019048 s\n","229 \t 18.839516 \t 9.801797 \t9.037718 \t 0.000000 \t 196.675590 s\n","230 \t 18.183186 \t 9.453999 \t8.729187 \t 0.000000 \t 197.314850 s\n","231 \t 18.472578 \t 8.900726 \t9.571852 \t 0.000000 \t 197.962696 s\n","232 \t 18.636292 \t 9.315855 \t9.320437 \t 0.000000 \t 198.610470 s\n","233 \t 17.752882 \t 8.926306 \t8.826575 \t 0.000000 \t 199.277203 s\n","234 \t 17.652686 \t 9.112902 \t8.539784 \t 0.000000 \t 199.927120 s\n","235 \t 18.389061 \t 9.519461 \t8.869600 \t 0.000000 \t 200.704108 s\n","236 \t 17.894127 \t 9.259187 \t8.634941 \t 0.000000 \t 201.492686 s\n","237 \t 17.492925 \t 8.720459 \t8.772466 \t 0.000000 \t 202.252440 s\n","238 \t 18.276999 \t 9.049981 \t9.227017 \t 0.000000 \t 202.994953 s\n","239 \t 17.750572 \t 8.890769 \t8.859802 \t 0.000000 \t 203.773868 s\n","240 \t 17.730906 \t 8.826568 \t8.904338 \t 0.000000 \t 204.432075 s\n","241 \t 18.888210 \t 9.031561 \t9.856649 \t 0.000000 \t 205.087779 s\n","242 \t 19.563112 \t 9.394034 \t10.169077 \t 0.000000 \t 205.729429 s\n","243 \t 17.847782 \t 8.712869 \t9.134912 \t 0.000000 \t 206.374327 s\n","244 \t 17.791512 \t 8.813376 \t8.978135 \t 0.000000 \t 207.029039 s\n","245 \t 18.767378 \t 9.319960 \t9.447419 \t 0.000000 \t 207.807765 s\n","246 \t 17.002608 \t 8.284166 \t8.718443 \t 0.000000 \t 208.459442 s\n","247 \t 18.150934 \t 9.085117 \t9.065818 \t 0.000000 \t 209.107909 s\n","248 \t 17.277580 \t 8.751176 \t8.526405 \t 0.000000 \t 209.755611 s\n","249 \t 17.629267 \t 8.770525 \t8.858742 \t 0.000000 \t 210.403293 s\n","250 \t 17.840049 \t 8.986115 \t8.853933 \t 0.000000 \t 211.056340 s\n","251 \t 18.108541 \t 9.024640 \t9.083901 \t 0.000000 \t 211.722220 s\n","252 \t 18.768673 \t 9.435062 \t9.333611 \t 0.000000 \t 212.370413 s\n","253 \t 18.712766 \t 9.700294 \t9.012472 \t 0.000000 \t 213.021727 s\n","254 \t 18.565896 \t 9.271603 \t9.294292 \t 0.000000 \t 213.846558 s\n","255 \t 17.591338 \t 8.144021 \t9.447318 \t 0.000000 \t 214.638722 s\n","256 \t 18.118297 \t 9.359292 \t8.759006 \t 0.000000 \t 215.407406 s\n","257 \t 18.206446 \t 9.089290 \t9.117157 \t 0.000000 \t 216.159074 s\n","258 \t 18.281317 \t 8.949331 \t9.331987 \t 0.000000 \t 216.943553 s\n","259 \t 17.876515 \t 8.575873 \t9.300642 \t 0.000000 \t 217.604876 s\n","260 \t 18.468479 \t 9.277974 \t9.190505 \t 0.000000 \t 218.261399 s\n","261 \t 18.066153 \t 8.877974 \t9.188179 \t 0.000000 \t 218.910268 s\n","262 \t 18.821420 \t 9.814488 \t9.006931 \t 0.000000 \t 219.557792 s\n","263 \t 18.770025 \t 9.424627 \t9.345399 \t 0.000000 \t 220.224980 s\n","264 \t 18.527555 \t 9.431216 \t9.096338 \t 0.000000 \t 220.874198 s\n","265 \t 18.014778 \t 9.376386 \t8.638392 \t 0.000000 \t 221.670266 s\n","266 \t 18.890713 \t 9.023915 \t9.866798 \t 0.000000 \t 222.328127 s\n","267 \t 17.749170 \t 9.081992 \t8.667178 \t 0.000000 \t 222.981641 s\n","268 \t 18.155521 \t 9.173280 \t8.982242 \t 0.000000 \t 223.629354 s\n","269 \t 18.230278 \t 8.932762 \t9.297516 \t 0.000000 \t 224.297050 s\n","270 \t 18.092160 \t 9.157675 \t8.934485 \t 0.000000 \t 224.952809 s\n","271 \t 17.717835 \t 9.112552 \t8.605283 \t 0.000000 \t 225.617725 s\n","272 \t 18.025052 \t 8.692062 \t9.332990 \t 0.000000 \t 226.267469 s\n","273 \t 19.155095 \t 9.272360 \t9.882735 \t 0.000000 \t 226.967351 s\n","274 \t 17.386466 \t 8.537855 \t8.848611 \t 0.000000 \t 227.728467 s\n","275 \t 18.362963 \t 8.897023 \t9.465940 \t 0.000000 \t 228.503896 s\n","276 \t 17.842897 \t 8.470145 \t9.372753 \t 0.000000 \t 229.439028 s\n","277 \t 18.072532 \t 8.988710 \t9.083821 \t 0.000000 \t 230.186955 s\n","278 \t 18.346514 \t 9.258680 \t9.087833 \t 0.000000 \t 230.832654 s\n","279 \t 18.679222 \t 8.997070 \t9.682153 \t 0.000000 \t 231.481061 s\n","280 \t 17.460604 \t 8.844156 \t8.616447 \t 0.000000 \t 232.137097 s\n","281 \t 18.984407 \t 9.557362 \t9.427045 \t 0.000000 \t 232.795416 s\n","282 \t 17.736814 \t 8.655361 \t9.081453 \t 0.000000 \t 233.446267 s\n","283 \t 19.218340 \t 9.604166 \t9.614174 \t 0.000000 \t 234.093489 s\n","284 \t 17.789190 \t 8.869903 \t8.919288 \t 0.000000 \t 234.756531 s\n","285 \t 18.358604 \t 9.076464 \t9.282139 \t 0.000000 \t 235.404181 s\n","286 \t 17.715982 \t 8.824809 \t8.891174 \t 0.000000 \t 236.051512 s\n","287 \t 17.798569 \t 9.126524 \t8.672044 \t 0.000000 \t 236.840626 s\n","288 \t 17.855072 \t 9.406880 \t8.448192 \t 0.000000 \t 237.494468 s\n","289 \t 17.701786 \t 8.528050 \t9.173737 \t 0.000000 \t 238.146822 s\n","290 \t 18.927331 \t 9.492278 \t9.435053 \t 0.000000 \t 238.822825 s\n","291 \t 17.992842 \t 8.569527 \t9.423314 \t 0.000000 \t 239.475447 s\n","292 \t 19.005588 \t 9.680929 \t9.324658 \t 0.000000 \t 240.177211 s\n","293 \t 18.918136 \t 10.187406 \t8.730730 \t 0.000000 \t 240.952405 s\n","294 \t 17.600230 \t 8.470179 \t9.130051 \t 0.000000 \t 241.715749 s\n","295 \t 17.849443 \t 8.653893 \t9.195551 \t 0.000000 \t 242.478843 s\n","296 \t 18.397058 \t 9.194634 \t9.202425 \t 0.000000 \t 243.233324 s\n","297 \t 18.152899 \t 9.261761 \t8.891139 \t 0.000000 \t 243.889437 s\n","298 \t 19.129204 \t 9.185420 \t9.943784 \t 0.000000 \t 244.547672 s\n","299 \t 17.601163 \t 8.975987 \t8.625175 \t 0.000000 \t 245.334197 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3508\n","Hit@10: 0.3962\n","Hit@3: 0.3269\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2339\n","Hit@10: 0.3415\n","Hit@3: 0.2265\n","Hit@1: 0.1742\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.1032\n","Hit@10: 0.3308\n","Hit@3: 0.0692\n","Hit@1: 0.0000\n","Relation Prediction on Validation Set (All)\n","MRR: 0.1840\n","Hit@10: 0.4414\n","Hit@3: 0.1779\n","Hit@1: 0.0743\n","300 \t 17.774009 \t 8.627113 \t9.146896 \t 0.000000 \t 282.832025 s\n","301 \t 17.836231 \t 9.217632 \t8.618599 \t 0.000000 \t 283.593773 s\n","302 \t 17.988348 \t 8.819680 \t9.168668 \t 0.000000 \t 284.247516 s\n","303 \t 17.515511 \t 8.530194 \t8.985316 \t 0.000000 \t 284.907911 s\n","304 \t 17.286167 \t 9.587305 \t7.698862 \t 0.000000 \t 285.566116 s\n","305 \t 17.266991 \t 8.652752 \t8.614239 \t 0.000000 \t 286.215002 s\n","306 \t 18.389203 \t 9.242298 \t9.146905 \t 0.000000 \t 286.875917 s\n","307 \t 18.299899 \t 8.863069 \t9.436831 \t 0.000000 \t 287.531425 s\n","308 \t 17.667890 \t 8.765270 \t8.902619 \t 0.000000 \t 288.191877 s\n","309 \t 17.087680 \t 8.397693 \t8.689988 \t 0.000000 \t 288.850033 s\n","310 \t 18.524024 \t 9.006514 \t9.517509 \t 0.000000 \t 289.508156 s\n","311 \t 18.840275 \t 9.548356 \t9.291918 \t 0.000000 \t 290.318154 s\n","312 \t 18.771051 \t 9.107290 \t9.663763 \t 0.000000 \t 291.042120 s\n","313 \t 17.981714 \t 9.336123 \t8.645591 \t 0.000000 \t 291.738825 s\n","314 \t 18.564123 \t 9.244584 \t9.319540 \t 0.000000 \t 292.447495 s\n","315 \t 18.020165 \t 8.897778 \t9.122388 \t 0.000000 \t 293.156678 s\n","316 \t 18.825558 \t 9.418450 \t9.407106 \t 0.000000 \t 293.943567 s\n","317 \t 17.412729 \t 8.385595 \t9.027133 \t 0.000000 \t 294.750255 s\n","318 \t 17.317513 \t 8.611460 \t8.706052 \t 0.000000 \t 295.602165 s\n","319 \t 17.551903 \t 8.586218 \t8.965685 \t 0.000000 \t 296.462936 s\n","320 \t 17.150054 \t 8.559300 \t8.590753 \t 0.000000 \t 297.288511 s\n","321 \t 18.235206 \t 9.383528 \t8.851677 \t 0.000000 \t 298.050068 s\n","322 \t 17.364868 \t 8.481391 \t8.883478 \t 0.000000 \t 298.763592 s\n","323 \t 17.847866 \t 8.975260 \t8.872605 \t 0.000000 \t 299.611557 s\n","324 \t 18.348648 \t 9.164507 \t9.184141 \t 0.000000 \t 300.302314 s\n","325 \t 18.045446 \t 8.974593 \t9.070855 \t 0.000000 \t 301.069189 s\n","326 \t 17.886460 \t 9.199655 \t8.686806 \t 0.000000 \t 301.803876 s\n","327 \t 18.431027 \t 9.297272 \t9.133756 \t 0.000000 \t 302.514201 s\n","328 \t 17.560015 \t 9.067257 \t8.492759 \t 0.000000 \t 303.202039 s\n","329 \t 18.112733 \t 8.737918 \t9.374815 \t 0.000000 \t 303.889855 s\n","330 \t 15.885062 \t 8.062135 \t7.822928 \t 0.000000 \t 304.577971 s\n","331 \t 18.500243 \t 9.366333 \t9.133910 \t 0.000000 \t 305.286055 s\n","332 \t 18.698027 \t 9.378722 \t9.319305 \t 0.000000 \t 305.975531 s\n","333 \t 17.098516 \t 8.380918 \t8.717598 \t 0.000000 \t 306.685196 s\n","334 \t 17.389602 \t 8.813165 \t8.576437 \t 0.000000 \t 307.561351 s\n","335 \t 17.891834 \t 8.379652 \t9.512183 \t 0.000000 \t 308.589155 s\n","336 \t 17.061083 \t 8.844296 \t8.216787 \t 0.000000 \t 309.392568 s\n","337 \t 17.792803 \t 9.148883 \t8.643919 \t 0.000000 \t 310.152074 s\n","338 \t 17.263179 \t 8.623458 \t8.639720 \t 0.000000 \t 310.807528 s\n","339 \t 17.102478 \t 8.375342 \t8.727135 \t 0.000000 \t 311.456858 s\n","340 \t 17.369908 \t 8.931184 \t8.438724 \t 0.000000 \t 312.118825 s\n","341 \t 17.333586 \t 8.834534 \t8.499052 \t 0.000000 \t 312.773399 s\n","342 \t 17.194875 \t 8.867006 \t8.327869 \t 0.000000 \t 313.434382 s\n","343 \t 17.574492 \t 8.830526 \t8.743966 \t 0.000000 \t 314.104989 s\n","344 \t 18.071363 \t 9.000229 \t9.071134 \t 0.000000 \t 314.910780 s\n","345 \t 17.265174 \t 8.248758 \t9.016416 \t 0.000000 \t 315.566977 s\n","346 \t 17.771155 \t 9.275012 \t8.496144 \t 0.000000 \t 316.242291 s\n","347 \t 17.048402 \t 8.959719 \t8.088683 \t 0.000000 \t 316.901247 s\n","348 \t 17.400322 \t 8.857691 \t8.542631 \t 0.000000 \t 317.564482 s\n","349 \t 17.709084 \t 8.361677 \t9.347406 \t 0.000000 \t 318.236956 s\n","350 \t 17.520111 \t 8.693543 \t8.826568 \t 0.000000 \t 318.892186 s\n","351 \t 16.715320 \t 8.211237 \t8.504082 \t 0.000000 \t 319.554863 s\n","352 \t 18.180073 \t 9.078344 \t9.101728 \t 0.000000 \t 320.412179 s\n","353 \t 16.831033 \t 8.708101 \t8.122931 \t 0.000000 \t 321.196268 s\n","354 \t 17.843864 \t 9.063648 \t8.780217 \t 0.000000 \t 321.954887 s\n","355 \t 16.786686 \t 8.659235 \t8.127450 \t 0.000000 \t 322.729687 s\n","356 \t 17.725721 \t 9.075101 \t8.650620 \t 0.000000 \t 323.448959 s\n","357 \t 17.441073 \t 8.873285 \t8.567789 \t 0.000000 \t 324.109290 s\n","358 \t 17.189320 \t 8.924909 \t8.264412 \t 0.000000 \t 324.769452 s\n","359 \t 17.637384 \t 8.514424 \t9.122961 \t 0.000000 \t 325.437869 s\n","360 \t 17.499100 \t 8.984869 \t8.514232 \t 0.000000 \t 326.115764 s\n","361 \t 18.097403 \t 8.897843 \t9.199559 \t 0.000000 \t 326.771092 s\n","362 \t 16.770657 \t 8.276443 \t8.494214 \t 0.000000 \t 327.558614 s\n","363 \t 17.059674 \t 8.878798 \t8.180876 \t 0.000000 \t 328.219841 s\n","364 \t 17.050414 \t 8.682364 \t8.368050 \t 0.000000 \t 328.873185 s\n","365 \t 17.670975 \t 9.115805 \t8.555170 \t 0.000000 \t 329.538774 s\n","366 \t 16.082969 \t 8.612890 \t7.470078 \t 0.000000 \t 330.192459 s\n","367 \t 18.025231 \t 9.048911 \t8.976320 \t 0.000000 \t 330.837786 s\n","368 \t 17.547743 \t 9.075274 \t8.472469 \t 0.000000 \t 331.514774 s\n","369 \t 16.600865 \t 8.113198 \t8.487668 \t 0.000000 \t 332.167142 s\n","370 \t 17.668947 \t 9.028474 \t8.640473 \t 0.000000 \t 332.838353 s\n","371 \t 17.508609 \t 8.973904 \t8.534705 \t 0.000000 \t 333.615324 s\n","372 \t 17.128898 \t 8.799562 \t8.329337 \t 0.000000 \t 334.547920 s\n","373 \t 18.346766 \t 9.040946 \t9.305820 \t 0.000000 \t 335.301222 s\n","374 \t 17.860909 \t 9.133580 \t8.727329 \t 0.000000 \t 336.067338 s\n","375 \t 16.795256 \t 8.204740 \t8.590516 \t 0.000000 \t 336.716518 s\n","376 \t 18.859771 \t 9.179803 \t9.679967 \t 0.000000 \t 337.361655 s\n","377 \t 17.667480 \t 8.809276 \t8.858205 \t 0.000000 \t 338.029628 s\n","378 \t 17.313801 \t 8.845305 \t8.468495 \t 0.000000 \t 338.683839 s\n","379 \t 18.114533 \t 8.806176 \t9.308357 \t 0.000000 \t 339.326276 s\n","380 \t 18.214384 \t 9.159336 \t9.055048 \t 0.000000 \t 339.973420 s\n","381 \t 16.255499 \t 8.203849 \t8.051651 \t 0.000000 \t 340.622951 s\n","382 \t 17.286121 \t 8.626589 \t8.659532 \t 0.000000 \t 341.267687 s\n","383 \t 17.565409 \t 8.836045 \t8.729364 \t 0.000000 \t 341.922463 s\n","384 \t 17.441836 \t 8.997159 \t8.444678 \t 0.000000 \t 342.706681 s\n","385 \t 16.997676 \t 9.241769 \t7.755907 \t 0.000000 \t 343.365357 s\n","386 \t 17.643768 \t 9.199074 \t8.444695 \t 0.000000 \t 344.026847 s\n","387 \t 16.390985 \t 8.479432 \t7.911553 \t 0.000000 \t 344.670042 s\n","388 \t 17.003289 \t 8.390940 \t8.612349 \t 0.000000 \t 345.334629 s\n","389 \t 16.560524 \t 8.124115 \t8.436409 \t 0.000000 \t 346.034816 s\n","390 \t 17.871166 \t 8.940856 \t8.930310 \t 0.000000 \t 346.792336 s\n","391 \t 17.442241 \t 9.123416 \t8.318825 \t 0.000000 \t 347.570865 s\n","392 \t 17.587051 \t 9.147060 \t8.439991 \t 0.000000 \t 348.355900 s\n","393 \t 17.483708 \t 8.700826 \t8.782882 \t 0.000000 \t 349.105195 s\n","394 \t 18.329096 \t 8.861566 \t9.467529 \t 0.000000 \t 349.756675 s\n","395 \t 18.000828 \t 9.121410 \t8.879417 \t 0.000000 \t 350.413331 s\n","396 \t 16.714808 \t 8.320661 \t8.394147 \t 0.000000 \t 351.206764 s\n","397 \t 17.502596 \t 8.797213 \t8.705382 \t 0.000000 \t 351.854403 s\n","398 \t 17.421204 \t 8.876163 \t8.545041 \t 0.000000 \t 352.519775 s\n","399 \t 17.513718 \t 8.520650 \t8.993068 \t 0.000000 \t 353.177543 s\n","400 \t 17.526001 \t 9.167370 \t8.358631 \t 0.000000 \t 353.837996 s\n","401 \t 17.794051 \t 9.081492 \t8.712559 \t 0.000000 \t 354.495979 s\n","402 \t 18.006365 \t 8.956013 \t9.050351 \t 0.000000 \t 355.155093 s\n","403 \t 17.691149 \t 8.591246 \t9.099903 \t 0.000000 \t 355.802922 s\n","404 \t 16.643337 \t 8.486615 \t8.156723 \t 0.000000 \t 356.454474 s\n","405 \t 15.862602 \t 7.934954 \t7.927647 \t 0.000000 \t 357.099806 s\n","406 \t 17.526704 \t 8.363084 \t9.163620 \t 0.000000 \t 357.768325 s\n","407 \t 17.676300 \t 8.743194 \t8.933106 \t 0.000000 \t 358.416236 s\n","408 \t 17.275424 \t 8.706449 \t8.568975 \t 0.000000 \t 359.238689 s\n","409 \t 17.520987 \t 8.799505 \t8.721481 \t 0.000000 \t 360.035023 s\n","410 \t 17.403555 \t 8.957745 \t8.445810 \t 0.000000 \t 360.767412 s\n","411 \t 17.217787 \t 8.702372 \t8.515414 \t 0.000000 \t 361.540191 s\n","412 \t 17.406387 \t 9.161418 \t8.244969 \t 0.000000 \t 362.258576 s\n","413 \t 17.179700 \t 8.736467 \t8.443233 \t 0.000000 \t 362.905869 s\n","414 \t 17.877604 \t 8.792298 \t9.085306 \t 0.000000 \t 363.564605 s\n","415 \t 17.725715 \t 9.404630 \t8.321084 \t 0.000000 \t 364.219600 s\n","416 \t 16.963408 \t 8.451312 \t8.512096 \t 0.000000 \t 364.872804 s\n","417 \t 16.406195 \t 8.015864 \t8.390331 \t 0.000000 \t 365.534915 s\n","418 \t 17.217290 \t 9.369513 \t7.847777 \t 0.000000 \t 366.205175 s\n","419 \t 18.237921 \t 9.442482 \t8.795438 \t 0.000000 \t 366.853669 s\n","420 \t 17.415867 \t 8.454409 \t8.961459 \t 0.000000 \t 367.643404 s\n","421 \t 17.538566 \t 9.036804 \t8.501762 \t 0.000000 \t 368.312024 s\n","422 \t 18.077248 \t 9.263186 \t8.814062 \t 0.000000 \t 368.973653 s\n","423 \t 17.878510 \t 9.038768 \t8.839742 \t 0.000000 \t 369.621354 s\n","424 \t 16.469702 \t 8.268091 \t8.201612 \t 0.000000 \t 370.290946 s\n","425 \t 16.553934 \t 8.065715 \t8.488219 \t 0.000000 \t 370.946130 s\n","426 \t 17.240643 \t 8.640984 \t8.599658 \t 0.000000 \t 371.587889 s\n","427 \t 17.194611 \t 8.621538 \t8.573073 \t 0.000000 \t 372.313642 s\n","428 \t 16.581250 \t 8.850483 \t7.730767 \t 0.000000 \t 373.080582 s\n","429 \t 17.055364 \t 8.872681 \t8.182682 \t 0.000000 \t 373.989738 s\n","430 \t 17.608296 \t 8.797091 \t8.811205 \t 0.000000 \t 374.779977 s\n","431 \t 17.931574 \t 9.143915 \t8.787659 \t 0.000000 \t 375.429357 s\n","432 \t 17.372752 \t 9.105878 \t8.266874 \t 0.000000 \t 376.085894 s\n","433 \t 17.986654 \t 8.872939 \t9.113717 \t 0.000000 \t 376.743666 s\n","434 \t 17.989225 \t 9.252893 \t8.736332 \t 0.000000 \t 377.389104 s\n","435 \t 17.182095 \t 8.855363 \t8.326732 \t 0.000000 \t 378.061635 s\n","436 \t 16.500138 \t 8.801393 \t7.698745 \t 0.000000 \t 378.725200 s\n","437 \t 16.822954 \t 8.562815 \t8.260140 \t 0.000000 \t 379.530639 s\n","438 \t 17.867558 \t 8.893585 \t8.973974 \t 0.000000 \t 380.190754 s\n","439 \t 17.460686 \t 8.699198 \t8.761487 \t 0.000000 \t 380.838860 s\n","440 \t 17.804745 \t 9.132157 \t8.672588 \t 0.000000 \t 381.500391 s\n","441 \t 17.738461 \t 8.648525 \t9.089936 \t 0.000000 \t 382.149921 s\n","442 \t 17.391796 \t 8.335366 \t9.056430 \t 0.000000 \t 382.813081 s\n","443 \t 16.423331 \t 8.248291 \t8.175040 \t 0.000000 \t 383.461773 s\n","444 \t 18.897480 \t 9.718417 \t9.179062 \t 0.000000 \t 384.114874 s\n","445 \t 17.821623 \t 9.190228 \t8.631394 \t 0.000000 \t 384.791312 s\n","446 \t 17.533110 \t 8.503670 \t9.029440 \t 0.000000 \t 385.562968 s\n","447 \t 17.251185 \t 8.983517 \t8.267668 \t 0.000000 \t 386.531211 s\n","448 \t 17.123851 \t 8.940440 \t8.183411 \t 0.000000 \t 387.289124 s\n","449 \t 16.956709 \t 8.427668 \t8.529040 \t 0.000000 \t 388.070943 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:33<00:00,  3.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3759\n","Hit@10: 0.4577\n","Hit@3: 0.3846\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2457\n","Hit@10: 0.3885\n","Hit@3: 0.2474\n","Hit@1: 0.1725\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3000\n","Hit@10: 0.5000\n","Hit@3: 0.3077\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2555\n","Hit@10: 0.5158\n","Hit@3: 0.2770\n","Hit@1: 0.1351\n","450 \t 17.195929 \t 8.735898 \t8.460030 \t 0.000000 \t 424.794453 s\n","451 \t 18.248779 \t 9.271912 \t8.976867 \t 0.000000 \t 425.565580 s\n","452 \t 18.704917 \t 8.990057 \t9.714859 \t 0.000000 \t 426.328246 s\n","453 \t 17.947573 \t 9.204838 \t8.742735 \t 0.000000 \t 427.098785 s\n","454 \t 17.470585 \t 8.696094 \t8.774490 \t 0.000000 \t 427.820024 s\n","455 \t 18.350706 \t 9.250104 \t9.100602 \t 0.000000 \t 428.482761 s\n","456 \t 18.825074 \t 9.840868 \t8.984206 \t 0.000000 \t 429.128726 s\n","457 \t 17.664265 \t 8.803636 \t8.860629 \t 0.000000 \t 429.937932 s\n","458 \t 17.236039 \t 8.551317 \t8.684722 \t 0.000000 \t 430.591738 s\n","459 \t 18.187923 \t 9.570293 \t8.617630 \t 0.000000 \t 431.257938 s\n","460 \t 17.727909 \t 8.667013 \t9.060897 \t 0.000000 \t 431.940280 s\n","461 \t 18.012487 \t 8.930669 \t9.081819 \t 0.000000 \t 432.625062 s\n","462 \t 17.353382 \t 8.688239 \t8.665144 \t 0.000000 \t 433.323870 s\n","463 \t 18.258584 \t 9.563723 \t8.694861 \t 0.000000 \t 434.025344 s\n","464 \t 17.644994 \t 8.435019 \t9.209976 \t 0.000000 \t 434.760815 s\n","465 \t 17.900793 \t 8.714260 \t9.186533 \t 0.000000 \t 435.479119 s\n","466 \t 18.434504 \t 9.665614 \t8.768890 \t 0.000000 \t 436.168666 s\n","467 \t 17.570951 \t 8.544401 \t9.026550 \t 0.000000 \t 436.856809 s\n","468 \t 17.484050 \t 9.030524 \t8.453526 \t 0.000000 \t 437.791618 s\n","469 \t 17.756589 \t 8.853408 \t8.903181 \t 0.000000 \t 438.611323 s\n","470 \t 17.374751 \t 9.127540 \t8.247212 \t 0.000000 \t 439.471959 s\n","471 \t 16.738223 \t 8.260437 \t8.477786 \t 0.000000 \t 440.294694 s\n","472 \t 17.761207 \t 8.652873 \t9.108334 \t 0.000000 \t 441.093045 s\n","473 \t 17.754827 \t 8.727751 \t9.027077 \t 0.000000 \t 441.781934 s\n","474 \t 17.757226 \t 8.697135 \t9.060091 \t 0.000000 \t 442.488946 s\n","475 \t 17.627592 \t 8.712640 \t8.914952 \t 0.000000 \t 443.219804 s\n","476 \t 17.549382 \t 9.227073 \t8.322309 \t 0.000000 \t 443.972613 s\n","477 \t 18.118567 \t 9.295849 \t8.822717 \t 0.000000 \t 444.698082 s\n","478 \t 18.113586 \t 8.761414 \t9.352172 \t 0.000000 \t 445.390020 s\n","479 \t 17.670853 \t 8.424626 \t9.246227 \t 0.000000 \t 446.281164 s\n","480 \t 18.317327 \t 9.222386 \t9.094940 \t 0.000000 \t 446.985049 s\n","481 \t 17.684954 \t 8.897487 \t8.787467 \t 0.000000 \t 447.707967 s\n","482 \t 17.279541 \t 8.811112 \t8.468430 \t 0.000000 \t 448.402768 s\n","483 \t 17.108627 \t 8.096584 \t9.012043 \t 0.000000 \t 449.088706 s\n","484 \t 17.095133 \t 7.840420 \t9.254713 \t 0.000000 \t 449.812545 s\n","485 \t 18.767089 \t 9.438783 \t9.328305 \t 0.000000 \t 450.616031 s\n","486 \t 18.023032 \t 9.653743 \t8.369289 \t 0.000000 \t 451.388691 s\n","487 \t 17.745732 \t 8.950169 \t8.795563 \t 0.000000 \t 452.133973 s\n","488 \t 18.887114 \t 9.352184 \t9.534929 \t 0.000000 \t 452.902527 s\n","489 \t 18.086271 \t 8.941332 \t9.144940 \t 0.000000 \t 453.685110 s\n","490 \t 17.716251 \t 8.833246 \t8.883006 \t 0.000000 \t 454.338223 s\n","491 \t 17.365619 \t 8.942691 \t8.422928 \t 0.000000 \t 455.163197 s\n","492 \t 17.280566 \t 8.518327 \t8.762239 \t 0.000000 \t 455.812937 s\n","493 \t 17.175851 \t 8.479836 \t8.696014 \t 0.000000 \t 456.461041 s\n","494 \t 18.829033 \t 9.613812 \t9.215220 \t 0.000000 \t 457.130195 s\n","495 \t 17.409085 \t 8.722781 \t8.686304 \t 0.000000 \t 457.793637 s\n","496 \t 17.855481 \t 8.890559 \t8.964922 \t 0.000000 \t 458.440066 s\n","497 \t 17.972101 \t 9.078681 \t8.893419 \t 0.000000 \t 459.106608 s\n","498 \t 17.995936 \t 8.801828 \t9.194108 \t 0.000000 \t 459.759696 s\n","499 \t 16.632965 \t 8.651506 \t7.981459 \t 0.000000 \t 460.411148 s\n","500 \t 18.779767 \t 9.449117 \t9.330650 \t 0.000000 \t 461.070265 s\n","501 \t 18.447547 \t 9.583066 \t8.864480 \t 0.000000 \t 461.729960 s\n","502 \t 18.290194 \t 9.516955 \t8.773238 \t 0.000000 \t 462.384675 s\n","503 \t 17.403128 \t 8.931293 \t8.471834 \t 0.000000 \t 463.172614 s\n","504 \t 17.261597 \t 8.713323 \t8.548275 \t 0.000000 \t 463.876280 s\n","505 \t 17.054483 \t 8.858970 \t8.195513 \t 0.000000 \t 464.625803 s\n","506 \t 17.897985 \t 9.146620 \t8.751365 \t 0.000000 \t 465.412012 s\n","507 \t 16.728975 \t 8.333776 \t8.395200 \t 0.000000 \t 466.170434 s\n","508 \t 17.298546 \t 8.479345 \t8.819201 \t 0.000000 \t 466.934515 s\n","509 \t 17.616463 \t 8.830330 \t8.786134 \t 0.000000 \t 467.601026 s\n","510 \t 17.470265 \t 8.916865 \t8.553401 \t 0.000000 \t 468.256646 s\n","511 \t 16.857833 \t 8.789282 \t8.068551 \t 0.000000 \t 468.915695 s\n","512 \t 17.601326 \t 8.838207 \t8.763119 \t 0.000000 \t 469.591990 s\n","513 \t 17.447069 \t 8.659162 \t8.787907 \t 0.000000 \t 470.250579 s\n","514 \t 17.359917 \t 8.868832 \t8.491085 \t 0.000000 \t 470.905529 s\n","515 \t 18.215448 \t 9.434498 \t8.780950 \t 0.000000 \t 471.709094 s\n","516 \t 17.743784 \t 9.008433 \t8.735351 \t 0.000000 \t 472.356265 s\n","517 \t 18.197715 \t 9.210414 \t8.987300 \t 0.000000 \t 473.009007 s\n","518 \t 16.873472 \t 8.232660 \t8.640812 \t 0.000000 \t 473.669266 s\n","519 \t 17.976562 \t 8.774059 \t9.202503 \t 0.000000 \t 474.326121 s\n","520 \t 17.990944 \t 9.052895 \t8.938049 \t 0.000000 \t 474.991320 s\n","521 \t 17.600354 \t 8.574049 \t9.026306 \t 0.000000 \t 475.654183 s\n","522 \t 18.034552 \t 9.407453 \t8.627099 \t 0.000000 \t 476.312365 s\n","523 \t 17.814495 \t 8.743994 \t9.070501 \t 0.000000 \t 477.015825 s\n","524 \t 17.559036 \t 8.986602 \t8.572435 \t 0.000000 \t 477.795003 s\n","525 \t 18.303300 \t 9.596613 \t8.706686 \t 0.000000 \t 478.572214 s\n","526 \t 17.309772 \t 8.674941 \t8.634830 \t 0.000000 \t 479.332789 s\n","527 \t 17.291076 \t 8.216483 \t9.074592 \t 0.000000 \t 480.242880 s\n","528 \t 18.078441 \t 8.724110 \t9.354330 \t 0.000000 \t 480.887868 s\n","529 \t 17.350019 \t 8.681005 \t8.669014 \t 0.000000 \t 481.541909 s\n","530 \t 17.796396 \t 8.915919 \t8.880477 \t 0.000000 \t 482.204495 s\n","531 \t 16.948172 \t 8.151517 \t8.796655 \t 0.000000 \t 482.855733 s\n","532 \t 17.856836 \t 8.781318 \t9.075519 \t 0.000000 \t 483.521352 s\n","533 \t 17.868793 \t 8.573350 \t9.295444 \t 0.000000 \t 484.193541 s\n","534 \t 18.491405 \t 8.929394 \t9.562010 \t 0.000000 \t 484.842597 s\n","535 \t 17.714613 \t 8.968716 \t8.745897 \t 0.000000 \t 485.494824 s\n","536 \t 17.538763 \t 8.617355 \t8.921408 \t 0.000000 \t 486.155137 s\n","537 \t 17.986799 \t 8.463163 \t9.523636 \t 0.000000 \t 486.813071 s\n","538 \t 16.174243 \t 7.693442 \t8.480801 \t 0.000000 \t 487.464839 s\n","539 \t 17.132797 \t 8.739404 \t8.393394 \t 0.000000 \t 488.282282 s\n","540 \t 16.746730 \t 8.813126 \t7.933604 \t 0.000000 \t 488.935258 s\n","541 \t 16.921423 \t 8.687182 \t8.234241 \t 0.000000 \t 489.579314 s\n","542 \t 17.513896 \t 9.170991 \t8.342905 \t 0.000000 \t 490.362829 s\n","543 \t 16.898330 \t 8.373744 \t8.524585 \t 0.000000 \t 491.116348 s\n","544 \t 18.261998 \t 8.929703 \t9.332295 \t 0.000000 \t 491.845053 s\n","545 \t 16.862334 \t 8.418136 \t8.444198 \t 0.000000 \t 492.638153 s\n","546 \t 17.610800 \t 9.062518 \t8.548283 \t 0.000000 \t 493.288345 s\n","547 \t 18.104766 \t 8.477401 \t9.627366 \t 0.000000 \t 493.938635 s\n","548 \t 17.038711 \t 8.986623 \t8.052088 \t 0.000000 \t 494.618326 s\n","549 \t 18.439854 \t 9.308078 \t9.131775 \t 0.000000 \t 495.274608 s\n","550 \t 17.013357 \t 8.397827 \t8.615530 \t 0.000000 \t 495.928171 s\n","551 \t 17.301838 \t 8.604700 \t8.697138 \t 0.000000 \t 496.582045 s\n","552 \t 17.067805 \t 8.583254 \t8.484551 \t 0.000000 \t 497.363192 s\n","553 \t 17.292721 \t 8.960865 \t8.331857 \t 0.000000 \t 498.019897 s\n","554 \t 17.039853 \t 8.786674 \t8.253180 \t 0.000000 \t 498.674859 s\n","555 \t 16.841198 \t 8.267072 \t8.574125 \t 0.000000 \t 499.335812 s\n","556 \t 17.393829 \t 9.080447 \t8.313382 \t 0.000000 \t 499.988009 s\n","557 \t 17.084145 \t 8.104187 \t8.979958 \t 0.000000 \t 500.633751 s\n","558 \t 17.119276 \t 8.721220 \t8.398056 \t 0.000000 \t 501.284127 s\n","559 \t 16.628193 \t 8.304335 \t8.323859 \t 0.000000 \t 501.928074 s\n","560 \t 17.036545 \t 9.007715 \t8.028830 \t 0.000000 \t 502.739004 s\n","561 \t 18.716513 \t 9.106946 \t9.609567 \t 0.000000 \t 503.511707 s\n","562 \t 16.847998 \t 8.805336 \t8.042660 \t 0.000000 \t 504.284993 s\n","563 \t 17.856261 \t 9.235846 \t8.620416 \t 0.000000 \t 505.081572 s\n","564 \t 17.325491 \t 8.873267 \t8.452224 \t 0.000000 \t 505.833966 s\n","565 \t 17.271977 \t 8.866117 \t8.405860 \t 0.000000 \t 506.477342 s\n","566 \t 17.247358 \t 8.232806 \t9.014552 \t 0.000000 \t 507.134258 s\n","567 \t 17.346960 \t 8.845649 \t8.501310 \t 0.000000 \t 507.790345 s\n","568 \t 18.371414 \t 9.393881 \t8.977532 \t 0.000000 \t 508.435878 s\n","569 \t 17.835373 \t 8.994122 \t8.841252 \t 0.000000 \t 509.239721 s\n","570 \t 17.420888 \t 8.834529 \t8.586358 \t 0.000000 \t 509.890976 s\n","571 \t 17.131541 \t 8.820388 \t8.311153 \t 0.000000 \t 510.540347 s\n","572 \t 17.421242 \t 8.559549 \t8.861692 \t 0.000000 \t 511.215072 s\n","573 \t 17.933786 \t 9.257966 \t8.675821 \t 0.000000 \t 511.875625 s\n","574 \t 17.176733 \t 8.595333 \t8.581400 \t 0.000000 \t 512.530712 s\n","575 \t 16.964584 \t 8.249005 \t8.715580 \t 0.000000 \t 513.190000 s\n","576 \t 17.779899 \t 8.478981 \t9.300918 \t 0.000000 \t 513.843613 s\n","577 \t 17.829656 \t 8.585879 \t9.243777 \t 0.000000 \t 514.496495 s\n","578 \t 17.239080 \t 8.634356 \t8.604724 \t 0.000000 \t 515.145285 s\n","579 \t 16.889987 \t 8.729233 \t8.160754 \t 0.000000 \t 515.992788 s\n","580 \t 17.158194 \t 8.762415 \t8.395779 \t 0.000000 \t 516.735059 s\n","581 \t 17.763039 \t 8.899670 \t8.863369 \t 0.000000 \t 517.521398 s\n","582 \t 17.207443 \t 8.711055 \t8.496388 \t 0.000000 \t 518.271018 s\n","583 \t 16.690501 \t 7.967255 \t8.723246 \t 0.000000 \t 519.017279 s\n","584 \t 17.204289 \t 8.519655 \t8.684634 \t 0.000000 \t 519.661297 s\n","585 \t 16.687138 \t 8.658366 \t8.028773 \t 0.000000 \t 520.319363 s\n","586 \t 17.240183 \t 8.250885 \t8.989299 \t 0.000000 \t 520.974184 s\n","587 \t 17.884167 \t 8.986220 \t8.897946 \t 0.000000 \t 521.638451 s\n","588 \t 16.360965 \t 7.868971 \t8.491994 \t 0.000000 \t 522.290262 s\n","589 \t 17.111681 \t 8.538744 \t8.572937 \t 0.000000 \t 522.946776 s\n","590 \t 17.724051 \t 9.043973 \t8.680077 \t 0.000000 \t 523.744336 s\n","591 \t 18.744867 \t 9.179889 \t9.564979 \t 0.000000 \t 524.395027 s\n","592 \t 17.210220 \t 8.833617 \t8.376604 \t 0.000000 \t 525.052845 s\n","593 \t 17.612498 \t 8.856389 \t8.756109 \t 0.000000 \t 525.710226 s\n","594 \t 17.030416 \t 8.573023 \t8.457393 \t 0.000000 \t 526.360966 s\n","595 \t 16.521952 \t 9.038480 \t7.483471 \t 0.000000 \t 527.026101 s\n","596 \t 17.125793 \t 8.810031 \t8.315763 \t 0.000000 \t 527.675994 s\n","597 \t 17.327649 \t 8.712932 \t8.614717 \t 0.000000 \t 528.346574 s\n","598 \t 16.799708 \t 8.862732 \t7.936977 \t 0.000000 \t 529.045765 s\n","599 \t 16.198234 \t 7.891070 \t8.307163 \t 0.000000 \t 529.804736 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:33<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3708\n","Hit@10: 0.4692\n","Hit@3: 0.3654\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2430\n","Hit@10: 0.3780\n","Hit@3: 0.2439\n","Hit@1: 0.1742\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3017\n","Hit@10: 0.5231\n","Hit@3: 0.3077\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2494\n","Hit@10: 0.5225\n","Hit@3: 0.2658\n","Hit@1: 0.1351\n","600 \t 17.407034 \t 9.023173 \t8.383860 \t 0.000000 \t 566.800576 s\n","601 \t 17.083047 \t 8.763192 \t8.319855 \t 0.000000 \t 567.599077 s\n","602 \t 17.377744 \t 8.797384 \t8.580360 \t 0.000000 \t 568.296608 s\n","603 \t 16.865906 \t 8.405042 \t8.460864 \t 0.000000 \t 569.052630 s\n","604 \t 16.632675 \t 8.140397 \t8.492277 \t 0.000000 \t 569.816843 s\n","605 \t 17.872320 \t 9.023189 \t8.849131 \t 0.000000 \t 570.579000 s\n","606 \t 17.597063 \t 8.787131 \t8.809932 \t 0.000000 \t 571.323758 s\n","607 \t 18.192991 \t 9.087173 \t9.105819 \t 0.000000 \t 571.967653 s\n","608 \t 18.037704 \t 9.376733 \t8.660972 \t 0.000000 \t 572.618951 s\n","609 \t 16.710134 \t 8.061692 \t8.648442 \t 0.000000 \t 573.289913 s\n","610 \t 17.474964 \t 8.866551 \t8.608414 \t 0.000000 \t 573.943003 s\n","611 \t 16.682309 \t 8.354584 \t8.327724 \t 0.000000 \t 574.669937 s\n","612 \t 18.081801 \t 8.932591 \t9.149209 \t 0.000000 \t 575.523773 s\n","613 \t 17.858569 \t 8.658867 \t9.199701 \t 0.000000 \t 576.254063 s\n","614 \t 17.666931 \t 8.874654 \t8.792277 \t 0.000000 \t 576.929779 s\n","615 \t 17.887981 \t 8.941762 \t8.946219 \t 0.000000 \t 577.632562 s\n","616 \t 18.072853 \t 9.046528 \t9.026325 \t 0.000000 \t 578.334232 s\n","617 \t 17.952798 \t 9.494794 \t8.458004 \t 0.000000 \t 579.071065 s\n","618 \t 17.726848 \t 8.548110 \t9.178738 \t 0.000000 \t 579.773678 s\n","619 \t 17.028079 \t 8.868927 \t8.159152 \t 0.000000 \t 580.474353 s\n","620 \t 17.500289 \t 9.108068 \t8.392222 \t 0.000000 \t 581.252156 s\n","621 \t 17.595262 \t 8.905520 \t8.689742 \t 0.000000 \t 582.020310 s\n","622 \t 17.206029 \t 8.614870 \t8.591159 \t 0.000000 \t 582.846540 s\n","623 \t 16.818842 \t 8.406207 \t8.412634 \t 0.000000 \t 583.613824 s\n","624 \t 16.857925 \t 8.348016 \t8.509910 \t 0.000000 \t 584.659971 s\n","625 \t 16.597888 \t 7.713764 \t8.884124 \t 0.000000 \t 585.397432 s\n","626 \t 18.006849 \t 9.046429 \t8.960422 \t 0.000000 \t 586.166817 s\n","627 \t 17.146991 \t 8.744967 \t8.402023 \t 0.000000 \t 586.914143 s\n","628 \t 17.383371 \t 8.980319 \t8.403052 \t 0.000000 \t 587.653339 s\n","629 \t 16.183381 \t 8.261934 \t7.921447 \t 0.000000 \t 588.355429 s\n","630 \t 18.409591 \t 9.285615 \t9.123975 \t 0.000000 \t 589.082863 s\n","631 \t 17.183583 \t 8.668077 \t8.515506 \t 0.000000 \t 589.769337 s\n","632 \t 17.681183 \t 9.102905 \t8.578278 \t 0.000000 \t 590.515625 s\n","633 \t 17.726110 \t 8.359674 \t9.366437 \t 0.000000 \t 591.189314 s\n","634 \t 18.005545 \t 8.809012 \t9.196533 \t 0.000000 \t 591.868865 s\n","635 \t 16.920672 \t 8.799438 \t8.121235 \t 0.000000 \t 592.557468 s\n","636 \t 17.665684 \t 9.251139 \t8.414545 \t 0.000000 \t 593.346869 s\n","637 \t 17.973687 \t 9.178576 \t8.795112 \t 0.000000 \t 594.001988 s\n","638 \t 17.341913 \t 8.569453 \t8.772460 \t 0.000000 \t 594.719308 s\n","639 \t 17.263134 \t 9.085801 \t8.177332 \t 0.000000 \t 595.500353 s\n","640 \t 17.674380 \t 8.596263 \t9.078117 \t 0.000000 \t 596.270066 s\n","641 \t 16.576191 \t 8.475106 \t8.101085 \t 0.000000 \t 597.035931 s\n","642 \t 17.832117 \t 9.217941 \t8.614176 \t 0.000000 \t 597.797947 s\n","643 \t 16.669407 \t 8.753559 \t7.915848 \t 0.000000 \t 598.457488 s\n","644 \t 18.097599 \t 9.110212 \t8.987387 \t 0.000000 \t 599.107320 s\n","645 \t 16.307853 \t 8.383942 \t7.923911 \t 0.000000 \t 599.899190 s\n","646 \t 16.856150 \t 8.330683 \t8.525466 \t 0.000000 \t 600.550094 s\n","647 \t 17.715033 \t 8.877882 \t8.837150 \t 0.000000 \t 601.211912 s\n","648 \t 17.627710 \t 8.722447 \t8.905264 \t 0.000000 \t 601.867030 s\n","649 \t 17.788666 \t 8.694365 \t9.094301 \t 0.000000 \t 602.538315 s\n","650 \t 17.452502 \t 9.176849 \t8.275653 \t 0.000000 \t 603.199310 s\n","651 \t 17.055812 \t 8.690279 \t8.365532 \t 0.000000 \t 603.842913 s\n","652 \t 17.853406 \t 8.948731 \t8.904675 \t 0.000000 \t 604.513482 s\n","653 \t 17.283075 \t 8.828545 \t8.454531 \t 0.000000 \t 605.303075 s\n","654 \t 16.914684 \t 8.876847 \t8.037838 \t 0.000000 \t 605.962637 s\n","655 \t 17.685610 \t 9.024001 \t8.661609 \t 0.000000 \t 606.625231 s\n","656 \t 17.874292 \t 8.997025 \t8.877269 \t 0.000000 \t 607.280917 s\n","657 \t 16.665054 \t 8.537861 \t8.127193 \t 0.000000 \t 608.060083 s\n","658 \t 17.256674 \t 8.718969 \t8.537705 \t 0.000000 \t 608.828138 s\n","659 \t 16.939775 \t 8.840518 \t8.099256 \t 0.000000 \t 609.558243 s\n","660 \t 16.773911 \t 8.400702 \t8.373209 \t 0.000000 \t 610.369113 s\n","661 \t 18.563466 \t 9.852875 \t8.710590 \t 0.000000 \t 611.027647 s\n","662 \t 17.139376 \t 8.313333 \t8.826043 \t 0.000000 \t 611.818397 s\n","663 \t 16.936559 \t 8.544136 \t8.392423 \t 0.000000 \t 612.464611 s\n","664 \t 17.499264 \t 8.801685 \t8.697578 \t 0.000000 \t 613.107028 s\n","665 \t 17.063561 \t 8.464427 \t8.599135 \t 0.000000 \t 613.763271 s\n","666 \t 17.501758 \t 9.058660 \t8.443099 \t 0.000000 \t 614.433658 s\n","667 \t 17.922041 \t 8.534372 \t9.387669 \t 0.000000 \t 615.102224 s\n","668 \t 16.796655 \t 8.556929 \t8.239727 \t 0.000000 \t 615.744870 s\n","669 \t 17.081419 \t 8.694676 \t8.386743 \t 0.000000 \t 616.387782 s\n","670 \t 17.762488 \t 9.011959 \t8.750529 \t 0.000000 \t 617.050527 s\n","671 \t 18.140741 \t 9.242638 \t8.898103 \t 0.000000 \t 617.715591 s\n","672 \t 17.838011 \t 9.155210 \t8.682800 \t 0.000000 \t 618.513743 s\n","673 \t 17.160075 \t 8.803910 \t8.356165 \t 0.000000 \t 619.162765 s\n","674 \t 17.302725 \t 8.410834 \t8.891891 \t 0.000000 \t 619.809803 s\n","675 \t 16.676897 \t 8.750189 \t7.926708 \t 0.000000 \t 620.501226 s\n","676 \t 17.479085 \t 8.365509 \t9.113575 \t 0.000000 \t 621.269974 s\n","677 \t 16.594234 \t 8.369806 \t8.224427 \t 0.000000 \t 622.046878 s\n","678 \t 17.365747 \t 8.469457 \t8.896290 \t 0.000000 \t 622.824134 s\n","679 \t 16.284373 \t 7.888518 \t8.395854 \t 0.000000 \t 623.579873 s\n","680 \t 17.561239 \t 9.348635 \t8.212605 \t 0.000000 \t 624.228041 s\n","681 \t 16.760612 \t 8.264219 \t8.496393 \t 0.000000 \t 624.897620 s\n","682 \t 17.556131 \t 8.388214 \t9.167917 \t 0.000000 \t 625.553396 s\n","683 \t 17.266495 \t 8.228731 \t9.037764 \t 0.000000 \t 626.357118 s\n","684 \t 18.548294 \t 9.042978 \t9.505316 \t 0.000000 \t 627.007327 s\n","685 \t 17.450902 \t 8.646593 \t8.804308 \t 0.000000 \t 627.650636 s\n","686 \t 18.120254 \t 9.100818 \t9.019435 \t 0.000000 \t 628.299659 s\n","687 \t 17.389422 \t 8.859055 \t8.530368 \t 0.000000 \t 628.966428 s\n","688 \t 17.650227 \t 8.746442 \t8.903784 \t 0.000000 \t 629.616607 s\n","689 \t 18.211392 \t 9.307484 \t8.903908 \t 0.000000 \t 630.261858 s\n","690 \t 17.947746 \t 8.467424 \t9.480323 \t 0.000000 \t 630.919114 s\n","691 \t 17.382506 \t 9.187224 \t8.195282 \t 0.000000 \t 631.572079 s\n","692 \t 17.480855 \t 8.850707 \t8.630148 \t 0.000000 \t 632.223730 s\n","693 \t 18.006342 \t 9.113034 \t8.893309 \t 0.000000 \t 632.874506 s\n","694 \t 16.995114 \t 8.886671 \t8.108443 \t 0.000000 \t 633.594916 s\n","695 \t 17.090850 \t 8.667564 \t8.423286 \t 0.000000 \t 634.552485 s\n","696 \t 16.405749 \t 8.465683 \t7.940067 \t 0.000000 \t 635.308090 s\n","697 \t 17.100981 \t 8.685446 \t8.415535 \t 0.000000 \t 636.073078 s\n","698 \t 18.020999 \t 9.317588 \t8.703412 \t 0.000000 \t 636.824014 s\n","699 \t 17.287070 \t 8.286496 \t9.000574 \t 0.000000 \t 637.478221 s\n","700 \t 17.835386 \t 9.090866 \t8.744521 \t 0.000000 \t 638.128099 s\n","701 \t 17.658419 \t 8.881419 \t8.776999 \t 0.000000 \t 638.777277 s\n","702 \t 17.210446 \t 8.862019 \t8.348428 \t 0.000000 \t 639.445086 s\n","703 \t 17.054340 \t 8.748137 \t8.306203 \t 0.000000 \t 640.093210 s\n","704 \t 17.596245 \t 8.802949 \t8.793296 \t 0.000000 \t 640.737302 s\n","705 \t 16.756598 \t 8.646643 \t8.109955 \t 0.000000 \t 641.385967 s\n","706 \t 17.227551 \t 9.002890 \t8.224662 \t 0.000000 \t 642.047647 s\n","707 \t 17.198296 \t 8.741756 \t8.456539 \t 0.000000 \t 642.825575 s\n","708 \t 17.107396 \t 8.386761 \t8.720635 \t 0.000000 \t 643.493296 s\n","709 \t 17.172710 \t 8.843454 \t8.329256 \t 0.000000 \t 644.161082 s\n","710 \t 17.621286 \t 8.838021 \t8.783266 \t 0.000000 \t 644.821503 s\n","711 \t 16.463724 \t 8.319957 \t8.143768 \t 0.000000 \t 645.477469 s\n","712 \t 16.976481 \t 8.460765 \t8.515716 \t 0.000000 \t 646.151046 s\n","713 \t 17.263139 \t 8.650575 \t8.612564 \t 0.000000 \t 646.855358 s\n","714 \t 18.370620 \t 8.651755 \t9.718865 \t 0.000000 \t 647.628274 s\n","715 \t 17.647613 \t 9.028986 \t8.618627 \t 0.000000 \t 648.381310 s\n","716 \t 17.540828 \t 8.805403 \t8.735425 \t 0.000000 \t 649.352311 s\n","717 \t 16.805989 \t 8.355180 \t8.450809 \t 0.000000 \t 650.001489 s\n","718 \t 18.574173 \t 9.393438 \t9.180735 \t 0.000000 \t 650.643802 s\n","719 \t 17.541022 \t 8.900664 \t8.640357 \t 0.000000 \t 651.300350 s\n","720 \t 17.342844 \t 8.732737 \t8.610107 \t 0.000000 \t 651.959980 s\n","721 \t 17.136817 \t 8.886058 \t8.250759 \t 0.000000 \t 652.610318 s\n","722 \t 16.783700 \t 8.344514 \t8.439186 \t 0.000000 \t 653.258892 s\n","723 \t 16.722753 \t 8.177549 \t8.545203 \t 0.000000 \t 653.922808 s\n","724 \t 17.223061 \t 8.807549 \t8.415510 \t 0.000000 \t 654.574948 s\n","725 \t 16.931638 \t 8.665076 \t8.266562 \t 0.000000 \t 655.350436 s\n","726 \t 17.707623 \t 8.965426 \t8.742196 \t 0.000000 \t 656.026944 s\n","727 \t 17.151799 \t 8.811075 \t8.340724 \t 0.000000 \t 656.701801 s\n","728 \t 17.262206 \t 8.701463 \t8.560743 \t 0.000000 \t 657.355137 s\n","729 \t 17.393066 \t 8.893517 \t8.499549 \t 0.000000 \t 658.026688 s\n","730 \t 17.500521 \t 8.991438 \t8.509084 \t 0.000000 \t 658.697402 s\n","731 \t 17.238152 \t 8.922910 \t8.315242 \t 0.000000 \t 659.355047 s\n","732 \t 17.682451 \t 8.689649 \t8.992802 \t 0.000000 \t 660.127483 s\n","733 \t 17.628925 \t 9.246079 \t8.382846 \t 0.000000 \t 660.875480 s\n","734 \t 17.201195 \t 8.605063 \t8.596132 \t 0.000000 \t 661.608301 s\n","735 \t 17.367434 \t 8.770607 \t8.596827 \t 0.000000 \t 662.390860 s\n","736 \t 16.861778 \t 8.826976 \t8.034802 \t 0.000000 \t 663.189515 s\n","737 \t 17.843789 \t 8.315552 \t9.528236 \t 0.000000 \t 663.841076 s\n","738 \t 18.033325 \t 9.129811 \t8.903514 \t 0.000000 \t 664.492958 s\n","739 \t 18.172553 \t 9.029558 \t9.142995 \t 0.000000 \t 665.144464 s\n","740 \t 17.025091 \t 8.413023 \t8.612068 \t 0.000000 \t 665.802790 s\n","741 \t 16.458714 \t 8.189050 \t8.269664 \t 0.000000 \t 666.454103 s\n","742 \t 17.252453 \t 8.739144 \t8.513309 \t 0.000000 \t 667.099280 s\n","743 \t 16.807998 \t 8.818482 \t7.989515 \t 0.000000 \t 667.760282 s\n","744 \t 17.287928 \t 8.400735 \t8.887193 \t 0.000000 \t 668.431923 s\n","745 \t 16.558423 \t 8.326040 \t8.232383 \t 0.000000 \t 669.086212 s\n","746 \t 17.192203 \t 8.570350 \t8.621852 \t 0.000000 \t 669.729775 s\n","747 \t 15.691381 \t 8.180763 \t7.510618 \t 0.000000 \t 670.520118 s\n","748 \t 16.262130 \t 8.057827 \t8.204303 \t 0.000000 \t 671.170080 s\n","749 \t 17.391097 \t 8.472513 \t8.918584 \t 0.000000 \t 671.812209 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3750\n","Hit@10: 0.4423\n","Hit@3: 0.3846\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2431\n","Hit@10: 0.3815\n","Hit@3: 0.2404\n","Hit@1: 0.1707\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2990\n","Hit@10: 0.4769\n","Hit@3: 0.3231\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2553\n","Hit@10: 0.5248\n","Hit@3: 0.2815\n","Hit@1: 0.1351\n","750 \t 18.134061 \t 8.624055 \t9.510006 \t 0.000000 \t 710.762105 s\n","751 \t 16.706996 \t 8.593270 \t8.113727 \t 0.000000 \t 711.410576 s\n","752 \t 17.529661 \t 8.959689 \t8.569972 \t 0.000000 \t 712.120793 s\n","753 \t 17.065269 \t 8.735076 \t8.330193 \t 0.000000 \t 712.885962 s\n","754 \t 17.454708 \t 8.570196 \t8.884512 \t 0.000000 \t 713.648807 s\n","755 \t 18.297598 \t 9.496033 \t8.801565 \t 0.000000 \t 714.431501 s\n","756 \t 16.725861 \t 8.211704 \t8.514157 \t 0.000000 \t 715.195392 s\n","757 \t 17.501933 \t 8.824574 \t8.677359 \t 0.000000 \t 715.843977 s\n","758 \t 17.742847 \t 9.066278 \t8.676569 \t 0.000000 \t 716.649415 s\n","759 \t 17.199530 \t 8.446954 \t8.752575 \t 0.000000 \t 717.396490 s\n","760 \t 17.744111 \t 9.117888 \t8.626223 \t 0.000000 \t 718.093742 s\n","761 \t 16.851479 \t 8.767009 \t8.084470 \t 0.000000 \t 718.770587 s\n","762 \t 17.961326 \t 9.033478 \t8.927848 \t 0.000000 \t 719.448034 s\n","763 \t 17.187666 \t 8.757318 \t8.430349 \t 0.000000 \t 720.154749 s\n","764 \t 17.648031 \t 9.261805 \t8.386227 \t 0.000000 \t 720.820656 s\n","765 \t 17.304497 \t 8.613181 \t8.691316 \t 0.000000 \t 721.506268 s\n","766 \t 17.021236 \t 8.349889 \t8.671347 \t 0.000000 \t 722.184147 s\n","767 \t 17.668291 \t 8.872395 \t8.795897 \t 0.000000 \t 722.864515 s\n","768 \t 16.673948 \t 8.205298 \t8.468651 \t 0.000000 \t 723.570225 s\n","769 \t 15.818894 \t 7.922623 \t7.896271 \t 0.000000 \t 724.297210 s\n","770 \t 17.709309 \t 8.887086 \t8.822222 \t 0.000000 \t 725.157845 s\n","771 \t 17.571306 \t 8.958776 \t8.612532 \t 0.000000 \t 725.959021 s\n","772 \t 17.158607 \t 8.745076 \t8.413531 \t 0.000000 \t 726.757550 s\n","773 \t 18.321842 \t 9.356706 \t8.965137 \t 0.000000 \t 727.553848 s\n","774 \t 17.176435 \t 8.572097 \t8.604337 \t 0.000000 \t 728.464797 s\n","775 \t 17.612824 \t 8.883918 \t8.728907 \t 0.000000 \t 729.212297 s\n","776 \t 17.506242 \t 8.777275 \t8.728967 \t 0.000000 \t 729.938589 s\n","777 \t 17.657278 \t 9.059636 \t8.597642 \t 0.000000 \t 730.697612 s\n","778 \t 17.969084 \t 8.658986 \t9.310098 \t 0.000000 \t 731.422828 s\n","779 \t 16.592794 \t 8.434811 \t8.157983 \t 0.000000 \t 732.123140 s\n","780 \t 17.469728 \t 8.749916 \t8.719811 \t 0.000000 \t 732.822127 s\n","781 \t 17.865376 \t 9.001075 \t8.864300 \t 0.000000 \t 733.514954 s\n","782 \t 16.996685 \t 8.923828 \t8.072857 \t 0.000000 \t 734.389568 s\n","783 \t 17.350126 \t 8.857501 \t8.492626 \t 0.000000 \t 735.059845 s\n","784 \t 18.266975 \t 8.976761 \t9.290214 \t 0.000000 \t 735.710650 s\n","785 \t 16.431726 \t 7.933019 \t8.498706 \t 0.000000 \t 736.370445 s\n","786 \t 17.191112 \t 8.853317 \t8.337795 \t 0.000000 \t 737.035622 s\n","787 \t 17.323471 \t 8.745952 \t8.577519 \t 0.000000 \t 737.688729 s\n","788 \t 17.183313 \t 8.735079 \t8.448235 \t 0.000000 \t 738.361433 s\n","789 \t 17.310925 \t 8.996467 \t8.314458 \t 0.000000 \t 739.127754 s\n","790 \t 17.342132 \t 8.682791 \t8.659340 \t 0.000000 \t 739.868570 s\n","791 \t 17.064453 \t 8.624204 \t8.440249 \t 0.000000 \t 740.796768 s\n","792 \t 18.008026 \t 8.990651 \t9.017375 \t 0.000000 \t 741.562357 s\n","793 \t 17.461899 \t 8.828260 \t8.633639 \t 0.000000 \t 742.231998 s\n","794 \t 17.247377 \t 8.540724 \t8.706654 \t 0.000000 \t 742.874883 s\n","795 \t 17.910439 \t 9.186937 \t8.723503 \t 0.000000 \t 743.541470 s\n","796 \t 16.891633 \t 8.373441 \t8.518192 \t 0.000000 \t 744.192710 s\n","797 \t 17.088977 \t 8.524182 \t8.564795 \t 0.000000 \t 744.837745 s\n","798 \t 17.611734 \t 8.700648 \t8.911086 \t 0.000000 \t 745.503335 s\n","799 \t 17.893761 \t 8.558549 \t9.335212 \t 0.000000 \t 746.314566 s\n","800 \t 18.143444 \t 9.106385 \t9.037059 \t 0.000000 \t 746.979058 s\n","801 \t 17.294133 \t 8.989146 \t8.304987 \t 0.000000 \t 747.637777 s\n","802 \t 16.738716 \t 8.267928 \t8.470789 \t 0.000000 \t 748.297743 s\n","803 \t 17.239902 \t 8.632032 \t8.607872 \t 0.000000 \t 748.947945 s\n","804 \t 18.099153 \t 9.311026 \t8.788126 \t 0.000000 \t 749.611106 s\n","805 \t 17.936315 \t 9.230571 \t8.705744 \t 0.000000 \t 750.264009 s\n","806 \t 17.219350 \t 8.210942 \t9.008407 \t 0.000000 \t 750.932092 s\n","807 \t 16.754298 \t 7.944675 \t8.809623 \t 0.000000 \t 751.648847 s\n","808 \t 18.080463 \t 8.615833 \t9.464630 \t 0.000000 \t 752.593780 s\n","809 \t 17.269262 \t 8.253335 \t9.015928 \t 0.000000 \t 753.335532 s\n","810 \t 17.434580 \t 8.606968 \t8.827610 \t 0.000000 \t 754.119373 s\n","811 \t 17.404677 \t 9.111645 \t8.293032 \t 0.000000 \t 754.835528 s\n","812 \t 17.228165 \t 8.769126 \t8.459038 \t 0.000000 \t 755.483628 s\n","813 \t 17.284240 \t 8.158976 \t9.125264 \t 0.000000 \t 756.152085 s\n","814 \t 17.271532 \t 8.630472 \t8.641060 \t 0.000000 \t 756.817297 s\n","815 \t 16.997688 \t 8.543271 \t8.454417 \t 0.000000 \t 757.467957 s\n","816 \t 16.904449 \t 8.246022 \t8.658427 \t 0.000000 \t 758.117239 s\n","817 \t 17.642708 \t 8.657709 \t8.984999 \t 0.000000 \t 758.782352 s\n","818 \t 16.740582 \t 8.404248 \t8.336334 \t 0.000000 \t 759.575715 s\n","819 \t 17.082747 \t 8.451238 \t8.631509 \t 0.000000 \t 760.245712 s\n","820 \t 17.462635 \t 8.728892 \t8.733743 \t 0.000000 \t 760.898717 s\n","821 \t 17.479015 \t 9.106430 \t8.372586 \t 0.000000 \t 761.557784 s\n","822 \t 17.920740 \t 9.009511 \t8.911228 \t 0.000000 \t 762.207387 s\n","823 \t 17.103681 \t 8.754347 \t8.349333 \t 0.000000 \t 762.883721 s\n","824 \t 17.752626 \t 8.858219 \t8.894408 \t 0.000000 \t 763.540857 s\n","825 \t 17.510017 \t 8.898899 \t8.611118 \t 0.000000 \t 764.194300 s\n","826 \t 17.483249 \t 8.961946 \t8.521302 \t 0.000000 \t 764.950043 s\n","827 \t 17.683650 \t 8.892598 \t8.791052 \t 0.000000 \t 765.707500 s\n","828 \t 18.003496 \t 8.904792 \t9.098704 \t 0.000000 \t 766.458620 s\n","829 \t 16.097579 \t 8.216759 \t7.880820 \t 0.000000 \t 767.403720 s\n","830 \t 17.446605 \t 8.191977 \t9.254627 \t 0.000000 \t 768.061059 s\n","831 \t 16.968639 \t 8.518592 \t8.450047 \t 0.000000 \t 768.722058 s\n","832 \t 17.668396 \t 8.812144 \t8.856253 \t 0.000000 \t 769.382551 s\n","833 \t 18.320849 \t 9.188654 \t9.132196 \t 0.000000 \t 770.042427 s\n","834 \t 17.009842 \t 8.434066 \t8.575777 \t 0.000000 \t 770.712783 s\n","835 \t 17.265113 \t 9.004817 \t8.260296 \t 0.000000 \t 771.372095 s\n","836 \t 17.120716 \t 8.791012 \t8.329704 \t 0.000000 \t 772.039111 s\n","837 \t 17.900632 \t 8.841474 \t9.059158 \t 0.000000 \t 772.689088 s\n","838 \t 17.765748 \t 8.739032 \t9.026716 \t 0.000000 \t 773.356766 s\n","839 \t 17.117542 \t 8.689994 \t8.427547 \t 0.000000 \t 774.012952 s\n","840 \t 17.193977 \t 8.634543 \t8.559434 \t 0.000000 \t 774.670171 s\n","841 \t 16.842337 \t 8.417513 \t8.424824 \t 0.000000 \t 775.459563 s\n","842 \t 16.512197 \t 7.802142 \t8.710055 \t 0.000000 \t 776.114801 s\n","843 \t 16.612069 \t 8.139137 \t8.472932 \t 0.000000 \t 776.769432 s\n","844 \t 17.228724 \t 8.232529 \t8.996196 \t 0.000000 \t 777.474819 s\n","845 \t 17.637531 \t 8.973448 \t8.664084 \t 0.000000 \t 778.249293 s\n","846 \t 16.790667 \t 8.421943 \t8.368724 \t 0.000000 \t 779.044225 s\n","847 \t 17.891145 \t 8.938996 \t8.952149 \t 0.000000 \t 779.799503 s\n","848 \t 18.047812 \t 8.825544 \t9.222268 \t 0.000000 \t 780.556220 s\n","849 \t 17.453567 \t 8.812804 \t8.640763 \t 0.000000 \t 781.221133 s\n","850 \t 17.368686 \t 9.288725 \t8.079960 \t 0.000000 \t 782.210384 s\n","851 \t 17.487930 \t 8.964626 \t8.523304 \t 0.000000 \t 783.506023 s\n","852 \t 17.043602 \t 8.983646 \t8.059955 \t 0.000000 \t 784.568898 s\n","853 \t 17.630447 \t 8.906246 \t8.724201 \t 0.000000 \t 785.623104 s\n","854 \t 17.121412 \t 8.644010 \t8.477402 \t 0.000000 \t 786.544058 s\n","855 \t 17.425887 \t 8.764109 \t8.661778 \t 0.000000 \t 787.211545 s\n","856 \t 17.251784 \t 8.775432 \t8.476352 \t 0.000000 \t 787.865717 s\n","857 \t 18.421604 \t 9.271755 \t9.149848 \t 0.000000 \t 788.529789 s\n","858 \t 16.693172 \t 7.811641 \t8.881531 \t 0.000000 \t 789.319985 s\n","859 \t 16.875914 \t 8.394127 \t8.481787 \t 0.000000 \t 789.973781 s\n","860 \t 17.201277 \t 9.058699 \t8.142578 \t 0.000000 \t 790.676238 s\n","861 \t 17.110997 \t 8.797805 \t8.313192 \t 0.000000 \t 791.453919 s\n","862 \t 16.667725 \t 8.611849 \t8.055875 \t 0.000000 \t 792.231773 s\n","863 \t 17.121992 \t 8.694747 \t8.427245 \t 0.000000 \t 792.986181 s\n","864 \t 16.824682 \t 8.646096 \t8.178586 \t 0.000000 \t 793.746602 s\n","865 \t 18.000339 \t 9.076619 \t8.923720 \t 0.000000 \t 794.413556 s\n","866 \t 16.738847 \t 8.107792 \t8.631055 \t 0.000000 \t 795.066076 s\n","867 \t 17.308722 \t 8.569589 \t8.739133 \t 0.000000 \t 795.730561 s\n","868 \t 17.282138 \t 8.525838 \t8.756299 \t 0.000000 \t 796.540203 s\n","869 \t 18.172974 \t 9.317763 \t8.855211 \t 0.000000 \t 797.186995 s\n","870 \t 17.154434 \t 8.927094 \t8.227340 \t 0.000000 \t 797.849677 s\n","871 \t 17.397081 \t 8.414147 \t8.982934 \t 0.000000 \t 798.502819 s\n","872 \t 17.683352 \t 8.931206 \t8.752147 \t 0.000000 \t 799.141697 s\n","873 \t 17.609094 \t 8.472233 \t9.136861 \t 0.000000 \t 799.792046 s\n","874 \t 16.538980 \t 8.235000 \t8.303980 \t 0.000000 \t 800.451793 s\n","875 \t 17.252121 \t 8.811075 \t8.441046 \t 0.000000 \t 801.103563 s\n","876 \t 16.695417 \t 7.953198 \t8.742220 \t 0.000000 \t 801.773475 s\n","877 \t 17.036787 \t 8.269966 \t8.766821 \t 0.000000 \t 802.429476 s\n","878 \t 17.513603 \t 8.839675 \t8.673927 \t 0.000000 \t 803.213089 s\n","879 \t 16.839177 \t 8.851226 \t7.987951 \t 0.000000 \t 803.934382 s\n","880 \t 16.806579 \t 8.622301 \t8.184278 \t 0.000000 \t 804.701002 s\n","881 \t 16.803830 \t 8.160026 \t8.643805 \t 0.000000 \t 805.446041 s\n","882 \t 16.778287 \t 8.466393 \t8.311894 \t 0.000000 \t 806.241524 s\n","883 \t 17.666626 \t 8.713642 \t8.952984 \t 0.000000 \t 806.917562 s\n","884 \t 17.311948 \t 8.293355 \t9.018593 \t 0.000000 \t 807.565832 s\n","885 \t 17.070181 \t 8.370729 \t8.699452 \t 0.000000 \t 808.233783 s\n","886 \t 17.803776 \t 8.868474 \t8.935302 \t 0.000000 \t 808.881638 s\n","887 \t 17.554001 \t 8.902053 \t8.651947 \t 0.000000 \t 809.528849 s\n","888 \t 17.334873 \t 8.342101 \t8.992772 \t 0.000000 \t 810.188105 s\n","889 \t 16.731987 \t 9.057352 \t7.674634 \t 0.000000 \t 810.981500 s\n","890 \t 17.680121 \t 9.117322 \t8.562799 \t 0.000000 \t 811.645303 s\n","891 \t 17.068985 \t 8.938704 \t8.130281 \t 0.000000 \t 812.296349 s\n","892 \t 16.969622 \t 8.757621 \t8.212001 \t 0.000000 \t 812.967424 s\n","893 \t 17.005709 \t 8.548013 \t8.457696 \t 0.000000 \t 813.630692 s\n","894 \t 17.551637 \t 8.952478 \t8.599158 \t 0.000000 \t 814.295806 s\n","895 \t 18.450621 \t 8.743164 \t9.707456 \t 0.000000 \t 814.951973 s\n","896 \t 17.104491 \t 8.854345 \t8.250146 \t 0.000000 \t 815.608633 s\n","897 \t 16.678638 \t 8.424524 \t8.254114 \t 0.000000 \t 816.263854 s\n","898 \t 16.716769 \t 8.684908 \t8.031861 \t 0.000000 \t 817.035001 s\n","899 \t 16.909091 \t 8.571302 \t8.337789 \t 0.000000 \t 817.779970 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3743\n","Hit@10: 0.4538\n","Hit@3: 0.3654\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2465\n","Hit@10: 0.3815\n","Hit@3: 0.2439\n","Hit@1: 0.1742\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3011\n","Hit@10: 0.4923\n","Hit@3: 0.3231\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2551\n","Hit@10: 0.5135\n","Hit@3: 0.2815\n","Hit@1: 0.1351\n","900 \t 16.504772 \t 8.364305 \t8.140467 \t 0.000000 \t 855.565893 s\n","901 \t 17.259410 \t 8.525999 \t8.733411 \t 0.000000 \t 856.288741 s\n","902 \t 17.258373 \t 8.548185 \t8.710188 \t 0.000000 \t 857.060434 s\n","903 \t 16.882820 \t 8.762963 \t8.119857 \t 0.000000 \t 857.812723 s\n","904 \t 17.849572 \t 8.539182 \t9.310390 \t 0.000000 \t 858.571193 s\n","905 \t 17.743702 \t 9.033090 \t8.710612 \t 0.000000 \t 859.320621 s\n","906 \t 17.817890 \t 8.838552 \t8.979339 \t 0.000000 \t 859.972600 s\n","907 \t 17.895058 \t 9.028893 \t8.866165 \t 0.000000 \t 860.616041 s\n","908 \t 17.094087 \t 8.462449 \t8.631638 \t 0.000000 \t 861.271218 s\n","909 \t 17.401873 \t 9.030191 \t8.371682 \t 0.000000 \t 861.925406 s\n","910 \t 16.184556 \t 7.958320 \t8.226236 \t 0.000000 \t 862.719499 s\n","911 \t 18.397406 \t 9.078493 \t9.318913 \t 0.000000 \t 863.404415 s\n","912 \t 17.757298 \t 8.615669 \t9.141628 \t 0.000000 \t 864.134524 s\n","913 \t 17.103220 \t 8.726031 \t8.377188 \t 0.000000 \t 864.820220 s\n","914 \t 16.470015 \t 8.208836 \t8.261179 \t 0.000000 \t 865.559664 s\n","915 \t 16.982926 \t 8.953973 \t8.028953 \t 0.000000 \t 866.273567 s\n","916 \t 16.580435 \t 8.497017 \t8.083418 \t 0.000000 \t 866.995509 s\n","917 \t 17.200758 \t 8.556419 \t8.644338 \t 0.000000 \t 867.699219 s\n","918 \t 17.157869 \t 8.362403 \t8.795466 \t 0.000000 \t 868.449393 s\n","919 \t 17.951200 \t 9.703291 \t8.247908 \t 0.000000 \t 869.278674 s\n","920 \t 17.526093 \t 8.829275 \t8.696818 \t 0.000000 \t 870.130895 s\n","921 \t 15.648563 \t 7.649220 \t7.999343 \t 0.000000 \t 871.179096 s\n","922 \t 16.559748 \t 8.335714 \t8.224033 \t 0.000000 \t 872.078395 s\n","923 \t 17.262369 \t 8.595526 \t8.666842 \t 0.000000 \t 872.773180 s\n","924 \t 16.439522 \t 8.119028 \t8.320495 \t 0.000000 \t 873.496641 s\n","925 \t 17.003356 \t 8.453556 \t8.549800 \t 0.000000 \t 874.245289 s\n","926 \t 17.399237 \t 8.795397 \t8.603840 \t 0.000000 \t 874.992711 s\n","927 \t 16.476414 \t 8.162874 \t8.313540 \t 0.000000 \t 875.740146 s\n","928 \t 17.937006 \t 8.915475 \t9.021532 \t 0.000000 \t 876.434472 s\n","929 \t 18.112019 \t 9.558571 \t8.553448 \t 0.000000 \t 877.172445 s\n","930 \t 17.047853 \t 8.972074 \t8.075779 \t 0.000000 \t 877.887874 s\n","931 \t 16.992262 \t 8.833001 \t8.159261 \t 0.000000 \t 878.568131 s\n","932 \t 18.236102 \t 9.251419 \t8.984684 \t 0.000000 \t 879.496548 s\n","933 \t 18.777262 \t 9.502361 \t9.274901 \t 0.000000 \t 880.202827 s\n","934 \t 16.919772 \t 8.261441 \t8.658331 \t 0.000000 \t 880.900340 s\n","935 \t 18.134651 \t 9.258777 \t8.875875 \t 0.000000 \t 881.559072 s\n","936 \t 17.559912 \t 9.282747 \t8.277164 \t 0.000000 \t 882.275601 s\n","937 \t 17.224101 \t 9.154805 \t8.069296 \t 0.000000 \t 883.065872 s\n","938 \t 16.852720 \t 8.343380 \t8.509339 \t 0.000000 \t 883.837472 s\n","939 \t 17.972762 \t 8.985999 \t8.986764 \t 0.000000 \t 884.608509 s\n","940 \t 17.267290 \t 8.535830 \t8.731460 \t 0.000000 \t 885.365730 s\n","941 \t 17.001345 \t 8.847857 \t8.153487 \t 0.000000 \t 886.019398 s\n","942 \t 16.994923 \t 8.833048 \t8.161874 \t 0.000000 \t 886.691664 s\n","943 \t 17.616164 \t 8.883334 \t8.732830 \t 0.000000 \t 887.354754 s\n","944 \t 18.661139 \t 9.595013 \t9.066126 \t 0.000000 \t 888.157240 s\n","945 \t 16.564568 \t 8.299057 \t8.265510 \t 0.000000 \t 888.828022 s\n","946 \t 17.526648 \t 8.755814 \t8.770834 \t 0.000000 \t 889.477510 s\n","947 \t 16.742590 \t 8.669545 \t8.073044 \t 0.000000 \t 890.120783 s\n","948 \t 17.164598 \t 8.458355 \t8.706242 \t 0.000000 \t 890.794486 s\n","949 \t 16.544215 \t 7.977718 \t8.566497 \t 0.000000 \t 891.458589 s\n","950 \t 17.352680 \t 8.697461 \t8.655220 \t 0.000000 \t 892.108537 s\n","951 \t 17.375257 \t 8.879488 \t8.495768 \t 0.000000 \t 892.777780 s\n","952 \t 17.596533 \t 8.962268 \t8.634265 \t 0.000000 \t 893.447787 s\n","953 \t 17.090797 \t 8.557120 \t8.533677 \t 0.000000 \t 894.096552 s\n","954 \t 16.726105 \t 8.387027 \t8.339077 \t 0.000000 \t 894.760239 s\n","955 \t 16.955114 \t 8.639008 \t8.316106 \t 0.000000 \t 895.463434 s\n","956 \t 16.411268 \t 8.074036 \t8.337232 \t 0.000000 \t 896.219612 s\n","957 \t 17.204496 \t 8.801910 \t8.402586 \t 0.000000 \t 897.144096 s\n","958 \t 17.633735 \t 8.757308 \t8.876427 \t 0.000000 \t 897.949036 s\n","959 \t 17.459041 \t 8.796411 \t8.662630 \t 0.000000 \t 898.611758 s\n","960 \t 16.787814 \t 8.413490 \t8.374324 \t 0.000000 \t 899.287940 s\n","961 \t 17.049125 \t 8.879284 \t8.169841 \t 0.000000 \t 899.944152 s\n","962 \t 16.694841 \t 8.235521 \t8.459321 \t 0.000000 \t 900.588272 s\n","963 \t 17.262418 \t 9.202686 \t8.059731 \t 0.000000 \t 901.261239 s\n","964 \t 17.377286 \t 8.802288 \t8.574998 \t 0.000000 \t 901.915352 s\n","965 \t 16.761945 \t 8.576148 \t8.185797 \t 0.000000 \t 902.700310 s\n","966 \t 17.820460 \t 9.150979 \t8.669481 \t 0.000000 \t 903.384273 s\n","967 \t 17.694625 \t 8.762415 \t8.932209 \t 0.000000 \t 904.037126 s\n","968 \t 16.798151 \t 8.245550 \t8.552602 \t 0.000000 \t 904.680240 s\n","969 \t 17.131956 \t 8.676150 \t8.455806 \t 0.000000 \t 905.344919 s\n","970 \t 17.713394 \t 9.296923 \t8.416471 \t 0.000000 \t 906.006069 s\n","971 \t 16.622739 \t 8.419312 \t8.203426 \t 0.000000 \t 906.660297 s\n","972 \t 17.214745 \t 8.626389 \t8.588356 \t 0.000000 \t 907.321340 s\n","973 \t 16.625687 \t 8.124853 \t8.500833 \t 0.000000 \t 907.998111 s\n","974 \t 17.176929 \t 8.862252 \t8.314677 \t 0.000000 \t 909.022465 s\n","975 \t 16.923552 \t 8.900091 \t8.023460 \t 0.000000 \t 909.802605 s\n","976 \t 17.704872 \t 8.191770 \t9.513101 \t 0.000000 \t 910.560801 s\n","977 \t 16.682518 \t 8.192831 \t8.489688 \t 0.000000 \t 911.318065 s\n","978 \t 17.273177 \t 8.854911 \t8.418266 \t 0.000000 \t 911.981101 s\n","979 \t 18.240638 \t 8.699577 \t9.541061 \t 0.000000 \t 912.629683 s\n","980 \t 18.165019 \t 9.193651 \t8.971367 \t 0.000000 \t 913.297618 s\n","981 \t 16.934927 \t 8.651373 \t8.283554 \t 0.000000 \t 913.962536 s\n","982 \t 17.183355 \t 8.656113 \t8.527242 \t 0.000000 \t 914.629262 s\n","983 \t 18.460371 \t 9.628500 \t8.831871 \t 0.000000 \t 915.305850 s\n","984 \t 16.415925 \t 8.383812 \t8.032113 \t 0.000000 \t 916.103104 s\n","985 \t 16.701475 \t 8.643231 \t8.058244 \t 0.000000 \t 916.765457 s\n","986 \t 16.532550 \t 8.851200 \t7.681350 \t 0.000000 \t 917.430669 s\n","987 \t 17.456886 \t 8.873003 \t8.583884 \t 0.000000 \t 918.086931 s\n","988 \t 18.172954 \t 8.912179 \t9.260775 \t 0.000000 \t 918.732619 s\n","989 \t 16.449588 \t 8.438334 \t8.011254 \t 0.000000 \t 919.408833 s\n","990 \t 17.655703 \t 8.863599 \t8.792103 \t 0.000000 \t 920.086389 s\n","991 \t 17.869055 \t 9.020209 \t8.848845 \t 0.000000 \t 920.735949 s\n","992 \t 16.164250 \t 8.215538 \t7.948713 \t 0.000000 \t 921.444813 s\n","993 \t 17.611588 \t 8.739226 \t8.872361 \t 0.000000 \t 922.228062 s\n","994 \t 17.427523 \t 8.626246 \t8.801277 \t 0.000000 \t 923.142936 s\n","995 \t 18.617121 \t 9.296224 \t9.320897 \t 0.000000 \t 923.936313 s\n","996 \t 17.655974 \t 9.128267 \t8.527708 \t 0.000000 \t 924.602404 s\n","997 \t 17.700265 \t 8.980134 \t8.720131 \t 0.000000 \t 925.286975 s\n","998 \t 16.977158 \t 8.161844 \t8.815312 \t 0.000000 \t 925.952621 s\n","999 \t 17.505945 \t 8.970236 \t8.535710 \t 0.000000 \t 926.601292 s\n","1000 \t 17.037209 \t 8.858508 \t8.178702 \t 0.000000 \t 927.269383 s\n","1001 \t 18.194202 \t 9.022398 \t9.171805 \t 0.000000 \t 927.920923 s\n","1002 \t 17.225645 \t 8.492631 \t8.733015 \t 0.000000 \t 928.578076 s\n","1003 \t 17.448647 \t 8.579937 \t8.868710 \t 0.000000 \t 929.248631 s\n","1004 \t 17.296152 \t 8.774778 \t8.521374 \t 0.000000 \t 930.039600 s\n","1005 \t 16.918183 \t 8.596622 \t8.321561 \t 0.000000 \t 930.685992 s\n","1006 \t 16.969633 \t 8.845957 \t8.123675 \t 0.000000 \t 931.347272 s\n","1007 \t 15.975415 \t 8.175112 \t7.800304 \t 0.000000 \t 932.011304 s\n","1008 \t 17.313630 \t 8.315101 \t8.998529 \t 0.000000 \t 932.663331 s\n","1009 \t 17.221338 \t 8.532881 \t8.688458 \t 0.000000 \t 933.319129 s\n","1010 \t 17.590445 \t 9.041911 \t8.548534 \t 0.000000 \t 934.009292 s\n","1011 \t 17.062057 \t 8.733066 \t8.328991 \t 0.000000 \t 934.767760 s\n","1012 \t 16.744923 \t 8.219460 \t8.525463 \t 0.000000 \t 935.526958 s\n","1013 \t 17.771165 \t 8.927102 \t8.844063 \t 0.000000 \t 936.292552 s\n","1014 \t 16.991979 \t 8.778417 \t8.213562 \t 0.000000 \t 937.077543 s\n","1015 \t 17.645735 \t 8.782518 \t8.863217 \t 0.000000 \t 937.886337 s\n","1016 \t 17.317212 \t 8.888062 \t8.429150 \t 0.000000 \t 938.539032 s\n","1017 \t 16.869898 \t 8.679807 \t8.190091 \t 0.000000 \t 939.203390 s\n","1018 \t 17.502913 \t 9.315292 \t8.187621 \t 0.000000 \t 939.872029 s\n","1019 \t 17.080629 \t 8.452395 \t8.628234 \t 0.000000 \t 940.533716 s\n","1020 \t 17.563997 \t 8.887356 \t8.676641 \t 0.000000 \t 941.188507 s\n","1021 \t 17.558520 \t 9.190241 \t8.368279 \t 0.000000 \t 941.858153 s\n","1022 \t 17.181090 \t 8.472910 \t8.708181 \t 0.000000 \t 942.521489 s\n","1023 \t 16.641889 \t 8.182426 \t8.459463 \t 0.000000 \t 943.169867 s\n","1024 \t 18.104691 \t 8.848752 \t9.255938 \t 0.000000 \t 943.817525 s\n","1025 \t 16.501894 \t 8.480610 \t8.021283 \t 0.000000 \t 944.478990 s\n","1026 \t 17.279678 \t 8.616155 \t8.663524 \t 0.000000 \t 945.143168 s\n","1027 \t 17.075223 \t 8.229138 \t8.846085 \t 0.000000 \t 945.963621 s\n","1028 \t 17.419875 \t 8.706958 \t8.712917 \t 0.000000 \t 946.621464 s\n","1029 \t 16.791401 \t 8.470041 \t8.321359 \t 0.000000 \t 947.340001 s\n","1030 \t 16.782162 \t 8.181974 \t8.600188 \t 0.000000 \t 948.115302 s\n","1031 \t 17.438651 \t 8.198588 \t9.240062 \t 0.000000 \t 948.891059 s\n","1032 \t 16.765646 \t 8.755857 \t8.009789 \t 0.000000 \t 949.660114 s\n","1033 \t 17.599965 \t 9.007326 \t8.592640 \t 0.000000 \t 950.429946 s\n","1034 \t 17.155180 \t 8.472617 \t8.682562 \t 0.000000 \t 951.086678 s\n","1035 \t 17.505650 \t 9.051563 \t8.454087 \t 0.000000 \t 951.750315 s\n","1036 \t 16.791192 \t 8.623474 \t8.167719 \t 0.000000 \t 952.412187 s\n","1037 \t 17.619548 \t 8.818478 \t8.801070 \t 0.000000 \t 953.068627 s\n","1038 \t 17.169750 \t 8.527470 \t8.642280 \t 0.000000 \t 953.728118 s\n","1039 \t 17.076367 \t 8.128642 \t8.947726 \t 0.000000 \t 954.532305 s\n","1040 \t 16.745596 \t 8.637923 \t8.107673 \t 0.000000 \t 955.186743 s\n","1041 \t 16.826384 \t 8.287351 \t8.539032 \t 0.000000 \t 955.837947 s\n","1042 \t 17.731456 \t 9.072159 \t8.659297 \t 0.000000 \t 956.505254 s\n","1043 \t 17.451315 \t 8.923659 \t8.527657 \t 0.000000 \t 957.149608 s\n","1044 \t 18.246275 \t 9.211131 \t9.035144 \t 0.000000 \t 957.796854 s\n","1045 \t 16.314183 \t 8.393182 \t7.921002 \t 0.000000 \t 958.454437 s\n","1046 \t 16.513767 \t 8.354983 \t8.158785 \t 0.000000 \t 959.117241 s\n","1047 \t 17.255395 \t 8.787559 \t8.467837 \t 0.000000 \t 959.782056 s\n","1048 \t 18.574283 \t 9.658183 \t8.916100 \t 0.000000 \t 960.487808 s\n","1049 \t 17.091794 \t 8.632288 \t8.459506 \t 0.000000 \t 961.266329 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3762\n","Hit@10: 0.4615\n","Hit@3: 0.3846\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2462\n","Hit@10: 0.3902\n","Hit@3: 0.2526\n","Hit@1: 0.1725\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3001\n","Hit@10: 0.5000\n","Hit@3: 0.3077\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2553\n","Hit@10: 0.5158\n","Hit@3: 0.2770\n","Hit@1: 0.1351\n"]}]},{"cell_type":"markdown","source":["# Test.py"],"metadata":{"id":"YXPk_zZ1JQHo"}},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/code/VTHNKG-OA/checkpoint/Reproduce/VTHNKG-OA_seed0_Weighted_512dim/lr_0.0004_dim_512__1050.ckpt\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-CQI_Retry\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","def load_id_mapping(file_path):\n","    id2name = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip() == \"\" or line.startswith(\"#\"):  # 주석 또는 공백 무시\n","                continue\n","            parts = line.strip().split('\\t')\n","            if len(parts) != 2:\n","                continue\n","            name, idx = parts\n","            id2name[int(idx)] = name\n","    return id2name\n","\n","id2ent = load_id_mapping(\"entity2id.txt\")\n","id2rel = load_id_mapping(\"relation2id.txt\")\n","\n","def convert_triplet_ids_to_names(triplet, id2ent, id2rel, num_ent, num_rel):\n","    triplet_named = []\n","    for idx, val in enumerate(triplet):\n","        if idx % 2 == 0:  # entity or numeric value\n","            if val < num_ent:\n","                triplet_named.append(id2ent.get(val, f\"[ENT:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[NUM:{val - num_ent}]\")\n","        else:  # relation\n","            if val < num_rel:\n","                triplet_named.append(id2rel.get(val, f\"[REL:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[MASK_REL]\")\n","    return triplet_named\n","\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    entity_pred_log = []\n","    relation_pred_log = []\n","    numeric_pred_log = []\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    # sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    # nvp_tri_se += sq_error\n","                    # nvp_tri_se_num += 1\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","\n","                else: # qualifier\n","                  pred = score_num[0, 2, tri[ent_idx*2] - KG.num_ent]\n","                  gt = tri_num[ent_idx - 1]\n","                  sq_error = (pred - gt) ** 2\n","                  named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                  numeric_pred_log.append({\n","                      \"triplet_id\": str(tri),\n","                      \"triplet_named\": \":\".join(named_triplet),\n","                      \"position\": ent_idx,\n","                      \"type\": \"qualifier\",\n","                      \"gt\": float(gt),\n","                      \"pred\": float(pred),\n","                      \"se\": float(sq_error)\n","                  })\n","                    # sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                    topk = np.argsort(-score_ent[0,1+2*ent_idx])[:5]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    try:\n","                      topk = np.argsort(-score_ent[0,2])[:5]\n","                    except:\n","                      topk = np.argsort(-score_ent[0,2])[:]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","                topk = np.argsort(-score_rel[0,2])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"relation\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                topk = np.argsort(-score_rel[0,1])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"qualifier\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","pd.DataFrame(entity_pred_log).to_csv(\"entity_predictions.csv\", index=False)\n","pd.DataFrame(relation_pred_log).to_csv(\"relation_predictions.csv\", index=False)\n","pd.DataFrame(numeric_pred_log).to_csv(\"numeric_predictions.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"N5Xppmy1JPrv","executionInfo":{"status":"error","timestamp":1749782780635,"user_tz":-540,"elapsed":1022,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"3660a4cc-f497-4cbc-e26c-9f02ea608eb6"},"execution_count":11,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-1318446068>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtriplet_named\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mKG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVTHNKG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vis_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_img_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mKG_DataLoader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-2996243231>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, max_vis_len, test)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_vis_len_ent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_vis_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_vis_len_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_vis_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_vis_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_txt_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-2996243231>\u001b[0m in \u001b[0;36mgather_vis_feature\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# self.logger.info(\"Found sorted entity visual features!\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment2vis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/code/VTKG-I/visual_features_ent.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;31m# self.logger.info(\"Entity visual features are not sorted! sorting...\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1463\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m                         \u001b[0;34mf\"Only persistent_load of storage is allowed, but got {pid[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                     )\n\u001b[0;32m--> 512\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLONG_BINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   1929\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1886\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m             storage = (\n\u001b[0;32m-> 1888\u001b[0;31m                 \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1889\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_untyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTHNKG-OA/checkpoint/Reproduce/VTHNKG-OA_seed42_Weighted/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                else: # qualifier\n","                    sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChVIC_5BHELi","executionInfo":{"status":"ok","timestamp":1748536543461,"user_tz":-540,"elapsed":34867,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"269735ce-6b89-4045-feca-8ec41c67f0b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 132/132 [00:30<00:00,  4.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4764\n","Hit@10: 0.6326\n","Hit@3: 0.5303\n","Hit@1: 0.3826\n","Link Prediction on Validation Set (All)\n","MRR: 0.4804\n","Hit@10: 0.6917\n","Hit@3: 0.5383\n","Hit@1: 0.3697\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3079\n","Hit@10: 0.4924\n","Hit@3: 0.3485\n","Hit@1: 0.2197\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4564\n","Hit@10: 0.6593\n","Hit@3: 0.5187\n","Hit@1: 0.3516\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}