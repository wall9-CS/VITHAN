{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyNByyMQZaVmg/OOhr6ITT83"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746718680317,"user_tz":-540,"elapsed":19883,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"c2be06a8-bac1-4f4e-8528-300536cceda4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1746718684866,"user_tz":-540,"elapsed":3685,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1746718684875,"user_tz":-540,"elapsed":5,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask.cuda(), false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask.cuda(), false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0:\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","            nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","            nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","            self.proj_ent_vis.bias.data.zero_()\n","            self.proj_rel_vis.bias.data.zero_()\n","        else:\n","            # vis feature가 없는 경우: 항상 0을 출력하는 Linear layer\n","            self.proj_ent_vis = nn.Linear(1, dim_str)\n","            self.proj_rel_vis = nn.Linear(1, dim_str)\n","            with torch.no_grad():\n","                self.proj_ent_vis.weight.zero_()\n","                self.proj_ent_vis.bias.zero_()\n","                self.proj_rel_vis.weight.zero_()\n","                self.proj_rel_vis.bias.zero_()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)\n"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1746719840368,"user_tz":-540,"elapsed":14,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1746719627395,"user_tz":-540,"elapsed":49,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-NT/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","        self.filter_dict = self.construct_filter_dict()\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","        if self.max_vis_len_ent * self.vis_feat_size > 100000:\n","            raise RuntimeError(\"Too large visual feature size. Consider downsampling or truncating.\")\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        device = torch.device(\"cuda\")\n","\n","        # 1. 시각 feature 파일 로드\n","        if os.path.isfile(self.dir + 'visual_features_ent_sorted.pt'):\n","            self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_ent.pt'):\n","            self.ent2vis = self.sort_vis_features(item='entity')\n","        else:\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(self.dir + 'visual_features_rel_sorted.pt'):\n","            self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_rel.pt'):\n","            self.rel2vis = self.sort_vis_features(item='relation')\n","        else:\n","            self.rel2vis = {}\n","\n","        # 시각 feature가 존재한다면 dim을 얻기\n","        if self.ent2vis:\n","            first_ent = next(iter(self.ent2vis.values()))\n","            if isinstance(first_ent, (list, torch.Tensor)):\n","                self.vis_feat_size = first_ent[0].shape[-1] if isinstance(first_ent[0], torch.Tensor) else len(first_ent[0])\n","            else:\n","                self.vis_feat_size = 0\n","        elif self.rel2vis:\n","            first_rel = next(iter(self.rel2vis.values()))\n","            self.vis_feat_size = first_rel[0].shape[-1] // 3 if isinstance(first_rel[0], torch.Tensor) else len(first_rel[0]) // 3\n","        else:\n","            self.vis_feat_size = 0\n","\n","        # numeric entity만 있는 경우 fallback용\n","        if self.vis_feat_size == 0:\n","            self.vis_feat_size = 1\n","            self.ent_vis_matrix = torch.zeros((self.num_ent, 1, self.vis_feat_size))\n","            self.rel_vis_matrix = torch.zeros((self.num_rel, 1, self.vis_feat_size * 3))\n","            self.ent_vis_mask = torch.ones((self.num_ent, 1), dtype=torch.bool)\n","            self.rel_vis_mask = torch.ones((self.num_rel, 1), dtype=torch.bool)\n","            return\n","\n","        # matrix 초기화 (CPU에서 생성)\n","        self.max_vis_len_ent = max([len(v) for v in self.ent2vis.values()], default=0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True, dtype=torch.bool)\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size))\n","\n","        self.max_vis_len_rel = max([len(v) for v in self.rel2vis.values()], default=0)\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True, dtype=torch.bool)\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3 * self.vis_feat_size))\n","\n","        # 실제 visual feature 채우기\n","        for ent_name, vis_feats in self.ent2vis.items():\n","            if ent_name not in self.ent2id:\n","                continue\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(vis_feats)\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = vis_feats[:self.max_vis_len_ent]\n","\n","        for rel_name, vis_feats in self.rel2vis.items():\n","            if rel_name not in self.rel2id:\n","                continue\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(vis_feats)\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = vis_feats[:self.max_vis_len_rel]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-NT/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1746718698920,"user_tz":-540,"elapsed":414,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"1b6ee9bc-ccd0-4977-bf90-81c004e3565b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-NT\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHN\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","device = torch.device(\"cuda\")\n","\n","model = VTHN(\n","    num_ent = KG.num_ent,\n","    num_rel = KG.num_rel,\n","    ent_vis = KG.ent_vis_matrix.to(device),\n","    rel_vis = KG.rel_vis_matrix.to(device),\n","    ent_vis_mask = KG.ent_vis_mask.to(device),\n","    rel_vis_mask = KG.rel_vis_mask.to(device),\n","    ent_txt = KG.ent_txt_matrix.to(device),\n","    rel_txt = KG.rel_txt_matrix.to(device),\n","    dim_vis = KG.vis_feat_size,\n","    dim_txt = KG.txt_feat_size,\n","    dim_str = args.dim,\n","    num_head = args.num_head,\n","    dim_hid = args.hidden_dim,\n","    num_layer_enc_ent = args.num_layer_enc_ent,\n","    num_layer_enc_rel = args.num_layer_enc_rel,\n","    num_layer_prediction = args.num_layer_prediction,\n","    num_layer_context = args.num_layer_context,\n","    dropout = args.dropout,\n","    emb_dropout = args.emb_dropout,\n","    vis_dropout = args.vis_dropout,\n","    txt_dropout = args.txt_dropout,\n","    emb_as_proj = False\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746719535425,"user_tz":-540,"elapsed":820494,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"54b664bf-e814-4738-ddbb-69d0e634c323"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 53.428660 \t 11.293248 \t11.570591 \t 30.564820 \t 2.153371 s\n","1 \t 29.356013 \t 10.978270 \t11.051913 \t 7.325831 \t 2.837585 s\n","2 \t 24.399767 \t 10.824288 \t10.936637 \t 2.638841 \t 3.445451 s\n","3 \t 23.037764 \t 10.534465 \t10.685199 \t 1.818098 \t 4.136162 s\n","4 \t 21.460013 \t 10.236959 \t10.521168 \t 0.701886 \t 4.743441 s\n","5 \t 21.377315 \t 10.154375 \t10.543665 \t 0.679273 \t 5.261576 s\n","6 \t 20.992183 \t 10.188689 \t10.230542 \t 0.572951 \t 5.763429 s\n","7 \t 20.589815 \t 9.996105 \t10.167500 \t 0.426210 \t 6.294579 s\n","8 \t 20.662721 \t 9.988612 \t10.188813 \t 0.485295 \t 6.799882 s\n","9 \t 20.302685 \t 9.832566 \t10.129248 \t 0.340871 \t 7.319980 s\n","10 \t 20.344391 \t 9.922853 \t10.052317 \t 0.369223 \t 7.999028 s\n","11 \t 20.220962 \t 9.945228 \t10.024646 \t 0.251087 \t 8.513766 s\n","12 \t 20.322292 \t 9.878709 \t10.091211 \t 0.352372 \t 9.166395 s\n","13 \t 20.192531 \t 9.839448 \t9.973579 \t 0.379503 \t 9.816116 s\n","14 \t 20.311554 \t 9.992345 \t10.113805 \t 0.205403 \t 10.466455 s\n","15 \t 20.376981 \t 10.008190 \t10.113358 \t 0.255433 \t 10.984178 s\n","16 \t 20.374885 \t 9.930821 \t10.112008 \t 0.332055 \t 11.493525 s\n","17 \t 20.158821 \t 9.851840 \t10.074666 \t 0.232315 \t 12.003999 s\n","18 \t 20.496772 \t 9.929576 \t10.064786 \t 0.502408 \t 12.511060 s\n","19 \t 20.241507 \t 9.939321 \t9.976160 \t 0.326026 \t 13.012974 s\n","20 \t 20.090024 \t 9.963709 \t9.836535 \t 0.289780 \t 13.533553 s\n","21 \t 19.998759 \t 9.863210 \t9.925716 \t 0.209833 \t 14.039150 s\n","22 \t 19.977175 \t 9.982763 \t9.783869 \t 0.210543 \t 14.624381 s\n","23 \t 19.920712 \t 9.849562 \t9.743142 \t 0.328009 \t 15.451241 s\n","24 \t 20.024516 \t 9.889939 \t9.789522 \t 0.345054 \t 16.053133 s\n","25 \t 20.066039 \t 9.943985 \t9.870215 \t 0.251840 \t 16.732201 s\n","26 \t 19.912338 \t 9.879065 \t9.715627 \t 0.317646 \t 17.422634 s\n","27 \t 19.776971 \t 9.849424 \t9.679706 \t 0.247840 \t 17.920380 s\n","28 \t 19.650014 \t 9.897117 \t9.576481 \t 0.176416 \t 18.437633 s\n","29 \t 19.643531 \t 9.749444 \t9.759062 \t 0.135027 \t 18.951394 s\n","30 \t 19.421599 \t 9.740092 \t9.597657 \t 0.083850 \t 19.465681 s\n","31 \t 19.595550 \t 9.842535 \t9.622149 \t 0.130866 \t 19.965175 s\n","32 \t 19.535722 \t 9.778172 \t9.563885 \t 0.193666 \t 20.491359 s\n","33 \t 19.791011 \t 9.834617 \t9.620110 \t 0.336285 \t 20.993232 s\n","34 \t 19.762786 \t 9.896884 \t9.652454 \t 0.213448 \t 21.519489 s\n","35 \t 20.007964 \t 9.822386 \t9.797005 \t 0.388574 \t 22.036729 s\n","36 \t 19.593489 \t 9.737110 \t9.615358 \t 0.241022 \t 22.716834 s\n","37 \t 19.673719 \t 9.802685 \t9.606599 \t 0.264436 \t 23.216212 s\n","38 \t 19.468828 \t 9.719172 \t9.651698 \t 0.097959 \t 23.718332 s\n","39 \t 19.645798 \t 9.703661 \t9.751457 \t 0.190680 \t 24.472309 s\n","40 \t 19.657894 \t 9.851721 \t9.538422 \t 0.267751 \t 25.167842 s\n","41 \t 19.701745 \t 9.799543 \t9.517276 \t 0.384925 \t 25.915589 s\n","42 \t 19.640146 \t 9.772316 \t9.616862 \t 0.250967 \t 26.423633 s\n","43 \t 19.730734 \t 9.648993 \t9.667902 \t 0.413838 \t 26.936715 s\n","44 \t 19.868339 \t 9.909888 \t9.752733 \t 0.205718 \t 27.449166 s\n","45 \t 19.579802 \t 9.836804 \t9.607716 \t 0.135282 \t 28.094011 s\n","46 \t 19.670931 \t 9.828546 \t9.572733 \t 0.269653 \t 28.704259 s\n","47 \t 19.486238 \t 9.672736 \t9.584049 \t 0.229453 \t 29.329737 s\n","48 \t 19.621576 \t 9.780310 \t9.588694 \t 0.252573 \t 29.977046 s\n","49 \t 19.374371 \t 9.729716 \t9.538750 \t 0.105905 \t 30.794498 s\n","50 \t 19.641263 \t 9.794322 \t9.605170 \t 0.241771 \t 31.309396 s\n","51 \t 19.517743 \t 9.799411 \t9.616087 \t 0.102245 \t 31.826623 s\n","52 \t 19.443261 \t 9.778703 \t9.516259 \t 0.148299 \t 32.346855 s\n","53 \t 19.668330 \t 9.629871 \t9.855915 \t 0.182544 \t 32.850433 s\n","54 \t 19.711776 \t 9.799320 \t9.770246 \t 0.142209 \t 33.372888 s\n","55 \t 19.324481 \t 9.781599 \t9.502243 \t 0.040639 \t 33.878568 s\n","56 \t 19.337683 \t 9.668421 \t9.497167 \t 0.172095 \t 34.394407 s\n","57 \t 19.663044 \t 9.779335 \t9.628676 \t 0.255032 \t 34.895767 s\n","58 \t 19.454636 \t 9.689123 \t9.515058 \t 0.250456 \t 35.416984 s\n","59 \t 19.632838 \t 9.761515 \t9.652268 \t 0.219055 \t 35.915819 s\n","60 \t 19.530779 \t 9.763387 \t9.646244 \t 0.121148 \t 36.444753 s\n","61 \t 19.516561 \t 9.831604 \t9.476977 \t 0.207979 \t 37.131376 s\n","62 \t 19.459840 \t 9.694736 \t9.546082 \t 0.219022 \t 37.633071 s\n","63 \t 19.535739 \t 9.762714 \t9.648600 \t 0.124425 \t 38.148669 s\n","64 \t 19.832590 \t 9.785583 \t9.567884 \t 0.479123 \t 38.653221 s\n","65 \t 19.516070 \t 9.693604 \t9.619161 \t 0.203305 \t 39.164334 s\n","66 \t 19.634382 \t 9.745583 \t9.731081 \t 0.157718 \t 39.710063 s\n","67 \t 19.511824 \t 9.753153 \t9.606449 \t 0.152222 \t 40.231130 s\n","68 \t 19.529264 \t 9.800908 \t9.613758 \t 0.114598 \t 40.797206 s\n","69 \t 19.493090 \t 9.733562 \t9.645345 \t 0.114184 \t 41.416792 s\n","70 \t 19.386123 \t 9.593828 \t9.615842 \t 0.176452 \t 42.018194 s\n","71 \t 19.385883 \t 9.711466 \t9.530812 \t 0.143606 \t 42.654328 s\n","72 \t 19.253933 \t 9.664122 \t9.535334 \t 0.054477 \t 43.370316 s\n","73 \t 19.446927 \t 9.569705 \t9.622817 \t 0.254404 \t 43.900644 s\n","74 \t 19.395888 \t 9.729828 \t9.538264 \t 0.127797 \t 44.584304 s\n","75 \t 19.216435 \t 9.571790 \t9.504954 \t 0.139692 \t 45.086051 s\n","76 \t 19.390303 \t 9.648439 \t9.630713 \t 0.111151 \t 45.601119 s\n","77 \t 19.437251 \t 9.684798 \t9.637805 \t 0.114647 \t 46.103084 s\n","78 \t 19.384117 \t 9.713271 \t9.531041 \t 0.139806 \t 46.615275 s\n","79 \t 19.319997 \t 9.679957 \t9.477926 \t 0.162114 \t 47.122702 s\n","80 \t 19.518157 \t 9.734559 \t9.524120 \t 0.259479 \t 47.633877 s\n","81 \t 19.166334 \t 9.561765 \t9.539668 \t 0.064902 \t 48.140635 s\n","82 \t 19.332919 \t 9.657317 \t9.498516 \t 0.177087 \t 48.652349 s\n","83 \t 19.427354 \t 9.638638 \t9.572438 \t 0.216279 \t 49.156529 s\n","84 \t 19.313679 \t 9.719109 \t9.498744 \t 0.095827 \t 49.675205 s\n","85 \t 19.419604 \t 9.708510 \t9.507679 \t 0.203416 \t 50.183077 s\n","86 \t 19.261018 \t 9.596018 \t9.583375 \t 0.081625 \t 50.694144 s\n","87 \t 19.052852 \t 9.519381 \t9.421501 \t 0.111970 \t 51.364721 s\n","88 \t 19.344866 \t 9.665857 \t9.544865 \t 0.134144 \t 51.869339 s\n","89 \t 19.017375 \t 9.628020 \t9.305153 \t 0.084201 \t 52.387994 s\n","90 \t 19.273247 \t 9.626691 \t9.455481 \t 0.191074 \t 52.894420 s\n","91 \t 19.213928 \t 9.531473 \t9.504513 \t 0.177943 \t 53.406360 s\n","92 \t 19.065125 \t 9.668573 \t9.304155 \t 0.092397 \t 54.066440 s\n","93 \t 18.972825 \t 9.450920 \t9.452446 \t 0.069459 \t 54.667752 s\n","94 \t 19.256797 \t 9.597500 \t9.577651 \t 0.081645 \t 55.321845 s\n","95 \t 19.349727 \t 9.670891 \t9.482306 \t 0.196529 \t 55.967696 s\n","96 \t 19.072274 \t 9.512314 \t9.370745 \t 0.189215 \t 56.588584 s\n","97 \t 19.126162 \t 9.618628 \t9.367675 \t 0.139860 \t 57.096692 s\n","98 \t 19.264059 \t 9.621301 \t9.527138 \t 0.115619 \t 57.599101 s\n","99 \t 19.273161 \t 9.638887 \t9.506470 \t 0.127803 \t 58.101631 s\n","100 \t 19.049773 \t 9.560148 \t9.396912 \t 0.092714 \t 58.769248 s\n","101 \t 18.978096 \t 9.578502 \t9.328596 \t 0.070998 \t 59.280527 s\n","102 \t 19.020018 \t 9.399620 \t9.490659 \t 0.129739 \t 59.791615 s\n","103 \t 18.888812 \t 9.573972 \t9.235778 \t 0.079061 \t 60.306445 s\n","104 \t 18.984418 \t 9.503571 \t9.397243 \t 0.083604 \t 60.810529 s\n","105 \t 18.650311 \t 9.421424 \t9.147153 \t 0.081733 \t 61.324852 s\n","106 \t 18.741206 \t 9.428105 \t9.171225 \t 0.141877 \t 61.830994 s\n","107 \t 18.868531 \t 9.393563 \t9.393063 \t 0.081905 \t 62.343252 s\n","108 \t 18.772356 \t 9.424261 \t9.293790 \t 0.054305 \t 62.846773 s\n","109 \t 18.898303 \t 9.459750 \t9.383982 \t 0.054571 \t 63.362085 s\n","110 \t 19.058320 \t 9.429095 \t9.462846 \t 0.166379 \t 63.867611 s\n","111 \t 18.838319 \t 9.446851 \t9.311137 \t 0.080331 \t 64.382999 s\n","112 \t 18.730447 \t 9.477572 \t9.176701 \t 0.076175 \t 64.889147 s\n","113 \t 18.856919 \t 9.451083 \t9.350700 \t 0.055137 \t 65.575232 s\n","114 \t 18.716365 \t 9.474376 \t9.199133 \t 0.042856 \t 66.081923 s\n","115 \t 18.668497 \t 9.334464 \t9.231130 \t 0.102903 \t 66.685571 s\n","116 \t 18.709813 \t 9.455007 \t9.201119 \t 0.053687 \t 67.308288 s\n","117 \t 18.624303 \t 9.435083 \t9.154758 \t 0.034460 \t 67.920142 s\n","118 \t 18.802435 \t 9.423574 \t9.298787 \t 0.080074 \t 68.565848 s\n","119 \t 18.920467 \t 9.416697 \t9.441302 \t 0.062469 \t 69.276204 s\n","120 \t 18.502975 \t 9.346820 \t9.120595 \t 0.035561 \t 69.885298 s\n","121 \t 18.599371 \t 9.332551 \t9.211055 \t 0.055765 \t 70.418839 s\n","122 \t 18.627787 \t 9.377765 \t9.197132 \t 0.052890 \t 70.928374 s\n","123 \t 18.786268 \t 9.424693 \t9.327667 \t 0.033907 \t 71.442366 s\n","124 \t 18.690848 \t 9.339305 \t9.309143 \t 0.042400 \t 71.961086 s\n","125 \t 18.548412 \t 9.402686 \t9.118615 \t 0.027111 \t 72.491604 s\n","126 \t 18.357928 \t 9.178374 \t9.144086 \t 0.035469 \t 73.185721 s\n","127 \t 18.659648 \t 9.438715 \t9.183583 \t 0.037349 \t 73.706180 s\n","128 \t 18.842792 \t 9.323612 \t9.478782 \t 0.040397 \t 74.219842 s\n","129 \t 18.654666 \t 9.346026 \t9.278020 \t 0.030619 \t 74.740741 s\n","130 \t 18.643762 \t 9.260869 \t9.337056 \t 0.045836 \t 75.262644 s\n","131 \t 18.620610 \t 9.396509 \t9.186893 \t 0.037208 \t 75.783099 s\n","132 \t 18.563885 \t 9.232126 \t9.303866 \t 0.027892 \t 76.305972 s\n","133 \t 18.804949 \t 9.393157 \t9.380881 \t 0.030911 \t 76.835836 s\n","134 \t 18.841130 \t 9.599732 \t9.192559 \t 0.048838 \t 77.358700 s\n","135 \t 18.657970 \t 9.404838 \t9.199914 \t 0.053219 \t 77.884379 s\n","136 \t 18.501505 \t 9.287601 \t9.179840 \t 0.034064 \t 78.406406 s\n","137 \t 18.669927 \t 9.306203 \t9.322430 \t 0.041293 \t 78.932514 s\n","138 \t 18.465385 \t 9.248744 \t9.192327 \t 0.024315 \t 79.470212 s\n","139 \t 18.520449 \t 9.301897 \t9.195132 \t 0.023419 \t 80.340278 s\n","140 \t 18.645088 \t 9.452077 \t9.163468 \t 0.029542 \t 80.978299 s\n","141 \t 18.579167 \t 9.344872 \t9.199132 \t 0.035164 \t 81.607088 s\n","142 \t 18.467944 \t 9.247851 \t9.183136 \t 0.036957 \t 82.316823 s\n","143 \t 18.600158 \t 9.342269 \t9.215465 \t 0.042425 \t 82.922005 s\n","144 \t 18.598698 \t 9.463217 \t9.102282 \t 0.033200 \t 83.438596 s\n","145 \t 18.476903 \t 9.272292 \t9.170069 \t 0.034542 \t 83.949764 s\n","146 \t 18.459161 \t 9.221890 \t9.208000 \t 0.029270 \t 84.468513 s\n","147 \t 18.497983 \t 9.311074 \t9.148146 \t 0.038763 \t 84.978389 s\n","148 \t 18.355433 \t 9.255940 \t9.075834 \t 0.023659 \t 85.500454 s\n","149 \t 18.618067 \t 9.353506 \t9.226267 \t 0.038295 \t 86.011757 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/159 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 159/159 [00:27<00:00,  5.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3191\n","Hit@10: 0.4113\n","Hit@3: 0.3050\n","Hit@1: 0.2695\n","Link Prediction on Validation Set (All)\n","MRR: 0.2492\n","Hit@10: 0.4098\n","Hit@3: 0.2532\n","Hit@1: 0.1725\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4158\n","Hit@10: 0.6352\n","Hit@3: 0.4591\n","Hit@1: 0.3019\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3264\n","Hit@10: 0.5265\n","Hit@3: 0.3654\n","Hit@1: 0.2122\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.1476\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1476\n","150 \t 19.108603 \t 9.464749 \t9.581732 \t 0.062121 \t 115.040036 s\n","151 \t 19.557035 \t 9.570666 \t9.904008 \t 0.082362 \t 115.735334 s\n","152 \t 19.466551 \t 9.489481 \t9.760754 \t 0.216317 \t 116.253214 s\n","153 \t 19.863388 \t 9.675520 \t9.831287 \t 0.356580 \t 116.765696 s\n","154 \t 19.441869 \t 9.557272 \t9.740310 \t 0.144287 \t 117.285615 s\n","155 \t 19.012820 \t 9.408918 \t9.424466 \t 0.179436 \t 117.792558 s\n","156 \t 19.046652 \t 9.536758 \t9.276872 \t 0.233022 \t 118.310562 s\n","157 \t 19.173409 \t 9.549898 \t9.495100 \t 0.128412 \t 118.892822 s\n","158 \t 18.981946 \t 9.483636 \t9.367663 \t 0.130647 \t 119.509156 s\n","159 \t 18.811591 \t 9.347801 \t9.387334 \t 0.076457 \t 120.116572 s\n","160 \t 18.724300 \t 9.265770 \t9.334443 \t 0.124087 \t 120.765258 s\n","161 \t 18.809813 \t 9.422180 \t9.310688 \t 0.076945 \t 121.478419 s\n","162 \t 18.622438 \t 9.392561 \t9.152610 \t 0.077268 \t 121.996828 s\n","163 \t 18.673349 \t 9.289621 \t9.336903 \t 0.046826 \t 122.524694 s\n","164 \t 18.659983 \t 9.327187 \t9.259803 \t 0.072993 \t 123.353112 s\n","165 \t 18.575335 \t 9.179084 \t9.307958 \t 0.088293 \t 124.037714 s\n","166 \t 18.479130 \t 9.370043 \t9.065649 \t 0.043438 \t 124.665097 s\n","167 \t 18.374890 \t 9.307045 \t9.005891 \t 0.061955 \t 125.298438 s\n","168 \t 18.660340 \t 9.244236 \t9.185112 \t 0.230992 \t 125.832821 s\n","169 \t 18.466772 \t 9.311794 \t9.050134 \t 0.104843 \t 126.352177 s\n","170 \t 18.473153 \t 9.261939 \t9.062408 \t 0.148807 \t 126.871951 s\n","171 \t 18.309810 \t 9.188939 \t9.026230 \t 0.094641 \t 127.383252 s\n","172 \t 18.468390 \t 9.327816 \t9.059908 \t 0.080666 \t 127.898648 s\n","173 \t 18.239403 \t 9.184930 \t8.994002 \t 0.060470 \t 128.416149 s\n","174 \t 18.266995 \t 9.279897 \t8.948937 \t 0.038160 \t 128.934188 s\n","175 \t 18.033439 \t 9.133679 \t8.835839 \t 0.063921 \t 129.453273 s\n","176 \t 18.303678 \t 9.240253 \t9.003583 \t 0.059843 \t 129.983049 s\n","177 \t 18.225810 \t 9.256246 \t8.891456 \t 0.078109 \t 130.675937 s\n","178 \t 18.267284 \t 9.229774 \t8.967405 \t 0.070105 \t 131.207659 s\n","179 \t 17.855013 \t 9.124516 \t8.659239 \t 0.071257 \t 131.792202 s\n","180 \t 18.086907 \t 9.105562 \t8.915985 \t 0.065359 \t 132.416881 s\n","181 \t 18.323643 \t 9.254470 \t9.021505 \t 0.047668 \t 133.046602 s\n","182 \t 18.311651 \t 9.208394 \t9.060014 \t 0.043243 \t 133.673571 s\n","183 \t 18.170485 \t 9.164353 \t8.948070 \t 0.058063 \t 134.360999 s\n","184 \t 18.231753 \t 9.174326 \t9.018190 \t 0.039237 \t 134.920821 s\n","185 \t 18.001446 \t 9.137199 \t8.815427 \t 0.048820 \t 135.449297 s\n","186 \t 18.024183 \t 9.298803 \t8.699379 \t 0.026001 \t 135.975630 s\n","187 \t 17.874768 \t 9.023880 \t8.804869 \t 0.046020 \t 136.491891 s\n","188 \t 17.806022 \t 9.020653 \t8.700785 \t 0.084583 \t 137.004936 s\n","189 \t 18.048708 \t 9.248500 \t8.740618 \t 0.059590 \t 137.526886 s\n","190 \t 17.965569 \t 9.098119 \t8.720109 \t 0.147340 \t 138.211835 s\n","191 \t 18.013622 \t 9.120820 \t8.856002 \t 0.036800 \t 138.722857 s\n","192 \t 18.208070 \t 9.129634 \t8.802780 \t 0.275655 \t 139.242381 s\n","193 \t 18.005408 \t 9.057545 \t8.814798 \t 0.133066 \t 139.756482 s\n","194 \t 18.113358 \t 9.167925 \t8.857517 \t 0.087917 \t 140.280799 s\n","195 \t 17.929822 \t 9.090851 \t8.746648 \t 0.092323 \t 140.795379 s\n","196 \t 18.138664 \t 9.220736 \t8.805108 \t 0.112821 \t 141.321764 s\n","197 \t 18.015563 \t 9.159707 \t8.748113 \t 0.107743 \t 141.838930 s\n","198 \t 17.583828 \t 8.974860 \t8.536732 \t 0.072238 \t 142.375839 s\n","199 \t 17.717491 \t 9.045347 \t8.595116 \t 0.077028 \t 142.916354 s\n","200 \t 17.720119 \t 9.069823 \t8.591218 \t 0.059078 \t 143.436608 s\n","201 \t 17.825787 \t 8.994621 \t8.711626 \t 0.119539 \t 143.957132 s\n","202 \t 17.732746 \t 9.023001 \t8.667097 \t 0.042650 \t 144.484438 s\n","203 \t 17.724765 \t 8.940277 \t8.736484 \t 0.048004 \t 145.361758 s\n","204 \t 17.665648 \t 8.976485 \t8.638601 \t 0.050562 \t 145.982164 s\n","205 \t 17.474163 \t 8.830831 \t8.588420 \t 0.054913 \t 146.635395 s\n","206 \t 17.728120 \t 9.045584 \t8.594838 \t 0.087697 \t 147.319254 s\n","207 \t 17.712809 \t 9.015749 \t8.657396 \t 0.039664 \t 147.889653 s\n","208 \t 17.811449 \t 9.024652 \t8.753674 \t 0.033124 \t 148.404785 s\n","209 \t 17.497826 \t 8.980153 \t8.488200 \t 0.029474 \t 148.939878 s\n","210 \t 17.505648 \t 8.955221 \t8.515146 \t 0.035280 \t 149.454013 s\n","211 \t 17.619067 \t 8.850368 \t8.641148 \t 0.127552 \t 149.981910 s\n","212 \t 17.634750 \t 8.899874 \t8.654445 \t 0.080432 \t 150.502982 s\n","213 \t 17.722621 \t 8.999260 \t8.680785 \t 0.042575 \t 151.023165 s\n","214 \t 17.524676 \t 8.992302 \t8.439855 \t 0.092521 \t 151.558965 s\n","215 \t 17.582870 \t 9.024483 \t8.501178 \t 0.057208 \t 152.077039 s\n","216 \t 17.627707 \t 8.981274 \t8.597231 \t 0.049203 \t 152.783926 s\n","217 \t 17.495121 \t 8.892748 \t8.543744 \t 0.058628 \t 153.305450 s\n","218 \t 17.522835 \t 8.988461 \t8.495235 \t 0.039139 \t 153.824126 s\n","219 \t 17.532749 \t 8.992500 \t8.506194 \t 0.034055 \t 154.340976 s\n","220 \t 17.645286 \t 9.057735 \t8.534241 \t 0.053309 \t 154.861995 s\n","221 \t 17.430111 \t 8.858706 \t8.530679 \t 0.040725 \t 155.386626 s\n","222 \t 17.596251 \t 8.960430 \t8.597556 \t 0.038265 \t 155.912423 s\n","223 \t 17.627753 \t 8.935906 \t8.627606 \t 0.064240 \t 156.441581 s\n","224 \t 17.280687 \t 8.876205 \t8.362967 \t 0.041515 \t 156.962094 s\n","225 \t 17.309179 \t 8.844738 \t8.435745 \t 0.028697 \t 157.507251 s\n","226 \t 17.173682 \t 8.770424 \t8.358580 \t 0.044679 \t 158.158454 s\n","227 \t 17.386230 \t 8.902116 \t8.448418 \t 0.035695 \t 158.777602 s\n","228 \t 17.539630 \t 8.873395 \t8.550989 \t 0.115246 \t 159.414095 s\n","229 \t 17.620766 \t 8.969218 \t8.571679 \t 0.079869 \t 160.091795 s\n","230 \t 17.301307 \t 8.878616 \t8.377514 \t 0.045177 \t 160.868525 s\n","231 \t 17.357769 \t 8.882494 \t8.428076 \t 0.047200 \t 161.397380 s\n","232 \t 17.375888 \t 8.918672 \t8.425122 \t 0.032094 \t 161.916658 s\n","233 \t 17.235940 \t 8.903668 \t8.295023 \t 0.037248 \t 162.455081 s\n","234 \t 17.370328 \t 8.829078 \t8.465285 \t 0.075965 \t 162.984666 s\n","235 \t 17.282876 \t 8.865511 \t8.359872 \t 0.057494 \t 163.502374 s\n","236 \t 17.315510 \t 8.947474 \t8.296654 \t 0.071383 \t 164.029302 s\n","237 \t 17.428926 \t 8.909032 \t8.453195 \t 0.066700 \t 164.552935 s\n","238 \t 17.400214 \t 8.870654 \t8.477456 \t 0.052103 \t 165.085661 s\n","239 \t 17.105827 \t 8.820229 \t8.245487 \t 0.040112 \t 165.601323 s\n","240 \t 17.307434 \t 8.940524 \t8.301398 \t 0.065513 \t 166.126924 s\n","241 \t 17.220327 \t 8.714597 \t8.454817 \t 0.050914 \t 166.646725 s\n","242 \t 17.285430 \t 8.850061 \t8.366058 \t 0.069310 \t 167.178122 s\n","243 \t 17.264521 \t 8.896707 \t8.306792 \t 0.061022 \t 167.873895 s\n","244 \t 17.079720 \t 8.773679 \t8.267251 \t 0.038790 \t 168.398044 s\n","245 \t 17.300233 \t 8.940132 \t8.306738 \t 0.053363 \t 168.921138 s\n","246 \t 17.269762 \t 8.841300 \t8.373775 \t 0.054686 \t 169.448899 s\n","247 \t 17.056031 \t 8.785048 \t8.225203 \t 0.045781 \t 169.970375 s\n","248 \t 17.180265 \t 8.878007 \t8.256936 \t 0.045323 \t 170.517878 s\n","249 \t 17.148420 \t 8.779060 \t8.325172 \t 0.044188 \t 171.162671 s\n","250 \t 17.110385 \t 8.856579 \t8.212384 \t 0.041422 \t 171.798314 s\n","251 \t 16.867057 \t 8.703125 \t8.116444 \t 0.047490 \t 172.437675 s\n","252 \t 17.101037 \t 8.833856 \t8.226292 \t 0.040890 \t 173.138555 s\n","253 \t 17.163868 \t 8.778893 \t8.343884 \t 0.041092 \t 173.765301 s\n","254 \t 16.933271 \t 8.723036 \t8.164839 \t 0.045396 \t 174.285282 s\n","255 \t 16.972587 \t 8.814979 \t8.121979 \t 0.035628 \t 174.803159 s\n","256 \t 17.024536 \t 8.717773 \t8.268083 \t 0.038679 \t 175.499483 s\n","257 \t 17.095628 \t 8.748988 \t8.289961 \t 0.056679 \t 176.024827 s\n","258 \t 16.854825 \t 8.699286 \t8.128778 \t 0.026760 \t 176.556687 s\n","259 \t 16.951692 \t 8.720696 \t8.136948 \t 0.094047 \t 177.079463 s\n","260 \t 17.121390 \t 8.878911 \t8.195453 \t 0.047027 \t 177.606513 s\n","261 \t 17.140717 \t 8.804660 \t8.290235 \t 0.045823 \t 178.139007 s\n","262 \t 16.950434 \t 8.741925 \t8.178928 \t 0.029580 \t 178.666769 s\n","263 \t 16.764557 \t 8.615796 \t8.100064 \t 0.048697 \t 179.185962 s\n","264 \t 16.865431 \t 8.693691 \t8.139880 \t 0.031859 \t 179.715698 s\n","265 \t 17.053707 \t 8.768951 \t8.259218 \t 0.025538 \t 180.233990 s\n","266 \t 16.887062 \t 8.813239 \t8.029237 \t 0.044586 \t 180.769778 s\n","267 \t 16.742053 \t 8.629451 \t8.066742 \t 0.045860 \t 181.293005 s\n","268 \t 16.970860 \t 8.701759 \t8.251686 \t 0.017414 \t 181.826354 s\n","269 \t 16.811168 \t 8.733470 \t8.056739 \t 0.020958 \t 182.515687 s\n","270 \t 16.917531 \t 8.693632 \t8.198248 \t 0.025651 \t 183.038706 s\n","271 \t 17.010327 \t 8.688166 \t8.283662 \t 0.038499 \t 183.614062 s\n","272 \t 16.753321 \t 8.655682 \t8.057028 \t 0.040611 \t 184.274783 s\n","273 \t 16.927119 \t 8.701573 \t8.189722 \t 0.035823 \t 184.917194 s\n","274 \t 16.631578 \t 8.586981 \t8.028477 \t 0.016121 \t 185.556185 s\n","275 \t 16.883497 \t 8.687262 \t8.168276 \t 0.027960 \t 186.260099 s\n","276 \t 16.875113 \t 8.770741 \t8.084833 \t 0.019540 \t 186.810571 s\n","277 \t 16.825056 \t 8.627734 \t8.176590 \t 0.020732 \t 187.354166 s\n","278 \t 16.586293 \t 8.638385 \t7.921718 \t 0.026191 \t 187.871513 s\n","279 \t 16.557748 \t 8.557090 \t7.972865 \t 0.027792 \t 188.424990 s\n","280 \t 16.630116 \t 8.565808 \t8.031945 \t 0.032362 \t 188.966079 s\n","281 \t 16.832592 \t 8.654693 \t8.129817 \t 0.048082 \t 189.502704 s\n","282 \t 16.837620 \t 8.709441 \t8.085801 \t 0.042377 \t 190.209373 s\n","283 \t 16.660784 \t 8.654383 \t7.971216 \t 0.035184 \t 190.732165 s\n","284 \t 16.602200 \t 8.663511 \t7.903490 \t 0.035198 \t 191.257325 s\n","285 \t 16.661048 \t 8.594843 \t8.045747 \t 0.020457 \t 191.769233 s\n","286 \t 16.711553 \t 8.622996 \t8.003191 \t 0.085365 \t 192.290774 s\n","287 \t 16.618224 \t 8.496480 \t8.093247 \t 0.028498 \t 192.811235 s\n","288 \t 16.633840 \t 8.640830 \t7.936166 \t 0.056844 \t 193.341483 s\n","289 \t 16.605414 \t 8.485870 \t8.077440 \t 0.042105 \t 193.858915 s\n","290 \t 16.751394 \t 8.660554 \t8.044782 \t 0.046058 \t 194.384120 s\n","291 \t 16.495236 \t 8.543135 \t7.906801 \t 0.045300 \t 194.903119 s\n","292 \t 16.435653 \t 8.553933 \t7.823690 \t 0.058030 \t 195.437008 s\n","293 \t 16.408811 \t 8.468110 \t7.915509 \t 0.025191 \t 195.948029 s\n","294 \t 16.609037 \t 8.572268 \t7.985103 \t 0.051667 \t 196.511200 s\n","295 \t 16.604403 \t 8.585375 \t7.978031 \t 0.040997 \t 197.142798 s\n","296 \t 16.552419 \t 8.571277 \t7.929536 \t 0.051605 \t 197.972668 s\n","297 \t 16.688998 \t 8.616461 \t8.045108 \t 0.027430 \t 198.651173 s\n","298 \t 16.394499 \t 8.528192 \t7.800628 \t 0.065679 \t 199.321866 s\n","299 \t 16.301410 \t 8.484430 \t7.786806 \t 0.030173 \t 199.854465 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:27<00:00,  5.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3434\n","Hit@10: 0.5071\n","Hit@3: 0.3475\n","Hit@1: 0.2624\n","Link Prediction on Validation Set (All)\n","MRR: 0.3014\n","Hit@10: 0.5142\n","Hit@3: 0.3275\n","Hit@1: 0.1978\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5698\n","Hit@10: 0.7547\n","Hit@3: 0.6164\n","Hit@1: 0.4780\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4682\n","Hit@10: 0.7407\n","Hit@3: 0.5639\n","Hit@1: 0.3183\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.1360\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1360\n","300 \t 16.451951 \t 8.462816 \t7.934517 \t 0.054618 \t 228.982322 s\n","301 \t 16.585763 \t 8.615551 \t7.942351 \t 0.027860 \t 229.507344 s\n","302 \t 16.500463 \t 8.575512 \t7.890787 \t 0.034164 \t 230.016366 s\n","303 \t 16.376472 \t 8.406479 \t7.943029 \t 0.026965 \t 230.545996 s\n","304 \t 16.278604 \t 8.385154 \t7.861317 \t 0.032132 \t 231.060032 s\n","305 \t 16.445395 \t 8.464086 \t7.954068 \t 0.027241 \t 231.586329 s\n","306 \t 16.269762 \t 8.476399 \t7.770580 \t 0.022782 \t 232.104057 s\n","307 \t 16.292510 \t 8.496312 \t7.771353 \t 0.024845 \t 232.624044 s\n","308 \t 16.296373 \t 8.477439 \t7.797293 \t 0.021642 \t 233.332118 s\n","309 \t 16.344377 \t 8.507561 \t7.811379 \t 0.025435 \t 233.855723 s\n","310 \t 16.474763 \t 8.505137 \t7.943490 \t 0.026136 \t 234.419398 s\n","311 \t 16.166412 \t 8.536462 \t7.606142 \t 0.023809 \t 235.094719 s\n","312 \t 16.143340 \t 8.253605 \t7.872191 \t 0.017544 \t 235.905176 s\n","313 \t 16.535126 \t 8.544524 \t7.971943 \t 0.018658 \t 236.618677 s\n","314 \t 16.513757 \t 8.426692 \t8.065495 \t 0.021570 \t 237.394422 s\n","315 \t 16.142248 \t 8.393585 \t7.717783 \t 0.030880 \t 238.141228 s\n","316 \t 16.390160 \t 8.474996 \t7.899207 \t 0.015957 \t 238.770272 s\n","317 \t 16.177087 \t 8.399655 \t7.760582 \t 0.016850 \t 239.315269 s\n","318 \t 16.223792 \t 8.456911 \t7.731329 \t 0.035552 \t 239.835763 s\n","319 \t 16.395674 \t 8.452067 \t7.916127 \t 0.027479 \t 240.375154 s\n","320 \t 16.286863 \t 8.349140 \t7.918711 \t 0.019013 \t 240.892070 s\n","321 \t 16.211334 \t 8.385765 \t7.793932 \t 0.031638 \t 241.605690 s\n","322 \t 16.447935 \t 8.528425 \t7.854200 \t 0.065310 \t 242.148395 s\n","323 \t 16.262728 \t 8.446480 \t7.771539 \t 0.044709 \t 242.667734 s\n","324 \t 16.128056 \t 8.277784 \t7.827714 \t 0.022558 \t 243.198328 s\n","325 \t 16.061966 \t 8.313224 \t7.723902 \t 0.024840 \t 243.722748 s\n","326 \t 16.307919 \t 8.420705 \t7.857256 \t 0.029956 \t 244.255192 s\n","327 \t 16.265928 \t 8.404342 \t7.826531 \t 0.035056 \t 244.782928 s\n","328 \t 16.212938 \t 8.474108 \t7.702852 \t 0.035979 \t 245.309948 s\n","329 \t 16.173306 \t 8.399545 \t7.747688 \t 0.026073 \t 245.831597 s\n","330 \t 16.341702 \t 8.464055 \t7.861284 \t 0.016363 \t 246.359441 s\n","331 \t 16.276596 \t 8.491140 \t7.763351 \t 0.022105 \t 246.878910 s\n","332 \t 16.341435 \t 8.460162 \t7.851647 \t 0.029626 \t 247.401720 s\n","333 \t 16.310175 \t 8.414944 \t7.853421 \t 0.041810 \t 247.921256 s\n","334 \t 16.180580 \t 8.434582 \t7.727211 \t 0.018789 \t 248.683973 s\n","335 \t 16.020751 \t 8.362089 \t7.632293 \t 0.026369 \t 249.336282 s\n","336 \t 16.156313 \t 8.417492 \t7.718428 \t 0.020392 \t 249.980117 s\n","337 \t 15.978384 \t 8.345906 \t7.611343 \t 0.021136 \t 250.643936 s\n","338 \t 16.310783 \t 8.483061 \t7.800626 \t 0.027096 \t 251.356333 s\n","339 \t 16.195911 \t 8.420436 \t7.747099 \t 0.028376 \t 251.903977 s\n","340 \t 16.157458 \t 8.402507 \t7.716884 \t 0.038068 \t 252.437895 s\n","341 \t 16.190520 \t 8.433839 \t7.736579 \t 0.020102 \t 252.958171 s\n","342 \t 15.954622 \t 8.304309 \t7.632898 \t 0.017414 \t 253.486820 s\n","343 \t 15.968050 \t 8.412385 \t7.530267 \t 0.025397 \t 254.000750 s\n","344 \t 15.993045 \t 8.349364 \t7.611956 \t 0.031725 \t 254.525992 s\n","345 \t 16.091039 \t 8.402397 \t7.658542 \t 0.030100 \t 255.044648 s\n","346 \t 15.853763 \t 8.169766 \t7.664049 \t 0.019947 \t 255.576207 s\n","347 \t 16.120923 \t 8.378134 \t7.720388 \t 0.022401 \t 256.267519 s\n","348 \t 16.199442 \t 8.453472 \t7.717919 \t 0.028052 \t 256.795482 s\n","349 \t 15.764818 \t 8.183300 \t7.567313 \t 0.014204 \t 257.317971 s\n","350 \t 16.025088 \t 8.388903 \t7.612087 \t 0.024097 \t 257.837945 s\n","351 \t 15.839406 \t 8.276192 \t7.553344 \t 0.009870 \t 258.361859 s\n","352 \t 15.903030 \t 8.268637 \t7.619058 \t 0.015335 \t 258.893083 s\n","353 \t 16.051088 \t 8.380074 \t7.654983 \t 0.016032 \t 259.415241 s\n","354 \t 16.015267 \t 8.361301 \t7.632315 \t 0.021650 \t 259.938972 s\n","355 \t 15.849586 \t 8.289916 \t7.520921 \t 0.038751 \t 260.475720 s\n","356 \t 15.844694 \t 8.215476 \t7.612851 \t 0.016366 \t 261.000234 s\n","357 \t 15.647063 \t 8.105474 \t7.516027 \t 0.025562 \t 261.560866 s\n","358 \t 15.859570 \t 8.314888 \t7.519767 \t 0.024914 \t 262.208295 s\n","359 \t 15.888974 \t 8.280242 \t7.580538 \t 0.028194 \t 262.850335 s\n","360 \t 15.763052 \t 8.132819 \t7.593713 \t 0.036521 \t 263.481170 s\n","361 \t 16.059243 \t 8.355195 \t7.678836 \t 0.025212 \t 264.397138 s\n","362 \t 15.830729 \t 8.341447 \t7.470844 \t 0.018438 \t 264.951747 s\n","363 \t 15.819108 \t 8.256021 \t7.538684 \t 0.024402 \t 265.473572 s\n","364 \t 15.777421 \t 8.147416 \t7.609312 \t 0.020692 \t 265.998749 s\n","365 \t 15.888125 \t 8.232224 \t7.628155 \t 0.027746 \t 266.519064 s\n","366 \t 15.859176 \t 8.210151 \t7.631713 \t 0.017312 \t 267.052476 s\n","367 \t 15.945642 \t 8.298759 \t7.622722 \t 0.024162 \t 267.575875 s\n","368 \t 15.628313 \t 7.965238 \t7.646973 \t 0.016102 \t 268.114953 s\n","369 \t 15.792264 \t 8.169436 \t7.593400 \t 0.029427 \t 268.635389 s\n","370 \t 15.848261 \t 8.247094 \t7.573767 \t 0.027400 \t 269.172446 s\n","371 \t 15.910648 \t 8.288304 \t7.595182 \t 0.027162 \t 269.692866 s\n","372 \t 15.832886 \t 8.143200 \t7.669500 \t 0.020186 \t 270.230014 s\n","373 \t 15.821977 \t 8.149113 \t7.641122 \t 0.031742 \t 270.749571 s\n","374 \t 15.728539 \t 8.158048 \t7.549581 \t 0.020911 \t 271.443951 s\n","375 \t 15.610212 \t 8.003501 \t7.586785 \t 0.019926 \t 271.968190 s\n","376 \t 15.727014 \t 8.102364 \t7.609836 \t 0.014814 \t 272.496142 s\n","377 \t 15.633273 \t 8.122252 \t7.490376 \t 0.020645 \t 273.017418 s\n","378 \t 15.809022 \t 8.202280 \t7.589446 \t 0.017296 \t 273.563458 s\n","379 \t 15.794525 \t 8.198167 \t7.574386 \t 0.021972 \t 274.088250 s\n","380 \t 15.767852 \t 8.195135 \t7.553675 \t 0.019042 \t 274.678513 s\n","381 \t 15.773926 \t 8.210189 \t7.549859 \t 0.013877 \t 275.333810 s\n","382 \t 15.669967 \t 8.204035 \t7.444616 \t 0.021315 \t 275.967227 s\n","383 \t 15.668429 \t 8.238110 \t7.417575 \t 0.012745 \t 276.611222 s\n","384 \t 15.754266 \t 8.173079 \t7.555257 \t 0.025930 \t 277.301594 s\n","385 \t 15.832701 \t 8.187320 \t7.628634 \t 0.016747 \t 277.877132 s\n","386 \t 15.867045 \t 8.263300 \t7.588790 \t 0.014955 \t 278.403849 s\n","387 \t 15.833079 \t 8.247544 \t7.553809 \t 0.031726 \t 279.096224 s\n","388 \t 15.814695 \t 8.227703 \t7.572916 \t 0.014076 \t 279.617471 s\n","389 \t 15.486039 \t 8.094506 \t7.378209 \t 0.013324 \t 280.139244 s\n","390 \t 15.785988 \t 8.130923 \t7.638724 \t 0.016341 \t 280.667752 s\n","391 \t 15.646336 \t 8.200760 \t7.428323 \t 0.017252 \t 281.185126 s\n","392 \t 15.887139 \t 8.222963 \t7.648956 \t 0.015221 \t 281.702890 s\n","393 \t 15.801033 \t 8.197613 \t7.590520 \t 0.012900 \t 282.219087 s\n","394 \t 15.640047 \t 8.112905 \t7.495142 \t 0.032000 \t 282.736470 s\n","395 \t 15.653385 \t 8.114732 \t7.523303 \t 0.015351 \t 283.250416 s\n","396 \t 15.698011 \t 8.128181 \t7.548937 \t 0.020893 \t 283.770112 s\n","397 \t 15.728240 \t 8.177623 \t7.534715 \t 0.015902 \t 284.286474 s\n","398 \t 15.701914 \t 8.140654 \t7.539686 \t 0.021574 \t 284.813004 s\n","399 \t 15.710906 \t 8.202199 \t7.493137 \t 0.015569 \t 285.341441 s\n","400 \t 15.555432 \t 8.111534 \t7.431078 \t 0.012821 \t 286.042257 s\n","401 \t 15.573596 \t 8.068974 \t7.493052 \t 0.011571 \t 286.555567 s\n","402 \t 15.671837 \t 8.093899 \t7.564518 \t 0.013420 \t 287.089794 s\n","403 \t 15.724520 \t 8.047324 \t7.652585 \t 0.024611 \t 287.675167 s\n","404 \t 15.546760 \t 8.096708 \t7.430627 \t 0.019425 \t 288.316751 s\n","405 \t 15.744448 \t 8.148619 \t7.571938 \t 0.023891 \t 288.947777 s\n","406 \t 15.759765 \t 8.174880 \t7.566514 \t 0.018371 \t 289.571388 s\n","407 \t 15.639288 \t 8.249629 \t7.372241 \t 0.017417 \t 290.281420 s\n","408 \t 15.650517 \t 8.183250 \t7.447327 \t 0.019941 \t 290.877403 s\n","409 \t 15.872020 \t 8.208511 \t7.645948 \t 0.017561 \t 291.399350 s\n","410 \t 15.547161 \t 8.110342 \t7.420075 \t 0.016744 \t 291.925925 s\n","411 \t 15.564385 \t 8.053625 \t7.499288 \t 0.011472 \t 292.440911 s\n","412 \t 15.497723 \t 8.132296 \t7.352685 \t 0.012742 \t 292.976487 s\n","413 \t 15.552926 \t 8.119717 \t7.413862 \t 0.019346 \t 293.661929 s\n","414 \t 15.641826 \t 8.156187 \t7.455359 \t 0.030280 \t 294.181038 s\n","415 \t 15.642626 \t 8.166927 \t7.465293 \t 0.010406 \t 294.694208 s\n","416 \t 15.626912 \t 8.236562 \t7.374904 \t 0.015446 \t 295.217701 s\n","417 \t 15.800858 \t 8.206574 \t7.574946 \t 0.019338 \t 295.733964 s\n","418 \t 15.608697 \t 8.076930 \t7.518106 \t 0.013662 \t 296.253689 s\n","419 \t 15.501929 \t 8.090126 \t7.388604 \t 0.023199 \t 296.772088 s\n","420 \t 15.620112 \t 8.118228 \t7.493147 \t 0.008737 \t 297.296769 s\n","421 \t 15.511584 \t 8.058316 \t7.435176 \t 0.018092 \t 297.809180 s\n","422 \t 15.529182 \t 8.093475 \t7.423940 \t 0.011767 \t 298.332118 s\n","423 \t 15.537547 \t 8.123814 \t7.402439 \t 0.011294 \t 298.849423 s\n","424 \t 15.353348 \t 8.014992 \t7.325295 \t 0.013061 \t 299.372922 s\n","425 \t 15.600071 \t 8.113340 \t7.472778 \t 0.013953 \t 299.891992 s\n","426 \t 15.584007 \t 8.085309 \t7.478174 \t 0.020524 \t 300.419052 s\n","427 \t 15.661229 \t 8.098618 \t7.544364 \t 0.018246 \t 301.313880 s\n","428 \t 15.568097 \t 8.158540 \t7.398786 \t 0.010770 \t 301.930764 s\n","429 \t 15.579810 \t 8.087327 \t7.472748 \t 0.019736 \t 302.562919 s\n","430 \t 15.762569 \t 8.214202 \t7.530723 \t 0.017644 \t 303.258611 s\n","431 \t 15.707885 \t 8.070800 \t7.619472 \t 0.017613 \t 303.841887 s\n","432 \t 15.699672 \t 8.182333 \t7.500911 \t 0.016428 \t 304.371360 s\n","433 \t 15.578652 \t 8.088548 \t7.479808 \t 0.010295 \t 304.887496 s\n","434 \t 15.909709 \t 8.256805 \t7.639903 \t 0.013001 \t 305.420512 s\n","435 \t 15.543880 \t 8.149389 \t7.379779 \t 0.014711 \t 305.940330 s\n","436 \t 15.559886 \t 8.051943 \t7.496158 \t 0.011785 \t 306.466466 s\n","437 \t 15.535715 \t 8.209064 \t7.313665 \t 0.012986 \t 306.986735 s\n","438 \t 15.690117 \t 8.170157 \t7.504710 \t 0.015249 \t 307.517771 s\n","439 \t 15.591094 \t 8.104399 \t7.468125 \t 0.018570 \t 308.045610 s\n","440 \t 15.797129 \t 8.183378 \t7.601235 \t 0.012516 \t 308.731516 s\n","441 \t 15.577501 \t 8.073066 \t7.491534 \t 0.012901 \t 309.254363 s\n","442 \t 15.719008 \t 8.123183 \t7.585964 \t 0.009862 \t 309.779884 s\n","443 \t 15.670542 \t 8.165209 \t7.487607 \t 0.017726 \t 310.302800 s\n","444 \t 15.583027 \t 8.143107 \t7.424581 \t 0.015339 \t 310.822242 s\n","445 \t 15.575918 \t 8.127869 \t7.433397 \t 0.014651 \t 311.357348 s\n","446 \t 15.494989 \t 8.124290 \t7.358341 \t 0.012359 \t 311.881789 s\n","447 \t 15.587116 \t 8.137044 \t7.431945 \t 0.018127 \t 312.408748 s\n","448 \t 15.508318 \t 8.061092 \t7.429154 \t 0.018072 \t 312.935178 s\n","449 \t 15.613206 \t 8.094101 \t7.500589 \t 0.018516 \t 313.467052 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:28<00:00,  5.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3500\n","Hit@10: 0.5496\n","Hit@3: 0.3617\n","Hit@1: 0.2553\n","Link Prediction on Validation Set (All)\n","MRR: 0.3335\n","Hit@10: 0.5680\n","Hit@3: 0.3655\n","Hit@1: 0.2184\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6010\n","Hit@10: 0.7862\n","Hit@3: 0.6289\n","Hit@1: 0.5220\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4953\n","Hit@10: 0.7505\n","Hit@3: 0.5815\n","Hit@1: 0.3595\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0370\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0370\n","450 \t 15.887301 \t 8.389873 \t7.472278 \t 0.025150 \t 343.408019 s\n","451 \t 15.838723 \t 8.240388 \t7.562363 \t 0.035971 \t 343.944293 s\n","452 \t 15.988314 \t 8.258714 \t7.648203 \t 0.081397 \t 344.459725 s\n","453 \t 16.072954 \t 8.347246 \t7.695311 \t 0.030397 \t 345.148337 s\n","454 \t 16.076487 \t 8.276468 \t7.714719 \t 0.085298 \t 345.670440 s\n","455 \t 15.947409 \t 8.280207 \t7.628127 \t 0.039075 \t 346.201242 s\n","456 \t 16.129969 \t 8.298442 \t7.770198 \t 0.061327 \t 346.718298 s\n","457 \t 16.109568 \t 8.323058 \t7.722957 \t 0.063553 \t 347.241532 s\n","458 \t 15.801461 \t 8.346399 \t7.425754 \t 0.029308 \t 347.763055 s\n","459 \t 16.064319 \t 8.358349 \t7.673018 \t 0.032951 \t 348.282824 s\n","460 \t 15.976275 \t 8.138836 \t7.789209 \t 0.048230 \t 348.802584 s\n","461 \t 15.914498 \t 8.254463 \t7.634151 \t 0.025885 \t 349.468441 s\n","462 \t 15.983109 \t 8.299569 \t7.660996 \t 0.022544 \t 350.060270 s\n","463 \t 16.202278 \t 8.418819 \t7.735475 \t 0.047985 \t 350.701104 s\n","464 \t 16.232106 \t 8.273451 \t7.925808 \t 0.032846 \t 351.332563 s\n","465 \t 16.013277 \t 8.227144 \t7.750314 \t 0.035819 \t 351.945640 s\n","466 \t 15.955249 \t 8.285992 \t7.630849 \t 0.038407 \t 352.702715 s\n","467 \t 16.104945 \t 8.312042 \t7.754216 \t 0.038687 \t 353.364713 s\n","468 \t 16.002922 \t 8.284681 \t7.682295 \t 0.035945 \t 353.983479 s\n","469 \t 15.890525 \t 8.215422 \t7.648219 \t 0.026885 \t 354.629295 s\n","470 \t 15.988031 \t 8.199413 \t7.747449 \t 0.041169 \t 355.331699 s\n","471 \t 15.903327 \t 8.256888 \t7.592678 \t 0.053762 \t 355.844147 s\n","472 \t 15.773866 \t 8.184125 \t7.544355 \t 0.045386 \t 356.382004 s\n","473 \t 15.549681 \t 8.022867 \t7.479906 \t 0.046908 \t 356.907736 s\n","474 \t 16.044768 \t 8.249494 \t7.739020 \t 0.056254 \t 357.443198 s\n","475 \t 15.876941 \t 8.243957 \t7.594776 \t 0.038208 \t 357.959996 s\n","476 \t 15.915060 \t 8.195460 \t7.688431 \t 0.031168 \t 358.484848 s\n","477 \t 15.786442 \t 8.154478 \t7.588073 \t 0.043891 \t 359.002605 s\n","478 \t 15.937634 \t 8.223004 \t7.688064 \t 0.026566 \t 359.537335 s\n","479 \t 15.805883 \t 8.054688 \t7.715914 \t 0.035282 \t 360.220974 s\n","480 \t 16.023738 \t 8.206867 \t7.791798 \t 0.025073 \t 360.739565 s\n","481 \t 15.902613 \t 8.230486 \t7.614214 \t 0.057913 \t 361.264908 s\n","482 \t 15.755145 \t 8.173077 \t7.549784 \t 0.032284 \t 361.787305 s\n","483 \t 15.488484 \t 7.990244 \t7.471699 \t 0.026542 \t 362.324845 s\n","484 \t 15.765333 \t 8.158050 \t7.572360 \t 0.034922 \t 362.844358 s\n","485 \t 15.906067 \t 8.251405 \t7.612087 \t 0.042576 \t 363.372555 s\n","486 \t 15.775305 \t 8.180398 \t7.555939 \t 0.038968 \t 363.900824 s\n","487 \t 15.730291 \t 8.080522 \t7.601669 \t 0.048100 \t 364.437445 s\n","488 \t 15.905344 \t 8.297279 \t7.574411 \t 0.033654 \t 364.970216 s\n","489 \t 15.452499 \t 7.966497 \t7.440055 \t 0.045947 \t 365.552309 s\n","490 \t 15.622067 \t 8.109278 \t7.449471 \t 0.063319 \t 366.200835 s\n","491 \t 15.743948 \t 8.056305 \t7.639122 \t 0.048522 \t 367.019399 s\n","492 \t 15.916794 \t 8.209356 \t7.606349 \t 0.101090 \t 367.668789 s\n","493 \t 15.590411 \t 8.058022 \t7.479275 \t 0.053114 \t 368.366055 s\n","494 \t 15.883695 \t 8.172843 \t7.680354 \t 0.030498 \t 368.901741 s\n","495 \t 15.702399 \t 8.156512 \t7.506279 \t 0.039608 \t 369.440386 s\n","496 \t 15.483979 \t 7.966910 \t7.484221 \t 0.032848 \t 369.989817 s\n","497 \t 15.390249 \t 7.854956 \t7.485634 \t 0.049659 \t 370.510890 s\n","498 \t 15.647037 \t 8.063174 \t7.543993 \t 0.039870 \t 371.042658 s\n","499 \t 15.369177 \t 7.941184 \t7.392400 \t 0.035594 \t 371.573566 s\n","500 \t 15.610421 \t 8.009273 \t7.572409 \t 0.028739 \t 372.110672 s\n","501 \t 15.691542 \t 8.002161 \t7.659071 \t 0.030310 \t 372.627768 s\n","502 \t 15.563915 \t 8.043555 \t7.494550 \t 0.025811 \t 373.151182 s\n","503 \t 15.451458 \t 7.894641 \t7.527118 \t 0.029699 \t 373.676997 s\n","504 \t 15.696960 \t 8.117656 \t7.550796 \t 0.028509 \t 374.375910 s\n","505 \t 15.553945 \t 7.953976 \t7.537862 \t 0.062107 \t 374.893219 s\n","506 \t 15.618361 \t 8.044907 \t7.540607 \t 0.032846 \t 375.437296 s\n","507 \t 15.484242 \t 7.966035 \t7.457222 \t 0.060985 \t 375.961291 s\n","508 \t 15.537925 \t 8.002394 \t7.497190 \t 0.038340 \t 376.498520 s\n","509 \t 15.520901 \t 7.892058 \t7.550580 \t 0.078264 \t 377.014446 s\n","510 \t 15.596527 \t 8.099863 \t7.454899 \t 0.041765 \t 377.554708 s\n","511 \t 15.435462 \t 7.974724 \t7.379536 \t 0.081202 \t 378.077351 s\n","512 \t 15.389054 \t 7.955767 \t7.404833 \t 0.028453 \t 378.683568 s\n","513 \t 15.618375 \t 7.984684 \t7.546971 \t 0.086721 \t 379.330082 s\n","514 \t 15.503432 \t 8.028747 \t7.442203 \t 0.032482 \t 379.951755 s\n","515 \t 15.414861 \t 7.957831 \t7.407345 \t 0.049685 \t 380.609102 s\n","516 \t 15.422565 \t 7.927244 \t7.466059 \t 0.029262 \t 381.473473 s\n","517 \t 15.395187 \t 7.898061 \t7.427782 \t 0.069345 \t 381.998068 s\n","518 \t 15.358737 \t 7.907055 \t7.414599 \t 0.037083 \t 382.537979 s\n","519 \t 15.514362 \t 8.043582 \t7.439171 \t 0.031609 \t 383.063959 s\n","520 \t 15.382612 \t 7.947479 \t7.382194 \t 0.052939 \t 383.600096 s\n","521 \t 15.473135 \t 7.949233 \t7.498020 \t 0.025882 \t 384.121761 s\n","522 \t 15.279526 \t 7.911368 \t7.328902 \t 0.039255 \t 384.643054 s\n","523 \t 15.373095 \t 7.906953 \t7.434282 \t 0.031860 \t 385.167017 s\n","524 \t 15.335853 \t 7.889714 \t7.411524 \t 0.034615 \t 385.684000 s\n","525 \t 15.390775 \t 7.929751 \t7.404359 \t 0.056665 \t 386.198664 s\n","526 \t 15.393548 \t 7.971719 \t7.392102 \t 0.029727 \t 386.725528 s\n","527 \t 15.378851 \t 7.936706 \t7.411386 \t 0.030759 \t 387.250013 s\n","528 \t 15.393707 \t 7.986365 \t7.382696 \t 0.024645 \t 387.773455 s\n","529 \t 15.298686 \t 7.890165 \t7.375147 \t 0.033374 \t 388.459934 s\n","530 \t 15.290767 \t 7.799625 \t7.449373 \t 0.041769 \t 388.990035 s\n","531 \t 15.312596 \t 7.868410 \t7.414720 \t 0.029466 \t 389.513131 s\n","532 \t 15.203970 \t 7.850668 \t7.327827 \t 0.025474 \t 390.058529 s\n","533 \t 15.348056 \t 7.863307 \t7.463528 \t 0.021220 \t 390.591034 s\n","534 \t 15.215196 \t 7.887013 \t7.302680 \t 0.025503 \t 391.116733 s\n","535 \t 15.133492 \t 7.771624 \t7.342018 \t 0.019850 \t 391.729898 s\n","536 \t 15.309871 \t 7.936640 \t7.353166 \t 0.020065 \t 392.346900 s\n","537 \t 15.419596 \t 7.883652 \t7.523500 \t 0.012443 \t 393.000135 s\n","538 \t 15.325188 \t 7.825406 \t7.478698 \t 0.021083 \t 393.656173 s\n","539 \t 15.210841 \t 7.855488 \t7.336453 \t 0.018900 \t 394.345123 s\n","540 \t 15.242939 \t 7.762473 \t7.445515 \t 0.034952 \t 394.867066 s\n","541 \t 15.040720 \t 7.675548 \t7.349214 \t 0.015957 \t 395.562226 s\n","542 \t 15.216466 \t 7.744986 \t7.448868 \t 0.022613 \t 396.081300 s\n","543 \t 15.062801 \t 7.756453 \t7.285018 \t 0.021329 \t 396.601319 s\n","544 \t 15.261872 \t 7.804398 \t7.431736 \t 0.025737 \t 397.122792 s\n","545 \t 15.258470 \t 7.842889 \t7.392960 \t 0.022622 \t 397.639069 s\n","546 \t 15.326503 \t 7.794754 \t7.504802 \t 0.026946 \t 398.173375 s\n","547 \t 15.126944 \t 7.845452 \t7.257975 \t 0.023517 \t 398.691534 s\n","548 \t 15.050129 \t 7.663151 \t7.354259 \t 0.032719 \t 399.212849 s\n","549 \t 15.320310 \t 7.800521 \t7.492957 \t 0.026832 \t 399.730049 s\n","550 \t 15.216512 \t 7.817108 \t7.374450 \t 0.024954 \t 400.254668 s\n","551 \t 14.993990 \t 7.677567 \t7.294702 \t 0.021721 \t 400.766969 s\n","552 \t 15.090755 \t 7.719679 \t7.345863 \t 0.025212 \t 401.290165 s\n","553 \t 15.140083 \t 7.836927 \t7.255239 \t 0.047916 \t 401.806707 s\n","554 \t 15.205398 \t 7.804720 \t7.365774 \t 0.034903 \t 402.335427 s\n","555 \t 14.988008 \t 7.610058 \t7.321440 \t 0.056510 \t 403.033136 s\n","556 \t 14.950198 \t 7.728323 \t7.182540 \t 0.039336 \t 403.561313 s\n","557 \t 14.987381 \t 7.655494 \t7.291901 \t 0.039985 \t 404.082364 s\n","558 \t 14.873802 \t 7.602642 \t7.251172 \t 0.019988 \t 404.667866 s\n","559 \t 15.082976 \t 7.688988 \t7.365383 \t 0.028605 \t 405.315640 s\n","560 \t 14.952382 \t 7.609892 \t7.309565 \t 0.032924 \t 405.928116 s\n","561 \t 14.768371 \t 7.562406 \t7.166734 \t 0.039231 \t 406.589800 s\n","562 \t 14.970284 \t 7.631953 \t7.306911 \t 0.031421 \t 407.281996 s\n","563 \t 14.973243 \t 7.627828 \t7.310624 \t 0.034792 \t 407.802228 s\n","564 \t 14.865334 \t 7.587050 \t7.249919 \t 0.028365 \t 408.336437 s\n","565 \t 14.681939 \t 7.450556 \t7.216907 \t 0.014476 \t 408.851400 s\n","566 \t 14.952765 \t 7.628772 \t7.301010 \t 0.022983 \t 409.379354 s\n","567 \t 14.825047 \t 7.581190 \t7.205049 \t 0.038808 \t 409.894687 s\n","568 \t 14.894332 \t 7.556206 \t7.315739 \t 0.022386 \t 410.591153 s\n","569 \t 14.903875 \t 7.610574 \t7.269477 \t 0.023825 \t 411.111490 s\n","570 \t 14.990080 \t 7.660084 \t7.295134 \t 0.034863 \t 411.635470 s\n","571 \t 14.617092 \t 7.515771 \t7.073149 \t 0.028171 \t 412.150409 s\n","572 \t 14.628517 \t 7.545098 \t7.046695 \t 0.036724 \t 412.672837 s\n","573 \t 14.931981 \t 7.558283 \t7.352011 \t 0.021686 \t 413.205645 s\n","574 \t 14.839534 \t 7.529524 \t7.285126 \t 0.024884 \t 413.731984 s\n","575 \t 14.825020 \t 7.531876 \t7.277303 \t 0.015842 \t 414.248302 s\n","576 \t 14.686029 \t 7.538588 \t7.109827 \t 0.037614 \t 414.761577 s\n","577 \t 14.878408 \t 7.615105 \t7.240307 \t 0.022996 \t 415.277786 s\n","578 \t 14.893876 \t 7.724275 \t7.151752 \t 0.017849 \t 415.794313 s\n","579 \t 14.763077 \t 7.558043 \t7.185611 \t 0.019423 \t 416.310070 s\n","580 \t 14.696937 \t 7.527100 \t7.149757 \t 0.020080 \t 416.826260 s\n","581 \t 14.668101 \t 7.541595 \t7.112684 \t 0.013822 \t 417.578340 s\n","582 \t 14.695999 \t 7.480467 \t7.180869 \t 0.034664 \t 418.213269 s\n","583 \t 14.745361 \t 7.502554 \t7.217913 \t 0.024895 \t 418.845795 s\n","584 \t 14.607064 \t 7.502895 \t7.081002 \t 0.023168 \t 419.494250 s\n","585 \t 14.769716 \t 7.528539 \t7.226175 \t 0.015002 \t 420.185374 s\n","586 \t 14.497259 \t 7.369417 \t7.110824 \t 0.017019 \t 420.706524 s\n","587 \t 14.654377 \t 7.481028 \t7.133230 \t 0.040119 \t 421.242836 s\n","588 \t 14.733470 \t 7.535359 \t7.175333 \t 0.022778 \t 421.763081 s\n","589 \t 14.652596 \t 7.486053 \t7.150434 \t 0.016108 \t 422.296663 s\n","590 \t 14.599756 \t 7.426843 \t7.160676 \t 0.012237 \t 422.811596 s\n","591 \t 14.560311 \t 7.476354 \t7.062582 \t 0.021375 \t 423.364433 s\n","592 \t 14.631320 \t 7.511314 \t7.108527 \t 0.011479 \t 423.888521 s\n","593 \t 14.714337 \t 7.544658 \t7.155759 \t 0.013919 \t 424.424238 s\n","594 \t 14.733306 \t 7.441472 \t7.263775 \t 0.028059 \t 425.118889 s\n","595 \t 14.616236 \t 7.482822 \t7.110728 \t 0.022686 \t 425.638976 s\n","596 \t 14.484604 \t 7.286813 \t7.183830 \t 0.013961 \t 426.168116 s\n","597 \t 14.555521 \t 7.476315 \t7.049098 \t 0.030107 \t 426.695528 s\n","598 \t 14.284370 \t 7.225315 \t7.044114 \t 0.014941 \t 427.229361 s\n","599 \t 14.555068 \t 7.357525 \t7.178412 \t 0.019132 \t 427.754582 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:27<00:00,  5.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4302\n","Hit@10: 0.6454\n","Hit@3: 0.4858\n","Hit@1: 0.3191\n","Link Prediction on Validation Set (All)\n","MRR: 0.4262\n","Hit@10: 0.6899\n","Hit@3: 0.4937\n","Hit@1: 0.2943\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5856\n","Hit@10: 0.7673\n","Hit@3: 0.6101\n","Hit@1: 0.5094\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5067\n","Hit@10: 0.7446\n","Hit@3: 0.5756\n","Hit@1: 0.3831\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.1092\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1092\n","600 \t 14.509650 \t 7.425312 \t7.065795 \t 0.018543 \t 457.282095 s\n","601 \t 14.547490 \t 7.347104 \t7.185466 \t 0.014920 \t 457.917111 s\n","602 \t 14.573628 \t 7.390180 \t7.170304 \t 0.013144 \t 458.579316 s\n","603 \t 14.382338 \t 7.396656 \t6.975518 \t 0.010164 \t 459.261218 s\n","604 \t 14.497213 \t 7.400588 \t7.068385 \t 0.028241 \t 459.804704 s\n","605 \t 14.432961 \t 7.355006 \t7.062549 \t 0.015407 \t 460.348021 s\n","606 \t 14.527711 \t 7.408607 \t7.093917 \t 0.025186 \t 460.872093 s\n","607 \t 14.249327 \t 7.288355 \t6.935012 \t 0.025960 \t 461.566776 s\n","608 \t 14.428120 \t 7.323789 \t7.083625 \t 0.020706 \t 462.082853 s\n","609 \t 14.618392 \t 7.426421 \t7.172698 \t 0.019272 \t 462.648396 s\n","610 \t 14.341138 \t 7.344919 \t6.977370 \t 0.018850 \t 463.336500 s\n","611 \t 14.416141 \t 7.411553 \t6.982267 \t 0.022321 \t 464.016323 s\n","612 \t 14.344716 \t 7.324090 \t7.011246 \t 0.009379 \t 464.688242 s\n","613 \t 14.207909 \t 7.318861 \t6.868652 \t 0.020396 \t 465.314298 s\n","614 \t 14.351895 \t 7.301139 \t7.024171 \t 0.026585 \t 465.905068 s\n","615 \t 14.314132 \t 7.333210 \t6.965127 \t 0.015794 \t 466.442757 s\n","616 \t 14.413757 \t 7.356799 \t7.042535 \t 0.014423 \t 466.983912 s\n","617 \t 14.392163 \t 7.260021 \t7.111143 \t 0.020999 \t 467.505419 s\n","618 \t 14.318901 \t 7.255620 \t7.048408 \t 0.014873 \t 468.048592 s\n","619 \t 14.259390 \t 7.205431 \t7.039120 \t 0.014839 \t 468.574730 s\n","620 \t 14.469943 \t 7.352875 \t7.093484 \t 0.023584 \t 469.291066 s\n","621 \t 14.250326 \t 7.256406 \t6.982420 \t 0.011500 \t 469.961800 s\n","622 \t 14.321270 \t 7.275046 \t7.034761 \t 0.011463 \t 470.581433 s\n","623 \t 14.213002 \t 7.225574 \t6.965779 \t 0.021649 \t 471.235997 s\n","624 \t 14.323097 \t 7.387076 \t6.918458 \t 0.017563 \t 471.919833 s\n","625 \t 14.328478 \t 7.309305 \t7.004089 \t 0.015084 \t 472.547567 s\n","626 \t 14.369228 \t 7.279424 \t7.076149 \t 0.013655 \t 473.076822 s\n","627 \t 14.029515 \t 7.149918 \t6.854138 \t 0.025459 \t 473.602045 s\n","628 \t 14.186314 \t 7.122959 \t7.018978 \t 0.044377 \t 474.156040 s\n","629 \t 14.107081 \t 7.171302 \t6.915754 \t 0.020026 \t 474.673440 s\n","630 \t 14.201074 \t 7.225223 \t6.958425 \t 0.017426 \t 475.197751 s\n","631 \t 14.139517 \t 7.237470 \t6.877215 \t 0.024833 \t 475.719179 s\n","632 \t 14.209495 \t 7.230907 \t6.961516 \t 0.017072 \t 476.245816 s\n","633 \t 14.107514 \t 7.173590 \t6.902959 \t 0.030964 \t 476.934984 s\n","634 \t 14.009338 \t 7.099951 \t6.899143 \t 0.010245 \t 477.478786 s\n","635 \t 14.282707 \t 7.289194 \t6.952450 \t 0.041064 \t 478.003830 s\n","636 \t 14.200968 \t 7.157534 \t7.024521 \t 0.018913 \t 478.546738 s\n","637 \t 14.081374 \t 7.154176 \t6.911401 \t 0.015797 \t 479.071842 s\n","638 \t 14.040879 \t 7.120612 \t6.905710 \t 0.014556 \t 479.611196 s\n","639 \t 14.234191 \t 7.256779 \t6.963563 \t 0.013850 \t 480.143739 s\n","640 \t 14.281882 \t 7.277599 \t6.986614 \t 0.017669 \t 480.663755 s\n","641 \t 14.175610 \t 7.209822 \t6.948084 \t 0.017704 \t 481.193632 s\n","642 \t 14.185176 \t 7.159482 \t7.008957 \t 0.016737 \t 481.711473 s\n","643 \t 14.123706 \t 7.146771 \t6.959869 \t 0.017066 \t 482.248137 s\n","644 \t 13.927025 \t 7.128930 \t6.787987 \t 0.010108 \t 482.901011 s\n","645 \t 14.010142 \t 7.068755 \t6.920278 \t 0.021110 \t 483.535314 s\n","646 \t 13.856966 \t 7.035912 \t6.811776 \t 0.009279 \t 484.402405 s\n","647 \t 13.933657 \t 7.090084 \t6.827197 \t 0.016377 \t 485.103496 s\n","648 \t 14.052878 \t 7.194258 \t6.839496 \t 0.019124 \t 485.650378 s\n","649 \t 14.055174 \t 7.155073 \t6.888974 \t 0.011127 \t 486.172245 s\n","650 \t 14.183158 \t 7.188829 \t6.981285 \t 0.013044 \t 486.695521 s\n","651 \t 14.150126 \t 7.287607 \t6.849605 \t 0.012913 \t 487.216161 s\n","652 \t 13.915314 \t 7.045061 \t6.860422 \t 0.009831 \t 487.742121 s\n","653 \t 14.083874 \t 7.199024 \t6.864984 \t 0.019866 \t 488.262796 s\n","654 \t 13.976093 \t 7.104694 \t6.853484 \t 0.017915 \t 488.795292 s\n","655 \t 14.030670 \t 7.138654 \t6.873939 \t 0.018077 \t 489.329070 s\n","656 \t 13.866300 \t 7.014157 \t6.833383 \t 0.018759 \t 489.875426 s\n","657 \t 13.968487 \t 7.099062 \t6.847696 \t 0.021729 \t 490.405845 s\n","658 \t 14.050760 \t 7.192466 \t6.844807 \t 0.013486 \t 490.954111 s\n","659 \t 13.939321 \t 7.187567 \t6.729569 \t 0.022185 \t 491.666517 s\n","660 \t 13.972508 \t 7.021849 \t6.938416 \t 0.012243 \t 492.185850 s\n","661 \t 14.011344 \t 7.080928 \t6.918313 \t 0.012104 \t 492.708876 s\n","662 \t 13.901900 \t 7.070042 \t6.817833 \t 0.014025 \t 493.225935 s\n","663 \t 13.981744 \t 7.074262 \t6.891328 \t 0.016154 \t 493.748502 s\n","664 \t 13.950913 \t 7.014936 \t6.921982 \t 0.013995 \t 494.268010 s\n","665 \t 13.794803 \t 7.071461 \t6.707072 \t 0.016270 \t 494.792386 s\n","666 \t 13.935442 \t 7.107169 \t6.814480 \t 0.013793 \t 495.388677 s\n","667 \t 13.843387 \t 7.024024 \t6.804318 \t 0.015044 \t 496.029503 s\n","668 \t 13.710877 \t 6.908353 \t6.788094 \t 0.014430 \t 496.653110 s\n","669 \t 13.884543 \t 7.034398 \t6.841497 \t 0.008647 \t 497.306286 s\n","670 \t 13.819158 \t 7.102893 \t6.693077 \t 0.023188 \t 498.012480 s\n","671 \t 13.839906 \t 6.974980 \t6.845516 \t 0.019410 \t 498.581979 s\n","672 \t 13.791467 \t 6.938560 \t6.845706 \t 0.007201 \t 499.278833 s\n","673 \t 13.901940 \t 6.971213 \t6.919781 \t 0.010946 \t 499.793712 s\n","674 \t 13.673837 \t 6.919649 \t6.731802 \t 0.022387 \t 500.328991 s\n","675 \t 13.843559 \t 7.058160 \t6.773432 \t 0.011968 \t 500.856338 s\n","676 \t 13.706731 \t 6.936129 \t6.757032 \t 0.013571 \t 501.398679 s\n","677 \t 13.829165 \t 7.006718 \t6.804444 \t 0.018003 \t 501.942059 s\n","678 \t 13.708376 \t 7.007969 \t6.685843 \t 0.014565 \t 502.462744 s\n","679 \t 13.743936 \t 6.986534 \t6.741440 \t 0.015962 \t 502.996212 s\n","680 \t 13.670852 \t 6.929729 \t6.717912 \t 0.023210 \t 503.520427 s\n","681 \t 13.824186 \t 7.001842 \t6.806391 \t 0.015953 \t 504.056234 s\n","682 \t 13.802780 \t 7.006689 \t6.783051 \t 0.013040 \t 504.569058 s\n","683 \t 13.814011 \t 6.951580 \t6.832822 \t 0.029609 \t 505.099279 s\n","684 \t 13.698920 \t 6.895724 \t6.772887 \t 0.030309 \t 505.623897 s\n","685 \t 13.694607 \t 6.976530 \t6.703383 \t 0.014694 \t 506.313294 s\n","686 \t 13.717186 \t 6.982581 \t6.720075 \t 0.014530 \t 506.825065 s\n","687 \t 13.685114 \t 6.937295 \t6.701006 \t 0.046813 \t 507.348625 s\n","688 \t 13.605181 \t 6.853937 \t6.731405 \t 0.019838 \t 507.865936 s\n","689 \t 13.607993 \t 6.850932 \t6.729367 \t 0.027694 \t 508.461283 s\n","690 \t 13.598993 \t 6.927132 \t6.656422 \t 0.015439 \t 509.081007 s\n","691 \t 13.553096 \t 6.845200 \t6.686566 \t 0.021330 \t 509.721946 s\n","692 \t 13.781922 \t 6.975434 \t6.779805 \t 0.026683 \t 510.365381 s\n","693 \t 13.686271 \t 6.891200 \t6.775903 \t 0.019168 \t 511.074374 s\n","694 \t 13.773042 \t 6.967693 \t6.762684 \t 0.042664 \t 511.594820 s\n","695 \t 13.560962 \t 6.902960 \t6.646120 \t 0.011882 \t 512.115955 s\n","696 \t 13.717208 \t 7.046675 \t6.649436 \t 0.021096 \t 512.639132 s\n","697 \t 13.558361 \t 6.819611 \t6.725504 \t 0.013247 \t 513.166061 s\n","698 \t 13.565021 \t 6.851346 \t6.697525 \t 0.016150 \t 513.870488 s\n","699 \t 13.654558 \t 7.038089 \t6.591447 \t 0.025022 \t 514.392896 s\n","700 \t 13.506269 \t 6.876357 \t6.601898 \t 0.028012 \t 514.930490 s\n","701 \t 13.424444 \t 6.776401 \t6.632474 \t 0.015569 \t 515.464150 s\n","702 \t 13.531662 \t 6.912555 \t6.604613 \t 0.014493 \t 515.982496 s\n","703 \t 13.581078 \t 6.941852 \t6.616882 \t 0.022343 \t 516.523490 s\n","704 \t 13.456007 \t 6.796471 \t6.652734 \t 0.006803 \t 517.040564 s\n","705 \t 13.517303 \t 6.929555 \t6.573791 \t 0.013957 \t 517.569607 s\n","706 \t 13.559433 \t 6.878463 \t6.667416 \t 0.013554 \t 518.090806 s\n","707 \t 13.566177 \t 6.841903 \t6.712487 \t 0.011787 \t 518.606333 s\n","708 \t 13.532680 \t 6.802412 \t6.714268 \t 0.016000 \t 519.126571 s\n","709 \t 13.613008 \t 6.937131 \t6.666223 \t 0.009654 \t 519.656794 s\n","710 \t 13.392817 \t 6.819291 \t6.560006 \t 0.013520 \t 520.179523 s\n","711 \t 13.442270 \t 6.765025 \t6.665730 \t 0.011515 \t 520.725067 s\n","712 \t 13.562137 \t 6.834676 \t6.714193 \t 0.013268 \t 521.469335 s\n","713 \t 13.495478 \t 6.868718 \t6.618234 \t 0.008526 \t 522.102831 s\n","714 \t 13.388828 \t 6.799697 \t6.573716 \t 0.015414 \t 522.726118 s\n","715 \t 13.503233 \t 6.841864 \t6.651435 \t 0.009935 \t 523.408441 s\n","716 \t 13.402114 \t 6.815922 \t6.573288 \t 0.012904 \t 524.097517 s\n","717 \t 13.396361 \t 6.738646 \t6.644917 \t 0.012798 \t 524.618104 s\n","718 \t 13.359738 \t 6.767759 \t6.575365 \t 0.016614 \t 525.139906 s\n","719 \t 13.471420 \t 6.909726 \t6.548661 \t 0.013034 \t 525.666413 s\n","720 \t 13.602332 \t 6.945871 \t6.621731 \t 0.034730 \t 526.188935 s\n","721 \t 13.454385 \t 6.813845 \t6.624245 \t 0.016295 \t 526.706128 s\n","722 \t 13.519303 \t 6.914137 \t6.585568 \t 0.019598 \t 527.220897 s\n","723 \t 13.493100 \t 6.828032 \t6.653252 \t 0.011816 \t 527.732441 s\n","724 \t 13.582067 \t 6.970815 \t6.594875 \t 0.016377 \t 528.254779 s\n","725 \t 13.375802 \t 6.725548 \t6.637637 \t 0.012616 \t 528.935639 s\n","726 \t 13.300126 \t 6.744543 \t6.545805 \t 0.009778 \t 529.465661 s\n","727 \t 13.455071 \t 6.871287 \t6.562089 \t 0.021695 \t 529.992892 s\n","728 \t 13.414754 \t 6.770415 \t6.630697 \t 0.013642 \t 530.528833 s\n","729 \t 13.228137 \t 6.793962 \t6.417580 \t 0.016596 \t 531.042219 s\n","730 \t 13.349998 \t 6.744312 \t6.589170 \t 0.016516 \t 531.581264 s\n","731 \t 13.428223 \t 6.879642 \t6.529088 \t 0.019494 \t 532.104675 s\n","732 \t 13.447883 \t 6.883218 \t6.556136 \t 0.008528 \t 532.628431 s\n","733 \t 13.393465 \t 6.859792 \t6.522341 \t 0.011331 \t 533.155326 s\n","734 \t 13.390228 \t 6.923818 \t6.445632 \t 0.020778 \t 533.669535 s\n","735 \t 13.289961 \t 6.684597 \t6.594097 \t 0.011267 \t 534.258681 s\n","736 \t 13.376059 \t 6.816704 \t6.552072 \t 0.007283 \t 534.898660 s\n","737 \t 13.339861 \t 6.799304 \t6.527288 \t 0.013270 \t 535.544960 s\n","738 \t 13.400461 \t 6.884689 \t6.505813 \t 0.009959 \t 536.424199 s\n","739 \t 13.308596 \t 6.775454 \t6.523393 \t 0.009749 \t 537.091582 s\n","740 \t 13.295119 \t 6.713608 \t6.569392 \t 0.012119 \t 537.614636 s\n","741 \t 13.412796 \t 6.806373 \t6.587569 \t 0.018855 \t 538.145782 s\n","742 \t 13.265965 \t 6.704472 \t6.552318 \t 0.009176 \t 538.680136 s\n","743 \t 13.094425 \t 6.619043 \t6.456634 \t 0.018748 \t 539.208473 s\n","744 \t 13.189199 \t 6.743925 \t6.433442 \t 0.011832 \t 539.760104 s\n","745 \t 13.315002 \t 6.744104 \t6.556748 \t 0.014150 \t 540.294401 s\n","746 \t 13.316418 \t 6.742795 \t6.548805 \t 0.024818 \t 540.820528 s\n","747 \t 13.210613 \t 6.676576 \t6.524068 \t 0.009969 \t 541.369972 s\n","748 \t 13.242989 \t 6.714579 \t6.514161 \t 0.014249 \t 541.897338 s\n","749 \t 13.277156 \t 6.792484 \t6.474105 \t 0.010568 \t 542.421058 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:27<00:00,  5.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4903\n","Hit@10: 0.6844\n","Hit@3: 0.5319\n","Hit@1: 0.3972\n","Link Prediction on Validation Set (All)\n","MRR: 0.4849\n","Hit@10: 0.7247\n","Hit@3: 0.5332\n","Hit@1: 0.3718\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6277\n","Hit@10: 0.7862\n","Hit@3: 0.6667\n","Hit@1: 0.5535\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5291\n","Hit@10: 0.7525\n","Hit@3: 0.5855\n","Hit@1: 0.4145\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0535\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0535\n","750 \t 13.017792 \t 6.574474 \t6.432729 \t 0.010588 \t 571.642187 s\n","751 \t 13.267171 \t 6.761592 \t6.491703 \t 0.013876 \t 572.344869 s\n","752 \t 13.237112 \t 6.736103 \t6.492472 \t 0.008537 \t 572.878780 s\n","753 \t 13.265102 \t 6.732464 \t6.523517 \t 0.009123 \t 573.540181 s\n","754 \t 13.163155 \t 6.659104 \t6.487588 \t 0.016463 \t 574.171038 s\n","755 \t 13.199490 \t 6.744809 \t6.444568 \t 0.010113 \t 574.812114 s\n","756 \t 13.135226 \t 6.715650 \t6.400387 \t 0.019190 \t 575.540446 s\n","757 \t 13.118303 \t 6.662621 \t6.445570 \t 0.010111 \t 576.194019 s\n","758 \t 13.167066 \t 6.599641 \t6.558552 \t 0.008872 \t 576.827161 s\n","759 \t 13.118266 \t 6.668734 \t6.438896 \t 0.010636 \t 577.487738 s\n","760 \t 13.240061 \t 6.788742 \t6.437619 \t 0.013700 \t 578.128494 s\n","761 \t 13.100452 \t 6.601425 \t6.488689 \t 0.010337 \t 578.734571 s\n","762 \t 13.139369 \t 6.640933 \t6.483001 \t 0.015437 \t 579.362269 s\n","763 \t 13.120647 \t 6.659550 \t6.452272 \t 0.008825 \t 579.962698 s\n","764 \t 13.145388 \t 6.688960 \t6.450991 \t 0.005436 \t 580.661742 s\n","765 \t 13.143945 \t 6.623630 \t6.510870 \t 0.009445 \t 581.200758 s\n","766 \t 13.221889 \t 6.764886 \t6.449267 \t 0.007737 \t 581.725888 s\n","767 \t 13.019414 \t 6.604519 \t6.398535 \t 0.016360 \t 582.258764 s\n","768 \t 13.161069 \t 6.729380 \t6.416299 \t 0.015391 \t 582.781338 s\n","769 \t 13.095129 \t 6.663991 \t6.424149 \t 0.006988 \t 583.317657 s\n","770 \t 13.229297 \t 6.703036 \t6.516562 \t 0.009700 \t 583.842617 s\n","771 \t 13.150424 \t 6.696999 \t6.443992 \t 0.009434 \t 584.381151 s\n","772 \t 13.065681 \t 6.646464 \t6.407384 \t 0.011833 \t 584.908786 s\n","773 \t 13.140711 \t 6.712987 \t6.418812 \t 0.008911 \t 585.483135 s\n","774 \t 13.035262 \t 6.666723 \t6.355737 \t 0.012802 \t 586.042006 s\n","775 \t 12.947268 \t 6.547154 \t6.389080 \t 0.011034 \t 586.752380 s\n","776 \t 13.009292 \t 6.599167 \t6.402367 \t 0.007759 \t 587.667128 s\n","777 \t 12.838453 \t 6.479700 \t6.342271 \t 0.016483 \t 588.322698 s\n","778 \t 12.958163 \t 6.537207 \t6.409860 \t 0.011096 \t 589.080700 s\n","779 \t 12.990934 \t 6.564350 \t6.418683 \t 0.007901 \t 589.681218 s\n","780 \t 13.156523 \t 6.703268 \t6.442915 \t 0.010340 \t 590.212180 s\n","781 \t 12.871730 \t 6.575209 \t6.291665 \t 0.004856 \t 590.742938 s\n","782 \t 13.069945 \t 6.715871 \t6.345698 \t 0.008376 \t 591.271911 s\n","783 \t 13.113815 \t 6.648673 \t6.450112 \t 0.015030 \t 591.797772 s\n","784 \t 12.951321 \t 6.590160 \t6.345116 \t 0.016045 \t 592.327981 s\n","785 \t 13.026692 \t 6.614667 \t6.396944 \t 0.015081 \t 592.854553 s\n","786 \t 12.906811 \t 6.535587 \t6.361497 \t 0.009727 \t 593.399075 s\n","787 \t 12.999824 \t 6.573060 \t6.411938 \t 0.014826 \t 593.953389 s\n","788 \t 13.032532 \t 6.591723 \t6.431999 \t 0.008810 \t 594.495627 s\n","789 \t 13.025694 \t 6.630172 \t6.387784 \t 0.007737 \t 595.195055 s\n","790 \t 12.968856 \t 6.574271 \t6.379939 \t 0.014645 \t 595.733149 s\n","791 \t 13.050258 \t 6.665095 \t6.369207 \t 0.015956 \t 596.263637 s\n","792 \t 12.971773 \t 6.595730 \t6.360869 \t 0.015174 \t 596.804961 s\n","793 \t 13.071857 \t 6.585985 \t6.470538 \t 0.015334 \t 597.330139 s\n","794 \t 12.948349 \t 6.532137 \t6.407542 \t 0.008669 \t 597.869193 s\n","795 \t 12.922986 \t 6.551105 \t6.356954 \t 0.014926 \t 598.401457 s\n","796 \t 13.000660 \t 6.628268 \t6.365116 \t 0.007277 \t 598.942813 s\n","797 \t 12.901614 \t 6.567653 \t6.325067 \t 0.008894 \t 599.529433 s\n","798 \t 12.996941 \t 6.609971 \t6.380893 \t 0.006077 \t 600.176097 s\n","799 \t 12.957905 \t 6.572304 \t6.374893 \t 0.010708 \t 600.805438 s\n","800 \t 13.050471 \t 6.637857 \t6.396591 \t 0.016022 \t 601.482739 s\n","801 \t 12.864885 \t 6.514834 \t6.341006 \t 0.009045 \t 602.347044 s\n","802 \t 12.905536 \t 6.599372 \t6.289895 \t 0.016269 \t 602.871058 s\n","803 \t 12.847685 \t 6.537313 \t6.297370 \t 0.013003 \t 603.409164 s\n","804 \t 12.924559 \t 6.523507 \t6.393230 \t 0.007822 \t 603.938421 s\n","805 \t 12.853946 \t 6.548921 \t6.294238 \t 0.010787 \t 604.488755 s\n","806 \t 12.931524 \t 6.579753 \t6.342560 \t 0.009211 \t 605.010274 s\n","807 \t 12.985277 \t 6.632606 \t6.340317 \t 0.012355 \t 605.554821 s\n","808 \t 12.973094 \t 6.639629 \t6.327965 \t 0.005499 \t 606.076374 s\n","809 \t 12.841137 \t 6.535918 \t6.292558 \t 0.012662 \t 606.613498 s\n","810 \t 12.838643 \t 6.516029 \t6.315268 \t 0.007347 \t 607.140615 s\n","811 \t 13.013448 \t 6.610003 \t6.388466 \t 0.014979 \t 607.671927 s\n","812 \t 12.710104 \t 6.451723 \t6.252210 \t 0.006171 \t 608.201221 s\n","813 \t 12.849316 \t 6.539391 \t6.301797 \t 0.008128 \t 608.740347 s\n","814 \t 12.939679 \t 6.577727 \t6.356471 \t 0.005482 \t 609.470458 s\n","815 \t 12.861546 \t 6.596460 \t6.257386 \t 0.007701 \t 610.000860 s\n","816 \t 12.820687 \t 6.527650 \t6.287759 \t 0.005278 \t 610.538039 s\n","817 \t 12.795993 \t 6.466772 \t6.321866 \t 0.007355 \t 611.062602 s\n","818 \t 12.779906 \t 6.509011 \t6.260713 \t 0.010184 \t 611.590269 s\n","819 \t 12.975026 \t 6.630388 \t6.322512 \t 0.022125 \t 612.121291 s\n","820 \t 12.930153 \t 6.568191 \t6.351280 \t 0.010683 \t 612.779965 s\n","821 \t 12.762511 \t 6.466657 \t6.285817 \t 0.010036 \t 613.410586 s\n","822 \t 12.911371 \t 6.563753 \t6.334987 \t 0.012630 \t 614.062726 s\n","823 \t 12.818987 \t 6.520461 \t6.291451 \t 0.007075 \t 614.735342 s\n","824 \t 12.744701 \t 6.459427 \t6.277667 \t 0.007606 \t 615.375175 s\n","825 \t 12.806486 \t 6.513973 \t6.285858 \t 0.006653 \t 615.916781 s\n","826 \t 12.765100 \t 6.474819 \t6.281024 \t 0.009258 \t 616.634428 s\n","827 \t 12.857212 \t 6.558798 \t6.291736 \t 0.006678 \t 617.187459 s\n","828 \t 12.870478 \t 6.518557 \t6.341546 \t 0.010375 \t 617.725258 s\n","829 \t 12.765189 \t 6.472270 \t6.285180 \t 0.007739 \t 618.257215 s\n","830 \t 12.819120 \t 6.502932 \t6.305990 \t 0.010198 \t 618.796248 s\n","831 \t 12.917885 \t 6.583151 \t6.330568 \t 0.004167 \t 619.330699 s\n","832 \t 12.868046 \t 6.552719 \t6.308592 \t 0.006735 \t 619.871578 s\n","833 \t 12.630222 \t 6.409728 \t6.210957 \t 0.009537 \t 620.401414 s\n","834 \t 12.743479 \t 6.447685 \t6.285522 \t 0.010273 \t 620.935192 s\n","835 \t 12.827505 \t 6.526350 \t6.293155 \t 0.008000 \t 621.468423 s\n","836 \t 12.672968 \t 6.432303 \t6.229420 \t 0.011245 \t 621.995900 s\n","837 \t 12.707315 \t 6.442208 \t6.256335 \t 0.008773 \t 622.530867 s\n","838 \t 12.684241 \t 6.485090 \t6.195843 \t 0.003307 \t 623.064886 s\n","839 \t 12.712777 \t 6.480618 \t6.226492 \t 0.005666 \t 623.775757 s\n","840 \t 12.832425 \t 6.561374 \t6.257768 \t 0.013282 \t 624.340890 s\n","841 \t 12.809465 \t 6.472693 \t6.323291 \t 0.013481 \t 624.879939 s\n","842 \t 12.776346 \t 6.527355 \t6.241781 \t 0.007210 \t 625.484735 s\n","843 \t 12.717086 \t 6.446994 \t6.265188 \t 0.004904 \t 626.124046 s\n","844 \t 12.747898 \t 6.454782 \t6.280674 \t 0.012441 \t 626.747107 s\n","845 \t 12.638553 \t 6.378312 \t6.254744 \t 0.005497 \t 627.424214 s\n","846 \t 12.704367 \t 6.478601 \t6.219156 \t 0.006611 \t 628.150142 s\n","847 \t 12.774980 \t 6.484591 \t6.286570 \t 0.003818 \t 628.671740 s\n","848 \t 12.800332 \t 6.554297 \t6.237463 \t 0.008571 \t 629.209830 s\n","849 \t 12.676260 \t 6.460150 \t6.207318 \t 0.008792 \t 629.738791 s\n","850 \t 12.782442 \t 6.547984 \t6.221393 \t 0.013065 \t 630.279249 s\n","851 \t 12.771134 \t 6.469031 \t6.292972 \t 0.009132 \t 630.995527 s\n","852 \t 12.878607 \t 6.569916 \t6.302017 \t 0.006674 \t 631.542727 s\n","853 \t 12.627868 \t 6.379481 \t6.240904 \t 0.007484 \t 632.083103 s\n","854 \t 12.664748 \t 6.467505 \t6.188784 \t 0.008459 \t 632.628198 s\n","855 \t 12.645927 \t 6.441908 \t6.194971 \t 0.009048 \t 633.166448 s\n","856 \t 12.693724 \t 6.459422 \t6.228137 \t 0.006166 \t 633.694048 s\n","857 \t 12.761144 \t 6.538123 \t6.217679 \t 0.005343 \t 634.226713 s\n","858 \t 12.586463 \t 6.384099 \t6.195789 \t 0.006577 \t 634.752332 s\n","859 \t 12.724033 \t 6.484917 \t6.232019 \t 0.007097 \t 635.291654 s\n","860 \t 12.704545 \t 6.492358 \t6.200567 \t 0.011619 \t 635.818553 s\n","861 \t 12.622201 \t 6.406077 \t6.208691 \t 0.007434 \t 636.359977 s\n","862 \t 12.576982 \t 6.392855 \t6.174772 \t 0.009356 \t 636.890646 s\n","863 \t 12.701377 \t 6.506624 \t6.184862 \t 0.009892 \t 637.443096 s\n","864 \t 12.752970 \t 6.521365 \t6.220000 \t 0.011604 \t 638.144718 s\n","865 \t 12.683836 \t 6.461507 \t6.213196 \t 0.009133 \t 638.824183 s\n","866 \t 12.733499 \t 6.515900 \t6.210825 \t 0.006773 \t 639.482069 s\n","867 \t 12.655914 \t 6.444365 \t6.205410 \t 0.006139 \t 640.115203 s\n","868 \t 12.715009 \t 6.481955 \t6.225795 \t 0.007259 \t 640.818496 s\n","869 \t 12.729799 \t 6.435230 \t6.286823 \t 0.007746 \t 641.456091 s\n","870 \t 12.662848 \t 6.452577 \t6.198779 \t 0.011493 \t 641.996206 s\n","871 \t 12.678012 \t 6.443341 \t6.226611 \t 0.008060 \t 642.526962 s\n","872 \t 12.626220 \t 6.412985 \t6.206095 \t 0.007140 \t 643.070050 s\n","873 \t 12.702852 \t 6.457448 \t6.240175 \t 0.005229 \t 643.609064 s\n","874 \t 12.783343 \t 6.493304 \t6.284996 \t 0.005042 \t 644.136889 s\n","875 \t 12.686239 \t 6.451151 \t6.229650 \t 0.005438 \t 644.671163 s\n","876 \t 12.611616 \t 6.365845 \t6.237408 \t 0.008363 \t 645.204691 s\n","877 \t 12.687174 \t 6.473677 \t6.205798 \t 0.007699 \t 645.913777 s\n","878 \t 12.775866 \t 6.546468 \t6.223053 \t 0.006346 \t 646.450927 s\n","879 \t 12.601480 \t 6.446577 \t6.148249 \t 0.006654 \t 646.988629 s\n","880 \t 12.623868 \t 6.411133 \t6.208380 \t 0.004355 \t 647.511608 s\n","881 \t 12.620841 \t 6.426701 \t6.185603 \t 0.008536 \t 648.039391 s\n","882 \t 12.680402 \t 6.476482 \t6.189081 \t 0.014839 \t 648.561988 s\n","883 \t 12.605698 \t 6.390741 \t6.204570 \t 0.010386 \t 649.081798 s\n","884 \t 12.579700 \t 6.400606 \t6.173911 \t 0.005183 \t 649.594191 s\n","885 \t 12.554203 \t 6.393527 \t6.153954 \t 0.006722 \t 650.120643 s\n","886 \t 12.581833 \t 6.416265 \t6.158054 \t 0.007515 \t 650.635608 s\n","887 \t 12.623456 \t 6.411932 \t6.204950 \t 0.006573 \t 651.164913 s\n","888 \t 12.492352 \t 6.337111 \t6.147053 \t 0.008188 \t 651.812180 s\n","889 \t 12.657834 \t 6.448033 \t6.199746 \t 0.010055 \t 652.461282 s\n","890 \t 12.623546 \t 6.468050 \t6.149964 \t 0.005532 \t 653.330589 s\n","891 \t 12.634162 \t 6.445308 \t6.182050 \t 0.006804 \t 654.031588 s\n","892 \t 12.628804 \t 6.444168 \t6.177697 \t 0.006939 \t 654.608427 s\n","893 \t 12.507202 \t 6.372178 \t6.124037 \t 0.010987 \t 655.134110 s\n","894 \t 12.640974 \t 6.409831 \t6.223771 \t 0.007371 \t 655.680912 s\n","895 \t 12.559985 \t 6.394261 \t6.153101 \t 0.012623 \t 656.209428 s\n","896 \t 12.591105 \t 6.432127 \t6.152751 \t 0.006227 \t 656.725956 s\n","897 \t 12.587067 \t 6.438604 \t6.144084 \t 0.004379 \t 657.252950 s\n","898 \t 12.534385 \t 6.410940 \t6.117536 \t 0.005909 \t 657.772904 s\n","899 \t 12.641197 \t 6.503065 \t6.124280 \t 0.013852 \t 658.306508 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:27<00:00,  5.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5012\n","Hit@10: 0.6560\n","Hit@3: 0.5319\n","Hit@1: 0.4149\n","Link Prediction on Validation Set (All)\n","MRR: 0.5062\n","Hit@10: 0.7152\n","Hit@3: 0.5570\n","Hit@1: 0.3956\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6315\n","Hit@10: 0.7610\n","Hit@3: 0.6667\n","Hit@1: 0.5597\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5549\n","Hit@10: 0.7466\n","Hit@3: 0.6169\n","Hit@1: 0.4538\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0330\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0330\n","900 \t 12.566323 \t 6.361806 \t6.196944 \t 0.007572 \t 687.451681 s\n","901 \t 12.480313 \t 6.372347 \t6.102943 \t 0.005022 \t 687.964744 s\n","902 \t 12.473140 \t 6.329574 \t6.137442 \t 0.006125 \t 688.657372 s\n","903 \t 12.667759 \t 6.474715 \t6.185300 \t 0.007744 \t 689.182065 s\n","904 \t 12.572110 \t 6.403321 \t6.162485 \t 0.006305 \t 689.704909 s\n","905 \t 12.463570 \t 6.355586 \t6.101974 \t 0.006010 \t 690.282350 s\n","906 \t 12.511016 \t 6.346446 \t6.158347 \t 0.006223 \t 690.925620 s\n","907 \t 12.588036 \t 6.413332 \t6.167269 \t 0.007434 \t 691.584103 s\n","908 \t 12.620024 \t 6.414622 \t6.195032 \t 0.010369 \t 692.200532 s\n","909 \t 12.533945 \t 6.390157 \t6.137953 \t 0.005834 \t 692.881629 s\n","910 \t 12.605510 \t 6.459509 \t6.139090 \t 0.006911 \t 693.740763 s\n","911 \t 12.490130 \t 6.355635 \t6.125525 \t 0.008970 \t 694.341680 s\n","912 \t 12.640237 \t 6.463627 \t6.170166 \t 0.006444 \t 694.960811 s\n","913 \t 12.540942 \t 6.398816 \t6.132555 \t 0.009571 \t 695.620075 s\n","914 \t 12.533346 \t 6.384847 \t6.143477 \t 0.005023 \t 696.231863 s\n","915 \t 12.623168 \t 6.432845 \t6.180984 \t 0.009339 \t 696.919832 s\n","916 \t 12.444013 \t 6.308572 \t6.129555 \t 0.005885 \t 697.449281 s\n","917 \t 12.616760 \t 6.444674 \t6.164929 \t 0.007156 \t 697.979848 s\n","918 \t 12.466302 \t 6.300509 \t6.157139 \t 0.008655 \t 698.516590 s\n","919 \t 12.582182 \t 6.451434 \t6.123445 \t 0.007304 \t 699.048204 s\n","920 \t 12.592620 \t 6.439872 \t6.141892 \t 0.010856 \t 699.573658 s\n","921 \t 12.617034 \t 6.454033 \t6.155120 \t 0.007881 \t 700.100008 s\n","922 \t 12.577466 \t 6.408075 \t6.161994 \t 0.007397 \t 700.629984 s\n","923 \t 12.537184 \t 6.431961 \t6.099159 \t 0.006064 \t 701.160697 s\n","924 \t 12.575258 \t 6.368271 \t6.198866 \t 0.008122 \t 701.680106 s\n","925 \t 12.626199 \t 6.438202 \t6.183671 \t 0.004327 \t 702.225969 s\n","926 \t 12.541150 \t 6.422911 \t6.112538 \t 0.005700 \t 702.746374 s\n","927 \t 12.503879 \t 6.349734 \t6.146344 \t 0.007800 \t 703.303278 s\n","928 \t 12.559677 \t 6.372808 \t6.178901 \t 0.007968 \t 704.191208 s\n","929 \t 12.417183 \t 6.299943 \t6.111928 \t 0.005312 \t 704.812958 s\n","930 \t 12.538105 \t 6.402144 \t6.130545 \t 0.005416 \t 705.472520 s\n","931 \t 12.575364 \t 6.409402 \t6.161225 \t 0.004737 \t 706.184738 s\n","932 \t 12.417737 \t 6.295902 \t6.114519 \t 0.007316 \t 706.733310 s\n","933 \t 12.584791 \t 6.426099 \t6.151723 \t 0.006968 \t 707.259256 s\n","934 \t 12.593568 \t 6.452917 \t6.133411 \t 0.007241 \t 707.773766 s\n","935 \t 12.525899 \t 6.412915 \t6.107455 \t 0.005529 \t 708.302033 s\n","936 \t 12.490209 \t 6.359888 \t6.124100 \t 0.006221 \t 708.827784 s\n","937 \t 12.551955 \t 6.373599 \t6.172996 \t 0.005361 \t 709.355720 s\n","938 \t 12.389369 \t 6.268897 \t6.116189 \t 0.004282 \t 709.877977 s\n","939 \t 12.540590 \t 6.391009 \t6.140564 \t 0.009017 \t 710.403954 s\n","940 \t 12.512946 \t 6.364130 \t6.141743 \t 0.007072 \t 710.935843 s\n","941 \t 12.560792 \t 6.398678 \t6.155565 \t 0.006549 \t 711.637246 s\n","942 \t 12.455613 \t 6.327298 \t6.123048 \t 0.005266 \t 712.164384 s\n","943 \t 12.513740 \t 6.362357 \t6.141971 \t 0.009412 \t 712.705929 s\n","944 \t 12.503019 \t 6.378079 \t6.119285 \t 0.005655 \t 713.230924 s\n","945 \t 12.589274 \t 6.416054 \t6.169276 \t 0.003944 \t 713.762720 s\n","946 \t 12.460021 \t 6.319736 \t6.132919 \t 0.007366 \t 714.281141 s\n","947 \t 12.551867 \t 6.385049 \t6.160605 \t 0.006212 \t 714.820718 s\n","948 \t 12.451104 \t 6.364417 \t6.080526 \t 0.006161 \t 715.354569 s\n","949 \t 12.499581 \t 6.378300 \t6.114412 \t 0.006869 \t 715.876484 s\n","950 \t 12.404580 \t 6.300627 \t6.097109 \t 0.006844 \t 716.467704 s\n","951 \t 12.471103 \t 6.330666 \t6.134567 \t 0.005869 \t 717.110768 s\n","952 \t 12.524712 \t 6.372050 \t6.146569 \t 0.006092 \t 717.740433 s\n","953 \t 12.552089 \t 6.438067 \t6.105712 \t 0.008310 \t 718.357035 s\n","954 \t 12.490287 \t 6.394095 \t6.088729 \t 0.007463 \t 719.274831 s\n","955 \t 12.551141 \t 6.444461 \t6.101434 \t 0.005246 \t 719.796787 s\n","956 \t 12.555653 \t 6.412425 \t6.137418 \t 0.005809 \t 720.319498 s\n","957 \t 12.442284 \t 6.339805 \t6.096519 \t 0.005960 \t 720.842597 s\n","958 \t 12.512926 \t 6.335329 \t6.171537 \t 0.006060 \t 721.367275 s\n","959 \t 12.504566 \t 6.387913 \t6.108383 \t 0.008270 \t 721.890511 s\n","960 \t 12.511154 \t 6.340880 \t6.163051 \t 0.007222 \t 722.417652 s\n","961 \t 12.513995 \t 6.393015 \t6.114843 \t 0.006137 \t 722.985670 s\n","962 \t 12.554448 \t 6.411014 \t6.135342 \t 0.008091 \t 723.506945 s\n","963 \t 12.468637 \t 6.320456 \t6.142653 \t 0.005527 \t 724.035677 s\n","964 \t 12.554055 \t 6.431405 \t6.115297 \t 0.007353 \t 724.562735 s\n","965 \t 12.473485 \t 6.333988 \t6.133143 \t 0.006354 \t 725.100020 s\n","966 \t 12.449240 \t 6.319205 \t6.125488 \t 0.004548 \t 725.616912 s\n","967 \t 12.551438 \t 6.421338 \t6.123730 \t 0.006370 \t 726.303586 s\n","968 \t 12.520285 \t 6.367295 \t6.146332 \t 0.006657 \t 726.822795 s\n","969 \t 12.555734 \t 6.412268 \t6.134340 \t 0.009126 \t 727.343977 s\n","970 \t 12.519944 \t 6.396714 \t6.117061 \t 0.006170 \t 727.870630 s\n","971 \t 12.533705 \t 6.431436 \t6.097124 \t 0.005145 \t 728.386342 s\n","972 \t 12.474240 \t 6.364579 \t6.102858 \t 0.006802 \t 728.919755 s\n","973 \t 12.464559 \t 6.362254 \t6.096812 \t 0.005493 \t 729.522350 s\n","974 \t 12.583511 \t 6.451157 \t6.125906 \t 0.006449 \t 730.175879 s\n","975 \t 12.346554 \t 6.288234 \t6.052880 \t 0.005441 \t 730.797509 s\n","976 \t 12.522526 \t 6.388292 \t6.126754 \t 0.007480 \t 731.443998 s\n","977 \t 12.575688 \t 6.454094 \t6.114485 \t 0.007109 \t 732.164263 s\n","978 \t 12.403889 \t 6.298651 \t6.099316 \t 0.005922 \t 732.697564 s\n","979 \t 12.535843 \t 6.392859 \t6.137628 \t 0.005356 \t 733.219677 s\n","980 \t 12.471862 \t 6.376913 \t6.088468 \t 0.006480 \t 733.906615 s\n","981 \t 12.434747 \t 6.352903 \t6.072488 \t 0.009356 \t 734.437718 s\n","982 \t 12.517315 \t 6.374516 \t6.136944 \t 0.005856 \t 734.957548 s\n","983 \t 12.517985 \t 6.384247 \t6.127798 \t 0.005940 \t 735.484058 s\n","984 \t 12.518450 \t 6.382644 \t6.127456 \t 0.008350 \t 735.997290 s\n","985 \t 12.453018 \t 6.328053 \t6.117843 \t 0.007121 \t 736.522914 s\n","986 \t 12.442161 \t 6.346260 \t6.091109 \t 0.004792 \t 737.037614 s\n","987 \t 12.449984 \t 6.347521 \t6.095092 \t 0.007371 \t 737.576261 s\n","988 \t 12.330438 \t 6.257622 \t6.068770 \t 0.004046 \t 738.090160 s\n","989 \t 12.436538 \t 6.266739 \t6.156788 \t 0.013010 \t 738.620240 s\n","990 \t 12.458106 \t 6.355098 \t6.094732 \t 0.008277 \t 739.138583 s\n","991 \t 12.441584 \t 6.311880 \t6.121832 \t 0.007872 \t 739.671684 s\n","992 \t 12.467132 \t 6.354309 \t6.106786 \t 0.006038 \t 740.191399 s\n","993 \t 12.496292 \t 6.351821 \t6.137596 \t 0.006875 \t 740.888274 s\n","994 \t 12.543539 \t 6.388806 \t6.147605 \t 0.007127 \t 741.413450 s\n","995 \t 12.365701 \t 6.297266 \t6.063871 \t 0.004564 \t 741.932354 s\n","996 \t 12.463220 \t 6.315311 \t6.143337 \t 0.004570 \t 742.525306 s\n","997 \t 12.359334 \t 6.300097 \t6.050388 \t 0.008851 \t 743.158920 s\n","998 \t 12.493307 \t 6.374650 \t6.111332 \t 0.007325 \t 743.780907 s\n","999 \t 12.463128 \t 6.333003 \t6.124031 \t 0.006094 \t 744.400233 s\n","1000 \t 12.428211 \t 6.320808 \t6.101781 \t 0.005622 \t 745.127681 s\n","1001 \t 12.439628 \t 6.284242 \t6.150142 \t 0.005244 \t 745.698695 s\n","1002 \t 12.441786 \t 6.312181 \t6.123878 \t 0.005726 \t 746.216932 s\n","1003 \t 12.541352 \t 6.406571 \t6.131533 \t 0.003248 \t 746.745578 s\n","1004 \t 12.445629 \t 6.289704 \t6.148628 \t 0.007297 \t 747.262861 s\n","1005 \t 12.425170 \t 6.294887 \t6.123229 \t 0.007054 \t 747.782848 s\n","1006 \t 12.490210 \t 6.380088 \t6.102173 \t 0.007949 \t 748.462337 s\n","1007 \t 12.386294 \t 6.297504 \t6.079575 \t 0.009216 \t 748.978057 s\n","1008 \t 12.479206 \t 6.359085 \t6.115824 \t 0.004296 \t 749.496018 s\n","1009 \t 12.462843 \t 6.359553 \t6.099136 \t 0.004155 \t 750.014362 s\n","1010 \t 12.465565 \t 6.341259 \t6.118532 \t 0.005774 \t 750.536525 s\n","1011 \t 12.498736 \t 6.399478 \t6.094651 \t 0.004607 \t 751.057050 s\n","1012 \t 12.435346 \t 6.312275 \t6.115627 \t 0.007443 \t 751.569908 s\n","1013 \t 12.346202 \t 6.264516 \t6.074618 \t 0.007069 \t 752.090983 s\n","1014 \t 12.472657 \t 6.361973 \t6.105961 \t 0.004723 \t 752.609156 s\n","1015 \t 12.519481 \t 6.416960 \t6.097635 \t 0.004885 \t 753.141269 s\n","1016 \t 12.343142 \t 6.294065 \t6.044652 \t 0.004425 \t 753.656828 s\n","1017 \t 12.365849 \t 6.296343 \t6.062031 \t 0.007475 \t 754.191970 s\n","1018 \t 12.455798 \t 6.346504 \t6.100039 \t 0.009254 \t 754.884360 s\n","1019 \t 12.545426 \t 6.422927 \t6.116764 \t 0.005735 \t 755.475380 s\n","1020 \t 12.481745 \t 6.354848 \t6.117424 \t 0.009472 \t 756.114206 s\n","1021 \t 12.574895 \t 6.443470 \t6.123534 \t 0.007892 \t 756.728103 s\n","1022 \t 12.460863 \t 6.362443 \t6.094567 \t 0.003854 \t 757.365325 s\n","1023 \t 12.478076 \t 6.359508 \t6.114205 \t 0.004363 \t 758.081277 s\n","1024 \t 12.518275 \t 6.389241 \t6.124452 \t 0.004581 \t 758.657140 s\n","1025 \t 12.388423 \t 6.284573 \t6.097930 \t 0.005919 \t 759.206605 s\n","1026 \t 12.350727 \t 6.283452 \t6.060318 \t 0.006957 \t 759.737264 s\n","1027 \t 12.360809 \t 6.254944 \t6.096430 \t 0.009434 \t 760.272166 s\n","1028 \t 12.431534 \t 6.293074 \t6.131399 \t 0.007061 \t 760.798722 s\n","1029 \t 12.469462 \t 6.368386 \t6.093460 \t 0.007616 \t 761.333401 s\n","1030 \t 12.567100 \t 6.414410 \t6.147712 \t 0.004979 \t 761.856930 s\n","1031 \t 12.450382 \t 6.354038 \t6.089845 \t 0.006498 \t 762.567043 s\n","1032 \t 12.442972 \t 6.286510 \t6.150007 \t 0.006455 \t 763.097707 s\n","1033 \t 12.448406 \t 6.295644 \t6.144292 \t 0.008469 \t 763.633798 s\n","1034 \t 12.511804 \t 6.427676 \t6.078092 \t 0.006036 \t 764.159753 s\n","1035 \t 12.479864 \t 6.359724 \t6.115426 \t 0.004714 \t 764.700238 s\n","1036 \t 12.426377 \t 6.361354 \t6.060108 \t 0.004915 \t 765.229595 s\n","1037 \t 12.520766 \t 6.421015 \t6.093140 \t 0.006611 \t 765.766991 s\n","1038 \t 12.437226 \t 6.324630 \t6.108336 \t 0.004261 \t 766.289439 s\n","1039 \t 12.550789 \t 6.411990 \t6.134293 \t 0.004506 \t 766.812927 s\n","1040 \t 12.319951 \t 6.230368 \t6.082636 \t 0.006947 \t 767.346861 s\n","1041 \t 12.361857 \t 6.275043 \t6.082358 \t 0.004456 \t 767.863842 s\n","1042 \t 12.368880 \t 6.271954 \t6.092081 \t 0.004845 \t 768.472649 s\n","1043 \t 12.461953 \t 6.318254 \t6.134589 \t 0.009109 \t 769.092756 s\n","1044 \t 12.450620 \t 6.360777 \t6.084280 \t 0.005563 \t 769.925168 s\n","1045 \t 12.496510 \t 6.387629 \t6.103500 \t 0.005381 \t 770.598743 s\n","1046 \t 12.568990 \t 6.503735 \t6.059867 \t 0.005389 \t 771.267639 s\n","1047 \t 12.423486 \t 6.338169 \t6.079684 \t 0.005634 \t 771.786109 s\n","1048 \t 12.390287 \t 6.304883 \t6.080314 \t 0.005090 \t 772.308613 s\n","1049 \t 12.534338 \t 6.422214 \t6.106596 \t 0.005529 \t 772.826001 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:27<00:00,  5.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5075\n","Hit@10: 0.6525\n","Hit@3: 0.5390\n","Hit@1: 0.4184\n","Link Prediction on Validation Set (All)\n","MRR: 0.5085\n","Hit@10: 0.7136\n","Hit@3: 0.5633\n","Hit@1: 0.3972\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6375\n","Hit@10: 0.7610\n","Hit@3: 0.6604\n","Hit@1: 0.5660\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5583\n","Hit@10: 0.7505\n","Hit@3: 0.6189\n","Hit@1: 0.4558\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0307\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0307\n"]}]},{"cell_type":"markdown","source":["# Test.py\n"],"metadata":{"id":"-BsiHiMQoArV"}},{"cell_type":"code","source":["KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix.cuda(), # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix.cuda(), # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix.cuda(), # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix.cuda(), # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask.cuda(), # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask.cuda(), # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTHNKG-NT/checkpoint/Reproduce/VTHN/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","                else: # qualifier\n","                    sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n"],"metadata":{"id":"ChVIC_5BHELi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746719973861,"user_tz":-540,"elapsed":40202,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"52dfd0bc-40b6-46b9-b272-1ed55bf65c46"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["  1%|          | 1/161 [00:00<00:16,  9.63it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0033, SE: 0.000011\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 2/161 [00:00<00:35,  4.49it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 1.0000, Pred: 1.0914, SE: 0.008358\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 10/161 [00:02<00:28,  5.21it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 1.0000, Pred: 1.0802, SE: 0.006425\n","[Triplet Num] GT: 0.0508, Pred: 0.0130, SE: 0.001429\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 12/161 [00:02<00:28,  5.19it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0002, Pred: -0.0211, SE: 0.000453\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 17/161 [00:03<00:32,  4.44it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0169, Pred: -0.0112, SE: 0.000791\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 26/161 [00:05<00:23,  5.76it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0746, Pred: 0.0268, SE: 0.002282\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 27/161 [00:05<00:24,  5.37it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0154, SE: 0.000237\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|█▉        | 32/161 [00:06<00:25,  5.04it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.3467, Pred: 0.3412, SE: 0.000031\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 44/161 [00:09<00:24,  4.70it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0339, Pred: 0.0165, SE: 0.000302\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▊       | 46/161 [00:09<00:20,  5.60it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0239, SE: 0.000571\n"]},{"output_type":"stream","name":"stderr","text":[" 31%|███       | 50/161 [00:10<00:20,  5.49it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0022, SE: 0.000005\n"]},{"output_type":"stream","name":"stderr","text":[" 37%|███▋      | 59/161 [00:11<00:21,  4.79it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0007, Pred: -0.0131, SE: 0.000190\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 65/161 [00:13<00:21,  4.44it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0366, SE: 0.001337\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 67/161 [00:13<00:19,  4.93it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0007, Pred: -0.0061, SE: 0.000046\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 76/161 [00:15<00:18,  4.53it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.1206, Pred: 0.0998, SE: 0.000435\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 83/161 [00:17<00:15,  4.88it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0045, Pred: 0.0020, SE: 0.000006\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▌    | 89/161 [00:18<00:14,  5.01it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0508, Pred: 0.0203, SE: 0.000934\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 110/161 [00:22<00:10,  4.69it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0508, Pred: 0.0168, SE: 0.001159\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|██████▉   | 112/161 [00:22<00:08,  5.54it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0169, Pred: -0.0138, SE: 0.000948\n"]},{"output_type":"stream","name":"stderr","text":[" 81%|████████  | 130/161 [00:26<00:07,  4.07it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.1525, Pred: 0.1490, SE: 0.000013\n"]},{"output_type":"stream","name":"stderr","text":[" 83%|████████▎ | 133/161 [00:27<00:05,  4.72it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: 0.0118, SE: 0.000138\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 140/161 [00:28<00:05,  3.75it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0045, Pred: -0.0097, SE: 0.000201\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 145/161 [00:29<00:03,  4.54it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0795, Pred: -0.0073, SE: 0.007533\n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████▏| 147/161 [00:30<00:02,  5.32it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0273, SE: 0.000746\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 154/161 [00:31<00:00,  7.25it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 1.0000, Pred: 1.1110, SE: 0.012310\n","[Triplet Num] GT: 1.0000, Pred: 1.0950, SE: 0.009028\n","[Triplet Num] GT: 0.0000, Pred: -0.0066, SE: 0.000044\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 161/161 [00:32<00:00,  4.96it/s]"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5609\n","Hit@10: 0.7143\n","Hit@3: 0.5850\n","Hit@1: 0.4830\n","Link Prediction on Validation Set (All)\n","MRR: 0.5278\n","Hit@10: 0.7176\n","Hit@3: 0.5691\n","Hit@1: 0.4324\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5895\n","Hit@10: 0.7578\n","Hit@3: 0.6460\n","Hit@1: 0.4969\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5027\n","Hit@10: 0.7148\n","Hit@3: 0.5649\n","Hit@1: 0.3931\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0447\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0447\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}