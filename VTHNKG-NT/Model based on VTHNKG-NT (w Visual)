{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747360082237,"user_tz":-540,"elapsed":42951,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"706f3090-f92a-42e1-c2c9-c634285fb1e6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1747360091994,"user_tz":-540,"elapsed":4368,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1747360097837,"user_tz":-540,"elapsed":18,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0: # numeric triplet 처리\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","        else:\n","            self.proj_ent_vis = nn.Identity()\n","            self.proj_rel_vis = nn.Identity()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1747360099193,"user_tz":-540,"elapsed":23,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1747360101781,"user_tz":-540,"elapsed":36,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-NT/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile(self.dir + 'visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(self.dir + 'visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-NT/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1747361246510,"user_tz":-540,"elapsed":17,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"87ee3016-54f8-476e-a881-0057f90bbd48"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-NT\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNNTV_maximg==3\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()"],"metadata":{"id":"HmgS2m1upzp1","executionInfo":{"status":"ok","timestamp":1747361247747,"user_tz":-540,"elapsed":70,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747360981716,"user_tz":-540,"elapsed":325412,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"878f1953-126a-47e8-8dcc-c161bd0b0761"},"execution_count":8,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 65.350249 \t 11.200663 \t11.609435 \t 42.540151 \t 2.030296 s\n","1 \t 29.594442 \t 10.711087 \t11.311655 \t 7.571700 \t 2.582119 s\n","2 \t 23.523794 \t 10.594780 \t10.994748 \t 1.934266 \t 3.174025 s\n","3 \t 21.818169 \t 10.319765 \t10.873240 \t 0.625163 \t 3.963655 s\n","4 \t 21.197461 \t 10.075526 \t10.567182 \t 0.554754 \t 4.621186 s\n","5 \t 22.167296 \t 10.116246 \t10.421124 \t 1.629926 \t 5.290927 s\n","6 \t 21.072887 \t 10.136944 \t10.322690 \t 0.613254 \t 5.994136 s\n","7 \t 20.710898 \t 10.066248 \t10.244096 \t 0.400553 \t 6.533517 s\n","8 \t 20.504406 \t 10.015208 \t10.179862 \t 0.309336 \t 7.042187 s\n","9 \t 20.591018 \t 9.939610 \t10.173387 \t 0.478021 \t 7.560123 s\n","10 \t 20.231037 \t 9.881042 \t10.036578 \t 0.313418 \t 8.081583 s\n","11 \t 20.890537 \t 9.951040 \t10.151093 \t 0.788404 \t 8.613144 s\n","12 \t 20.153805 \t 10.029366 \t9.921636 \t 0.202803 \t 9.131023 s\n","13 \t 20.354206 \t 9.939482 \t9.852449 \t 0.562275 \t 9.781919 s\n","14 \t 20.123303 \t 9.971379 \t9.750419 \t 0.401506 \t 10.316199 s\n","15 \t 19.998781 \t 9.893540 \t9.788844 \t 0.316398 \t 10.831336 s\n","16 \t 19.946584 \t 9.885117 \t9.773100 \t 0.288367 \t 11.357372 s\n","17 \t 19.832839 \t 9.946856 \t9.593780 \t 0.292202 \t 11.871790 s\n","18 \t 20.065393 \t 10.047271 \t9.727942 \t 0.290180 \t 12.402901 s\n","19 \t 20.075672 \t 9.881717 \t9.779182 \t 0.414773 \t 12.928887 s\n","20 \t 20.004435 \t 9.902989 \t9.807667 \t 0.293779 \t 13.453612 s\n","21 \t 20.080687 \t 9.985553 \t9.811604 \t 0.283529 \t 13.972400 s\n","22 \t 19.908596 \t 9.926186 \t9.721272 \t 0.261138 \t 14.621774 s\n","23 \t 20.051180 \t 9.873779 \t9.745922 \t 0.431479 \t 15.151945 s\n","24 \t 19.864091 \t 9.864287 \t9.701328 \t 0.298475 \t 15.675200 s\n","25 \t 19.687250 \t 9.876010 \t9.635619 \t 0.175623 \t 16.259209 s\n","26 \t 19.747095 \t 9.870893 \t9.766181 \t 0.110020 \t 16.918905 s\n","27 \t 19.821667 \t 9.782864 \t9.663467 \t 0.375336 \t 17.536406 s\n","28 \t 19.936884 \t 9.998626 \t9.635893 \t 0.302365 \t 18.189819 s\n","29 \t 20.215473 \t 9.889005 \t9.871666 \t 0.454803 \t 18.908699 s\n","30 \t 19.689371 \t 9.815319 \t9.670778 \t 0.203273 \t 19.511364 s\n","31 \t 20.008555 \t 9.829859 \t9.660622 \t 0.518075 \t 20.034167 s\n","32 \t 19.763001 \t 9.794354 \t9.741468 \t 0.227178 \t 20.685939 s\n","33 \t 19.882916 \t 9.870544 \t9.695102 \t 0.317270 \t 21.221985 s\n","34 \t 19.833480 \t 9.812281 \t9.697413 \t 0.323786 \t 21.754391 s\n","35 \t 19.680855 \t 9.896960 \t9.584818 \t 0.199077 \t 22.290467 s\n","36 \t 19.396659 \t 9.721341 \t9.488415 \t 0.186902 \t 22.840540 s\n","37 \t 19.567526 \t 9.793105 \t9.530272 \t 0.244149 \t 23.358752 s\n","38 \t 19.609519 \t 9.769304 \t9.688035 \t 0.152179 \t 23.886508 s\n","39 \t 19.678250 \t 9.723080 \t9.650290 \t 0.304881 \t 24.402141 s\n","40 \t 19.418549 \t 9.842771 \t9.438106 \t 0.137673 \t 24.936139 s\n","41 \t 19.702127 \t 9.793667 \t9.594209 \t 0.314251 \t 25.581534 s\n","42 \t 19.679456 \t 9.821790 \t9.646565 \t 0.211100 \t 26.100069 s\n","43 \t 19.631966 \t 9.738360 \t9.743355 \t 0.150251 \t 26.616073 s\n","44 \t 19.364957 \t 9.733092 \t9.453847 \t 0.178017 \t 27.146043 s\n","45 \t 19.658902 \t 9.889294 \t9.618966 \t 0.150642 \t 27.665945 s\n","46 \t 19.349766 \t 9.681200 \t9.587522 \t 0.081044 \t 28.201105 s\n","47 \t 19.551975 \t 9.689363 \t9.640954 \t 0.221657 \t 28.717719 s\n","48 \t 19.383289 \t 9.805523 \t9.451091 \t 0.126676 \t 29.301276 s\n","49 \t 19.565269 \t 9.744691 \t9.596775 \t 0.223803 \t 29.944562 s\n","50 \t 19.589231 \t 9.677008 \t9.660799 \t 0.251424 \t 30.582921 s\n","51 \t 19.618443 \t 9.788548 \t9.624548 \t 0.205346 \t 31.400111 s\n","52 \t 19.580646 \t 9.711120 \t9.549495 \t 0.320031 \t 32.104131 s\n","53 \t 19.809726 \t 9.799672 \t9.714243 \t 0.295811 \t 32.643543 s\n","54 \t 19.388287 \t 9.636996 \t9.528740 \t 0.222549 \t 33.173058 s\n","55 \t 19.411450 \t 9.622123 \t9.653789 \t 0.135538 \t 33.700292 s\n","56 \t 19.322106 \t 9.616599 \t9.490780 \t 0.214728 \t 34.241117 s\n","57 \t 19.466061 \t 9.708428 \t9.629694 \t 0.127938 \t 34.759452 s\n","58 \t 19.515540 \t 9.734728 \t9.650881 \t 0.129930 \t 35.286877 s\n","59 \t 19.547616 \t 9.740088 \t9.627429 \t 0.180099 \t 35.817596 s\n","60 \t 19.346113 \t 9.547186 \t9.579224 \t 0.219704 \t 36.342681 s\n","61 \t 19.594619 \t 9.699663 \t9.635282 \t 0.259674 \t 36.993281 s\n","62 \t 19.622243 \t 9.800284 \t9.598083 \t 0.223875 \t 37.512530 s\n","63 \t 19.549370 \t 9.758155 \t9.552319 \t 0.238896 \t 38.032153 s\n","64 \t 19.790182 \t 9.754152 \t9.585805 \t 0.450226 \t 38.558862 s\n","65 \t 19.550947 \t 9.723267 \t9.634359 \t 0.193321 \t 39.080224 s\n","66 \t 19.334447 \t 9.595819 \t9.596632 \t 0.141996 \t 39.611150 s\n","67 \t 19.615802 \t 9.793355 \t9.699716 \t 0.122731 \t 40.129730 s\n","68 \t 19.632459 \t 9.761317 \t9.579815 \t 0.291327 \t 40.652770 s\n","69 \t 19.130548 \t 9.632846 \t9.392042 \t 0.105660 \t 41.177933 s\n","70 \t 19.546525 \t 9.666795 \t9.646664 \t 0.233066 \t 41.846860 s\n","71 \t 19.288390 \t 9.712322 \t9.493556 \t 0.082513 \t 42.429515 s\n","72 \t 19.468038 \t 9.579031 \t9.646546 \t 0.242461 \t 43.086553 s\n","73 \t 19.383701 \t 9.644104 \t9.584787 \t 0.154810 \t 43.735473 s\n","74 \t 19.393394 \t 9.689219 \t9.563627 \t 0.140548 \t 44.397681 s\n","75 \t 19.483078 \t 9.695793 \t9.641337 \t 0.145948 \t 45.112827 s\n","76 \t 19.350480 \t 9.640522 \t9.570273 \t 0.139685 \t 45.686463 s\n","77 \t 19.354080 \t 9.650029 \t9.618332 \t 0.085719 \t 46.220646 s\n","78 \t 19.301922 \t 9.696655 \t9.494055 \t 0.111212 \t 46.747457 s\n","79 \t 19.445129 \t 9.719605 \t9.530507 \t 0.195018 \t 47.268988 s\n","80 \t 19.423276 \t 9.736165 \t9.539132 \t 0.147979 \t 47.927117 s\n","81 \t 19.328419 \t 9.566427 \t9.627956 \t 0.134035 \t 48.454754 s\n","82 \t 19.378222 \t 9.597690 \t9.575506 \t 0.205027 \t 48.987354 s\n","83 \t 19.521939 \t 9.687701 \t9.594885 \t 0.239352 \t 49.508461 s\n","84 \t 19.418404 \t 9.704759 \t9.591300 \t 0.122345 \t 50.037279 s\n","85 \t 19.388046 \t 9.615691 \t9.590927 \t 0.181428 \t 50.566650 s\n","86 \t 19.372010 \t 9.524183 \t9.732194 \t 0.115634 \t 51.139300 s\n","87 \t 19.496003 \t 9.682157 \t9.623374 \t 0.190473 \t 51.659641 s\n","88 \t 19.566466 \t 9.771591 \t9.604194 \t 0.190681 \t 52.359079 s\n","89 \t 19.050406 \t 9.599937 \t9.360942 \t 0.089528 \t 53.026176 s\n","90 \t 19.361055 \t 9.599704 \t9.622418 \t 0.138933 \t 53.540544 s\n","91 \t 19.307585 \t 9.550487 \t9.564984 \t 0.192113 \t 54.073557 s\n","92 \t 19.218869 \t 9.587744 \t9.458021 \t 0.173105 \t 54.601727 s\n","93 \t 19.068774 \t 9.505813 \t9.404039 \t 0.158922 \t 55.143045 s\n","94 \t 19.310681 \t 9.599924 \t9.602536 \t 0.108222 \t 55.781970 s\n","95 \t 19.299707 \t 9.580863 \t9.631472 \t 0.087373 \t 56.425662 s\n","96 \t 19.223006 \t 9.509228 \t9.604910 \t 0.108868 \t 57.079875 s\n","97 \t 19.190238 \t 9.449686 \t9.542687 \t 0.197866 \t 57.756950 s\n","98 \t 19.099710 \t 9.542466 \t9.413128 \t 0.144118 \t 58.433121 s\n","99 \t 19.338585 \t 9.617502 \t9.582347 \t 0.138736 \t 59.090278 s\n","100 \t 19.172843 \t 9.466029 \t9.590788 \t 0.116025 \t 59.627367 s\n","101 \t 19.356101 \t 9.575294 \t9.619009 \t 0.161798 \t 60.158891 s\n","102 \t 19.300146 \t 9.622240 \t9.557516 \t 0.120390 \t 60.703065 s\n","103 \t 19.337327 \t 9.584882 \t9.635131 \t 0.117313 \t 61.251177 s\n","104 \t 19.175918 \t 9.564590 \t9.522780 \t 0.088547 \t 61.799985 s\n","105 \t 19.087123 \t 9.510643 \t9.515855 \t 0.060625 \t 62.333943 s\n","106 \t 19.128244 \t 9.469044 \t9.569012 \t 0.090189 \t 62.857913 s\n","107 \t 19.059063 \t 9.609781 \t9.331253 \t 0.118029 \t 63.387559 s\n","108 \t 18.905180 \t 9.407791 \t9.355696 \t 0.141694 \t 63.914873 s\n","109 \t 18.982473 \t 9.437233 \t9.493205 \t 0.052036 \t 64.578238 s\n","110 \t 19.149817 \t 9.468429 \t9.564848 \t 0.116540 \t 65.107315 s\n","111 \t 19.245035 \t 9.522803 \t9.605007 \t 0.117225 \t 65.635301 s\n","112 \t 19.410821 \t 9.553846 \t9.650692 \t 0.206283 \t 66.170994 s\n","113 \t 19.132774 \t 9.461608 \t9.530712 \t 0.140454 \t 66.699998 s\n","114 \t 19.274341 \t 9.516405 \t9.645694 \t 0.112241 \t 67.244076 s\n","115 \t 19.194477 \t 9.479066 \t9.613006 \t 0.102404 \t 67.777380 s\n","116 \t 19.132901 \t 9.438332 \t9.589154 \t 0.105415 \t 68.313536 s\n","117 \t 18.987751 \t 9.387292 \t9.440777 \t 0.159682 \t 68.996586 s\n","118 \t 19.173024 \t 9.536561 \t9.519667 \t 0.116796 \t 69.802956 s\n","119 \t 19.042862 \t 9.360658 \t9.475341 \t 0.206863 \t 70.442723 s\n","120 \t 19.237194 \t 9.489940 \t9.525522 \t 0.221733 \t 71.164352 s\n","121 \t 19.046051 \t 9.437100 \t9.501882 \t 0.107069 \t 71.793936 s\n","122 \t 18.962176 \t 9.217124 \t9.579082 \t 0.165969 \t 72.322434 s\n","123 \t 19.014802 \t 9.256253 \t9.570074 \t 0.188474 \t 72.862586 s\n","124 \t 19.152798 \t 9.483846 \t9.594321 \t 0.074631 \t 73.401041 s\n","125 \t 19.068638 \t 9.406850 \t9.593590 \t 0.068198 \t 73.951827 s\n","126 \t 18.960001 \t 9.438030 \t9.443419 \t 0.078550 \t 74.481436 s\n","127 \t 19.045020 \t 9.387094 \t9.579253 \t 0.078674 \t 75.146399 s\n","128 \t 18.889915 \t 9.296250 \t9.491659 \t 0.102007 \t 75.672754 s\n","129 \t 18.967459 \t 9.259361 \t9.502884 \t 0.205213 \t 76.207919 s\n","130 \t 19.009052 \t 9.315345 \t9.544428 \t 0.149278 \t 76.749693 s\n","131 \t 18.862264 \t 9.340527 \t9.424171 \t 0.097565 \t 77.289899 s\n","132 \t 19.216735 \t 9.479609 \t9.634872 \t 0.102253 \t 77.816350 s\n","133 \t 18.909163 \t 9.331827 \t9.506710 \t 0.070625 \t 78.360045 s\n","134 \t 19.173314 \t 9.491764 \t9.543219 \t 0.138330 \t 78.885786 s\n","135 \t 18.907564 \t 9.248225 \t9.504847 \t 0.154493 \t 79.438663 s\n","136 \t 18.970926 \t 9.343649 \t9.526466 \t 0.100811 \t 80.116606 s\n","137 \t 19.143758 \t 9.446368 \t9.574881 \t 0.122508 \t 80.645002 s\n","138 \t 19.086731 \t 9.434014 \t9.482351 \t 0.170366 \t 81.210452 s\n","139 \t 19.132883 \t 9.395265 \t9.646766 \t 0.090853 \t 81.812275 s\n","140 \t 18.999825 \t 9.397232 \t9.456204 \t 0.146388 \t 82.479840 s\n","141 \t 18.835068 \t 9.193016 \t9.527686 \t 0.114366 \t 83.109780 s\n","142 \t 19.297374 \t 9.425611 \t9.644940 \t 0.226823 \t 83.845588 s\n","143 \t 18.800763 \t 9.169904 \t9.543690 \t 0.087169 \t 84.551727 s\n","144 \t 18.953938 \t 9.368141 \t9.497653 \t 0.088145 \t 85.090690 s\n","145 \t 18.943624 \t 9.200801 \t9.559352 \t 0.183472 \t 85.955799 s\n","146 \t 18.850146 \t 9.276238 \t9.438993 \t 0.134915 \t 86.660058 s\n","147 \t 19.137140 \t 9.500864 \t9.444467 \t 0.191809 \t 87.196280 s\n","148 \t 19.044977 \t 9.390473 \t9.546452 \t 0.108052 \t 87.736212 s\n","149 \t 18.895342 \t 9.216472 \t9.571513 \t 0.107358 \t 88.274253 s\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/159 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 159/159 [00:28<00:00,  5.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3186\n","Hit@10: 0.4113\n","Hit@3: 0.3191\n","Hit@1: 0.2660\n","Link Prediction on Validation Set (All)\n","MRR: 0.2498\n","Hit@10: 0.3892\n","Hit@3: 0.2595\n","Hit@1: 0.1693\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3866\n","Hit@10: 0.5912\n","Hit@3: 0.4277\n","Hit@1: 0.2767\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2628\n","Hit@10: 0.4656\n","Hit@3: 0.2967\n","Hit@1: 0.1513\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2067\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2067\n","150 \t 19.164330 \t 9.409342 \t9.596263 \t 0.158725 \t 118.414109 s\n","151 \t 19.297112 \t 9.567307 \t9.566427 \t 0.163378 \t 118.961368 s\n","152 \t 19.454278 \t 9.596503 \t9.685993 \t 0.171782 \t 119.499269 s\n","153 \t 19.205520 \t 9.556760 \t9.491393 \t 0.157367 \t 120.037205 s\n","154 \t 19.480227 \t 9.669705 \t9.623580 \t 0.186943 \t 120.723706 s\n","155 \t 19.387341 \t 9.452950 \t9.639689 \t 0.294702 \t 121.336742 s\n","156 \t 19.302979 \t 9.550473 \t9.537800 \t 0.214705 \t 121.979779 s\n","157 \t 18.990286 \t 9.490853 \t9.407663 \t 0.091769 \t 122.616585 s\n","158 \t 19.046094 \t 9.356633 \t9.560646 \t 0.128816 \t 123.295830 s\n","159 \t 19.182852 \t 9.430552 \t9.650124 \t 0.102176 \t 124.000586 s\n","160 \t 19.145578 \t 9.558990 \t9.449679 \t 0.136909 \t 124.541019 s\n","161 \t 19.221424 \t 9.407624 \t9.655200 \t 0.158599 \t 125.087749 s\n","162 \t 19.112422 \t 9.445461 \t9.527360 \t 0.139602 \t 125.619342 s\n","163 \t 19.105117 \t 9.407105 \t9.502660 \t 0.195352 \t 126.156008 s\n","164 \t 19.258087 \t 9.446497 \t9.591326 \t 0.220265 \t 126.830540 s\n","165 \t 19.242874 \t 9.360337 \t9.650437 \t 0.232099 \t 127.375397 s\n","166 \t 18.875265 \t 9.256679 \t9.503953 \t 0.114635 \t 127.914002 s\n","167 \t 18.832176 \t 9.283987 \t9.451404 \t 0.096784 \t 128.477175 s\n","168 \t 19.122911 \t 9.351999 \t9.587358 \t 0.183555 \t 129.017351 s\n","169 \t 19.035460 \t 9.394230 \t9.530104 \t 0.111127 \t 129.570855 s\n","170 \t 18.768243 \t 9.350979 \t9.310194 \t 0.107070 \t 130.118308 s\n","171 \t 18.793202 \t 9.271428 \t9.399526 \t 0.122250 \t 130.647790 s\n","172 \t 18.682518 \t 9.330470 \t9.249739 \t 0.102309 \t 131.194181 s\n","173 \t 18.918881 \t 9.334105 \t9.414678 \t 0.170099 \t 131.867687 s\n","174 \t 18.907724 \t 9.427960 \t9.335713 \t 0.144052 \t 132.409800 s\n","175 \t 18.615618 \t 9.376929 \t9.172554 \t 0.066135 \t 132.946311 s\n","176 \t 18.782161 \t 9.306747 \t9.333412 \t 0.142001 \t 133.486960 s\n","177 \t 18.833724 \t 9.249162 \t9.447573 \t 0.136991 \t 134.038700 s\n","178 \t 18.657187 \t 9.293008 \t9.263794 \t 0.100384 \t 134.709349 s\n","179 \t 18.790627 \t 9.348420 \t9.318166 \t 0.124040 \t 135.369497 s\n","180 \t 18.733353 \t 9.321432 \t9.249906 \t 0.162014 \t 136.008458 s\n","181 \t 18.858158 \t 9.334661 \t9.297460 \t 0.226037 \t 136.725859 s\n","182 \t 18.729730 \t 9.344358 \t9.293928 \t 0.091444 \t 137.386029 s\n","183 \t 18.726677 \t 9.338081 \t9.243148 \t 0.145448 \t 138.064002 s\n","184 \t 18.723420 \t 9.428420 \t9.182219 \t 0.112780 \t 138.602930 s\n","185 \t 18.590981 \t 9.280579 \t9.226250 \t 0.084151 \t 139.133940 s\n","186 \t 18.443814 \t 9.203321 \t9.147409 \t 0.093085 \t 139.686375 s\n","187 \t 18.397694 \t 9.112723 \t9.166862 \t 0.118109 \t 140.227580 s\n","188 \t 18.287241 \t 9.142525 \t9.080080 \t 0.064635 \t 140.768140 s\n","189 \t 18.683235 \t 9.279102 \t9.262074 \t 0.142059 \t 141.304703 s\n","190 \t 18.166659 \t 9.102346 \t8.998551 \t 0.065762 \t 141.852650 s\n","191 \t 18.329849 \t 9.083734 \t9.167405 \t 0.078711 \t 142.393379 s\n","192 \t 18.574021 \t 9.291703 \t9.200211 \t 0.082109 \t 143.073895 s\n","193 \t 18.342262 \t 9.243881 \t9.058633 \t 0.039747 \t 143.603009 s\n","194 \t 18.348992 \t 9.255713 \t9.021819 \t 0.071460 \t 144.142246 s\n","195 \t 18.263597 \t 9.141491 \t9.083295 \t 0.038811 \t 144.682958 s\n","196 \t 18.259800 \t 9.127914 \t9.075627 \t 0.056259 \t 145.221533 s\n","197 \t 18.558008 \t 9.371428 \t9.112269 \t 0.074310 \t 145.765474 s\n","198 \t 18.430795 \t 9.176731 \t9.170385 \t 0.083679 \t 146.298538 s\n","199 \t 18.273291 \t 9.206944 \t8.957386 \t 0.108959 \t 146.837779 s\n","200 \t 18.243655 \t 9.216739 \t8.949324 \t 0.077593 \t 147.447137 s\n","201 \t 18.458731 \t 9.138952 \t9.179604 \t 0.140175 \t 148.099521 s\n","202 \t 18.649065 \t 9.226473 \t9.311281 \t 0.111309 \t 148.917562 s\n","203 \t 18.371580 \t 9.155207 \t9.067118 \t 0.149255 \t 149.593049 s\n","204 \t 18.496156 \t 9.278753 \t9.112409 \t 0.104995 \t 150.250848 s\n","205 \t 18.454000 \t 9.263754 \t9.112012 \t 0.078234 \t 150.781465 s\n","206 \t 18.088148 \t 9.056105 \t8.930945 \t 0.101098 \t 151.342180 s\n","207 \t 18.292954 \t 9.065492 \t8.975546 \t 0.251918 \t 151.879970 s\n","208 \t 18.490141 \t 9.211268 \t9.150985 \t 0.127888 \t 152.432615 s\n","209 \t 18.620408 \t 9.318919 \t9.123889 \t 0.177601 \t 152.978755 s\n","210 \t 18.188032 \t 9.143748 \t8.979744 \t 0.064540 \t 153.506439 s\n","211 \t 18.208140 \t 9.143860 \t8.985373 \t 0.078907 \t 154.173487 s\n","212 \t 18.214121 \t 9.151868 \t8.978021 \t 0.084232 \t 154.695723 s\n","213 \t 18.373073 \t 9.205164 \t9.096259 \t 0.071649 \t 155.245240 s\n","214 \t 18.298117 \t 9.180116 \t9.014182 \t 0.103818 \t 155.780132 s\n","215 \t 18.217209 \t 9.048999 \t9.073748 \t 0.094462 \t 156.319729 s\n","216 \t 18.307851 \t 9.179851 \t9.044122 \t 0.083879 \t 156.854918 s\n","217 \t 18.015403 \t 8.986106 \t8.956843 \t 0.072453 \t 157.386528 s\n","218 \t 18.392216 \t 9.189756 \t9.126003 \t 0.076456 \t 157.913489 s\n","219 \t 18.157022 \t 9.106606 \t8.941851 \t 0.108565 \t 158.453741 s\n","220 \t 18.066579 \t 9.131723 \t8.887397 \t 0.047459 \t 159.116172 s\n","221 \t 18.089389 \t 8.973271 \t8.966280 \t 0.149838 \t 159.665135 s\n","222 \t 18.047564 \t 9.064857 \t8.916082 \t 0.066625 \t 160.236114 s\n","223 \t 18.209337 \t 9.081984 \t8.961085 \t 0.166269 \t 160.898951 s\n","224 \t 18.024191 \t 9.059557 \t8.903036 \t 0.061598 \t 161.557492 s\n","225 \t 18.048880 \t 9.049076 \t8.899384 \t 0.100419 \t 162.182019 s\n","226 \t 17.877951 \t 9.008487 \t8.824843 \t 0.044621 \t 162.901546 s\n","227 \t 18.186616 \t 9.050510 \t9.046322 \t 0.089783 \t 163.553866 s\n","228 \t 17.948156 \t 9.099937 \t8.806318 \t 0.041900 \t 164.084556 s\n","229 \t 17.940637 \t 8.991035 \t8.842016 \t 0.107587 \t 164.749045 s\n","230 \t 17.999881 \t 9.107417 \t8.837124 \t 0.055340 \t 165.284856 s\n","231 \t 17.966819 \t 9.041841 \t8.839595 \t 0.085382 \t 165.811265 s\n","232 \t 18.026361 \t 9.110211 \t8.876375 \t 0.039774 \t 166.351954 s\n","233 \t 17.843949 \t 9.068892 \t8.703792 \t 0.071264 \t 166.902340 s\n","234 \t 17.988674 \t 9.094801 \t8.847390 \t 0.046482 \t 167.441810 s\n","235 \t 17.950255 \t 9.053147 \t8.827232 \t 0.069875 \t 167.994485 s\n","236 \t 17.879301 \t 9.080366 \t8.751070 \t 0.047865 \t 168.525741 s\n","237 \t 18.022403 \t 9.062551 \t8.909158 \t 0.050693 \t 169.073307 s\n","238 \t 18.094232 \t 9.202467 \t8.754426 \t 0.137339 \t 169.757235 s\n","239 \t 17.828606 \t 8.991251 \t8.800243 \t 0.037111 \t 170.302806 s\n","240 \t 17.890121 \t 9.057764 \t8.769501 \t 0.062856 \t 170.845291 s\n","241 \t 17.521679 \t 8.842071 \t8.628232 \t 0.051376 \t 171.379101 s\n","242 \t 18.001326 \t 9.173463 \t8.760669 \t 0.067194 \t 171.932685 s\n","243 \t 17.847796 \t 8.973373 \t8.839213 \t 0.035210 \t 172.463932 s\n","244 \t 17.628734 \t 8.922734 \t8.631265 \t 0.074735 \t 172.993254 s\n","245 \t 17.617864 \t 8.868354 \t8.713362 \t 0.036148 \t 173.595456 s\n","246 \t 17.823337 \t 9.022075 \t8.759002 \t 0.042260 \t 174.258605 s\n","247 \t 17.714357 \t 9.060515 \t8.574943 \t 0.078900 \t 175.076794 s\n","248 \t 17.908900 \t 9.020303 \t8.844935 \t 0.043662 \t 175.752186 s\n","249 \t 17.948312 \t 8.947218 \t8.949095 \t 0.051999 \t 176.410317 s\n","250 \t 17.902486 \t 8.858838 \t9.007159 \t 0.036488 \t 176.951144 s\n","251 \t 17.658405 \t 8.970217 \t8.650324 \t 0.037864 \t 177.493587 s\n","252 \t 17.759613 \t 8.998381 \t8.713270 \t 0.047961 \t 178.029391 s\n","253 \t 17.553483 \t 8.817464 \t8.700315 \t 0.035704 \t 178.558989 s\n","254 \t 17.732720 \t 8.949145 \t8.715662 \t 0.067914 \t 179.099116 s\n","255 \t 17.605845 \t 9.045912 \t8.513772 \t 0.046162 \t 179.629496 s\n","256 \t 17.741719 \t 9.015817 \t8.695249 \t 0.030654 \t 180.304081 s\n","257 \t 17.583135 \t 8.884974 \t8.655362 \t 0.042798 \t 180.844765 s\n","258 \t 17.595626 \t 8.927989 \t8.630478 \t 0.037159 \t 181.390720 s\n","259 \t 17.682578 \t 8.960927 \t8.675551 \t 0.046101 \t 181.924299 s\n","260 \t 17.401711 \t 8.944790 \t8.421074 \t 0.035847 \t 182.462853 s\n","261 \t 17.486732 \t 8.932105 \t8.510584 \t 0.044043 \t 182.999794 s\n","262 \t 17.403683 \t 8.816483 \t8.532338 \t 0.054861 \t 183.541419 s\n","263 \t 17.331985 \t 8.783143 \t8.517386 \t 0.031456 \t 184.081852 s\n","264 \t 17.648482 \t 8.934603 \t8.682570 \t 0.031309 \t 184.639502 s\n","265 \t 17.736198 \t 8.990004 \t8.682700 \t 0.063496 \t 185.313415 s\n","266 \t 17.691826 \t 8.911448 \t8.747129 \t 0.033249 \t 185.844085 s\n","267 \t 17.587364 \t 8.911052 \t8.648129 \t 0.028184 \t 186.467803 s\n","268 \t 17.638444 \t 8.899002 \t8.683840 \t 0.055602 \t 187.113699 s\n","269 \t 17.384979 \t 8.827819 \t8.523274 \t 0.033886 \t 187.759184 s\n","270 \t 17.307364 \t 8.759481 \t8.525548 \t 0.022333 \t 188.429609 s\n","271 \t 17.699005 \t 8.997452 \t8.668136 \t 0.033418 \t 189.153084 s\n","272 \t 17.366615 \t 8.835369 \t8.499539 \t 0.031708 \t 189.734090 s\n","273 \t 17.617782 \t 8.942207 \t8.635114 \t 0.040459 \t 190.277007 s\n","274 \t 17.514440 \t 8.863547 \t8.617279 \t 0.033614 \t 190.960179 s\n","275 \t 17.668505 \t 8.949302 \t8.678000 \t 0.041202 \t 191.496547 s\n","276 \t 17.456755 \t 8.853778 \t8.567090 \t 0.035887 \t 192.026659 s\n","277 \t 17.455707 \t 8.895095 \t8.529543 \t 0.031069 \t 192.567782 s\n","278 \t 17.456406 \t 8.784379 \t8.635077 \t 0.036951 \t 193.096365 s\n","279 \t 17.533299 \t 8.934323 \t8.562649 \t 0.036328 \t 193.645920 s\n","280 \t 17.362704 \t 8.947190 \t8.386358 \t 0.029157 \t 194.179569 s\n","281 \t 17.605136 \t 8.853936 \t8.715811 \t 0.035388 \t 194.731576 s\n","282 \t 17.284195 \t 8.889759 \t8.360070 \t 0.034366 \t 195.271257 s\n","283 \t 17.490745 \t 8.859247 \t8.607971 \t 0.023528 \t 195.948046 s\n","284 \t 17.302417 \t 8.872130 \t8.384604 \t 0.045684 \t 196.490017 s\n","285 \t 17.396147 \t 8.868488 \t8.506561 \t 0.021098 \t 197.034588 s\n","286 \t 17.219082 \t 8.738007 \t8.450685 \t 0.030390 \t 197.571290 s\n","287 \t 17.458130 \t 8.908970 \t8.521671 \t 0.027488 \t 198.123802 s\n","288 \t 17.523290 \t 8.818220 \t8.618850 \t 0.086220 \t 198.658164 s\n","289 \t 17.264770 \t 8.774925 \t8.443090 \t 0.046754 \t 199.214280 s\n","290 \t 17.161842 \t 8.791656 \t8.341134 \t 0.029053 \t 199.898281 s\n","291 \t 17.343640 \t 8.722131 \t8.583045 \t 0.038464 \t 200.533438 s\n","292 \t 17.331366 \t 8.778244 \t8.492607 \t 0.060513 \t 201.181205 s\n","293 \t 17.295562 \t 8.818070 \t8.448617 \t 0.028873 \t 202.045846 s\n","294 \t 17.099149 \t 8.757369 \t8.313278 \t 0.028503 \t 202.699253 s\n","295 \t 17.234936 \t 8.848201 \t8.354538 \t 0.032196 \t 203.236520 s\n","296 \t 17.268292 \t 8.808706 \t8.428603 \t 0.030983 \t 203.770477 s\n","297 \t 17.400090 \t 8.950180 \t8.414713 \t 0.035198 \t 204.309074 s\n","298 \t 16.983553 \t 8.676847 \t8.275396 \t 0.031311 \t 204.850036 s\n","299 \t 17.342473 \t 8.881733 \t8.428127 \t 0.032612 \t 205.387269 s\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 159/159 [00:28<00:00,  5.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3417\n","Hit@10: 0.5000\n","Hit@3: 0.3475\n","Hit@1: 0.2695\n","Link Prediction on Validation Set (All)\n","MRR: 0.2897\n","Hit@10: 0.5063\n","Hit@3: 0.3228\n","Hit@1: 0.1851\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5171\n","Hit@10: 0.6855\n","Hit@3: 0.5283\n","Hit@1: 0.4403\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4233\n","Hit@10: 0.6739\n","Hit@3: 0.4735\n","Hit@1: 0.3006\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.1114\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1114\n","300 \t 17.093562 \t 8.722967 \t8.346529 \t 0.024067 \t 235.430185 s\n","301 \t 16.817381 \t 8.578972 \t8.197165 \t 0.041245 \t 236.099208 s\n","302 \t 16.917510 \t 8.545891 \t8.338559 \t 0.033062 \t 236.639477 s\n","303 \t 17.011478 \t 8.576048 \t8.394147 \t 0.041283 \t 237.173473 s\n","304 \t 17.247426 \t 8.791129 \t8.437872 \t 0.018424 \t 237.721509 s\n","305 \t 17.252337 \t 8.803960 \t8.419838 \t 0.028539 \t 238.258219 s\n","306 \t 17.215806 \t 8.815492 \t8.378266 \t 0.022048 \t 238.847693 s\n","307 \t 17.010653 \t 8.759416 \t8.226670 \t 0.024569 \t 239.520828 s\n","308 \t 16.959717 \t 8.672307 \t8.239826 \t 0.047583 \t 240.162829 s\n","309 \t 16.970635 \t 8.726232 \t8.224070 \t 0.020334 \t 240.808213 s\n","310 \t 17.015114 \t 8.771507 \t8.215024 \t 0.028582 \t 241.532228 s\n","311 \t 16.837050 \t 8.647710 \t8.151631 \t 0.037709 \t 242.288403 s\n","312 \t 16.891465 \t 8.734284 \t8.137519 \t 0.019662 \t 242.828642 s\n","313 \t 17.045546 \t 8.644427 \t8.371161 \t 0.029958 \t 243.361260 s\n","314 \t 17.070171 \t 8.689876 \t8.354799 \t 0.025496 \t 243.899791 s\n","315 \t 17.057799 \t 8.781662 \t8.242612 \t 0.033527 \t 244.453805 s\n","316 \t 16.847961 \t 8.676987 \t8.147975 \t 0.022998 \t 244.993815 s\n","317 \t 16.961352 \t 8.698030 \t8.243900 \t 0.019422 \t 245.525554 s\n","318 \t 16.922948 \t 8.773370 \t8.124413 \t 0.025164 \t 246.073504 s\n","319 \t 17.029623 \t 8.830377 \t8.184388 \t 0.014858 \t 246.617072 s\n","320 \t 17.025677 \t 8.686107 \t8.323920 \t 0.015648 \t 247.304810 s\n","321 \t 16.665157 \t 8.632198 \t8.012156 \t 0.020802 \t 247.844958 s\n","322 \t 17.088739 \t 8.810849 \t8.261373 \t 0.016518 \t 248.378610 s\n","323 \t 16.791291 \t 8.724049 \t8.034800 \t 0.032442 \t 248.922057 s\n","324 \t 16.994287 \t 8.708972 \t8.265625 \t 0.019689 \t 249.450415 s\n","325 \t 16.840713 \t 8.659595 \t8.158358 \t 0.022760 \t 249.999124 s\n","326 \t 16.790961 \t 8.742866 \t8.025688 \t 0.022407 \t 250.530446 s\n","327 \t 16.832635 \t 8.775373 \t8.031996 \t 0.025265 \t 251.067240 s\n","328 \t 16.895070 \t 8.626698 \t8.243962 \t 0.024410 \t 251.602534 s\n","329 \t 16.862592 \t 8.644490 \t8.189464 \t 0.028638 \t 252.394577 s\n","330 \t 16.958908 \t 8.667631 \t8.269426 \t 0.021851 \t 253.061708 s\n","331 \t 16.715608 \t 8.600801 \t8.084770 \t 0.030038 \t 253.705715 s\n","332 \t 17.013474 \t 8.696663 \t8.289952 \t 0.026858 \t 254.403759 s\n","333 \t 16.838339 \t 8.579207 \t8.236156 \t 0.022976 \t 255.063698 s\n","334 \t 16.597214 \t 8.641356 \t7.927256 \t 0.028602 \t 255.599378 s\n","335 \t 16.606495 \t 8.513403 \t8.071906 \t 0.021187 \t 256.175517 s\n","336 \t 16.743136 \t 8.525594 \t8.194735 \t 0.022809 \t 256.712065 s\n","337 \t 16.717793 \t 8.628158 \t8.067651 \t 0.021985 \t 257.252811 s\n","338 \t 16.665795 \t 8.675694 \t7.973691 \t 0.016410 \t 257.910619 s\n","339 \t 16.660892 \t 8.601942 \t8.029535 \t 0.029415 \t 258.438444 s\n","340 \t 16.740964 \t 8.634115 \t8.077420 \t 0.029429 \t 258.976589 s\n","341 \t 16.537968 \t 8.559072 \t7.933272 \t 0.045623 \t 259.514817 s\n","342 \t 16.647663 \t 8.596338 \t8.021516 \t 0.029809 \t 260.051404 s\n","343 \t 16.733906 \t 8.630682 \t8.075606 \t 0.027618 \t 260.604085 s\n","344 \t 16.637626 \t 8.656412 \t7.961182 \t 0.020032 \t 261.146291 s\n","345 \t 16.666293 \t 8.607428 \t8.027750 \t 0.031116 \t 261.700501 s\n","346 \t 16.498938 \t 8.563677 \t7.920801 \t 0.014460 \t 262.247118 s\n","347 \t 16.645347 \t 8.583339 \t8.028688 \t 0.033319 \t 262.918824 s\n","348 \t 16.640883 \t 8.505120 \t8.120019 \t 0.015745 \t 263.459235 s\n","349 \t 16.461863 \t 8.481579 \t7.963789 \t 0.016495 \t 263.997775 s\n","350 \t 16.623175 \t 8.552090 \t8.030480 \t 0.040604 \t 264.525533 s\n","351 \t 16.516478 \t 8.496303 \t7.989878 \t 0.030297 \t 265.154414 s\n","352 \t 16.655148 \t 8.548873 \t8.094555 \t 0.011718 \t 265.808223 s\n","353 \t 16.571621 \t 8.564119 \t7.986591 \t 0.020911 \t 266.463148 s\n","354 \t 16.860953 \t 8.684765 \t8.155200 \t 0.020989 \t 267.137715 s\n","355 \t 16.608800 \t 8.564954 \t8.024736 \t 0.019108 \t 267.846271 s\n","356 \t 16.538383 \t 8.691973 \t7.831704 \t 0.014705 \t 268.508717 s\n","357 \t 16.489515 \t 8.542763 \t7.926305 \t 0.020448 \t 269.047195 s\n","358 \t 16.379132 \t 8.424400 \t7.940469 \t 0.014262 \t 269.585974 s\n","359 \t 16.384782 \t 8.504626 \t7.847700 \t 0.032456 \t 270.129568 s\n","360 \t 16.411283 \t 8.406746 \t7.984156 \t 0.020380 \t 270.665986 s\n","361 \t 16.556294 \t 8.597507 \t7.932406 \t 0.026380 \t 271.208703 s\n","362 \t 16.715217 \t 8.651088 \t8.038059 \t 0.026068 \t 271.760175 s\n","363 \t 16.455073 \t 8.452261 \t7.983489 \t 0.019324 \t 272.301130 s\n","364 \t 16.428191 \t 8.480830 \t7.921873 \t 0.025488 \t 272.845031 s\n","365 \t 16.755535 \t 8.654714 \t8.080434 \t 0.020388 \t 273.503551 s\n","366 \t 16.541484 \t 8.502300 \t8.022143 \t 0.017041 \t 274.037941 s\n","367 \t 16.548535 \t 8.532423 \t7.995198 \t 0.020915 \t 274.563731 s\n","368 \t 16.318084 \t 8.364680 \t7.933132 \t 0.020272 \t 275.098927 s\n","369 \t 16.511909 \t 8.560351 \t7.916337 \t 0.035221 \t 275.630280 s\n","370 \t 16.649570 \t 8.601748 \t8.023411 \t 0.024412 \t 276.182796 s\n","371 \t 16.495228 \t 8.609547 \t7.874400 \t 0.011281 \t 276.723052 s\n","372 \t 16.551200 \t 8.620507 \t7.917322 \t 0.013371 \t 277.272774 s\n","373 \t 16.200808 \t 8.388849 \t7.788262 \t 0.023697 \t 277.809206 s\n","374 \t 16.382977 \t 8.500138 \t7.863594 \t 0.019244 \t 278.628329 s\n","375 \t 16.511942 \t 8.520055 \t7.972732 \t 0.019154 \t 279.277248 s\n","376 \t 16.410273 \t 8.500895 \t7.882755 \t 0.026624 \t 279.909036 s\n","377 \t 16.265621 \t 8.386649 \t7.857350 \t 0.021622 \t 280.627022 s\n","378 \t 16.422083 \t 8.464608 \t7.939524 \t 0.017951 \t 281.274020 s\n","379 \t 16.301662 \t 8.481156 \t7.808674 \t 0.011833 \t 281.801060 s\n","380 \t 16.416417 \t 8.483243 \t7.913052 \t 0.020121 \t 282.343189 s\n","381 \t 16.467552 \t 8.560079 \t7.890143 \t 0.017330 \t 282.876073 s\n","382 \t 16.394705 \t 8.422781 \t7.952865 \t 0.019060 \t 283.408795 s\n","383 \t 16.394620 \t 8.479781 \t7.894811 \t 0.020028 \t 284.090641 s\n","384 \t 16.521607 \t 8.533977 \t7.971513 \t 0.016116 \t 284.615807 s\n","385 \t 16.428743 \t 8.405406 \t8.008625 \t 0.014713 \t 285.157707 s\n","386 \t 16.422462 \t 8.567377 \t7.839980 \t 0.015105 \t 285.685401 s\n","387 \t 16.464530 \t 8.477148 \t7.974800 \t 0.012584 \t 286.236296 s\n","388 \t 16.212356 \t 8.361282 \t7.838919 \t 0.012154 \t 286.763360 s\n","389 \t 16.238346 \t 8.443351 \t7.777781 \t 0.017214 \t 287.319902 s\n","390 \t 16.461753 \t 8.471875 \t7.974574 \t 0.015305 \t 287.849825 s\n","391 \t 16.248013 \t 8.396157 \t7.825828 \t 0.026027 \t 288.384223 s\n","392 \t 16.333166 \t 8.526673 \t7.790776 \t 0.015718 \t 289.052646 s\n","393 \t 16.348984 \t 8.499896 \t7.832212 \t 0.016875 \t 289.584365 s\n","394 \t 16.266288 \t 8.376772 \t7.873932 \t 0.015584 \t 290.125021 s\n","395 \t 16.138350 \t 8.331438 \t7.789033 \t 0.017879 \t 290.663189 s\n","396 \t 16.300462 \t 8.421702 \t7.855194 \t 0.023567 \t 291.258693 s\n","397 \t 16.244946 \t 8.409586 \t7.825624 \t 0.009736 \t 291.916592 s\n","398 \t 16.269461 \t 8.442207 \t7.803631 \t 0.023624 \t 292.568240 s\n","399 \t 15.932329 \t 8.320804 \t7.600979 \t 0.010545 \t 293.245150 s\n","400 \t 16.176354 \t 8.332683 \t7.821874 \t 0.021797 \t 293.963296 s\n","401 \t 16.271405 \t 8.443691 \t7.804772 \t 0.022942 \t 294.640141 s\n","402 \t 16.115639 \t 8.353518 \t7.746670 \t 0.015450 \t 295.175876 s\n","403 \t 16.271511 \t 8.405260 \t7.844124 \t 0.022127 \t 295.728596 s\n","404 \t 16.266644 \t 8.462793 \t7.777971 \t 0.025881 \t 296.271227 s\n","405 \t 16.526746 \t 8.545569 \t7.967453 \t 0.013724 \t 296.808757 s\n","406 \t 16.174027 \t 8.414597 \t7.747978 \t 0.011452 \t 297.342570 s\n","407 \t 16.167754 \t 8.484697 \t7.667209 \t 0.015849 \t 297.883596 s\n","408 \t 16.221286 \t 8.460684 \t7.743074 \t 0.017528 \t 298.418649 s\n","409 \t 16.409766 \t 8.522948 \t7.875946 \t 0.010873 \t 298.957639 s\n","410 \t 16.330719 \t 8.523560 \t7.792193 \t 0.014967 \t 299.625299 s\n","411 \t 16.162322 \t 8.400602 \t7.752655 \t 0.009065 \t 300.177957 s\n","412 \t 16.187523 \t 8.483942 \t7.686176 \t 0.017404 \t 300.708694 s\n","413 \t 16.401516 \t 8.537237 \t7.850902 \t 0.013377 \t 301.268473 s\n","414 \t 16.225525 \t 8.383343 \t7.830260 \t 0.011922 \t 301.803367 s\n","415 \t 16.197349 \t 8.395830 \t7.783307 \t 0.018212 \t 302.343505 s\n","416 \t 16.262997 \t 8.465228 \t7.785141 \t 0.012627 \t 302.882826 s\n","417 \t 16.270894 \t 8.531445 \t7.716513 \t 0.022936 \t 303.414779 s\n","418 \t 16.125659 \t 8.352593 \t7.749084 \t 0.023982 \t 303.955341 s\n","419 \t 15.857096 \t 8.344923 \t7.488181 \t 0.023992 \t 304.803113 s\n","420 \t 16.328951 \t 8.499588 \t7.815200 \t 0.014163 \t 305.465453 s\n","421 \t 16.239830 \t 8.340034 \t7.885440 \t 0.014356 \t 306.124078 s\n","422 \t 16.176970 \t 8.470784 \t7.693693 \t 0.012492 \t 306.867118 s\n","423 \t 16.112552 \t 8.456000 \t7.639805 \t 0.016747 \t 307.520107 s\n","424 \t 16.285077 \t 8.448302 \t7.821898 \t 0.014878 \t 308.050664 s\n","425 \t 16.330987 \t 8.434376 \t7.881051 \t 0.015559 \t 308.602159 s\n","426 \t 16.245183 \t 8.433038 \t7.792864 \t 0.019281 \t 309.148216 s\n","427 \t 16.181376 \t 8.409924 \t7.755567 \t 0.015885 \t 309.675327 s\n","428 \t 16.341152 \t 8.455272 \t7.871424 \t 0.014456 \t 310.358252 s\n","429 \t 16.341457 \t 8.411710 \t7.917400 \t 0.012348 \t 310.888572 s\n","430 \t 16.164780 \t 8.419122 \t7.723459 \t 0.022199 \t 311.427662 s\n","431 \t 16.298840 \t 8.416545 \t7.866824 \t 0.015470 \t 311.970761 s\n","432 \t 16.045109 \t 8.329832 \t7.690967 \t 0.024310 \t 312.494729 s\n","433 \t 16.455852 \t 8.591016 \t7.853382 \t 0.011453 \t 313.042019 s\n","434 \t 16.225854 \t 8.438828 \t7.770270 \t 0.016756 \t 313.576454 s\n","435 \t 16.095671 \t 8.273851 \t7.808787 \t 0.013033 \t 314.108673 s\n","436 \t 16.227090 \t 8.392437 \t7.819428 \t 0.015224 \t 314.641706 s\n","437 \t 16.249178 \t 8.511134 \t7.725428 \t 0.012616 \t 315.317527 s\n","438 \t 16.186129 \t 8.328120 \t7.841383 \t 0.016626 \t 315.853000 s\n","439 \t 16.146584 \t 8.294090 \t7.838915 \t 0.013578 \t 316.395516 s\n","440 \t 16.137942 \t 8.424127 \t7.697826 \t 0.015990 \t 316.936965 s\n","441 \t 16.390518 \t 8.496167 \t7.877706 \t 0.016645 \t 317.545301 s\n","442 \t 16.160195 \t 8.464883 \t7.681277 \t 0.014035 \t 318.179509 s\n","443 \t 16.114501 \t 8.370281 \t7.726973 \t 0.017247 \t 318.822226 s\n","444 \t 16.436996 \t 8.502741 \t7.912564 \t 0.021691 \t 319.493655 s\n","445 \t 16.267639 \t 8.466999 \t7.785505 \t 0.015135 \t 320.200655 s\n","446 \t 15.961370 \t 8.347812 \t7.601931 \t 0.011627 \t 320.859872 s\n","447 \t 16.425409 \t 8.514045 \t7.893253 \t 0.018112 \t 321.392314 s\n","448 \t 16.226723 \t 8.365322 \t7.841965 \t 0.019436 \t 321.945286 s\n","449 \t 16.122248 \t 8.269689 \t7.840612 \t 0.011947 \t 322.490656 s\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 159/159 [00:29<00:00,  5.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3606\n","Hit@10: 0.5071\n","Hit@3: 0.3582\n","Hit@1: 0.2837\n","Link Prediction on Validation Set (All)\n","MRR: 0.3186\n","Hit@10: 0.5269\n","Hit@3: 0.3354\n","Hit@1: 0.2136\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6236\n","Hit@10: 0.7610\n","Hit@3: 0.6667\n","Hit@1: 0.5409\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4936\n","Hit@10: 0.7544\n","Hit@3: 0.5619\n","Hit@1: 0.3576\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0431\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0431\n","450 \t 16.221685 \t 8.392893 \t7.803784 \t 0.025009 \t 353.073895 s\n","451 \t 16.587068 \t 8.536889 \t8.021764 \t 0.028414 \t 353.609823 s\n","452 \t 16.544312 \t 8.624634 \t7.894725 \t 0.024953 \t 354.163044 s\n","453 \t 16.820935 \t 8.642811 \t8.157587 \t 0.020537 \t 354.691975 s\n","454 \t 16.514749 \t 8.602315 \t7.864271 \t 0.048162 \t 355.229313 s\n","455 \t 16.658064 \t 8.579451 \t8.054332 \t 0.024281 \t 355.754369 s\n","456 \t 16.413937 \t 8.476353 \t7.892494 \t 0.045089 \t 356.451622 s\n","457 \t 16.688111 \t 8.426889 \t8.229581 \t 0.031642 \t 357.063099 s\n","458 \t 16.665629 \t 8.648128 \t7.990303 \t 0.027199 \t 357.695264 s\n","459 \t 16.414187 \t 8.475781 \t7.913893 \t 0.024513 \t 358.343968 s\n","460 \t 16.577442 \t 8.598059 \t7.953404 \t 0.025980 \t 359.011071 s\n","461 \t 16.630385 \t 8.479135 \t8.091375 \t 0.059877 \t 359.719603 s\n","462 \t 16.666725 \t 8.637136 \t8.000274 \t 0.029315 \t 360.265446 s\n","463 \t 16.491036 \t 8.590137 \t7.872703 \t 0.028196 \t 360.792132 s\n","464 \t 16.665822 \t 8.685133 \t7.951545 \t 0.029146 \t 361.456223 s\n","465 \t 16.753937 \t 8.613529 \t8.103175 \t 0.037233 \t 361.996539 s\n","466 \t 16.510755 \t 8.561220 \t7.928397 \t 0.021137 \t 362.544904 s\n","467 \t 16.307268 \t 8.464431 \t7.822682 \t 0.020155 \t 363.072077 s\n","468 \t 16.810338 \t 8.717159 \t8.068178 \t 0.025002 \t 363.606673 s\n","469 \t 16.388601 \t 8.525724 \t7.836059 \t 0.026819 \t 364.139189 s\n","470 \t 16.422943 \t 8.538361 \t7.862319 \t 0.022263 \t 364.668780 s\n","471 \t 16.637766 \t 8.574460 \t8.024409 \t 0.038897 \t 365.209352 s\n","472 \t 16.247300 \t 8.472247 \t7.739241 \t 0.035813 \t 365.744814 s\n","473 \t 16.576226 \t 8.543503 \t7.961810 \t 0.070914 \t 366.409437 s\n","474 \t 16.646882 \t 8.600699 \t8.025935 \t 0.020247 \t 366.971053 s\n","475 \t 16.553556 \t 8.589344 \t7.923335 \t 0.040878 \t 367.513295 s\n","476 \t 16.676917 \t 8.561498 \t8.074108 \t 0.041310 \t 368.047088 s\n","477 \t 16.647719 \t 8.521966 \t8.065490 \t 0.060263 \t 368.588152 s\n","478 \t 16.359646 \t 8.463297 \t7.859694 \t 0.036655 \t 369.121417 s\n","479 \t 16.394055 \t 8.487530 \t7.875976 \t 0.030549 \t 369.668720 s\n","480 \t 16.400798 \t 8.586357 \t7.779092 \t 0.035349 \t 370.336625 s\n","481 \t 16.480742 \t 8.637256 \t7.801066 \t 0.042419 \t 370.972611 s\n","482 \t 16.421611 \t 8.507782 \t7.812693 \t 0.101137 \t 371.766740 s\n","483 \t 16.182390 \t 8.388835 \t7.774289 \t 0.019265 \t 372.482706 s\n","484 \t 16.573414 \t 8.581908 \t7.907212 \t 0.084293 \t 373.127702 s\n","485 \t 16.167882 \t 8.369868 \t7.768935 \t 0.029080 \t 373.666014 s\n","486 \t 16.225976 \t 8.318276 \t7.850519 \t 0.057181 \t 374.216114 s\n","487 \t 16.278072 \t 8.603200 \t7.630388 \t 0.044484 \t 374.754300 s\n","488 \t 16.307002 \t 8.429208 \t7.852406 \t 0.025388 \t 375.303651 s\n","489 \t 16.177678 \t 8.437536 \t7.695987 \t 0.044155 \t 375.841882 s\n","490 \t 16.069195 \t 8.364412 \t7.676793 \t 0.027989 \t 376.373719 s\n","491 \t 16.337325 \t 8.518497 \t7.797884 \t 0.020944 \t 377.052952 s\n","492 \t 16.300440 \t 8.506842 \t7.769681 \t 0.023917 \t 377.578789 s\n","493 \t 16.243169 \t 8.464082 \t7.736011 \t 0.043076 \t 378.111102 s\n","494 \t 16.329736 \t 8.439468 \t7.827565 \t 0.062703 \t 378.640218 s\n","495 \t 16.323759 \t 8.288476 \t7.996125 \t 0.039159 \t 379.189866 s\n","496 \t 16.440331 \t 8.407311 \t7.955543 \t 0.077477 \t 379.733549 s\n","497 \t 16.344424 \t 8.435749 \t7.856843 \t 0.051831 \t 380.279868 s\n","498 \t 16.142385 \t 8.343516 \t7.743652 \t 0.055218 \t 380.801952 s\n","499 \t 16.065093 \t 8.373898 \t7.659936 \t 0.031260 \t 381.346598 s\n","500 \t 16.080771 \t 8.278957 \t7.756706 \t 0.045108 \t 382.016557 s\n","501 \t 15.987633 \t 8.259975 \t7.700649 \t 0.027010 \t 382.564072 s\n","502 \t 16.165607 \t 8.288679 \t7.838209 \t 0.038719 \t 383.173921 s\n","503 \t 16.166520 \t 8.439545 \t7.699799 \t 0.027178 \t 383.817225 s\n","504 \t 16.184987 \t 8.309820 \t7.851926 \t 0.023242 \t 384.530268 s\n","505 \t 16.156535 \t 8.378886 \t7.756586 \t 0.021063 \t 385.228676 s\n","506 \t 16.274572 \t 8.421638 \t7.779750 \t 0.073184 \t 385.904393 s\n","507 \t 16.205855 \t 8.387201 \t7.789895 \t 0.028758 \t 386.442800 s\n","508 \t 16.155394 \t 8.309972 \t7.809309 \t 0.036111 \t 386.988691 s\n","509 \t 16.018836 \t 8.296411 \t7.683529 \t 0.038896 \t 387.647736 s\n","510 \t 16.132109 \t 8.417957 \t7.694223 \t 0.019930 \t 388.172468 s\n","511 \t 15.916163 \t 8.242489 \t7.642263 \t 0.031412 \t 388.726668 s\n","512 \t 16.066382 \t 8.302821 \t7.738779 \t 0.024782 \t 389.258470 s\n","513 \t 16.079062 \t 8.449900 \t7.599309 \t 0.029852 \t 389.807404 s\n","514 \t 15.967533 \t 8.373416 \t7.567259 \t 0.026857 \t 390.344052 s\n","515 \t 16.080321 \t 8.363729 \t7.697385 \t 0.019209 \t 390.890404 s\n","516 \t 15.830618 \t 8.265492 \t7.533441 \t 0.031685 \t 391.431017 s\n","517 \t 15.918151 \t 8.307961 \t7.588725 \t 0.021465 \t 391.966975 s\n","518 \t 15.931020 \t 8.288765 \t7.622805 \t 0.019450 \t 392.627473 s\n","519 \t 15.988368 \t 8.352334 \t7.612273 \t 0.023760 \t 393.155856 s\n","520 \t 16.137980 \t 8.410522 \t7.705311 \t 0.022148 \t 393.690324 s\n","521 \t 15.967270 \t 8.325216 \t7.621269 \t 0.020786 \t 394.232876 s\n","522 \t 16.105482 \t 8.296844 \t7.768328 \t 0.040310 \t 394.767941 s\n","523 \t 15.856884 \t 8.301737 \t7.543497 \t 0.011650 \t 395.303950 s\n","524 \t 15.970986 \t 8.279461 \t7.660933 \t 0.030592 \t 395.847061 s\n","525 \t 16.100568 \t 8.295568 \t7.773366 \t 0.031635 \t 396.522685 s\n","526 \t 16.031853 \t 8.329044 \t7.673649 \t 0.029160 \t 397.213829 s\n","527 \t 15.772572 \t 8.165895 \t7.582239 \t 0.024437 \t 398.027498 s\n","528 \t 15.935035 \t 8.319487 \t7.570058 \t 0.045491 \t 398.760434 s\n","529 \t 15.709901 \t 8.120831 \t7.564797 \t 0.024273 \t 399.333869 s\n","530 \t 15.615682 \t 8.209042 \t7.375266 \t 0.031374 \t 399.874320 s\n","531 \t 15.940764 \t 8.178492 \t7.746880 \t 0.015392 \t 400.409870 s\n","532 \t 15.816078 \t 8.249786 \t7.543683 \t 0.022609 \t 400.953048 s\n","533 \t 15.861792 \t 8.279738 \t7.560640 \t 0.021413 \t 401.501081 s\n","534 \t 15.891582 \t 8.203287 \t7.667324 \t 0.020971 \t 402.042444 s\n","535 \t 15.713088 \t 8.139157 \t7.552848 \t 0.021083 \t 402.579730 s\n","536 \t 15.927647 \t 8.291077 \t7.618623 \t 0.017947 \t 403.247961 s\n","537 \t 15.540024 \t 8.047349 \t7.474266 \t 0.018409 \t 403.773448 s\n","538 \t 15.538896 \t 8.134055 \t7.384394 \t 0.020447 \t 404.312727 s\n","539 \t 15.661761 \t 8.099359 \t7.501395 \t 0.061008 \t 404.842079 s\n","540 \t 15.891315 \t 8.179930 \t7.685333 \t 0.026052 \t 405.386144 s\n","541 \t 15.782721 \t 8.229074 \t7.533755 \t 0.019892 \t 405.928873 s\n","542 \t 15.742444 \t 8.153801 \t7.472786 \t 0.115856 \t 406.478062 s\n","543 \t 15.868595 \t 8.233239 \t7.615668 \t 0.019689 \t 407.011379 s\n","544 \t 15.580634 \t 8.113149 \t7.447576 \t 0.019909 \t 407.562260 s\n","545 \t 15.776407 \t 8.130757 \t7.620776 \t 0.024874 \t 408.233304 s\n","546 \t 16.035892 \t 8.343012 \t7.644282 \t 0.048599 \t 408.774551 s\n","547 \t 15.647633 \t 8.114047 \t7.497050 \t 0.036535 \t 409.414871 s\n","548 \t 16.061530 \t 8.331029 \t7.692155 \t 0.038346 \t 410.047201 s\n","549 \t 15.703200 \t 8.190618 \t7.491838 \t 0.020745 \t 410.695563 s\n","550 \t 15.541516 \t 8.112464 \t7.403491 \t 0.025561 \t 411.374878 s\n","551 \t 15.523154 \t 8.051153 \t7.423731 \t 0.048270 \t 412.053573 s\n","552 \t 15.616551 \t 8.097654 \t7.500206 \t 0.018691 \t 412.598778 s\n","553 \t 15.563986 \t 8.157504 \t7.392609 \t 0.013873 \t 413.129487 s\n","554 \t 15.659575 \t 8.181321 \t7.463755 \t 0.014500 \t 413.796302 s\n","555 \t 15.533772 \t 8.123389 \t7.398156 \t 0.012227 \t 414.324091 s\n","556 \t 15.461036 \t 8.052872 \t7.388525 \t 0.019639 \t 414.892088 s\n","557 \t 15.648999 \t 8.164882 \t7.458312 \t 0.025805 \t 415.439012 s\n","558 \t 15.664219 \t 8.073899 \t7.554381 \t 0.035939 \t 415.976645 s\n","559 \t 15.681147 \t 8.067877 \t7.554844 \t 0.058426 \t 416.510995 s\n","560 \t 15.520330 \t 8.016139 \t7.482342 \t 0.021849 \t 417.042161 s\n","561 \t 15.649796 \t 8.155074 \t7.456486 \t 0.038236 \t 417.584601 s\n","562 \t 15.534105 \t 8.028712 \t7.456909 \t 0.048484 \t 418.110525 s\n","563 \t 15.410057 \t 7.920917 \t7.446949 \t 0.042191 \t 418.784283 s\n","564 \t 15.626649 \t 8.111476 \t7.484135 \t 0.031038 \t 419.324264 s\n","565 \t 15.264542 \t 7.892121 \t7.346138 \t 0.026283 \t 419.859609 s\n","566 \t 15.601195 \t 8.041268 \t7.529739 \t 0.030189 \t 420.401890 s\n","567 \t 15.375563 \t 7.926488 \t7.432913 \t 0.016162 \t 420.955820 s\n","568 \t 15.413797 \t 7.934645 \t7.456659 \t 0.022493 \t 421.492737 s\n","569 \t 15.640096 \t 8.063375 \t7.544948 \t 0.031773 \t 422.096376 s\n","570 \t 15.304380 \t 7.964630 \t7.319469 \t 0.020282 \t 422.782491 s\n","571 \t 15.407924 \t 7.981385 \t7.401767 \t 0.024772 \t 423.419770 s\n","572 \t 15.347682 \t 7.981570 \t7.343225 \t 0.022888 \t 424.249394 s\n","573 \t 15.124342 \t 7.881635 \t7.228461 \t 0.014245 \t 424.994018 s\n","574 \t 15.466706 \t 7.997496 \t7.448153 \t 0.021057 \t 425.531910 s\n","575 \t 15.136210 \t 7.877899 \t7.222291 \t 0.036021 \t 426.073679 s\n","576 \t 15.148921 \t 7.949668 \t7.177506 \t 0.021748 \t 426.607479 s\n","577 \t 15.259497 \t 7.877205 \t7.310222 \t 0.072070 \t 427.148926 s\n","578 \t 15.386619 \t 7.904707 \t7.448772 \t 0.033139 \t 427.679157 s\n","579 \t 15.298747 \t 7.910820 \t7.351604 \t 0.036323 \t 428.242311 s\n","580 \t 15.316141 \t 7.947250 \t7.324627 \t 0.044264 \t 428.773424 s\n","581 \t 15.339997 \t 7.874439 \t7.431895 \t 0.033662 \t 429.445194 s\n","582 \t 15.523840 \t 7.914373 \t7.582811 \t 0.026657 \t 429.986993 s\n","583 \t 15.057249 \t 7.784642 \t7.235013 \t 0.037595 \t 430.522820 s\n","584 \t 15.196076 \t 7.889096 \t7.269681 \t 0.037299 \t 431.068916 s\n","585 \t 15.336944 \t 7.997482 \t7.305174 \t 0.034287 \t 431.599997 s\n","586 \t 15.167207 \t 7.772201 \t7.361398 \t 0.033608 \t 432.144382 s\n","587 \t 15.096952 \t 7.803673 \t7.263788 \t 0.029491 \t 432.672103 s\n","588 \t 15.125508 \t 7.882966 \t7.227277 \t 0.015266 \t 433.217211 s\n","589 \t 15.147007 \t 7.942444 \t7.189529 \t 0.015033 \t 433.745666 s\n","590 \t 15.189219 \t 7.897336 \t7.275166 \t 0.016716 \t 434.416942 s\n","591 \t 15.204170 \t 7.885592 \t7.300763 \t 0.017816 \t 434.953507 s\n","592 \t 15.253080 \t 7.940427 \t7.275249 \t 0.037405 \t 435.624352 s\n","593 \t 15.070343 \t 7.861985 \t7.191637 \t 0.016722 \t 436.309571 s\n","594 \t 14.924975 \t 7.776603 \t7.124616 \t 0.023756 \t 436.958580 s\n","595 \t 15.197447 \t 7.836205 \t7.340788 \t 0.020454 \t 437.628511 s\n","596 \t 15.268551 \t 7.848852 \t7.404559 \t 0.015140 \t 438.273670 s\n","597 \t 14.985916 \t 7.651498 \t7.306155 \t 0.028262 \t 438.811454 s\n","598 \t 15.105934 \t 7.758832 \t7.324071 \t 0.023031 \t 439.347492 s\n","599 \t 14.915823 \t 7.672433 \t7.197845 \t 0.045545 \t 440.011986 s\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 159/159 [00:29<00:00,  5.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3992\n","Hit@10: 0.5887\n","Hit@3: 0.4255\n","Hit@1: 0.3050\n","Link Prediction on Validation Set (All)\n","MRR: 0.3805\n","Hit@10: 0.6187\n","Hit@3: 0.4241\n","Hit@1: 0.2642\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6285\n","Hit@10: 0.7799\n","Hit@3: 0.6604\n","Hit@1: 0.5535\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5314\n","Hit@10: 0.7957\n","Hit@3: 0.6130\n","Hit@1: 0.3929\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0999\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0999\n","600 \t 15.139826 \t 7.822448 \t7.303571 \t 0.013807 \t 470.124575 s\n","601 \t 14.984883 \t 7.772501 \t7.181666 \t 0.030717 \t 470.681046 s\n","602 \t 15.142941 \t 7.751791 \t7.361455 \t 0.029696 \t 471.213419 s\n","603 \t 15.061089 \t 7.859668 \t7.164703 \t 0.036718 \t 471.753009 s\n","604 \t 14.912574 \t 7.755101 \t7.125660 \t 0.031813 \t 472.283251 s\n","605 \t 14.891773 \t 7.698775 \t7.161894 \t 0.031103 \t 472.810632 s\n","606 \t 14.989188 \t 7.680095 \t7.263794 \t 0.045300 \t 473.343380 s\n","607 \t 15.100796 \t 7.864216 \t7.212604 \t 0.023977 \t 473.874475 s\n","608 \t 14.872955 \t 7.640830 \t7.203290 \t 0.028835 \t 474.543319 s\n","609 \t 15.094194 \t 7.792980 \t7.283384 \t 0.017830 \t 475.422479 s\n","610 \t 14.876850 \t 7.680126 \t7.169702 \t 0.027022 \t 476.056133 s\n","611 \t 14.895288 \t 7.702054 \t7.169985 \t 0.023250 \t 476.740590 s\n","612 \t 14.961225 \t 7.742526 \t7.183337 \t 0.035361 \t 477.292073 s\n","613 \t 14.928522 \t 7.721776 \t7.189882 \t 0.016863 \t 477.818125 s\n","614 \t 14.693972 \t 7.591348 \t7.085822 \t 0.016801 \t 478.360470 s\n","615 \t 14.969955 \t 7.654075 \t7.293445 \t 0.022435 \t 478.896945 s\n","616 \t 14.898710 \t 7.664748 \t7.212354 \t 0.021607 \t 479.417388 s\n","617 \t 14.783465 \t 7.628700 \t7.115160 \t 0.039605 \t 479.958300 s\n","618 \t 14.688475 \t 7.637712 \t7.036295 \t 0.014468 \t 480.616930 s\n","619 \t 14.795689 \t 7.673727 \t7.105914 \t 0.016047 \t 481.155104 s\n","620 \t 14.718384 \t 7.579587 \t7.121179 \t 0.017617 \t 481.685330 s\n","621 \t 14.955410 \t 7.774734 \t7.160270 \t 0.020405 \t 482.221761 s\n","622 \t 14.993430 \t 7.702765 \t7.269040 \t 0.021625 \t 482.749792 s\n","623 \t 14.704286 \t 7.645107 \t7.046062 \t 0.013117 \t 483.283223 s\n","624 \t 14.854603 \t 7.707811 \t7.129990 \t 0.016802 \t 483.807645 s\n","625 \t 14.865123 \t 7.642448 \t7.210417 \t 0.012258 \t 484.341089 s\n","626 \t 14.697010 \t 7.538481 \t7.130758 \t 0.027770 \t 484.866002 s\n","627 \t 14.648435 \t 7.561540 \t7.069914 \t 0.016981 \t 485.399416 s\n","628 \t 14.913660 \t 7.766119 \t7.122417 \t 0.025124 \t 486.057206 s\n","629 \t 14.676794 \t 7.519623 \t7.132935 \t 0.024235 \t 486.602998 s\n","630 \t 14.549551 \t 7.497728 \t7.024907 \t 0.026916 \t 487.268313 s\n","631 \t 14.531554 \t 7.405898 \t7.113579 \t 0.012077 \t 487.929680 s\n","632 \t 14.577544 \t 7.560475 \t6.976119 \t 0.040949 \t 488.568740 s\n","633 \t 14.703722 \t 7.610720 \t7.073041 \t 0.019961 \t 489.231998 s\n","634 \t 14.615195 \t 7.532867 \t7.051741 \t 0.030587 \t 489.859328 s\n","635 \t 14.715645 \t 7.614251 \t7.088785 \t 0.012609 \t 490.392825 s\n","636 \t 14.812696 \t 7.586570 \t7.179099 \t 0.047027 \t 490.928536 s\n","637 \t 14.534659 \t 7.576618 \t6.933492 \t 0.024550 \t 491.584244 s\n","638 \t 14.679160 \t 7.546660 \t7.111195 \t 0.021305 \t 492.122923 s\n","639 \t 14.568887 \t 7.527142 \t7.032954 \t 0.008792 \t 492.671974 s\n","640 \t 14.539696 \t 7.460613 \t7.057052 \t 0.022030 \t 493.208245 s\n","641 \t 14.538183 \t 7.525003 \t6.998222 \t 0.014958 \t 493.745013 s\n","642 \t 14.411393 \t 7.473839 \t6.920691 \t 0.016864 \t 494.276911 s\n","643 \t 14.417687 \t 7.385410 \t7.006897 \t 0.025380 \t 494.816066 s\n","644 \t 14.510934 \t 7.476928 \t7.014478 \t 0.019528 \t 495.348502 s\n","645 \t 14.487812 \t 7.468675 \t6.997899 \t 0.021238 \t 495.902363 s\n","646 \t 14.462675 \t 7.413464 \t7.021841 \t 0.027369 \t 496.569056 s\n","647 \t 14.469340 \t 7.428815 \t7.001442 \t 0.039083 \t 497.100380 s\n","648 \t 14.360716 \t 7.386163 \t6.958224 \t 0.016329 \t 497.637146 s\n","649 \t 14.496071 \t 7.511325 \t6.957436 \t 0.027310 \t 498.171682 s\n","650 \t 14.345980 \t 7.431546 \t6.888438 \t 0.025995 \t 498.707664 s\n","651 \t 14.466299 \t 7.520707 \t6.927092 \t 0.018500 \t 499.246504 s\n","652 \t 14.543661 \t 7.505148 \t7.020584 \t 0.017929 \t 499.852198 s\n","653 \t 14.658635 \t 7.456241 \t7.165318 \t 0.037076 \t 500.492014 s\n","654 \t 14.440159 \t 7.374417 \t7.041649 \t 0.024093 \t 501.170603 s\n","655 \t 14.615104 \t 7.493081 \t7.086696 \t 0.035328 \t 501.978768 s\n","656 \t 14.351521 \t 7.474680 \t6.852143 \t 0.024698 \t 502.608739 s\n","657 \t 14.407647 \t 7.391438 \t6.980820 \t 0.035389 \t 503.150458 s\n","658 \t 14.365851 \t 7.398768 \t6.945739 \t 0.021345 \t 503.683085 s\n","659 \t 14.228537 \t 7.294815 \t6.904023 \t 0.029698 \t 504.236257 s\n","660 \t 14.258848 \t 7.308039 \t6.931723 \t 0.019086 \t 504.761989 s\n","661 \t 14.191941 \t 7.270589 \t6.908756 \t 0.012597 \t 505.309448 s\n","662 \t 14.430448 \t 7.347390 \t7.045266 \t 0.037793 \t 505.856804 s\n","663 \t 14.291297 \t 7.370877 \t6.903742 \t 0.016679 \t 506.394475 s\n","664 \t 14.365296 \t 7.420495 \t6.917693 \t 0.027108 \t 507.059572 s\n","665 \t 14.384741 \t 7.386674 \t6.977520 \t 0.020547 \t 507.583525 s\n","666 \t 14.206667 \t 7.200686 \t6.979252 \t 0.026729 \t 508.117025 s\n","667 \t 14.249756 \t 7.293931 \t6.930727 \t 0.025098 \t 508.640265 s\n","668 \t 14.276537 \t 7.236413 \t7.018412 \t 0.021712 \t 509.175011 s\n","669 \t 14.348115 \t 7.397534 \t6.900247 \t 0.050335 \t 509.706340 s\n","670 \t 14.132150 \t 7.191393 \t6.920832 \t 0.019925 \t 510.241235 s\n","671 \t 14.370461 \t 7.402013 \t6.947167 \t 0.021281 \t 510.773723 s\n","672 \t 14.312862 \t 7.452471 \t6.834229 \t 0.026162 \t 511.318131 s\n","673 \t 14.241334 \t 7.312831 \t6.917932 \t 0.010572 \t 511.970340 s\n","674 \t 14.290222 \t 7.322233 \t6.943783 \t 0.024206 \t 512.582522 s\n","675 \t 14.313987 \t 7.402556 \t6.895919 \t 0.015512 \t 513.269298 s\n","676 \t 14.082808 \t 7.190739 \t6.879516 \t 0.012553 \t 513.929960 s\n","677 \t 14.252944 \t 7.291324 \t6.943494 \t 0.018126 \t 514.588034 s\n","678 \t 14.180609 \t 7.299531 \t6.861809 \t 0.019268 \t 515.229605 s\n","679 \t 14.233937 \t 7.331617 \t6.881426 \t 0.020894 \t 515.764630 s\n","680 \t 14.001602 \t 7.178524 \t6.807266 \t 0.015812 \t 516.303760 s\n","681 \t 14.136499 \t 7.318786 \t6.786454 \t 0.031259 \t 516.833242 s\n","682 \t 14.319610 \t 7.311489 \t6.966648 \t 0.041473 \t 517.497104 s\n","683 \t 14.141032 \t 7.237370 \t6.881669 \t 0.021993 \t 518.024038 s\n","684 \t 14.341238 \t 7.351267 \t6.975110 \t 0.014861 \t 518.554558 s\n","685 \t 14.196075 \t 7.313564 \t6.860857 \t 0.021654 \t 519.077202 s\n","686 \t 14.131869 \t 7.231338 \t6.889364 \t 0.011168 \t 519.615928 s\n","687 \t 14.136714 \t 7.205522 \t6.902344 \t 0.028848 \t 520.146889 s\n","688 \t 13.995634 \t 7.150782 \t6.832940 \t 0.011912 \t 520.669225 s\n","689 \t 13.878381 \t 7.106492 \t6.750974 \t 0.020916 \t 521.208508 s\n","690 \t 14.153353 \t 7.270178 \t6.864604 \t 0.018570 \t 521.746152 s\n","691 \t 13.984157 \t 7.196370 \t6.773538 \t 0.014248 \t 522.408995 s\n","692 \t 13.997363 \t 7.186077 \t6.794105 \t 0.017181 \t 522.957277 s\n","693 \t 13.974718 \t 7.180329 \t6.776183 \t 0.018206 \t 523.497536 s\n","694 \t 14.069484 \t 7.187435 \t6.859508 \t 0.022540 \t 524.024308 s\n","695 \t 13.939702 \t 7.183300 \t6.729274 \t 0.027128 \t 524.556558 s\n","696 \t 14.045187 \t 7.182160 \t6.843727 \t 0.019300 \t 525.127350 s\n","697 \t 13.924986 \t 7.184239 \t6.731547 \t 0.009201 \t 525.796870 s\n","698 \t 14.113516 \t 7.240525 \t6.853664 \t 0.019327 \t 526.454418 s\n","699 \t 14.017547 \t 7.216191 \t6.786860 \t 0.014496 \t 527.113611 s\n","700 \t 13.940324 \t 7.143946 \t6.782529 \t 0.013848 \t 527.939035 s\n","701 \t 13.847492 \t 7.189544 \t6.646754 \t 0.011194 \t 528.469764 s\n","702 \t 13.760994 \t 7.033424 \t6.709669 \t 0.017902 \t 528.997763 s\n","703 \t 13.966909 \t 7.187181 \t6.748562 \t 0.031165 \t 529.525941 s\n","704 \t 13.826085 \t 7.057054 \t6.754346 \t 0.014686 \t 530.061721 s\n","705 \t 14.032757 \t 7.224833 \t6.780878 \t 0.027046 \t 530.585042 s\n","706 \t 13.924301 \t 7.140564 \t6.765635 \t 0.018103 \t 531.115916 s\n","707 \t 13.909913 \t 7.064991 \t6.830450 \t 0.014473 \t 531.640671 s\n","708 \t 13.861402 \t 7.181123 \t6.658638 \t 0.021639 \t 532.175654 s\n","709 \t 13.804128 \t 7.062430 \t6.717553 \t 0.024144 \t 532.832919 s\n","710 \t 13.786506 \t 7.058667 \t6.711904 \t 0.015934 \t 533.379585 s\n","711 \t 13.750926 \t 7.031183 \t6.696127 \t 0.023616 \t 533.920536 s\n","712 \t 13.874153 \t 7.212816 \t6.646732 \t 0.014605 \t 534.459015 s\n","713 \t 13.733910 \t 7.076243 \t6.638241 \t 0.019427 \t 534.999008 s\n","714 \t 13.500600 \t 6.905947 \t6.584820 \t 0.009833 \t 535.528559 s\n","715 \t 13.911529 \t 7.180557 \t6.721255 \t 0.009716 \t 536.088138 s\n","716 \t 13.923164 \t 7.135413 \t6.774662 \t 0.013088 \t 536.623591 s\n","717 \t 13.877274 \t 7.127748 \t6.733801 \t 0.015724 \t 537.183371 s\n","718 \t 13.802033 \t 7.092388 \t6.700247 \t 0.009398 \t 537.901138 s\n","719 \t 13.811141 \t 7.129542 \t6.668269 \t 0.013330 \t 538.573608 s\n","720 \t 13.711247 \t 7.053759 \t6.649533 \t 0.007956 \t 539.236819 s\n","721 \t 13.964567 \t 7.183486 \t6.767823 \t 0.013258 \t 539.854677 s\n","722 \t 13.838556 \t 7.064820 \t6.760777 \t 0.012960 \t 540.553227 s\n","723 \t 13.668097 \t 7.042304 \t6.617281 \t 0.008512 \t 541.096375 s\n","724 \t 13.752093 \t 6.942505 \t6.791706 \t 0.017882 \t 541.652773 s\n","725 \t 13.701419 \t 7.040491 \t6.653483 \t 0.007444 \t 542.188151 s\n","726 \t 13.786723 \t 7.009326 \t6.761101 \t 0.016296 \t 542.732944 s\n","727 \t 13.631502 \t 6.986426 \t6.630397 \t 0.014678 \t 543.410623 s\n","728 \t 13.646351 \t 7.000890 \t6.630729 \t 0.014732 \t 543.957450 s\n","729 \t 13.683243 \t 6.964348 \t6.706398 \t 0.012497 \t 544.505642 s\n","730 \t 13.637925 \t 6.986340 \t6.635345 \t 0.016241 \t 545.036651 s\n","731 \t 13.705461 \t 7.008208 \t6.685800 \t 0.011453 \t 545.572209 s\n","732 \t 13.697188 \t 7.060697 \t6.622509 \t 0.013982 \t 546.102292 s\n","733 \t 13.706916 \t 7.050466 \t6.643255 \t 0.013195 \t 546.644107 s\n","734 \t 13.872459 \t 7.135185 \t6.728150 \t 0.009124 \t 547.176872 s\n","735 \t 13.642676 \t 6.974448 \t6.656712 \t 0.011517 \t 547.710101 s\n","736 \t 13.689267 \t 7.032463 \t6.633472 \t 0.023332 \t 548.382134 s\n","737 \t 13.651055 \t 6.940888 \t6.697924 \t 0.012242 \t 548.934452 s\n","738 \t 13.562561 \t 6.914955 \t6.633154 \t 0.014452 \t 549.470537 s\n","739 \t 13.658279 \t 6.986983 \t6.659736 \t 0.011560 \t 550.023550 s\n","740 \t 13.653348 \t 6.992633 \t6.647434 \t 0.013281 \t 550.565230 s\n","741 \t 13.706758 \t 7.033136 \t6.659054 \t 0.014567 \t 551.250304 s\n","742 \t 13.599918 \t 7.002284 \t6.587494 \t 0.010141 \t 551.922565 s\n","743 \t 13.530592 \t 6.941544 \t6.575585 \t 0.013463 \t 552.548106 s\n","744 \t 13.653609 \t 6.997908 \t6.647042 \t 0.008659 \t 553.243977 s\n","745 \t 13.586534 \t 6.970369 \t6.607991 \t 0.008173 \t 553.967046 s\n","746 \t 13.640031 \t 6.967398 \t6.654779 \t 0.017853 \t 554.500858 s\n","747 \t 13.705432 \t 7.044907 \t6.645752 \t 0.014774 \t 555.033331 s\n","748 \t 13.562553 \t 6.947012 \t6.600292 \t 0.015249 \t 555.574232 s\n","749 \t 13.499420 \t 6.960127 \t6.527993 \t 0.011300 \t 556.114813 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:29<00:00,  5.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4790\n","Hit@10: 0.6844\n","Hit@3: 0.5248\n","Hit@1: 0.3794\n","Link Prediction on Validation Set (All)\n","MRR: 0.4646\n","Hit@10: 0.7310\n","Hit@3: 0.5206\n","Hit@1: 0.3386\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6624\n","Hit@10: 0.7925\n","Hit@3: 0.6792\n","Hit@1: 0.5975\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5814\n","Hit@10: 0.8075\n","Hit@3: 0.6464\n","Hit@1: 0.4695\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0591\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0591\n","750 \t 13.633142 \t 6.965434 \t6.652369 \t 0.015339 \t 586.827412 s\n","751 \t 13.340364 \t 6.764786 \t6.568484 \t 0.007094 \t 587.365254 s\n","752 \t 13.491683 \t 6.912469 \t6.562994 \t 0.016220 \t 587.902982 s\n","753 \t 13.460567 \t 6.873350 \t6.577467 \t 0.009750 \t 588.435479 s\n","754 \t 13.464836 \t 6.855317 \t6.594682 \t 0.014837 \t 588.970590 s\n","755 \t 13.509131 \t 6.908897 \t6.591022 \t 0.009212 \t 589.652369 s\n","756 \t 13.544688 \t 6.882097 \t6.652070 \t 0.010521 \t 590.348365 s\n","757 \t 13.402101 \t 6.922702 \t6.467299 \t 0.012100 \t 591.017853 s\n","758 \t 13.382198 \t 6.850245 \t6.517498 \t 0.014455 \t 591.658757 s\n","759 \t 13.439144 \t 6.837695 \t6.592757 \t 0.008692 \t 592.337022 s\n","760 \t 13.569114 \t 6.963115 \t6.595304 \t 0.010695 \t 592.944628 s\n","761 \t 13.522424 \t 6.951204 \t6.548222 \t 0.022998 \t 593.490991 s\n","762 \t 13.245233 \t 6.743633 \t6.493563 \t 0.008037 \t 594.019273 s\n","763 \t 13.376718 \t 6.860780 \t6.503028 \t 0.012910 \t 594.683455 s\n","764 \t 13.406995 \t 6.808852 \t6.590091 \t 0.008052 \t 595.214419 s\n","765 \t 13.405710 \t 6.890287 \t6.507519 \t 0.007904 \t 595.746506 s\n","766 \t 13.310849 \t 6.821073 \t6.474879 \t 0.014897 \t 596.274052 s\n","767 \t 13.346584 \t 6.880068 \t6.461313 \t 0.005203 \t 596.833137 s\n","768 \t 13.418001 \t 6.863033 \t6.534419 \t 0.020549 \t 597.360406 s\n","769 \t 13.399208 \t 6.846658 \t6.537936 \t 0.014613 \t 597.904094 s\n","770 \t 13.299964 \t 6.849959 \t6.437289 \t 0.012717 \t 598.434001 s\n","771 \t 13.388978 \t 6.859476 \t6.517099 \t 0.012403 \t 598.982105 s\n","772 \t 13.273266 \t 6.757020 \t6.503879 \t 0.012367 \t 599.656031 s\n","773 \t 13.350382 \t 6.830691 \t6.507694 \t 0.011997 \t 600.188617 s\n","774 \t 13.405450 \t 6.863593 \t6.522419 \t 0.019438 \t 600.734795 s\n","775 \t 13.482289 \t 6.958248 \t6.511140 \t 0.012901 \t 601.274321 s\n","776 \t 13.365067 \t 6.842626 \t6.508119 \t 0.014322 \t 601.814646 s\n","777 \t 13.267864 \t 6.850200 \t6.405772 \t 0.011893 \t 602.346871 s\n","778 \t 13.329135 \t 6.814908 \t6.503310 \t 0.010917 \t 602.948896 s\n","779 \t 13.210203 \t 6.730410 \t6.462074 \t 0.017719 \t 603.591540 s\n","780 \t 13.369439 \t 6.825628 \t6.535402 \t 0.008408 \t 604.273729 s\n","781 \t 13.257552 \t 6.793020 \t6.454054 \t 0.010479 \t 605.081294 s\n","782 \t 13.422534 \t 6.933960 \t6.477715 \t 0.010860 \t 605.720169 s\n","783 \t 13.481664 \t 6.958337 \t6.512870 \t 0.010457 \t 606.271391 s\n","784 \t 13.309130 \t 6.856099 \t6.442711 \t 0.010320 \t 606.823158 s\n","785 \t 13.277883 \t 6.776664 \t6.493110 \t 0.008109 \t 607.356288 s\n","786 \t 13.264805 \t 6.809000 \t6.445554 \t 0.010251 \t 607.896240 s\n","787 \t 13.297701 \t 6.824371 \t6.463186 \t 0.010145 \t 608.422873 s\n","788 \t 13.204872 \t 6.706035 \t6.487657 \t 0.011179 \t 608.976875 s\n","789 \t 13.140097 \t 6.721483 \t6.403855 \t 0.014759 \t 609.509611 s\n","790 \t 13.277531 \t 6.751341 \t6.510002 \t 0.016188 \t 610.177872 s\n","791 \t 13.239599 \t 6.802854 \t6.420677 \t 0.016068 \t 610.701999 s\n","792 \t 13.173005 \t 6.720441 \t6.438359 \t 0.014205 \t 611.246536 s\n","793 \t 13.139626 \t 6.716257 \t6.412213 \t 0.011155 \t 611.786841 s\n","794 \t 13.291636 \t 6.761499 \t6.521206 \t 0.008931 \t 612.332693 s\n","795 \t 13.347539 \t 6.897722 \t6.439874 \t 0.009943 \t 612.870439 s\n","796 \t 13.230969 \t 6.842132 \t6.379164 \t 0.009674 \t 613.427301 s\n","797 \t 13.230617 \t 6.807854 \t6.407487 \t 0.015275 \t 613.963328 s\n","798 \t 13.215284 \t 6.788877 \t6.416866 \t 0.009541 \t 614.506416 s\n","799 \t 13.082792 \t 6.715118 \t6.358780 \t 0.008895 \t 615.169121 s\n","800 \t 13.215285 \t 6.819230 \t6.377298 \t 0.018757 \t 615.762867 s\n","801 \t 13.129934 \t 6.746858 \t6.368062 \t 0.015015 \t 616.414069 s\n","802 \t 13.105720 \t 6.745880 \t6.350125 \t 0.009715 \t 617.084811 s\n","803 \t 13.288982 \t 6.792366 \t6.479447 \t 0.017169 \t 617.761772 s\n","804 \t 13.230274 \t 6.832347 \t6.385450 \t 0.012476 \t 618.413486 s\n","805 \t 13.202997 \t 6.811728 \t6.375335 \t 0.015935 \t 618.944719 s\n","806 \t 13.123940 \t 6.693002 \t6.421937 \t 0.009001 \t 619.471283 s\n","807 \t 13.131040 \t 6.715823 \t6.405255 \t 0.009962 \t 620.005278 s\n","808 \t 13.206526 \t 6.741616 \t6.452439 \t 0.012471 \t 620.666468 s\n","809 \t 13.172326 \t 6.684524 \t6.476002 \t 0.011800 \t 621.197778 s\n","810 \t 13.005589 \t 6.668628 \t6.329429 \t 0.007531 \t 621.739866 s\n","811 \t 13.164830 \t 6.771410 \t6.383634 \t 0.009787 \t 622.263997 s\n","812 \t 13.148871 \t 6.731027 \t6.403724 \t 0.014121 \t 622.807195 s\n","813 \t 13.186584 \t 6.756389 \t6.423799 \t 0.006396 \t 623.331770 s\n","814 \t 13.101160 \t 6.694800 \t6.391876 \t 0.014484 \t 623.895143 s\n","815 \t 13.036009 \t 6.665184 \t6.361100 \t 0.009725 \t 624.437353 s\n","816 \t 13.096334 \t 6.700842 \t6.384752 \t 0.010739 \t 624.981451 s\n","817 \t 13.157595 \t 6.713193 \t6.435336 \t 0.009066 \t 625.656658 s\n","818 \t 13.109211 \t 6.697957 \t6.400899 \t 0.010356 \t 626.190499 s\n","819 \t 12.970430 \t 6.682628 \t6.279283 \t 0.008519 \t 626.737370 s\n","820 \t 12.954706 \t 6.639562 \t6.301824 \t 0.013321 \t 627.277190 s\n","821 \t 13.032403 \t 6.712252 \t6.307962 \t 0.012189 \t 627.819231 s\n","822 \t 13.088074 \t 6.737252 \t6.342698 \t 0.008125 \t 628.424282 s\n","823 \t 12.986244 \t 6.639828 \t6.333456 \t 0.012961 \t 629.089437 s\n","824 \t 13.048679 \t 6.759192 \t6.282667 \t 0.006820 \t 629.780957 s\n","825 \t 13.103259 \t 6.723546 \t6.371229 \t 0.008484 \t 630.426767 s\n","826 \t 12.992982 \t 6.674320 \t6.311049 \t 0.007613 \t 631.236362 s\n","827 \t 12.900564 \t 6.585757 \t6.305717 \t 0.009091 \t 631.777117 s\n","828 \t 13.068967 \t 6.719811 \t6.341346 \t 0.007810 \t 632.315215 s\n","829 \t 13.009652 \t 6.602062 \t6.397403 \t 0.010187 \t 632.852505 s\n","830 \t 13.057111 \t 6.709927 \t6.337301 \t 0.009883 \t 633.378018 s\n","831 \t 12.844615 \t 6.600585 \t6.237078 \t 0.006951 \t 633.920261 s\n","832 \t 12.915682 \t 6.576223 \t6.324040 \t 0.015419 \t 634.447453 s\n","833 \t 12.938572 \t 6.613338 \t6.314410 \t 0.010825 \t 635.005035 s\n","834 \t 12.964773 \t 6.621190 \t6.337046 \t 0.006536 \t 635.531466 s\n","835 \t 12.974118 \t 6.680722 \t6.288145 \t 0.005251 \t 636.194488 s\n","836 \t 12.967257 \t 6.645063 \t6.313263 \t 0.008930 \t 636.731572 s\n","837 \t 12.970158 \t 6.589429 \t6.373600 \t 0.007129 \t 637.269255 s\n","838 \t 13.098581 \t 6.738912 \t6.346845 \t 0.012824 \t 637.809072 s\n","839 \t 12.931701 \t 6.593455 \t6.327437 \t 0.010809 \t 638.351191 s\n","840 \t 13.069364 \t 6.684377 \t6.371484 \t 0.013504 \t 638.885449 s\n","841 \t 13.061097 \t 6.761782 \t6.292179 \t 0.007135 \t 639.431041 s\n","842 \t 12.909141 \t 6.602433 \t6.300039 \t 0.006670 \t 639.965588 s\n","843 \t 13.038561 \t 6.672247 \t6.359813 \t 0.006502 \t 640.511451 s\n","844 \t 13.011833 \t 6.710482 \t6.287088 \t 0.014263 \t 641.259970 s\n","845 \t 12.986232 \t 6.690851 \t6.284123 \t 0.011257 \t 641.906535 s\n","846 \t 12.963099 \t 6.658068 \t6.293842 \t 0.011188 \t 642.579718 s\n","847 \t 12.963334 \t 6.687351 \t6.267306 \t 0.008677 \t 643.236465 s\n","848 \t 12.983404 \t 6.628714 \t6.346423 \t 0.008267 \t 643.912724 s\n","849 \t 12.875825 \t 6.589097 \t6.279809 \t 0.006919 \t 644.439524 s\n","850 \t 12.971447 \t 6.711905 \t6.249181 \t 0.010361 \t 644.968323 s\n","851 \t 12.884021 \t 6.628576 \t6.244714 \t 0.010731 \t 645.498563 s\n","852 \t 12.946266 \t 6.638757 \t6.300677 \t 0.006831 \t 646.033351 s\n","853 \t 12.995619 \t 6.666171 \t6.316881 \t 0.012568 \t 646.713102 s\n","854 \t 12.996215 \t 6.630181 \t6.356702 \t 0.009333 \t 647.253387 s\n","855 \t 12.939535 \t 6.639686 \t6.288210 \t 0.011639 \t 647.799186 s\n","856 \t 12.943874 \t 6.640868 \t6.294660 \t 0.008346 \t 648.328346 s\n","857 \t 12.938746 \t 6.657696 \t6.270410 \t 0.010641 \t 648.881179 s\n","858 \t 12.875605 \t 6.571623 \t6.295664 \t 0.008318 \t 649.420014 s\n","859 \t 12.921273 \t 6.655172 \t6.256058 \t 0.010042 \t 649.954874 s\n","860 \t 12.943261 \t 6.644131 \t6.289736 \t 0.009394 \t 650.492807 s\n","861 \t 12.900333 \t 6.612655 \t6.281830 \t 0.005848 \t 651.026954 s\n","862 \t 12.827395 \t 6.555594 \t6.263510 \t 0.008292 \t 651.693555 s\n","863 \t 12.996785 \t 6.682178 \t6.307542 \t 0.007064 \t 652.240516 s\n","864 \t 13.007402 \t 6.658844 \t6.339643 \t 0.008916 \t 652.774169 s\n","865 \t 12.883927 \t 6.562352 \t6.312481 \t 0.009094 \t 653.305069 s\n","866 \t 12.895512 \t 6.667037 \t6.217776 \t 0.010699 \t 653.851199 s\n","867 \t 12.870041 \t 6.637300 \t6.223917 \t 0.008823 \t 654.512620 s\n","868 \t 12.858442 \t 6.656971 \t6.193684 \t 0.007786 \t 655.195616 s\n","869 \t 12.904472 \t 6.622763 \t6.271718 \t 0.009991 \t 655.854286 s\n","870 \t 12.804718 \t 6.533796 \t6.263876 \t 0.007046 \t 656.535992 s\n","871 \t 12.782374 \t 6.519241 \t6.257827 \t 0.005306 \t 657.236384 s\n","872 \t 12.842566 \t 6.569523 \t6.265607 \t 0.007435 \t 657.807853 s\n","873 \t 12.873329 \t 6.629153 \t6.235721 \t 0.008455 \t 658.342976 s\n","874 \t 12.887614 \t 6.634707 \t6.246983 \t 0.005924 \t 658.878019 s\n","875 \t 12.770424 \t 6.525417 \t6.233594 \t 0.011412 \t 659.408575 s\n","876 \t 12.749897 \t 6.481370 \t6.254182 \t 0.014345 \t 659.950262 s\n","877 \t 12.869633 \t 6.570500 \t6.292602 \t 0.006531 \t 660.480392 s\n","878 \t 12.859913 \t 6.585478 \t6.267861 \t 0.006574 \t 661.030866 s\n","879 \t 12.856933 \t 6.625383 \t6.218625 \t 0.012925 \t 661.558398 s\n","880 \t 12.807910 \t 6.503251 \t6.291892 \t 0.012767 \t 662.227663 s\n","881 \t 12.868646 \t 6.592022 \t6.268327 \t 0.008297 \t 662.764848 s\n","882 \t 12.762159 \t 6.509959 \t6.245564 \t 0.006636 \t 663.297937 s\n","883 \t 12.795862 \t 6.563700 \t6.225128 \t 0.007035 \t 663.840410 s\n","884 \t 12.938390 \t 6.642518 \t6.290472 \t 0.005401 \t 664.383534 s\n","885 \t 12.867240 \t 6.540902 \t6.317626 \t 0.008711 \t 664.913213 s\n","886 \t 12.822027 \t 6.581440 \t6.229533 \t 0.011054 \t 665.454792 s\n","887 \t 12.762694 \t 6.506891 \t6.249529 \t 0.006274 \t 665.977916 s\n","888 \t 12.716002 \t 6.501432 \t6.204064 \t 0.010506 \t 666.516624 s\n","889 \t 12.770473 \t 6.543240 \t6.220431 \t 0.006802 \t 667.315033 s\n","890 \t 12.699646 \t 6.518187 \t6.168419 \t 0.013041 \t 667.969443 s\n","891 \t 12.806743 \t 6.600854 \t6.197971 \t 0.007918 \t 668.611677 s\n","892 \t 12.749209 \t 6.528759 \t6.209894 \t 0.010556 \t 669.282457 s\n","893 \t 12.670511 \t 6.434300 \t6.228670 \t 0.007542 \t 669.911955 s\n","894 \t 12.724060 \t 6.487979 \t6.228714 \t 0.007367 \t 670.446032 s\n","895 \t 12.858726 \t 6.652520 \t6.200406 \t 0.005800 \t 670.975859 s\n","896 \t 12.842331 \t 6.640872 \t6.196009 \t 0.005450 \t 671.503913 s\n","897 \t 12.694036 \t 6.522963 \t6.163262 \t 0.007811 \t 672.033056 s\n","898 \t 12.854497 \t 6.665998 \t6.181068 \t 0.007430 \t 672.684982 s\n","899 \t 12.834970 \t 6.614505 \t6.212795 \t 0.007671 \t 673.220743 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:28<00:00,  5.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4823\n","Hit@10: 0.6915\n","Hit@3: 0.5213\n","Hit@1: 0.3830\n","Link Prediction on Validation Set (All)\n","MRR: 0.4725\n","Hit@10: 0.7278\n","Hit@3: 0.5269\n","Hit@1: 0.3513\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6478\n","Hit@10: 0.7736\n","Hit@3: 0.6855\n","Hit@1: 0.5786\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5701\n","Hit@10: 0.7682\n","Hit@3: 0.6542\n","Hit@1: 0.4519\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0528\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0528\n","900 \t 12.748009 \t 6.499470 \t6.240567 \t 0.007971 \t 702.790092 s\n","901 \t 12.798204 \t 6.558455 \t6.233899 \t 0.005850 \t 703.329455 s\n","902 \t 12.762193 \t 6.549290 \t6.202626 \t 0.010276 \t 703.860580 s\n","903 \t 12.811028 \t 6.608627 \t6.193975 \t 0.008424 \t 704.389450 s\n","904 \t 12.691208 \t 6.550536 \t6.134354 \t 0.006318 \t 704.929339 s\n","905 \t 12.761120 \t 6.513960 \t6.238571 \t 0.008589 \t 705.518589 s\n","906 \t 12.589207 \t 6.417098 \t6.163775 \t 0.008334 \t 706.170998 s\n","907 \t 12.716747 \t 6.589566 \t6.119142 \t 0.008039 \t 706.839186 s\n","908 \t 12.688386 \t 6.470037 \t6.212441 \t 0.005908 \t 707.642738 s\n","909 \t 12.745569 \t 6.559265 \t6.179004 \t 0.007299 \t 708.317718 s\n","910 \t 12.688681 \t 6.507655 \t6.172276 \t 0.008750 \t 708.857102 s\n","911 \t 12.676102 \t 6.475374 \t6.194293 \t 0.006435 \t 709.386094 s\n","912 \t 12.624451 \t 6.462711 \t6.154669 \t 0.007071 \t 709.937462 s\n","913 \t 12.809010 \t 6.588549 \t6.208585 \t 0.011876 \t 710.458310 s\n","914 \t 12.667283 \t 6.452615 \t6.209522 \t 0.005145 \t 710.999256 s\n","915 \t 12.762677 \t 6.531701 \t6.224639 \t 0.006337 \t 711.536762 s\n","916 \t 12.662302 \t 6.443542 \t6.211635 \t 0.007126 \t 712.068290 s\n","917 \t 12.720041 \t 6.537559 \t6.175624 \t 0.006859 \t 712.726102 s\n","918 \t 12.626754 \t 6.435917 \t6.184453 \t 0.006385 \t 713.264728 s\n","919 \t 12.671051 \t 6.477703 \t6.186923 \t 0.006424 \t 713.792774 s\n","920 \t 12.762108 \t 6.537074 \t6.217608 \t 0.007425 \t 714.353996 s\n","921 \t 12.685680 \t 6.478622 \t6.200343 \t 0.006715 \t 714.887063 s\n","922 \t 12.652830 \t 6.424849 \t6.217951 \t 0.010030 \t 715.434901 s\n","923 \t 12.689599 \t 6.518571 \t6.155676 \t 0.015352 \t 715.971390 s\n","924 \t 12.640146 \t 6.470282 \t6.162605 \t 0.007259 \t 716.504492 s\n","925 \t 12.799193 \t 6.591509 \t6.199815 \t 0.007869 \t 717.051671 s\n","926 \t 12.708239 \t 6.533598 \t6.167952 \t 0.006689 \t 717.574203 s\n","927 \t 12.733386 \t 6.532823 \t6.194299 \t 0.006264 \t 718.294246 s\n","928 \t 12.890260 \t 6.634497 \t6.248424 \t 0.007339 \t 718.939785 s\n","929 \t 12.654846 \t 6.473544 \t6.171536 \t 0.009766 \t 719.607327 s\n","930 \t 12.707321 \t 6.556050 \t6.138761 \t 0.012510 \t 720.279552 s\n","931 \t 12.743909 \t 6.501383 \t6.231657 \t 0.010869 \t 720.950079 s\n","932 \t 12.772743 \t 6.567390 \t6.195725 \t 0.009627 \t 721.483134 s\n","933 \t 12.681666 \t 6.500406 \t6.172689 \t 0.008571 \t 722.006649 s\n","934 \t 12.625805 \t 6.454782 \t6.162051 \t 0.008972 \t 722.531239 s\n","935 \t 12.778212 \t 6.581977 \t6.188672 \t 0.007563 \t 723.059651 s\n","936 \t 12.761807 \t 6.601617 \t6.147841 \t 0.012349 \t 723.721003 s\n","937 \t 12.705667 \t 6.535692 \t6.162455 \t 0.007521 \t 724.257225 s\n","938 \t 12.729807 \t 6.494873 \t6.228047 \t 0.006888 \t 724.796932 s\n","939 \t 12.747492 \t 6.552694 \t6.188421 \t 0.006376 \t 725.331825 s\n","940 \t 12.646427 \t 6.514479 \t6.125345 \t 0.006602 \t 725.877214 s\n","941 \t 12.641167 \t 6.489762 \t6.146614 \t 0.004791 \t 726.412302 s\n","942 \t 12.671849 \t 6.500422 \t6.161950 \t 0.009477 \t 726.942343 s\n","943 \t 12.592156 \t 6.426907 \t6.158102 \t 0.007147 \t 727.487387 s\n","944 \t 12.636758 \t 6.456901 \t6.172365 \t 0.007492 \t 728.019928 s\n","945 \t 12.713085 \t 6.564946 \t6.143058 \t 0.005081 \t 728.676859 s\n","946 \t 12.688340 \t 6.453233 \t6.227746 \t 0.007361 \t 729.210805 s\n","947 \t 12.590892 \t 6.468952 \t6.115273 \t 0.006667 \t 729.741093 s\n","948 \t 12.617731 \t 6.416096 \t6.195359 \t 0.006275 \t 730.270964 s\n","949 \t 12.700571 \t 6.563988 \t6.130141 \t 0.006442 \t 730.795027 s\n","950 \t 12.676526 \t 6.519359 \t6.151519 \t 0.005648 \t 731.446720 s\n","951 \t 12.626054 \t 6.490954 \t6.124230 \t 0.010870 \t 732.115345 s\n","952 \t 12.603554 \t 6.489293 \t6.109360 \t 0.004901 \t 732.777457 s\n","953 \t 12.623005 \t 6.475866 \t6.141184 \t 0.005955 \t 733.445624 s\n","954 \t 12.618526 \t 6.463873 \t6.148663 \t 0.005990 \t 734.187578 s\n","955 \t 12.742367 \t 6.551858 \t6.181065 \t 0.009444 \t 734.720804 s\n","956 \t 12.629689 \t 6.453972 \t6.167515 \t 0.008201 \t 735.258218 s\n","957 \t 12.702929 \t 6.507611 \t6.187112 \t 0.008206 \t 735.799482 s\n","958 \t 12.632653 \t 6.427634 \t6.198267 \t 0.006752 \t 736.332419 s\n","959 \t 12.768506 \t 6.586733 \t6.175565 \t 0.006208 \t 736.871449 s\n","960 \t 12.629438 \t 6.498350 \t6.125481 \t 0.005607 \t 737.414400 s\n","961 \t 12.718470 \t 6.473099 \t6.234490 \t 0.010881 \t 737.967636 s\n","962 \t 12.717978 \t 6.522092 \t6.191630 \t 0.004255 \t 738.504655 s\n","963 \t 12.560106 \t 6.427741 \t6.125370 \t 0.006995 \t 739.173058 s\n","964 \t 12.664380 \t 6.451735 \t6.204443 \t 0.008202 \t 739.696898 s\n","965 \t 12.636741 \t 6.498707 \t6.129950 \t 0.008084 \t 740.246796 s\n","966 \t 12.687654 \t 6.508351 \t6.172992 \t 0.006311 \t 740.778926 s\n","967 \t 12.644965 \t 6.447414 \t6.189684 \t 0.007867 \t 741.330744 s\n","968 \t 12.772859 \t 6.560630 \t6.205360 \t 0.006869 \t 741.869868 s\n","969 \t 12.644666 \t 6.456578 \t6.180051 \t 0.008037 \t 742.418277 s\n","970 \t 12.563878 \t 6.416064 \t6.137052 \t 0.010762 \t 742.958381 s\n","971 \t 12.573073 \t 6.445285 \t6.120465 \t 0.007324 \t 743.500118 s\n","972 \t 12.593101 \t 6.466629 \t6.121135 \t 0.005337 \t 744.272709 s\n","973 \t 12.761145 \t 6.546701 \t6.208308 \t 0.006135 \t 744.941995 s\n","974 \t 12.695440 \t 6.517680 \t6.170187 \t 0.007573 \t 745.596143 s\n","975 \t 12.781633 \t 6.584036 \t6.191922 \t 0.005676 \t 746.261220 s\n","976 \t 12.748112 \t 6.604892 \t6.135926 \t 0.007294 \t 746.906013 s\n","977 \t 12.619249 \t 6.495138 \t6.117631 \t 0.006480 \t 747.434819 s\n","978 \t 12.604115 \t 6.469852 \t6.124260 \t 0.010004 \t 747.967185 s\n","979 \t 12.628942 \t 6.447270 \t6.175621 \t 0.006050 \t 748.508549 s\n","980 \t 12.529138 \t 6.384176 \t6.138752 \t 0.006209 \t 749.042140 s\n","981 \t 12.691783 \t 6.555523 \t6.130285 \t 0.005975 \t 749.704997 s\n","982 \t 12.706049 \t 6.508848 \t6.186370 \t 0.010831 \t 750.243006 s\n","983 \t 12.577510 \t 6.442495 \t6.131037 \t 0.003978 \t 750.783954 s\n","984 \t 12.687481 \t 6.555284 \t6.125113 \t 0.007084 \t 751.324028 s\n","985 \t 12.719386 \t 6.506174 \t6.205505 \t 0.007708 \t 751.843725 s\n","986 \t 12.633062 \t 6.471490 \t6.154238 \t 0.007333 \t 752.382680 s\n","987 \t 12.534513 \t 6.381861 \t6.144935 \t 0.007718 \t 752.913730 s\n","988 \t 12.582832 \t 6.452777 \t6.123338 \t 0.006717 \t 753.446661 s\n","989 \t 12.550992 \t 6.400037 \t6.146782 \t 0.004173 \t 753.982955 s\n","990 \t 12.607458 \t 6.444494 \t6.155455 \t 0.007508 \t 754.637773 s\n","991 \t 12.606550 \t 6.451722 \t6.145905 \t 0.008923 \t 755.166194 s\n","992 \t 12.618209 \t 6.504360 \t6.106690 \t 0.007158 \t 755.697411 s\n","993 \t 12.622120 \t 6.455400 \t6.160726 \t 0.005994 \t 756.236440 s\n","994 \t 12.756157 \t 6.541122 \t6.209250 \t 0.005785 \t 756.838260 s\n","995 \t 12.469931 \t 6.387570 \t6.074592 \t 0.007768 \t 757.460873 s\n","996 \t 12.720275 \t 6.543443 \t6.168185 \t 0.008648 \t 758.148281 s\n","997 \t 12.637735 \t 6.463689 \t6.168282 \t 0.005764 \t 758.789313 s\n","998 \t 12.702738 \t 6.522901 \t6.172671 \t 0.007167 \t 759.458855 s\n","999 \t 12.588728 \t 6.461071 \t6.118725 \t 0.008931 \t 760.127090 s\n","1000 \t 12.586756 \t 6.477883 \t6.103888 \t 0.004985 \t 760.661636 s\n","1001 \t 12.568176 \t 6.386884 \t6.176399 \t 0.004894 \t 761.200302 s\n","1002 \t 12.629035 \t 6.491541 \t6.129586 \t 0.007908 \t 761.740473 s\n","1003 \t 12.667277 \t 6.515393 \t6.146424 \t 0.005460 \t 762.271211 s\n","1004 \t 12.687325 \t 6.493905 \t6.187069 \t 0.006351 \t 762.803366 s\n","1005 \t 12.508138 \t 6.427888 \t6.073422 \t 0.006828 \t 763.329618 s\n","1006 \t 12.588223 \t 6.495719 \t6.086594 \t 0.005910 \t 763.876079 s\n","1007 \t 12.564292 \t 6.379241 \t6.176739 \t 0.008311 \t 764.418261 s\n","1008 \t 12.560977 \t 6.428407 \t6.126989 \t 0.005581 \t 765.092738 s\n","1009 \t 12.548230 \t 6.444949 \t6.094350 \t 0.008932 \t 765.629496 s\n","1010 \t 12.587997 \t 6.462742 \t6.121320 \t 0.003935 \t 766.170849 s\n","1011 \t 12.546728 \t 6.380252 \t6.157976 \t 0.008500 \t 766.700515 s\n","1012 \t 12.554223 \t 6.481240 \t6.066973 \t 0.006010 \t 767.256250 s\n","1013 \t 12.616386 \t 6.453143 \t6.153296 \t 0.009948 \t 767.785009 s\n","1014 \t 12.687810 \t 6.517915 \t6.162557 \t 0.007339 \t 768.327180 s\n","1015 \t 12.656129 \t 6.477000 \t6.172542 \t 0.006587 \t 768.856016 s\n","1016 \t 12.714046 \t 6.549139 \t6.160128 \t 0.004780 \t 769.418793 s\n","1017 \t 12.565658 \t 6.400879 \t6.158671 \t 0.006108 \t 770.241507 s\n","1018 \t 12.622041 \t 6.449043 \t6.168573 \t 0.004425 \t 770.890639 s\n","1019 \t 12.589276 \t 6.460610 \t6.122759 \t 0.005907 \t 771.532162 s\n","1020 \t 12.748446 \t 6.577202 \t6.164784 \t 0.006460 \t 772.228895 s\n","1021 \t 12.639968 \t 6.525124 \t6.108075 \t 0.006768 \t 772.761035 s\n","1022 \t 12.429936 \t 6.331143 \t6.090433 \t 0.008360 \t 773.306803 s\n","1023 \t 12.680003 \t 6.528955 \t6.140115 \t 0.010932 \t 773.830678 s\n","1024 \t 12.629715 \t 6.485868 \t6.134627 \t 0.009219 \t 774.371804 s\n","1025 \t 12.632301 \t 6.441237 \t6.185340 \t 0.005724 \t 774.899300 s\n","1026 \t 12.639988 \t 6.499089 \t6.133836 \t 0.007063 \t 775.554599 s\n","1027 \t 12.606261 \t 6.472664 \t6.127452 \t 0.006145 \t 776.085251 s\n","1028 \t 12.529776 \t 6.396031 \t6.126436 \t 0.007310 \t 776.622093 s\n","1029 \t 12.674461 \t 6.486884 \t6.181341 \t 0.006235 \t 777.155248 s\n","1030 \t 12.546247 \t 6.405246 \t6.134623 \t 0.006378 \t 777.699856 s\n","1031 \t 12.695917 \t 6.542611 \t6.146022 \t 0.007283 \t 778.240106 s\n","1032 \t 12.610888 \t 6.445744 \t6.158674 \t 0.006472 \t 778.808639 s\n","1033 \t 12.549259 \t 6.399957 \t6.141818 \t 0.007484 \t 779.346049 s\n","1034 \t 12.626327 \t 6.480913 \t6.139530 \t 0.005884 \t 779.872443 s\n","1035 \t 12.722465 \t 6.539552 \t6.170999 \t 0.011914 \t 780.541165 s\n","1036 \t 12.622244 \t 6.482327 \t6.132331 \t 0.007587 \t 781.066965 s\n","1037 \t 12.566411 \t 6.439704 \t6.118111 \t 0.008596 \t 781.603795 s\n","1038 \t 12.631947 \t 6.487629 \t6.138219 \t 0.006099 \t 782.142616 s\n","1039 \t 12.639925 \t 6.465061 \t6.167671 \t 0.007193 \t 782.796588 s\n","1040 \t 12.563670 \t 6.397075 \t6.159737 \t 0.006858 \t 783.456728 s\n","1041 \t 12.611187 \t 6.415164 \t6.190040 \t 0.005982 \t 784.102653 s\n","1042 \t 12.646702 \t 6.478700 \t6.163386 \t 0.004617 \t 784.776402 s\n","1043 \t 12.541222 \t 6.347827 \t6.186642 \t 0.006752 \t 785.409172 s\n","1044 \t 12.578920 \t 6.449346 \t6.123919 \t 0.005656 \t 786.069128 s\n","1045 \t 12.665299 \t 6.526264 \t6.134148 \t 0.004887 \t 786.592424 s\n","1046 \t 12.635170 \t 6.462039 \t6.165936 \t 0.007194 \t 787.142721 s\n","1047 \t 12.622488 \t 6.472696 \t6.140873 \t 0.008919 \t 787.684582 s\n","1048 \t 12.680734 \t 6.559917 \t6.115402 \t 0.005415 \t 788.226447 s\n","1049 \t 12.633162 \t 6.474006 \t6.151743 \t 0.007413 \t 788.768680 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 159/159 [00:28<00:00,  5.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4836\n","Hit@10: 0.7021\n","Hit@3: 0.5213\n","Hit@1: 0.3865\n","Link Prediction on Validation Set (All)\n","MRR: 0.4774\n","Hit@10: 0.7278\n","Hit@3: 0.5222\n","Hit@1: 0.3608\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6549\n","Hit@10: 0.8050\n","Hit@3: 0.6792\n","Hit@1: 0.5849\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5811\n","Hit@10: 0.7839\n","Hit@3: 0.6582\n","Hit@1: 0.4695\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0286\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0286\n"]}]},{"cell_type":"markdown","source":["# Test.py\n"],"metadata":{"id":"-BsiHiMQoArV"}},{"cell_type":"code","source":["KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix.cuda(), # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix.cuda(), # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix.cuda(), # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix.cuda(), # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask.cuda(), # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask.cuda(), # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTHNKG-NT/checkpoint/Reproduce/VTHNNTV_maximg==3/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","                else: # qualifier\n","                    sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n"],"metadata":{"id":"ChVIC_5BHELi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747361295377,"user_tz":-540,"elapsed":40248,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"e35bf98f-dc2c-4765-88af-ea136b63fe46"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/161 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: 0.0053, SE: 0.000028\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 2/161 [00:00<00:29,  5.36it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 1.0000, Pred: 1.0630, SE: 0.003966\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 10/161 [00:01<00:22,  6.65it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 1.0000, Pred: 1.0547, SE: 0.002995\n","[Triplet Num] GT: 0.0508, Pred: 0.0139, SE: 0.001362\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 11/161 [00:01<00:25,  5.97it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0002, Pred: -0.0012, SE: 0.000002\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 17/161 [00:03<00:28,  5.11it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0169, Pred: 0.0167, SE: 0.000000\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▍        | 24/161 [00:04<00:27,  4.96it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0746, Pred: 0.0131, SE: 0.003785\n"]},{"output_type":"stream","name":"stderr","text":[" 17%|█▋        | 27/161 [00:05<00:26,  5.07it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0093, SE: 0.000087\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|█▉        | 32/161 [00:06<00:29,  4.35it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.3467, Pred: 0.3587, SE: 0.000142\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 44/161 [00:09<00:26,  4.37it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0339, Pred: 0.0218, SE: 0.000146\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▊       | 46/161 [00:09<00:21,  5.24it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: 0.0019, SE: 0.000004\n"]},{"output_type":"stream","name":"stderr","text":[" 31%|███       | 50/161 [00:10<00:20,  5.35it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: 0.0016, SE: 0.000003\n"]},{"output_type":"stream","name":"stderr","text":[" 37%|███▋      | 59/161 [00:11<00:21,  4.84it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0007, Pred: -0.0115, SE: 0.000150\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 65/161 [00:12<00:19,  4.95it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: 0.0068, SE: 0.000046\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 69/161 [00:13<00:14,  6.41it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0007, Pred: -0.0023, SE: 0.000009\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 75/161 [00:14<00:17,  4.98it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.1206, Pred: 0.1417, SE: 0.000447\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 83/161 [00:16<00:14,  5.44it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0045, Pred: -0.0236, SE: 0.000788\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▌    | 89/161 [00:17<00:13,  5.31it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0508, Pred: 0.0076, SE: 0.001867\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 110/161 [00:22<00:11,  4.31it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0508, Pred: 0.0295, SE: 0.000454\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|██████▉   | 112/161 [00:22<00:09,  5.26it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0169, Pred: 0.0295, SE: 0.000157\n"]},{"output_type":"stream","name":"stderr","text":[" 82%|████████▏ | 132/161 [00:26<00:04,  6.01it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.1525, Pred: 0.1651, SE: 0.000158\n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 133/161 [00:26<00:04,  5.63it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: 0.0520, SE: 0.002703\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 140/161 [00:27<00:04,  5.11it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0045, Pred: -0.0251, SE: 0.000874\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 145/161 [00:28<00:03,  5.24it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0795, Pred: 0.0508, SE: 0.000824\n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████▏| 147/161 [00:29<00:02,  5.88it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0175, SE: 0.000306\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 154/161 [00:30<00:00,  7.57it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 1.0000, Pred: 1.1001, SE: 0.010023\n","[Triplet Num] GT: 1.0000, Pred: 1.1301, SE: 0.016932\n","[Triplet Num] GT: 0.0000, Pred: -0.0333, SE: 0.001110\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 161/161 [00:31<00:00,  5.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5337\n","Hit@10: 0.6939\n","Hit@3: 0.5646\n","Hit@1: 0.4490\n","Link Prediction on Validation Set (All)\n","MRR: 0.4899\n","Hit@10: 0.6985\n","Hit@3: 0.5397\n","Hit@1: 0.3838\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6064\n","Hit@10: 0.7950\n","Hit@3: 0.6398\n","Hit@1: 0.5217\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5215\n","Hit@10: 0.7349\n","Hit@3: 0.5868\n","Hit@1: 0.4059\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0420\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0420\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}