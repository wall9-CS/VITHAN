{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyMhZ8G7UpZ9m0wSk/aSvqQ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747355413454,"user_tz":-540,"elapsed":25374,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"3889cd31-45e4-4397-b2e2-fa7f87bd0d05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n","\n","import pandas as pd"],"metadata":{"id":"xWGfSBgsm1r2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","        self.proj_rel_vis = nn.Linear(dim_vis * 3, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)\n"],"metadata":{"id":"2CgXgeAXmg-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTMmNF8Cl5it"},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-CQI/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        # 경로 수정 visual feature는 VTHNKG 인용\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + \"visual_fetures_ent_sorted\")\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel_sorted')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile(self.dir + 'visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(self.dir + 'visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-CQI/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1747355484698,"user_tz":-540,"elapsed":9,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"6ae3f4f1-be3f-4312-ba85-1a8d1b54047c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-CQI\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-CQI_Retry\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    print(f\"[DEBUG] Total RP (Tri) samples collected: {len(rp_tri_list_rank)}\")\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747357090260,"user_tz":-540,"elapsed":1004237,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"fe8c1b7f-2588-47bb-af72-a69514c393cf","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 44.253408 \t 11.332339 \t11.075620 \t 21.845449 \t 0.687901 s\n","1 \t 29.226954 \t 11.102822 \t10.867203 \t 7.256928 \t 1.414813 s\n","2 \t 23.533183 \t 10.440206 \t10.015895 \t 3.077082 \t 1.981277 s\n","3 \t 21.345242 \t 10.339816 \t9.520745 \t 1.484681 \t 2.543401 s\n","4 \t 20.656482 \t 10.126626 \t9.688233 \t 0.841623 \t 3.111599 s\n","5 \t 20.530315 \t 10.253610 \t9.834796 \t 0.441908 \t 3.681434 s\n","6 \t 20.019605 \t 9.922863 \t9.596168 \t 0.500575 \t 4.240871 s\n","7 \t 19.753604 \t 10.195607 \t9.191381 \t 0.366615 \t 4.795542 s\n","8 \t 20.156082 \t 10.228010 \t9.454879 \t 0.473193 \t 5.434353 s\n","9 \t 19.520997 \t 10.010941 \t9.163506 \t 0.346551 \t 6.117238 s\n","10 \t 19.615347 \t 9.989136 \t9.294997 \t 0.331214 \t 6.769661 s\n","11 \t 19.563332 \t 9.873165 \t9.560370 \t 0.129796 \t 7.717638 s\n","12 \t 19.196070 \t 9.523149 \t9.218548 \t 0.454373 \t 8.404260 s\n","13 \t 18.566321 \t 9.571149 \t8.808692 \t 0.186480 \t 8.963995 s\n","14 \t 19.759119 \t 10.414293 \t9.088380 \t 0.256445 \t 9.531125 s\n","15 \t 19.547473 \t 9.894313 \t9.294504 \t 0.358656 \t 10.098681 s\n","16 \t 18.807611 \t 9.862202 \t8.770645 \t 0.174765 \t 10.678647 s\n","17 \t 18.888338 \t 9.663603 \t8.929639 \t 0.295096 \t 11.240454 s\n","18 \t 19.626235 \t 10.041631 \t9.236346 \t 0.348258 \t 11.795639 s\n","19 \t 18.971379 \t 10.123412 \t8.650548 \t 0.197419 \t 12.366719 s\n","20 \t 19.600518 \t 9.939093 \t9.166556 \t 0.494870 \t 12.930495 s\n","21 \t 19.740716 \t 10.066204 \t9.457251 \t 0.217262 \t 13.488737 s\n","22 \t 18.954569 \t 9.814083 \t8.923818 \t 0.216669 \t 14.199399 s\n","23 \t 18.512791 \t 9.540704 \t8.771995 \t 0.200092 \t 14.765476 s\n","24 \t 19.365940 \t 10.103655 \t8.962658 \t 0.299626 \t 15.324728 s\n","25 \t 19.217935 \t 10.192343 \t8.676493 \t 0.349098 \t 15.895912 s\n","26 \t 18.408571 \t 9.986780 \t8.193118 \t 0.228673 \t 16.464683 s\n","27 \t 19.304473 \t 10.280064 \t8.884891 \t 0.139519 \t 17.023221 s\n","28 \t 18.631537 \t 9.673937 \t8.721498 \t 0.236103 \t 17.586972 s\n","29 \t 18.709254 \t 9.720720 \t8.807854 \t 0.180680 \t 18.149837 s\n","30 \t 18.453795 \t 9.707720 \t8.556859 \t 0.189215 \t 18.842939 s\n","31 \t 19.607224 \t 10.176456 \t9.300178 \t 0.130591 \t 19.506048 s\n","32 \t 18.943368 \t 10.198390 \t8.626473 \t 0.118506 \t 20.180840 s\n","33 \t 18.396158 \t 9.640387 \t8.529786 \t 0.225986 \t 21.142933 s\n","34 \t 18.653518 \t 10.107641 \t8.330610 \t 0.215266 \t 21.825790 s\n","35 \t 18.203613 \t 9.720055 \t8.314496 \t 0.169063 \t 22.389754 s\n","36 \t 19.086963 \t 10.071135 \t8.717844 \t 0.297985 \t 22.961250 s\n","37 \t 18.905542 \t 9.678612 \t9.015692 \t 0.211238 \t 23.524168 s\n","38 \t 19.498347 \t 10.357655 \t8.611336 \t 0.529357 \t 24.093962 s\n","39 \t 19.343419 \t 10.245266 \t8.807759 \t 0.290394 \t 24.661731 s\n","40 \t 18.279421 \t 9.646912 \t8.511257 \t 0.121252 \t 25.230042 s\n","41 \t 18.155625 \t 9.520464 \t8.413600 \t 0.221561 \t 25.801258 s\n","42 \t 18.570537 \t 9.534093 \t8.879525 \t 0.156918 \t 26.360000 s\n","43 \t 19.202899 \t 10.040244 \t8.931686 \t 0.230969 \t 26.942596 s\n","44 \t 18.521659 \t 9.747396 \t8.493342 \t 0.280921 \t 27.496572 s\n","45 \t 18.692744 \t 9.797436 \t8.680307 \t 0.215001 \t 28.063418 s\n","46 \t 19.833914 \t 10.186642 \t9.277042 \t 0.370230 \t 28.777809 s\n","47 \t 18.974076 \t 9.945748 \t8.771769 \t 0.256560 \t 29.360808 s\n","48 \t 18.010807 \t 9.102625 \t8.719938 \t 0.188244 \t 29.932477 s\n","49 \t 18.298779 \t 9.458991 \t8.488634 \t 0.351154 \t 30.487946 s\n","50 \t 18.073123 \t 9.536685 \t8.400125 \t 0.136313 \t 31.065939 s\n","51 \t 19.242023 \t 10.000247 \t8.967672 \t 0.274102 \t 31.676157 s\n","52 \t 18.221868 \t 9.369557 \t8.724453 \t 0.127858 \t 32.364396 s\n","53 \t 18.985758 \t 9.943733 \t8.711962 \t 0.330063 \t 33.005398 s\n","54 \t 19.429688 \t 9.993687 \t9.134162 \t 0.301839 \t 33.726129 s\n","55 \t 18.888704 \t 9.910300 \t8.860614 \t 0.117790 \t 34.611812 s\n","56 \t 18.612068 \t 9.775729 \t8.662319 \t 0.174021 \t 35.161519 s\n","57 \t 18.219891 \t 9.514391 \t8.551780 \t 0.153719 \t 35.741287 s\n","58 \t 18.385070 \t 9.670928 \t8.571311 \t 0.142832 \t 36.295544 s\n","59 \t 17.867849 \t 9.503493 \t8.186935 \t 0.177420 \t 36.867932 s\n","60 \t 18.683323 \t 9.480969 \t8.798480 \t 0.403874 \t 37.439013 s\n","61 \t 18.191251 \t 9.081543 \t8.933246 \t 0.176463 \t 37.990382 s\n","62 \t 18.823435 \t 9.349988 \t9.312492 \t 0.160954 \t 38.557939 s\n","63 \t 18.816302 \t 10.240438 \t8.439623 \t 0.136242 \t 39.121703 s\n","64 \t 18.747513 \t 9.682850 \t8.593912 \t 0.470751 \t 39.690138 s\n","65 \t 18.827582 \t 9.912828 \t8.681873 \t 0.232881 \t 40.397773 s\n","66 \t 18.582449 \t 9.715274 \t8.685459 \t 0.181715 \t 40.981154 s\n","67 \t 18.642264 \t 9.789048 \t8.601009 \t 0.252208 \t 41.547634 s\n","68 \t 18.743283 \t 10.027637 \t8.382894 \t 0.332754 \t 42.105697 s\n","69 \t 19.013224 \t 9.909643 \t8.858712 \t 0.244868 \t 42.670522 s\n","70 \t 19.202317 \t 9.736567 \t9.315936 \t 0.149815 \t 43.227180 s\n","71 \t 18.800570 \t 9.453225 \t9.089816 \t 0.257531 \t 43.792237 s\n","72 \t 18.585791 \t 9.924150 \t8.484489 \t 0.177152 \t 44.344776 s\n","73 \t 19.352859 \t 10.057994 \t9.121556 \t 0.173310 \t 44.994079 s\n","74 \t 18.960993 \t 9.774805 \t9.086728 \t 0.099460 \t 45.663139 s\n","75 \t 18.431570 \t 9.644024 \t8.661321 \t 0.126225 \t 46.524939 s\n","76 \t 19.085197 \t 9.798195 \t9.137825 \t 0.149179 \t 47.258535 s\n","77 \t 18.517406 \t 9.878591 \t8.448729 \t 0.190088 \t 47.927666 s\n","78 \t 18.313787 \t 9.883431 \t8.239082 \t 0.191273 \t 48.485673 s\n","79 \t 18.799783 \t 9.734137 \t8.942641 \t 0.123003 \t 49.042719 s\n","80 \t 18.653916 \t 9.863226 \t8.602651 \t 0.188039 \t 49.596464 s\n","81 \t 18.901178 \t 10.019877 \t8.772480 \t 0.108821 \t 50.154728 s\n","82 \t 17.737152 \t 9.114071 \t8.465406 \t 0.157675 \t 50.722062 s\n","83 \t 19.155425 \t 9.828643 \t8.786817 \t 0.539965 \t 51.287517 s\n","84 \t 18.505194 \t 9.443633 \t8.769677 \t 0.291884 \t 51.853522 s\n","85 \t 17.803822 \t 9.550503 \t8.065825 \t 0.187492 \t 52.424634 s\n","86 \t 18.140523 \t 9.282058 \t8.737346 \t 0.121118 \t 53.146465 s\n","87 \t 17.601850 \t 9.157708 \t8.336488 \t 0.107653 \t 53.713308 s\n","88 \t 18.502938 \t 9.965602 \t8.428847 \t 0.108488 \t 54.278793 s\n","89 \t 19.164566 \t 10.278816 \t8.755794 \t 0.129957 \t 54.836977 s\n","90 \t 17.643739 \t 9.314941 \t8.224044 \t 0.104752 \t 55.396854 s\n","91 \t 18.843738 \t 9.511861 \t8.855547 \t 0.476329 \t 55.953469 s\n","92 \t 18.335390 \t 9.729921 \t8.477126 \t 0.128344 \t 56.520132 s\n","93 \t 19.471324 \t 9.670957 \t9.496796 \t 0.303570 \t 57.085274 s\n","94 \t 18.468996 \t 9.504594 \t8.848971 \t 0.115432 \t 57.718777 s\n","95 \t 18.407685 \t 9.454833 \t8.865758 \t 0.087095 \t 58.398370 s\n","96 \t 19.232198 \t 10.191761 \t8.896772 \t 0.143664 \t 59.046584 s\n","97 \t 18.423127 \t 9.732359 \t8.595726 \t 0.095042 \t 59.715254 s\n","98 \t 18.655182 \t 10.014205 \t8.530664 \t 0.110311 \t 60.633728 s\n","99 \t 19.178426 \t 9.876514 \t9.167334 \t 0.134578 \t 61.187418 s\n","100 \t 18.934460 \t 10.065818 \t8.752574 \t 0.116068 \t 61.754004 s\n","101 \t 18.792316 \t 9.953043 \t8.718601 \t 0.120672 \t 62.312608 s\n","102 \t 18.899413 \t 9.849495 \t8.937430 \t 0.112487 \t 62.890637 s\n","103 \t 18.534619 \t 9.662101 \t8.685153 \t 0.187366 \t 63.461560 s\n","104 \t 18.799761 \t 9.840474 \t8.868511 \t 0.090775 \t 64.029738 s\n","105 \t 18.886831 \t 10.253701 \t8.470782 \t 0.162348 \t 64.595366 s\n","106 \t 18.987081 \t 9.887181 \t9.009844 \t 0.090055 \t 65.162974 s\n","107 \t 18.217047 \t 9.366683 \t8.684892 \t 0.165472 \t 65.720083 s\n","108 \t 18.879094 \t 10.029037 \t8.740175 \t 0.109882 \t 66.272815 s\n","109 \t 17.905357 \t 9.130883 \t8.680955 \t 0.093518 \t 67.001436 s\n","110 \t 18.092237 \t 9.532677 \t8.301466 \t 0.258094 \t 67.556413 s\n","111 \t 18.352115 \t 9.922820 \t8.246944 \t 0.182350 \t 68.129152 s\n","112 \t 18.541603 \t 9.681667 \t8.650383 \t 0.209552 \t 68.701101 s\n","113 \t 19.132442 \t 9.970078 \t9.091634 \t 0.070729 \t 69.253925 s\n","114 \t 18.106749 \t 9.672500 \t8.337926 \t 0.096322 \t 69.826199 s\n","115 \t 17.536923 \t 9.423428 \t8.011878 \t 0.101618 \t 70.391472 s\n","116 \t 18.009584 \t 9.623985 \t8.298843 \t 0.086755 \t 71.004849 s\n","117 \t 18.100497 \t 9.402079 \t8.590575 \t 0.107844 \t 71.655560 s\n","118 \t 18.607888 \t 9.828863 \t8.704253 \t 0.074772 \t 72.501265 s\n","119 \t 17.778625 \t 9.431446 \t8.245100 \t 0.102081 \t 73.184434 s\n","120 \t 19.059381 \t 9.856015 \t8.966843 \t 0.236524 \t 73.851629 s\n","121 \t 19.468534 \t 10.078950 \t9.270457 \t 0.119126 \t 74.436342 s\n","122 \t 18.647574 \t 9.943497 \t8.537654 \t 0.166424 \t 75.004747 s\n","123 \t 18.346802 \t 9.752761 \t8.492456 \t 0.101583 \t 75.561528 s\n","124 \t 18.802714 \t 9.690236 \t8.981231 \t 0.131248 \t 76.119946 s\n","125 \t 17.820735 \t 9.322500 \t8.284844 \t 0.213391 \t 76.676286 s\n","126 \t 18.194025 \t 9.821232 \t8.281042 \t 0.091751 \t 77.238867 s\n","127 \t 18.063027 \t 9.505073 \t8.449404 \t 0.108551 \t 77.796736 s\n","128 \t 18.560160 \t 9.605603 \t8.759495 \t 0.195062 \t 78.354936 s\n","129 \t 19.133145 \t 9.940662 \t9.098197 \t 0.094286 \t 79.066180 s\n","130 \t 17.571975 \t 9.253860 \t8.241390 \t 0.076724 \t 79.632111 s\n","131 \t 17.556225 \t 9.495782 \t7.946158 \t 0.114285 \t 80.200776 s\n","132 \t 18.763407 \t 9.807242 \t8.809202 \t 0.146963 \t 80.762963 s\n","133 \t 19.061593 \t 10.210874 \t8.767653 \t 0.083067 \t 81.326329 s\n","134 \t 17.900908 \t 9.045194 \t8.752201 \t 0.103513 \t 81.904839 s\n","135 \t 18.306920 \t 9.889532 \t8.344242 \t 0.073146 \t 82.483229 s\n","136 \t 18.588049 \t 9.711395 \t8.791451 \t 0.085201 \t 83.041109 s\n","137 \t 17.566794 \t 9.066519 \t8.417237 \t 0.083037 \t 83.610857 s\n","138 \t 18.072015 \t 9.333037 \t8.606781 \t 0.132196 \t 84.316170 s\n","139 \t 17.837992 \t 9.437015 \t8.341356 \t 0.059623 \t 84.968982 s\n","140 \t 18.481851 \t 9.453164 \t8.898884 \t 0.129802 \t 85.658718 s\n","141 \t 18.676785 \t 10.067616 \t8.305074 \t 0.304094 \t 86.592712 s\n","142 \t 18.439536 \t 9.748353 \t8.603398 \t 0.087785 \t 87.185062 s\n","143 \t 18.505410 \t 9.963160 \t8.481277 \t 0.060974 \t 87.746611 s\n","144 \t 18.909170 \t 9.504432 \t9.106793 \t 0.297944 \t 88.299791 s\n","145 \t 18.004169 \t 9.573350 \t8.327602 \t 0.103217 \t 88.864363 s\n","146 \t 18.626925 \t 9.797089 \t8.718821 \t 0.111016 \t 89.420281 s\n","147 \t 17.559874 \t 9.462688 \t8.003897 \t 0.093287 \t 89.991886 s\n","148 \t 18.182004 \t 9.630251 \t8.465724 \t 0.086030 \t 90.543874 s\n","149 \t 18.768058 \t 9.920740 \t8.737228 \t 0.110089 \t 91.127664 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:51<00:00,  2.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3894\n","Hit@10: 0.4346\n","Hit@3: 0.3769\n","Hit@1: 0.3577\n","Link Prediction on Validation Set (All)\n","MRR: 0.2621\n","Hit@10: 0.3698\n","Hit@3: 0.2603\n","Hit@1: 0.2000\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2571\n","Hit@10: 0.4385\n","Hit@3: 0.2615\n","Hit@1: 0.1769\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4297\n","Hit@10: 0.6824\n","Hit@3: 0.5733\n","Hit@1: 0.2952\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1713\n","150 \t 18.976846 \t 9.805022 \t9.023780 \t 0.148044 \t 143.878738 s\n","151 \t 18.276997 \t 9.582710 \t8.206789 \t 0.487497 \t 144.437158 s\n","152 \t 18.795418 \t 9.735630 \t8.751911 \t 0.307876 \t 145.003503 s\n","153 \t 18.484481 \t 9.611543 \t8.643102 \t 0.229836 \t 145.560484 s\n","154 \t 19.157188 \t 10.173449 \t8.666557 \t 0.317183 \t 146.282264 s\n","155 \t 18.550070 \t 9.905893 \t8.497725 \t 0.146451 \t 146.842904 s\n","156 \t 18.091272 \t 9.409436 \t8.524032 \t 0.157805 \t 147.428920 s\n","157 \t 18.192794 \t 9.366736 \t8.668412 \t 0.157645 \t 147.985677 s\n","158 \t 18.094247 \t 9.550964 \t8.390474 \t 0.152809 \t 148.548888 s\n","159 \t 18.968266 \t 9.863991 \t8.828575 \t 0.275700 \t 149.170386 s\n","160 \t 18.454308 \t 8.955556 \t9.320886 \t 0.177866 \t 149.846411 s\n","161 \t 18.876267 \t 9.981022 \t8.671706 \t 0.223540 \t 150.521165 s\n","162 \t 18.717869 \t 10.107683 \t8.401641 \t 0.208545 \t 151.249175 s\n","163 \t 18.644009 \t 9.755473 \t8.805939 \t 0.082596 \t 152.310206 s\n","164 \t 19.325452 \t 9.788453 \t9.312580 \t 0.224419 \t 153.053599 s\n","165 \t 19.016934 \t 9.954036 \t8.959887 \t 0.103012 \t 153.725247 s\n","166 \t 18.520895 \t 9.472152 \t8.735126 \t 0.313617 \t 154.388148 s\n","167 \t 18.299610 \t 9.750169 \t8.228081 \t 0.321360 \t 155.164464 s\n","168 \t 18.422934 \t 9.444261 \t8.799297 \t 0.179376 \t 155.717750 s\n","169 \t 17.453025 \t 8.363583 \t8.958957 \t 0.130486 \t 156.284662 s\n","170 \t 18.513980 \t 9.560901 \t8.562835 \t 0.390243 \t 156.866024 s\n","171 \t 18.428947 \t 9.557297 \t8.768181 \t 0.103471 \t 157.422554 s\n","172 \t 19.107732 \t 10.338678 \t8.445092 \t 0.323961 \t 158.001683 s\n","173 \t 18.901860 \t 9.694690 \t8.970195 \t 0.236974 \t 158.580834 s\n","174 \t 18.915272 \t 9.928285 \t8.851076 \t 0.135911 \t 159.142955 s\n","175 \t 18.184891 \t 9.385931 \t8.647516 \t 0.151444 \t 159.725080 s\n","176 \t 19.479557 \t 9.819624 \t9.327173 \t 0.332760 \t 160.289400 s\n","177 \t 18.298582 \t 9.630248 \t8.522009 \t 0.146325 \t 161.035856 s\n","178 \t 18.660954 \t 9.629158 \t8.939389 \t 0.092409 \t 161.609518 s\n","179 \t 18.770739 \t 9.980977 \t8.469503 \t 0.320257 \t 162.173130 s\n","180 \t 17.805026 \t 9.449125 \t8.241583 \t 0.114317 \t 162.797169 s\n","181 \t 19.384259 \t 9.958398 \t9.132684 \t 0.293178 \t 163.465977 s\n","182 \t 18.732887 \t 9.799562 \t8.857038 \t 0.076288 \t 164.123690 s\n","183 \t 19.035472 \t 10.112080 \t8.800364 \t 0.123028 \t 164.815500 s\n","184 \t 17.927037 \t 9.068392 \t8.790058 \t 0.068588 \t 165.551115 s\n","185 \t 18.225259 \t 9.400659 \t8.592016 \t 0.232584 \t 166.134071 s\n","186 \t 19.177612 \t 9.857488 \t9.010213 \t 0.309911 \t 166.702332 s\n","187 \t 18.057312 \t 9.737644 \t8.232489 \t 0.087179 \t 167.417619 s\n","188 \t 18.619425 \t 9.552841 \t8.967347 \t 0.099236 \t 167.986242 s\n","189 \t 18.912873 \t 9.770496 \t8.976086 \t 0.166292 \t 168.538491 s\n","190 \t 19.519590 \t 10.042089 \t9.033406 \t 0.444097 \t 169.111238 s\n","191 \t 18.523982 \t 9.563344 \t8.771693 \t 0.188946 \t 169.679371 s\n","192 \t 18.353421 \t 9.422005 \t8.595047 \t 0.336370 \t 170.237241 s\n","193 \t 19.071142 \t 10.122131 \t8.532122 \t 0.416889 \t 170.810533 s\n","194 \t 18.068931 \t 9.582503 \t8.159407 \t 0.327021 \t 171.376241 s\n","195 \t 18.690730 \t 9.591217 \t9.015028 \t 0.084485 \t 171.938854 s\n","196 \t 18.058051 \t 9.614429 \t8.348931 \t 0.094690 \t 172.509158 s\n","197 \t 18.608614 \t 9.830367 \t8.667144 \t 0.111103 \t 173.088715 s\n","198 \t 17.581357 \t 8.753632 \t8.638614 \t 0.189111 \t 173.819787 s\n","199 \t 17.710877 \t 9.124660 \t8.503337 \t 0.082880 \t 174.389509 s\n","200 \t 19.348799 \t 10.233525 \t8.878813 \t 0.236462 \t 174.947678 s\n","201 \t 18.446455 \t 9.938878 \t8.441900 \t 0.065678 \t 175.500435 s\n","202 \t 18.629572 \t 9.672782 \t8.869012 \t 0.087778 \t 176.135171 s\n","203 \t 17.763749 \t 9.014720 \t8.627464 \t 0.121566 \t 176.794435 s\n","204 \t 18.075709 \t 9.621221 \t8.385836 \t 0.068653 \t 177.470419 s\n","205 \t 17.974014 \t 9.352961 \t8.525941 \t 0.095113 \t 178.156999 s\n","206 \t 18.264292 \t 9.503778 \t8.702096 \t 0.058418 \t 178.840450 s\n","207 \t 18.341697 \t 9.788467 \t8.414233 \t 0.138996 \t 179.404414 s\n","208 \t 18.465080 \t 10.084784 \t8.093257 \t 0.287040 \t 179.960876 s\n","209 \t 18.664169 \t 9.450415 \t9.004403 \t 0.209353 \t 180.676833 s\n","210 \t 18.176750 \t 9.478764 \t8.507532 \t 0.190455 \t 181.232922 s\n","211 \t 18.598892 \t 10.115125 \t8.247045 \t 0.236722 \t 181.793758 s\n","212 \t 18.110927 \t 9.551630 \t8.462812 \t 0.096484 \t 182.366398 s\n","213 \t 18.839454 \t 10.013071 \t8.673088 \t 0.153295 \t 182.949029 s\n","214 \t 18.489527 \t 9.399920 \t8.985872 \t 0.103735 \t 183.513180 s\n","215 \t 18.924623 \t 9.993708 \t8.852563 \t 0.078354 \t 184.104150 s\n","216 \t 18.105906 \t 9.676325 \t8.328661 \t 0.100920 \t 184.681150 s\n","217 \t 18.148151 \t 9.443060 \t8.625271 \t 0.079820 \t 185.239833 s\n","218 \t 17.953571 \t 9.527449 \t8.312849 \t 0.113274 \t 185.799590 s\n","219 \t 18.676723 \t 9.802009 \t8.769369 \t 0.105344 \t 186.357412 s\n","220 \t 18.508988 \t 9.617539 \t8.754126 \t 0.137323 \t 187.087520 s\n","221 \t 18.678975 \t 9.690406 \t8.886166 \t 0.102404 \t 187.640964 s\n","222 \t 17.330458 \t 8.880365 \t8.316402 \t 0.133691 \t 188.223175 s\n","223 \t 18.416595 \t 9.759956 \t8.393364 \t 0.263275 \t 188.802374 s\n","224 \t 17.642202 \t 9.265181 \t8.222310 \t 0.154712 \t 189.488833 s\n","225 \t 18.011473 \t 9.434045 \t8.490129 \t 0.087298 \t 190.152786 s\n","226 \t 18.207637 \t 9.698952 \t8.289301 \t 0.219383 \t 190.810174 s\n","227 \t 18.924911 \t 10.251154 \t8.613073 \t 0.060684 \t 191.560436 s\n","228 \t 18.451061 \t 9.269796 \t8.968349 \t 0.212915 \t 192.220299 s\n","229 \t 19.225110 \t 9.774547 \t9.288520 \t 0.162043 \t 192.782655 s\n","230 \t 17.719775 \t 9.320489 \t8.080582 \t 0.318704 \t 193.353731 s\n","231 \t 18.353559 \t 9.859031 \t8.122265 \t 0.372263 \t 194.083782 s\n","232 \t 18.816235 \t 9.802080 \t8.859001 \t 0.155154 \t 194.648506 s\n","233 \t 18.398944 \t 9.812612 \t8.512708 \t 0.073624 \t 195.200831 s\n","234 \t 18.246271 \t 9.780641 \t8.388667 \t 0.076963 \t 195.753980 s\n","235 \t 17.138032 \t 8.746869 \t8.291596 \t 0.099566 \t 196.309148 s\n","236 \t 18.177167 \t 9.842510 \t8.262227 \t 0.072430 \t 196.872432 s\n","237 \t 18.172327 \t 9.530275 \t8.509420 \t 0.132631 \t 197.433389 s\n","238 \t 18.800632 \t 9.614091 \t9.125346 \t 0.061196 \t 197.988537 s\n","239 \t 18.666542 \t 9.678540 \t8.893622 \t 0.094380 \t 198.566381 s\n","240 \t 17.880794 \t 9.856118 \t7.905543 \t 0.119132 \t 199.119558 s\n","241 \t 18.053083 \t 9.867616 \t8.103984 \t 0.081483 \t 199.703562 s\n","242 \t 18.706894 \t 9.791208 \t8.798636 \t 0.117049 \t 200.421743 s\n","243 \t 18.639629 \t 9.894143 \t8.441309 \t 0.304178 \t 200.984098 s\n","244 \t 18.406593 \t 9.694042 \t8.635046 \t 0.077505 \t 201.541656 s\n","245 \t 18.147362 \t 9.430753 \t8.562069 \t 0.154539 \t 202.152894 s\n","246 \t 17.847128 \t 9.646423 \t8.029721 \t 0.170983 \t 202.821554 s\n","247 \t 18.000053 \t 9.394845 \t8.526830 \t 0.078378 \t 203.498122 s\n","248 \t 18.297099 \t 9.665508 \t8.516222 \t 0.115370 \t 204.176398 s\n","249 \t 18.659613 \t 9.659076 \t8.930601 \t 0.069936 \t 204.915060 s\n","250 \t 17.880260 \t 9.592370 \t8.198681 \t 0.089209 \t 205.490966 s\n","251 \t 18.318278 \t 9.518298 \t8.683916 \t 0.116064 \t 206.053690 s\n","252 \t 17.759817 \t 9.612679 \t8.062458 \t 0.084681 \t 206.626863 s\n","253 \t 17.997510 \t 9.370852 \t8.350152 \t 0.276504 \t 207.180622 s\n","254 \t 17.551659 \t 9.168834 \t8.294378 \t 0.088447 \t 207.738659 s\n","255 \t 17.695301 \t 9.137621 \t8.422164 \t 0.135516 \t 208.453505 s\n","256 \t 18.501330 \t 9.795518 \t8.570755 \t 0.135059 \t 209.033005 s\n","257 \t 18.599279 \t 9.811837 \t8.502188 \t 0.285254 \t 209.585508 s\n","258 \t 17.448117 \t 9.444092 \t7.884850 \t 0.119175 \t 210.153330 s\n","259 \t 18.589123 \t 9.681858 \t8.826772 \t 0.080493 \t 210.730001 s\n","260 \t 19.014018 \t 9.840330 \t8.801297 \t 0.372391 \t 211.288722 s\n","261 \t 18.171693 \t 9.561394 \t8.463487 \t 0.146813 \t 211.856277 s\n","262 \t 18.941360 \t 9.632553 \t9.129354 \t 0.179452 \t 212.421175 s\n","263 \t 18.230088 \t 9.742021 \t8.392481 \t 0.095585 \t 212.983156 s\n","264 \t 18.439282 \t 9.741038 \t8.628852 \t 0.069393 \t 213.546334 s\n","265 \t 18.701151 \t 9.753097 \t8.859022 \t 0.089032 \t 214.136476 s\n","266 \t 18.903527 \t 9.736368 \t9.047873 \t 0.119286 \t 214.695607 s\n","267 \t 18.688020 \t 9.863736 \t8.651589 \t 0.172694 \t 215.310834 s\n","268 \t 17.772191 \t 9.687976 \t7.847983 \t 0.236231 \t 216.180678 s\n","269 \t 18.184149 \t 9.652321 \t8.419195 \t 0.112632 \t 216.879059 s\n","270 \t 18.236837 \t 9.680251 \t8.465460 \t 0.091126 \t 218.089443 s\n","271 \t 18.060948 \t 9.682230 \t8.087005 \t 0.291714 \t 218.735611 s\n","272 \t 18.413602 \t 9.970921 \t8.322961 \t 0.119721 \t 219.306984 s\n","273 \t 18.412501 \t 9.529203 \t8.785271 \t 0.098027 \t 219.870788 s\n","274 \t 18.484521 \t 9.788164 \t8.449697 \t 0.246660 \t 220.435690 s\n","275 \t 18.019652 \t 9.374188 \t8.578547 \t 0.066918 \t 220.999630 s\n","276 \t 17.945539 \t 9.300850 \t8.578103 \t 0.066586 \t 221.571853 s\n","277 \t 17.977057 \t 9.510990 \t8.317033 \t 0.149033 \t 222.143025 s\n","278 \t 18.773680 \t 9.638093 \t9.036988 \t 0.098598 \t 222.705989 s\n","279 \t 18.158641 \t 9.774008 \t8.302355 \t 0.082277 \t 223.277708 s\n","280 \t 18.630472 \t 9.943524 \t8.597840 \t 0.089109 \t 223.840006 s\n","281 \t 18.240096 \t 9.703936 \t8.419172 \t 0.116987 \t 224.404583 s\n","282 \t 17.695967 \t 9.568532 \t8.044015 \t 0.083420 \t 225.107937 s\n","283 \t 17.545820 \t 9.336747 \t8.089234 \t 0.119839 \t 225.679545 s\n","284 \t 18.294367 \t 10.095700 \t8.142572 \t 0.056095 \t 226.243849 s\n","285 \t 17.796474 \t 9.340758 \t8.304835 \t 0.150881 \t 226.831744 s\n","286 \t 17.908739 \t 9.698872 \t8.134563 \t 0.075304 \t 227.405284 s\n","287 \t 18.598420 \t 10.002063 \t8.495475 \t 0.100882 \t 227.960543 s\n","288 \t 17.906589 \t 9.480972 \t8.245307 \t 0.180309 \t 228.572804 s\n","289 \t 17.779412 \t 9.263849 \t8.365347 \t 0.150216 \t 229.247905 s\n","290 \t 17.827232 \t 9.090000 \t8.656489 \t 0.080743 \t 229.915632 s\n","291 \t 19.022227 \t 10.253526 \t8.665307 \t 0.103394 \t 230.615036 s\n","292 \t 17.879933 \t 9.378175 \t8.438006 \t 0.063752 \t 231.354107 s\n","293 \t 17.325636 \t 8.938156 \t8.314479 \t 0.073003 \t 231.923957 s\n","294 \t 18.351431 \t 9.651273 \t8.583950 \t 0.116207 \t 232.482092 s\n","295 \t 18.354800 \t 9.527717 \t8.727935 \t 0.099149 \t 233.067282 s\n","296 \t 17.515417 \t 8.972354 \t8.455060 \t 0.088001 \t 233.816553 s\n","297 \t 18.199363 \t 9.253506 \t8.314283 \t 0.631575 \t 234.367992 s\n","298 \t 18.511746 \t 9.841600 \t8.506477 \t 0.163670 \t 234.932406 s\n","299 \t 17.219371 \t 9.082216 \t8.060043 \t 0.077112 \t 235.481364 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:50<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4016\n","Hit@10: 0.4731\n","Hit@3: 0.3962\n","Hit@1: 0.3577\n","Link Prediction on Validation Set (All)\n","MRR: 0.2564\n","Hit@10: 0.3714\n","Hit@3: 0.2619\n","Hit@1: 0.1937\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2572\n","Hit@10: 0.5308\n","Hit@3: 0.3000\n","Hit@1: 0.1308\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4579\n","Hit@10: 0.7155\n","Hit@3: 0.6064\n","Hit@1: 0.2995\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2030\n","300 \t 17.961808 \t 9.434178 \t8.414103 \t 0.113527 \t 287.599778 s\n","301 \t 17.888983 \t 9.454326 \t8.374981 \t 0.059676 \t 288.165849 s\n","302 \t 17.802780 \t 9.549010 \t8.187103 \t 0.066667 \t 288.726076 s\n","303 \t 18.080765 \t 9.774140 \t8.233256 \t 0.073369 \t 289.289142 s\n","304 \t 18.244607 \t 9.302799 \t8.857314 \t 0.084494 \t 289.859175 s\n","305 \t 17.997171 \t 9.545317 \t8.361526 \t 0.090329 \t 290.429192 s\n","306 \t 17.775101 \t 9.486783 \t8.222937 \t 0.065380 \t 290.991801 s\n","307 \t 17.854095 \t 9.718675 \t8.056986 \t 0.078435 \t 291.724607 s\n","308 \t 17.792965 \t 9.435590 \t8.268201 \t 0.089175 \t 292.292880 s\n","309 \t 18.045800 \t 9.615743 \t8.359466 \t 0.070592 \t 292.863612 s\n","310 \t 17.224988 \t 9.117831 \t8.047693 \t 0.059464 \t 293.434781 s\n","311 \t 18.567486 \t 10.110470 \t8.394785 \t 0.062231 \t 294.090184 s\n","312 \t 17.650378 \t 9.538755 \t8.044247 \t 0.067376 \t 294.784314 s\n","313 \t 17.843527 \t 9.932509 \t7.776231 \t 0.134787 \t 295.647488 s\n","314 \t 17.764859 \t 9.300297 \t8.387240 \t 0.077322 \t 296.772785 s\n","315 \t 17.782373 \t 9.169980 \t8.550202 \t 0.062193 \t 297.729488 s\n","316 \t 18.033196 \t 9.403979 \t8.427288 \t 0.201930 \t 298.592485 s\n","317 \t 18.487240 \t 9.318234 \t8.885961 \t 0.283045 \t 299.211450 s\n","318 \t 17.767383 \t 9.510504 \t8.197386 \t 0.059492 \t 299.779030 s\n","319 \t 17.456524 \t 8.765291 \t8.624089 \t 0.067144 \t 300.341932 s\n","320 \t 17.635139 \t 9.614149 \t7.911407 \t 0.109581 \t 300.914075 s\n","321 \t 18.491295 \t 9.745159 \t8.521822 \t 0.224314 \t 301.468352 s\n","322 \t 17.472199 \t 9.252178 \t8.161260 \t 0.058761 \t 302.047556 s\n","323 \t 18.120706 \t 9.599492 \t8.437460 \t 0.083753 \t 302.601231 s\n","324 \t 18.902725 \t 9.866918 \t8.972466 \t 0.063342 \t 303.184442 s\n","325 \t 17.187298 \t 9.255100 \t7.860485 \t 0.071714 \t 303.751918 s\n","326 \t 17.930716 \t 9.531985 \t8.325472 \t 0.073258 \t 304.461248 s\n","327 \t 18.033125 \t 9.598653 \t8.331362 \t 0.103110 \t 305.025370 s\n","328 \t 17.781988 \t 9.387816 \t8.321287 \t 0.072885 \t 305.589483 s\n","329 \t 18.678737 \t 10.205256 \t8.273457 \t 0.200025 \t 306.156823 s\n","330 \t 17.874069 \t 9.600515 \t8.179821 \t 0.093731 \t 306.719443 s\n","331 \t 18.276631 \t 9.389630 \t8.829056 \t 0.057945 \t 307.301947 s\n","332 \t 18.233279 \t 9.992085 \t8.169884 \t 0.071309 \t 307.988303 s\n","333 \t 17.936515 \t 9.458546 \t8.400109 \t 0.077860 \t 308.653443 s\n","334 \t 17.940563 \t 9.342126 \t8.333560 \t 0.264876 \t 309.317918 s\n","335 \t 17.981350 \t 9.334186 \t8.565036 \t 0.082128 \t 310.016720 s\n","336 \t 17.378477 \t 9.165268 \t8.119346 \t 0.093862 \t 310.732890 s\n","337 \t 17.996511 \t 9.798981 \t8.121409 \t 0.076122 \t 311.453562 s\n","338 \t 18.439608 \t 9.772700 \t8.593893 \t 0.073014 \t 312.011810 s\n","339 \t 18.345096 \t 9.575512 \t8.665827 \t 0.103757 \t 312.584130 s\n","340 \t 17.824739 \t 9.811159 \t7.804864 \t 0.208716 \t 313.154481 s\n","341 \t 17.230598 \t 9.060898 \t8.066886 \t 0.102815 \t 313.710310 s\n","342 \t 17.988625 \t 9.451315 \t8.453745 \t 0.083565 \t 314.272748 s\n","343 \t 17.604853 \t 9.474669 \t8.043334 \t 0.086851 \t 314.844674 s\n","344 \t 17.349289 \t 9.482468 \t7.795404 \t 0.071417 \t 315.411400 s\n","345 \t 17.153397 \t 9.197401 \t7.887070 \t 0.068926 \t 315.976377 s\n","346 \t 18.647200 \t 9.969130 \t8.551372 \t 0.126697 \t 316.543389 s\n","347 \t 17.415060 \t 9.147375 \t8.168257 \t 0.099428 \t 317.110262 s\n","348 \t 17.753240 \t 9.324996 \t8.352812 \t 0.075432 \t 317.704401 s\n","349 \t 17.937180 \t 9.309465 \t8.558257 \t 0.069457 \t 318.434304 s\n","350 \t 18.271402 \t 9.665253 \t8.556895 \t 0.049256 \t 318.994046 s\n","351 \t 17.252933 \t 9.160836 \t8.016423 \t 0.075673 \t 319.556533 s\n","352 \t 18.054156 \t 9.581933 \t8.399869 \t 0.072354 \t 320.134447 s\n","353 \t 18.174047 \t 9.666611 \t8.431488 \t 0.075948 \t 320.760392 s\n","354 \t 18.442948 \t 9.674686 \t8.675466 \t 0.092797 \t 321.450997 s\n","355 \t 17.276252 \t 9.442747 \t7.746594 \t 0.086910 \t 322.110581 s\n","356 \t 18.408427 \t 9.916219 \t8.426048 \t 0.066161 \t 322.812252 s\n","357 \t 18.002730 \t 9.710120 \t8.222063 \t 0.070546 \t 323.579845 s\n","358 \t 17.688332 \t 9.253585 \t8.364559 \t 0.070187 \t 324.181548 s\n","359 \t 17.532517 \t 9.124378 \t8.312874 \t 0.095266 \t 324.764134 s\n","360 \t 17.878599 \t 9.504849 \t8.126854 \t 0.246896 \t 325.325682 s\n","361 \t 18.263535 \t 9.836168 \t8.348592 \t 0.078775 \t 326.055373 s\n","362 \t 17.723952 \t 9.379701 \t8.236843 \t 0.107408 \t 326.617955 s\n","363 \t 18.187788 \t 9.420415 \t8.670757 \t 0.096615 \t 327.195936 s\n","364 \t 18.073930 \t 9.654669 \t8.335363 \t 0.083898 \t 327.760305 s\n","365 \t 17.739036 \t 9.439402 \t8.252698 \t 0.046935 \t 328.317370 s\n","366 \t 18.067713 \t 9.668953 \t8.329409 \t 0.069350 \t 328.884981 s\n","367 \t 17.372657 \t 9.529422 \t7.783292 \t 0.059942 \t 329.454715 s\n","368 \t 17.757807 \t 9.349972 \t8.234369 \t 0.173466 \t 330.017885 s\n","369 \t 18.155602 \t 9.631482 \t8.451753 \t 0.072366 \t 330.591033 s\n","370 \t 18.373272 \t 9.672232 \t8.660823 \t 0.040217 \t 331.153418 s\n","371 \t 18.448345 \t 9.714293 \t8.594543 \t 0.139510 \t 331.719082 s\n","372 \t 17.593038 \t 9.281679 \t8.259537 \t 0.051822 \t 332.300918 s\n","373 \t 18.026276 \t 9.357046 \t8.617284 \t 0.051945 \t 333.025705 s\n","374 \t 18.051567 \t 9.826214 \t8.189471 \t 0.035883 \t 333.579299 s\n","375 \t 17.722317 \t 9.716862 \t7.960029 \t 0.045425 \t 334.253227 s\n","376 \t 18.070949 \t 9.393512 \t8.602137 \t 0.075300 \t 334.900312 s\n","377 \t 18.477509 \t 9.773346 \t8.629914 \t 0.074249 \t 335.584571 s\n","378 \t 18.198839 \t 9.867652 \t8.259244 \t 0.071944 \t 336.274439 s\n","379 \t 18.257946 \t 9.588408 \t8.506386 \t 0.163151 \t 336.955047 s\n","380 \t 17.379707 \t 9.462796 \t7.872872 \t 0.044040 \t 337.512091 s\n","381 \t 18.076748 \t 9.554889 \t8.274484 \t 0.247376 \t 338.077868 s\n","382 \t 17.599506 \t 9.642767 \t7.879621 \t 0.077118 \t 338.651287 s\n","383 \t 17.819639 \t 9.327905 \t8.437209 \t 0.054526 \t 339.217847 s\n","384 \t 17.699628 \t 9.368048 \t8.245853 \t 0.085726 \t 339.780235 s\n","385 \t 18.590019 \t 9.688675 \t8.830451 \t 0.070892 \t 340.496713 s\n","386 \t 17.446466 \t 8.907707 \t8.463316 \t 0.075444 \t 341.061143 s\n","387 \t 18.052240 \t 9.691413 \t8.304253 \t 0.056575 \t 341.624181 s\n","388 \t 17.903223 \t 9.632003 \t8.201186 \t 0.070033 \t 342.184902 s\n","389 \t 18.165352 \t 9.369113 \t8.745994 \t 0.050244 \t 342.767333 s\n","390 \t 17.709841 \t 9.052044 \t8.603909 \t 0.053887 \t 343.321121 s\n","391 \t 18.272052 \t 9.396937 \t8.817044 \t 0.058071 \t 343.901243 s\n","392 \t 18.201579 \t 9.780923 \t8.345417 \t 0.075238 \t 344.462045 s\n","393 \t 17.995981 \t 9.486714 \t8.354879 \t 0.154387 \t 345.019422 s\n","394 \t 17.919655 \t 9.350773 \t8.498562 \t 0.070320 \t 345.585123 s\n","395 \t 17.519682 \t 9.616657 \t7.810957 \t 0.092068 \t 346.154488 s\n","396 \t 17.692269 \t 9.192004 \t8.401769 \t 0.098496 \t 346.717329 s\n","397 \t 17.621494 \t 9.392259 \t8.122119 \t 0.107116 \t 347.377552 s\n","398 \t 17.323243 \t 9.238501 \t7.959990 \t 0.124752 \t 348.246374 s\n","399 \t 18.251925 \t 10.052749 \t8.135821 \t 0.063356 \t 348.904145 s\n","400 \t 17.530691 \t 9.096446 \t8.371318 \t 0.062927 \t 349.616567 s\n","401 \t 18.157284 \t 9.341210 \t8.757374 \t 0.058701 \t 350.309303 s\n","402 \t 17.799502 \t 9.500039 \t8.242322 \t 0.057141 \t 350.877677 s\n","403 \t 17.620648 \t 9.689977 \t7.784241 \t 0.146429 \t 351.443821 s\n","404 \t 18.064134 \t 9.531137 \t8.218758 \t 0.314238 \t 352.008553 s\n","405 \t 17.282399 \t 9.520759 \t7.701764 \t 0.059876 \t 352.577240 s\n","406 \t 17.729072 \t 9.567462 \t8.075650 \t 0.085960 \t 353.147975 s\n","407 \t 17.987878 \t 9.519677 \t8.409185 \t 0.059017 \t 353.714526 s\n","408 \t 17.840744 \t 9.666343 \t8.087331 \t 0.087071 \t 354.291896 s\n","409 \t 18.373253 \t 9.579627 \t8.631660 \t 0.161965 \t 354.871338 s\n","410 \t 17.601830 \t 9.191103 \t8.320255 \t 0.090471 \t 355.430784 s\n","411 \t 17.498016 \t 8.909824 \t8.517983 \t 0.070209 \t 356.133987 s\n","412 \t 17.894416 \t 9.425343 \t8.403158 \t 0.065915 \t 356.687558 s\n","413 \t 17.545534 \t 9.659734 \t7.646404 \t 0.239396 \t 357.248729 s\n","414 \t 17.920882 \t 9.410862 \t8.417871 \t 0.092150 \t 357.814960 s\n","415 \t 17.826765 \t 9.184052 \t8.576645 \t 0.066068 \t 358.385087 s\n","416 \t 17.584684 \t 9.424883 \t8.112125 \t 0.047676 \t 358.947937 s\n","417 \t 17.562875 \t 9.267097 \t8.245333 \t 0.050446 \t 359.519086 s\n","418 \t 18.159062 \t 9.568874 \t8.522273 \t 0.067915 \t 360.141362 s\n","419 \t 17.841430 \t 9.620148 \t8.166745 \t 0.054536 \t 360.822885 s\n","420 \t 17.717445 \t 9.519838 \t8.038202 \t 0.159406 \t 361.665096 s\n","421 \t 17.790686 \t 9.845097 \t7.869487 \t 0.076103 \t 362.344534 s\n","422 \t 18.144887 \t 9.702093 \t8.385567 \t 0.057226 \t 363.091908 s\n","423 \t 17.897294 \t 9.636699 \t8.218611 \t 0.041984 \t 363.680256 s\n","424 \t 18.131059 \t 9.370128 \t8.560685 \t 0.200246 \t 364.237955 s\n","425 \t 17.591935 \t 9.375180 \t8.157275 \t 0.059479 \t 364.812978 s\n","426 \t 18.201332 \t 9.501638 \t8.624318 \t 0.075377 \t 365.385635 s\n","427 \t 17.395625 \t 9.448067 \t7.892610 \t 0.054948 \t 365.945561 s\n","428 \t 18.087379 \t 9.567406 \t8.457765 \t 0.062210 \t 366.497562 s\n","429 \t 17.500718 \t 8.971322 \t8.191696 \t 0.337701 \t 367.062410 s\n","430 \t 18.327734 \t 9.972114 \t8.282845 \t 0.072776 \t 367.788891 s\n","431 \t 18.791927 \t 10.070351 \t8.670760 \t 0.050817 \t 368.351508 s\n","432 \t 18.290173 \t 9.908641 \t8.310780 \t 0.070751 \t 368.920892 s\n","433 \t 17.509600 \t 8.923979 \t8.532970 \t 0.052650 \t 369.477728 s\n","434 \t 17.795139 \t 9.330114 \t8.080627 \t 0.384398 \t 370.051514 s\n","435 \t 17.801510 \t 9.337510 \t8.297961 \t 0.166038 \t 370.615625 s\n","436 \t 17.634286 \t 9.399572 \t8.166093 \t 0.068620 \t 371.167523 s\n","437 \t 18.158637 \t 9.709310 \t8.207024 \t 0.242305 \t 371.731470 s\n","438 \t 17.638209 \t 9.216277 \t8.361248 \t 0.060683 \t 372.293999 s\n","439 \t 17.449135 \t 9.087307 \t8.240622 \t 0.121206 \t 372.864653 s\n","440 \t 18.223717 \t 9.510653 \t8.614730 \t 0.098333 \t 373.664039 s\n","441 \t 17.788453 \t 9.715523 \t8.041380 \t 0.031551 \t 374.320930 s\n","442 \t 18.218394 \t 9.714262 \t8.437380 \t 0.066752 \t 375.005515 s\n","443 \t 17.909218 \t 9.689591 \t8.156236 \t 0.063389 \t 375.689368 s\n","444 \t 18.026366 \t 9.699345 \t8.258596 \t 0.068425 \t 376.378158 s\n","445 \t 17.957648 \t 9.394495 \t8.488303 \t 0.074850 \t 376.944203 s\n","446 \t 17.627340 \t 9.452246 \t8.095832 \t 0.079261 \t 377.503562 s\n","447 \t 17.226830 \t 9.221198 \t7.945609 \t 0.060024 \t 378.073651 s\n","448 \t 17.647424 \t 9.372462 \t8.217814 \t 0.057147 \t 378.628996 s\n","449 \t 18.016092 \t 9.592628 \t8.344169 \t 0.079295 \t 379.184332 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:50<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3924\n","Hit@10: 0.4885\n","Hit@3: 0.3885\n","Hit@1: 0.3423\n","Link Prediction on Validation Set (All)\n","MRR: 0.2672\n","Hit@10: 0.4127\n","Hit@3: 0.2746\n","Hit@1: 0.1952\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2780\n","Hit@10: 0.5154\n","Hit@3: 0.3077\n","Hit@1: 0.1692\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4787\n","Hit@10: 0.7380\n","Hit@3: 0.6214\n","Hit@1: 0.3230\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1716\n","450 \t 17.636385 \t 9.364385 \t8.214193 \t 0.057806 \t 431.431391 s\n","451 \t 17.627729 \t 9.494646 \t8.044681 \t 0.088404 \t 432.155901 s\n","452 \t 18.199282 \t 9.480090 \t8.531225 \t 0.187965 \t 432.710619 s\n","453 \t 19.089476 \t 9.813642 \t9.223433 \t 0.052402 \t 433.272044 s\n","454 \t 17.550547 \t 9.492643 \t7.996404 \t 0.061500 \t 433.840934 s\n","455 \t 18.028111 \t 9.946213 \t8.013395 \t 0.068502 \t 434.405350 s\n","456 \t 17.792629 \t 9.137708 \t8.469472 \t 0.185450 \t 434.969860 s\n","457 \t 17.951076 \t 9.739233 \t8.159664 \t 0.052178 \t 435.551894 s\n","458 \t 18.009018 \t 9.382171 \t8.571401 \t 0.055446 \t 436.109529 s\n","459 \t 19.120560 \t 9.807940 \t9.023006 \t 0.289614 \t 436.685560 s\n","460 \t 18.703476 \t 9.651339 \t8.976884 \t 0.075253 \t 437.260095 s\n","461 \t 18.176646 \t 9.426021 \t8.497902 \t 0.252723 \t 437.885708 s\n","462 \t 17.652010 \t 9.549973 \t8.015331 \t 0.086707 \t 438.698509 s\n","463 \t 17.514729 \t 9.193191 \t8.249837 \t 0.071702 \t 439.465175 s\n","464 \t 17.896398 \t 9.291977 \t8.304863 \t 0.299557 \t 440.287570 s\n","465 \t 18.287251 \t 9.464681 \t8.761981 \t 0.060589 \t 441.069739 s\n","466 \t 18.077260 \t 9.783669 \t8.131657 \t 0.161934 \t 441.877338 s\n","467 \t 17.992436 \t 9.366161 \t8.432953 \t 0.193323 \t 442.626064 s\n","468 \t 18.072647 \t 9.537285 \t8.470444 \t 0.064919 \t 443.284244 s\n","469 \t 17.962306 \t 9.544431 \t8.270713 \t 0.147161 \t 443.843282 s\n","470 \t 18.539460 \t 9.722006 \t8.693785 \t 0.123670 \t 444.409843 s\n","471 \t 18.482830 \t 9.829877 \t8.458673 \t 0.194281 \t 444.984585 s\n","472 \t 18.043547 \t 9.756456 \t8.201746 \t 0.085344 \t 445.550021 s\n","473 \t 17.930251 \t 9.298514 \t8.553524 \t 0.078213 \t 446.132536 s\n","474 \t 17.967087 \t 9.736955 \t8.132907 \t 0.097225 \t 446.871078 s\n","475 \t 18.878697 \t 9.981359 \t8.745530 \t 0.151808 \t 447.436781 s\n","476 \t 17.135395 \t 8.458329 \t8.604702 \t 0.072364 \t 448.000306 s\n","477 \t 18.234939 \t 9.582294 \t8.551375 \t 0.101270 \t 448.565048 s\n","478 \t 18.304245 \t 9.540678 \t8.700600 \t 0.062966 \t 449.131811 s\n","479 \t 18.315249 \t 9.810899 \t8.405623 \t 0.098727 \t 449.705254 s\n","480 \t 18.453006 \t 9.500407 \t8.857954 \t 0.094645 \t 450.280872 s\n","481 \t 18.718628 \t 9.886415 \t8.740392 \t 0.091822 \t 450.858492 s\n","482 \t 18.467139 \t 9.549436 \t8.815651 \t 0.102053 \t 451.418634 s\n","483 \t 18.515277 \t 9.586071 \t8.866239 \t 0.062967 \t 451.991359 s\n","484 \t 18.587291 \t 9.834559 \t8.637111 \t 0.115620 \t 452.610405 s\n","485 \t 18.222317 \t 9.609395 \t8.542197 \t 0.070725 \t 453.285506 s\n","486 \t 17.834884 \t 9.308856 \t8.378217 \t 0.147810 \t 454.150452 s\n","487 \t 17.982843 \t 9.661661 \t8.248545 \t 0.072638 \t 454.838901 s\n","488 \t 18.865401 \t 9.867507 \t8.891360 \t 0.106534 \t 455.541200 s\n","489 \t 18.340636 \t 9.700711 \t8.506158 \t 0.133767 \t 456.114585 s\n","490 \t 18.981063 \t 9.970036 \t8.788058 \t 0.222969 \t 456.683557 s\n","491 \t 19.023126 \t 10.089244 \t8.686131 \t 0.247750 \t 457.250198 s\n","492 \t 18.608489 \t 9.842211 \t8.686007 \t 0.080271 \t 457.816480 s\n","493 \t 18.412500 \t 9.922686 \t8.265461 \t 0.224354 \t 458.376168 s\n","494 \t 18.858142 \t 9.922186 \t8.856615 \t 0.079341 \t 458.937455 s\n","495 \t 18.652874 \t 9.761249 \t8.767709 \t 0.123917 \t 459.499886 s\n","496 \t 18.610922 \t 9.686554 \t8.825958 \t 0.098410 \t 460.074117 s\n","497 \t 18.575639 \t 9.787802 \t8.571981 \t 0.215857 \t 460.647729 s\n","498 \t 17.955951 \t 9.240705 \t8.577737 \t 0.137510 \t 461.212505 s\n","499 \t 18.766265 \t 9.852304 \t8.653409 \t 0.260552 \t 461.945978 s\n","500 \t 18.149946 \t 9.975989 \t8.093554 \t 0.080404 \t 462.512310 s\n","501 \t 18.837848 \t 9.915145 \t8.808681 \t 0.114022 \t 463.068690 s\n","502 \t 19.073028 \t 9.953900 \t8.560440 \t 0.558688 \t 463.630962 s\n","503 \t 18.766670 \t 9.848515 \t8.819403 \t 0.098753 \t 464.189891 s\n","504 \t 18.643441 \t 9.844146 \t8.689221 \t 0.110074 \t 464.757263 s\n","505 \t 18.744624 \t 9.743286 \t8.904209 \t 0.097129 \t 465.352378 s\n","506 \t 18.686609 \t 9.752516 \t8.853943 \t 0.080150 \t 466.060497 s\n","507 \t 18.315753 \t 9.649849 \t8.593986 \t 0.071917 \t 466.743390 s\n","508 \t 18.302745 \t 9.988485 \t8.219167 \t 0.095094 \t 467.415818 s\n","509 \t 18.676707 \t 9.708163 \t8.844965 \t 0.123578 \t 468.162904 s\n","510 \t 18.518039 \t 9.617995 \t8.814939 \t 0.085105 \t 468.842304 s\n","511 \t 18.272157 \t 9.879932 \t8.287493 \t 0.104731 \t 469.417577 s\n","512 \t 18.623446 \t 9.822769 \t8.687136 \t 0.113541 \t 470.151239 s\n","513 \t 18.118071 \t 9.296755 \t8.686443 \t 0.134873 \t 470.729042 s\n","514 \t 17.742793 \t 9.827972 \t7.842505 \t 0.072315 \t 471.292779 s\n","515 \t 17.782948 \t 9.613863 \t8.090265 \t 0.078819 \t 471.866655 s\n","516 \t 18.605098 \t 9.939593 \t8.495080 \t 0.170424 \t 472.444502 s\n","517 \t 18.471015 \t 9.996033 \t8.396760 \t 0.078221 \t 473.005091 s\n","518 \t 18.089552 \t 9.361473 \t8.593400 \t 0.134680 \t 473.570370 s\n","519 \t 18.248556 \t 9.995695 \t8.193596 \t 0.059264 \t 474.133669 s\n","520 \t 18.785150 \t 9.619263 \t9.007163 \t 0.158723 \t 474.696700 s\n","521 \t 17.835279 \t 9.068371 \t8.697153 \t 0.069756 \t 475.264076 s\n","522 \t 18.072536 \t 9.468675 \t8.460150 \t 0.143711 \t 475.832129 s\n","523 \t 18.130284 \t 9.624224 \t8.415525 \t 0.090535 \t 476.413135 s\n","524 \t 18.537997 \t 9.796311 \t8.630148 \t 0.111537 \t 477.009851 s\n","525 \t 17.759035 \t 9.419600 \t8.261437 \t 0.077998 \t 477.571938 s\n","526 \t 18.877211 \t 9.965173 \t8.790241 \t 0.121796 \t 478.299040 s\n","527 \t 18.508574 \t 9.771186 \t8.680196 \t 0.057193 \t 478.920159 s\n","528 \t 17.995740 \t 9.409975 \t8.496506 \t 0.089259 \t 479.601732 s\n","529 \t 18.907498 \t 9.459673 \t9.331999 \t 0.115826 \t 480.263520 s\n","530 \t 18.646262 \t 9.782111 \t8.757681 \t 0.106471 \t 480.942445 s\n","531 \t 18.084689 \t 9.571778 \t8.420722 \t 0.092189 \t 481.627097 s\n","532 \t 19.222556 \t 9.838534 \t9.260566 \t 0.123456 \t 482.176331 s\n","533 \t 18.555609 \t 9.752435 \t8.728471 \t 0.074703 \t 482.761633 s\n","534 \t 18.176113 \t 9.679885 \t8.418427 \t 0.077800 \t 483.341545 s\n","535 \t 18.182789 \t 9.474308 \t8.625477 \t 0.083003 \t 483.912251 s\n","536 \t 18.462294 \t 10.122159 \t8.267488 \t 0.072646 \t 484.477497 s\n","537 \t 19.108764 \t 9.737354 \t9.307858 \t 0.063551 \t 485.044394 s\n","538 \t 18.366262 \t 9.640106 \t8.637476 \t 0.088680 \t 485.599828 s\n","539 \t 18.231468 \t 9.907563 \t8.137242 \t 0.186663 \t 486.157620 s\n","540 \t 19.046745 \t 10.231050 \t8.744726 \t 0.070969 \t 486.886083 s\n","541 \t 18.339769 \t 9.360990 \t8.924753 \t 0.054026 \t 487.447256 s\n","542 \t 18.621871 \t 9.505340 \t9.029965 \t 0.086566 \t 487.999118 s\n","543 \t 18.913168 \t 9.674588 \t8.996111 \t 0.242469 \t 488.570041 s\n","544 \t 19.002200 \t 9.847656 \t9.038171 \t 0.116373 \t 489.126811 s\n","545 \t 18.571054 \t 9.671315 \t8.710356 \t 0.189383 \t 489.691287 s\n","546 \t 18.524016 \t 10.212545 \t8.228285 \t 0.083186 \t 490.245096 s\n","547 \t 18.202151 \t 9.565299 \t8.534995 \t 0.101857 \t 490.811731 s\n","548 \t 18.065093 \t 9.863432 \t8.113948 \t 0.087715 \t 491.363729 s\n","549 \t 18.765061 \t 9.693457 \t8.610490 \t 0.461115 \t 492.190990 s\n","550 \t 18.281469 \t 9.895953 \t8.288601 \t 0.096915 \t 492.869585 s\n","551 \t 18.949096 \t 9.901166 \t8.952946 \t 0.094983 \t 493.543767 s\n","552 \t 18.780590 \t 9.919431 \t8.747174 \t 0.113985 \t 494.252593 s\n","553 \t 18.470025 \t 9.794752 \t8.588567 \t 0.086705 \t 494.934065 s\n","554 \t 17.962052 \t 9.495920 \t8.359676 \t 0.106455 \t 495.481454 s\n","555 \t 18.052924 \t 9.750566 \t8.199478 \t 0.102880 \t 496.043130 s\n","556 \t 18.737732 \t 9.894205 \t8.792263 \t 0.051265 \t 496.604736 s\n","557 \t 18.358548 \t 9.801813 \t8.481133 \t 0.075603 \t 497.163562 s\n","558 \t 18.236888 \t 9.786427 \t8.345712 \t 0.104748 \t 497.724027 s\n","559 \t 17.868653 \t 9.701561 \t8.117745 \t 0.049347 \t 498.453171 s\n","560 \t 18.493940 \t 9.784156 \t8.652085 \t 0.057699 \t 499.017616 s\n","561 \t 18.573550 \t 9.908008 \t8.578783 \t 0.086760 \t 499.569923 s\n","562 \t 18.301284 \t 9.313014 \t8.696094 \t 0.292176 \t 500.128786 s\n","563 \t 18.375098 \t 9.518088 \t8.705792 \t 0.151217 \t 500.697243 s\n","564 \t 17.769772 \t 9.268572 \t8.375216 \t 0.125984 \t 501.262172 s\n","565 \t 18.444567 \t 9.368765 \t8.964894 \t 0.110907 \t 501.828687 s\n","566 \t 18.156049 \t 9.396948 \t8.695481 \t 0.063620 \t 502.401915 s\n","567 \t 18.060532 \t 9.399094 \t8.401915 \t 0.259523 \t 502.965625 s\n","568 \t 18.682597 \t 9.831370 \t8.669996 \t 0.181231 \t 503.537294 s\n","569 \t 17.998776 \t 9.741279 \t8.068968 \t 0.188530 \t 504.255231 s\n","570 \t 19.060919 \t 9.586457 \t9.392714 \t 0.081748 \t 504.851190 s\n","571 \t 18.539289 \t 9.769399 \t8.676066 \t 0.093825 \t 505.567059 s\n","572 \t 18.419086 \t 9.707126 \t8.639829 \t 0.072131 \t 506.239223 s\n","573 \t 18.075897 \t 9.797218 \t8.196342 \t 0.082338 \t 506.939906 s\n","574 \t 18.526666 \t 10.092867 \t8.339341 \t 0.094457 \t 507.666269 s\n","575 \t 18.269346 \t 9.363787 \t8.810027 \t 0.095533 \t 508.270111 s\n","576 \t 18.326938 \t 9.384002 \t8.825456 \t 0.117480 \t 508.840097 s\n","577 \t 18.221083 \t 9.613527 \t8.537842 \t 0.069713 \t 509.420770 s\n","578 \t 17.391880 \t 9.114238 \t8.225732 \t 0.051911 \t 509.971945 s\n","579 \t 18.246378 \t 9.849195 \t8.305570 \t 0.091614 \t 510.688865 s\n","580 \t 19.043639 \t 10.176521 \t8.562833 \t 0.304285 \t 511.238030 s\n","581 \t 17.846584 \t 9.458574 \t8.304322 \t 0.083688 \t 511.792669 s\n","582 \t 17.914538 \t 9.383001 \t8.454944 \t 0.076593 \t 512.354435 s\n","583 \t 17.858608 \t 9.422505 \t8.310807 \t 0.125296 \t 512.937283 s\n","584 \t 17.643880 \t 9.403334 \t7.950903 \t 0.289643 \t 513.498104 s\n","585 \t 18.519437 \t 9.669971 \t8.784819 \t 0.064646 \t 514.085536 s\n","586 \t 18.136537 \t 9.596150 \t8.454318 \t 0.086068 \t 514.677928 s\n","587 \t 17.651225 \t 9.198418 \t8.380199 \t 0.072608 \t 515.226706 s\n","588 \t 18.032005 \t 9.326584 \t8.650489 \t 0.054933 \t 515.800114 s\n","589 \t 18.327975 \t 9.560070 \t8.716121 \t 0.051785 \t 516.353676 s\n","590 \t 18.299253 \t 9.881476 \t8.327347 \t 0.090430 \t 517.087025 s\n","591 \t 18.809711 \t 9.789733 \t8.954967 \t 0.065012 \t 517.663538 s\n","592 \t 17.921128 \t 9.357223 \t8.462750 \t 0.101156 \t 518.276724 s\n","593 \t 18.608024 \t 9.539860 \t8.993647 \t 0.074517 \t 518.933100 s\n","594 \t 18.759487 \t 9.581989 \t9.135389 \t 0.042109 \t 519.588236 s\n","595 \t 18.068906 \t 9.535301 \t8.428094 \t 0.105512 \t 520.279773 s\n","596 \t 18.006130 \t 9.193689 \t8.752394 \t 0.060047 \t 520.969465 s\n","597 \t 18.336417 \t 9.283257 \t8.957744 \t 0.095417 \t 521.523238 s\n","598 \t 18.138348 \t 9.738899 \t8.318143 \t 0.081307 \t 522.082286 s\n","599 \t 18.582874 \t 9.983342 \t8.523003 \t 0.076530 \t 522.649030 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:50<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3944\n","Hit@10: 0.4731\n","Hit@3: 0.3923\n","Hit@1: 0.3500\n","Link Prediction on Validation Set (All)\n","MRR: 0.2623\n","Hit@10: 0.3778\n","Hit@3: 0.2635\n","Hit@1: 0.1984\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2417\n","Hit@10: 0.4692\n","Hit@3: 0.2308\n","Hit@1: 0.1615\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3794\n","Hit@10: 0.6770\n","Hit@3: 0.5508\n","Hit@1: 0.1861\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1731\n","600 \t 18.113300 \t 9.662424 \t8.308973 \t 0.141903 \t 574.796801 s\n","601 \t 19.112867 \t 9.997143 \t9.044882 \t 0.070842 \t 575.525835 s\n","602 \t 19.077549 \t 10.063061 \t8.945869 \t 0.068619 \t 576.084043 s\n","603 \t 17.957829 \t 9.585558 \t8.314179 \t 0.058092 \t 576.659462 s\n","604 \t 17.949988 \t 9.594267 \t8.294270 \t 0.061451 \t 577.223795 s\n","605 \t 18.024501 \t 9.614485 \t8.360106 \t 0.049910 \t 577.785213 s\n","606 \t 18.546713 \t 9.449813 \t8.759129 \t 0.337771 \t 578.357967 s\n","607 \t 17.961496 \t 9.317635 \t8.551483 \t 0.092378 \t 578.923328 s\n","608 \t 18.184460 \t 9.532833 \t8.560517 \t 0.091109 \t 579.486390 s\n","609 \t 18.195351 \t 9.474913 \t8.640936 \t 0.079503 \t 580.054005 s\n","610 \t 19.074944 \t 10.018945 \t8.762746 \t 0.293254 \t 580.629363 s\n","611 \t 18.410499 \t 9.885155 \t8.461348 \t 0.063996 \t 581.278512 s\n","612 \t 18.576591 \t 9.782746 \t8.710379 \t 0.083465 \t 581.929449 s\n","613 \t 18.646042 \t 10.192426 \t8.400068 \t 0.053549 \t 582.802699 s\n","614 \t 18.552715 \t 9.767633 \t8.484378 \t 0.300704 \t 583.527416 s\n","615 \t 17.680711 \t 8.880938 \t8.716563 \t 0.083210 \t 584.267886 s\n","616 \t 18.338034 \t 9.499254 \t8.786592 \t 0.052187 \t 584.984667 s\n","617 \t 18.242212 \t 9.332935 \t8.863740 \t 0.045537 \t 585.751092 s\n","618 \t 17.941378 \t 9.656583 \t8.198326 \t 0.086467 \t 586.475263 s\n","619 \t 17.841660 \t 9.813932 \t7.929395 \t 0.098333 \t 587.139011 s\n","620 \t 18.204943 \t 9.577379 \t8.554574 \t 0.072989 \t 587.709046 s\n","621 \t 17.838033 \t 9.601037 \t8.172031 \t 0.064965 \t 588.271863 s\n","622 \t 17.637171 \t 9.428003 \t8.160729 \t 0.048438 \t 588.839430 s\n","623 \t 18.292427 \t 9.706239 \t8.525079 \t 0.061109 \t 589.406427 s\n","624 \t 18.963266 \t 9.907758 \t8.733677 \t 0.321831 \t 589.972003 s\n","625 \t 18.915590 \t 9.971790 \t8.872677 \t 0.071122 \t 590.529420 s\n","626 \t 17.989415 \t 9.583682 \t8.233557 \t 0.172175 \t 591.105094 s\n","627 \t 17.197819 \t 9.129719 \t7.945536 \t 0.122565 \t 591.819288 s\n","628 \t 18.182121 \t 9.592892 \t8.528636 \t 0.060594 \t 592.380832 s\n","629 \t 18.431162 \t 10.101851 \t8.276146 \t 0.053165 \t 592.953089 s\n","630 \t 18.526715 \t 9.671655 \t8.791323 \t 0.063737 \t 593.524930 s\n","631 \t 18.899184 \t 9.934072 \t8.697054 \t 0.268058 \t 594.092874 s\n","632 \t 17.758451 \t 9.096311 \t8.496694 \t 0.165448 \t 594.655252 s\n","633 \t 17.150017 \t 8.993722 \t7.903543 \t 0.252751 \t 595.221500 s\n","634 \t 18.846949 \t 9.590600 \t9.017563 \t 0.238785 \t 595.776814 s\n","635 \t 18.239124 \t 9.568861 \t8.591409 \t 0.078855 \t 596.354099 s\n","636 \t 18.125221 \t 9.675221 \t8.393599 \t 0.056401 \t 596.989660 s\n","637 \t 18.972180 \t 9.800603 \t8.934601 \t 0.236976 \t 597.646014 s\n","638 \t 19.265284 \t 9.981062 \t9.190203 \t 0.094018 \t 598.345979 s\n","639 \t 17.973663 \t 9.395504 \t8.529638 \t 0.048522 \t 599.031090 s\n","640 \t 17.952089 \t 9.440508 \t8.462910 \t 0.048671 \t 599.778722 s\n","641 \t 18.677661 \t 9.824304 \t8.737938 \t 0.115419 \t 600.491239 s\n","642 \t 18.232996 \t 9.405166 \t8.747685 \t 0.080146 \t 601.061528 s\n","643 \t 18.902752 \t 10.055557 \t8.781958 \t 0.065237 \t 601.632112 s\n","644 \t 18.040870 \t 9.557109 \t8.446804 \t 0.036957 \t 602.198000 s\n","645 \t 18.369060 \t 9.526328 \t8.746709 \t 0.096023 \t 602.756804 s\n","646 \t 18.746260 \t 9.755692 \t8.898863 \t 0.091705 \t 603.334270 s\n","647 \t 18.048965 \t 9.522287 \t8.416580 \t 0.110098 \t 603.893215 s\n","648 \t 18.980212 \t 9.921679 \t8.984435 \t 0.074098 \t 604.463261 s\n","649 \t 18.003160 \t 9.550720 \t8.393512 \t 0.058928 \t 605.023679 s\n","650 \t 18.530249 \t 9.612217 \t8.839808 \t 0.078224 \t 605.733768 s\n","651 \t 18.522090 \t 9.668124 \t8.792977 \t 0.060989 \t 606.296105 s\n","652 \t 18.761312 \t 9.553934 \t9.068285 \t 0.139093 \t 606.878675 s\n","653 \t 18.039537 \t 9.485029 \t8.455560 \t 0.098948 \t 607.446987 s\n","654 \t 18.797956 \t 9.936751 \t8.799180 \t 0.062024 \t 608.002055 s\n","655 \t 19.040145 \t 9.921546 \t8.928809 \t 0.189790 \t 608.562918 s\n","656 \t 18.443395 \t 9.622467 \t8.766006 \t 0.054922 \t 609.117661 s\n","657 \t 17.775672 \t 9.061565 \t8.660558 \t 0.053550 \t 609.679464 s\n","658 \t 19.183505 \t 9.794471 \t9.180576 \t 0.208458 \t 610.317237 s\n","659 \t 18.217307 \t 9.880910 \t8.057801 \t 0.278596 \t 610.988772 s\n","660 \t 17.986098 \t 9.626053 \t8.295539 \t 0.064505 \t 611.838072 s\n","661 \t 18.385376 \t 9.520812 \t8.810287 \t 0.054277 \t 612.558542 s\n","662 \t 18.018953 \t 9.562171 \t8.386858 \t 0.069924 \t 613.228390 s\n","663 \t 18.221032 \t 9.367881 \t8.773124 \t 0.080026 \t 613.788260 s\n","664 \t 18.966416 \t 9.862589 \t9.014278 \t 0.089550 \t 614.339486 s\n","665 \t 18.532513 \t 10.127930 \t8.330127 \t 0.074455 \t 614.909128 s\n","666 \t 18.721790 \t 9.821794 \t8.657959 \t 0.242037 \t 615.473450 s\n","667 \t 18.344413 \t 9.793946 \t8.468451 \t 0.082015 \t 616.041449 s\n","668 \t 19.022703 \t 10.032354 \t8.886510 \t 0.103839 \t 616.590944 s\n","669 \t 18.350092 \t 9.852129 \t8.328737 \t 0.169226 \t 617.155060 s\n","670 \t 18.106449 \t 9.268732 \t8.796491 \t 0.041226 \t 617.709811 s\n","671 \t 18.942264 \t 9.773332 \t9.092780 \t 0.076152 \t 618.281634 s\n","672 \t 17.614683 \t 9.675710 \t7.863989 \t 0.074984 \t 618.998013 s\n","673 \t 18.728464 \t 9.934255 \t8.722582 \t 0.071627 \t 619.550927 s\n","674 \t 18.537934 \t 9.629106 \t8.559401 \t 0.349429 \t 620.116270 s\n","675 \t 18.479923 \t 9.682127 \t8.704867 \t 0.092929 \t 620.677829 s\n","676 \t 18.422336 \t 9.498121 \t8.881469 \t 0.042744 \t 621.235896 s\n","677 \t 18.028337 \t 9.673328 \t8.264220 \t 0.090788 \t 621.811502 s\n","678 \t 17.573397 \t 9.336728 \t8.182787 \t 0.053883 \t 622.391711 s\n","679 \t 18.346059 \t 9.602537 \t8.702266 \t 0.041256 \t 622.993918 s\n","680 \t 18.958267 \t 10.016156 \t8.877570 \t 0.064542 \t 623.678831 s\n","681 \t 18.365674 \t 9.629030 \t8.676936 \t 0.059707 \t 624.340834 s\n","682 \t 19.472664 \t 10.029757 \t9.150945 \t 0.291962 \t 625.009482 s\n","683 \t 17.585632 \t 8.621519 \t8.900002 \t 0.064111 \t 625.744102 s\n","684 \t 18.619761 \t 9.991594 \t8.492375 \t 0.135793 \t 626.349805 s\n","685 \t 17.724604 \t 9.542359 \t8.118388 \t 0.063856 \t 627.075449 s\n","686 \t 18.078602 \t 9.760147 \t8.229601 \t 0.088856 \t 627.661835 s\n","687 \t 18.073741 \t 9.427536 \t8.594300 \t 0.051904 \t 628.230698 s\n","688 \t 18.385326 \t 9.676665 \t8.640119 \t 0.068543 \t 628.789378 s\n","689 \t 18.576032 \t 9.866521 \t8.638100 \t 0.071411 \t 629.353518 s\n","690 \t 18.448504 \t 9.796174 \t8.575488 \t 0.076843 \t 629.913093 s\n","691 \t 18.559595 \t 9.730596 \t8.771252 \t 0.057747 \t 630.472934 s\n","692 \t 18.143600 \t 9.506029 \t8.585885 \t 0.051688 \t 631.039515 s\n","693 \t 19.523129 \t 10.642516 \t8.632340 \t 0.248273 \t 631.596060 s\n","694 \t 18.414424 \t 9.883539 \t8.466061 \t 0.064824 \t 632.166118 s\n","695 \t 18.279938 \t 9.876343 \t8.315400 \t 0.088194 \t 632.736830 s\n","696 \t 18.438833 \t 9.788446 \t8.475674 \t 0.174712 \t 633.307448 s\n","697 \t 18.405814 \t 9.570978 \t8.784284 \t 0.050552 \t 633.880759 s\n","698 \t 18.872054 \t 10.045304 \t8.767358 \t 0.059392 \t 634.587275 s\n","699 \t 18.319151 \t 9.565710 \t8.594133 \t 0.159308 \t 635.149221 s\n","700 \t 18.200747 \t 9.859623 \t8.275172 \t 0.065951 \t 635.694657 s\n","701 \t 18.694971 \t 9.851853 \t8.776833 \t 0.066285 \t 636.298244 s\n","702 \t 18.523640 \t 10.011040 \t8.442760 \t 0.069840 \t 636.961955 s\n","703 \t 18.673629 \t 9.826259 \t8.766554 \t 0.080816 \t 637.633915 s\n","704 \t 18.469591 \t 9.932279 \t8.479772 \t 0.057540 \t 638.318423 s\n","705 \t 18.033781 \t 9.382728 \t8.565268 \t 0.085785 \t 639.015105 s\n","706 \t 18.971423 \t 10.009526 \t8.903374 \t 0.058524 \t 639.572902 s\n","707 \t 17.626991 \t 9.371698 \t8.186637 \t 0.068656 \t 640.145078 s\n","708 \t 18.896129 \t 10.212542 \t8.615896 \t 0.067691 \t 640.717105 s\n","709 \t 17.324628 \t 8.920980 \t8.244194 \t 0.159453 \t 641.287495 s\n","710 \t 18.278336 \t 9.469980 \t8.745884 \t 0.062472 \t 641.855244 s\n","711 \t 18.349620 \t 9.670914 \t8.631518 \t 0.047188 \t 642.415372 s\n","712 \t 17.453828 \t 9.264189 \t8.153967 \t 0.035672 \t 643.193035 s\n","713 \t 18.325844 \t 10.005767 \t8.266992 \t 0.053086 \t 643.747032 s\n","714 \t 17.679623 \t 9.109174 \t8.508062 \t 0.062385 \t 644.321502 s\n","715 \t 18.072489 \t 9.859286 \t8.161489 \t 0.051714 \t 644.888501 s\n","716 \t 18.013544 \t 9.394666 \t8.550587 \t 0.068291 \t 645.447437 s\n","717 \t 18.937675 \t 10.038324 \t8.820877 \t 0.078474 \t 646.011623 s\n","718 \t 18.344619 \t 9.749632 \t8.531550 \t 0.063438 \t 646.569606 s\n","719 \t 18.625076 \t 9.667253 \t8.886950 \t 0.070872 \t 647.135576 s\n","720 \t 18.332780 \t 9.763525 \t8.517383 \t 0.051874 \t 647.706350 s\n","721 \t 18.628963 \t 9.793239 \t8.743814 \t 0.091912 \t 648.269750 s\n","722 \t 18.125955 \t 9.421995 \t8.635616 \t 0.068343 \t 648.833341 s\n","723 \t 18.082044 \t 9.537954 \t8.476794 \t 0.067295 \t 649.488863 s\n","724 \t 17.792320 \t 9.460547 \t8.270152 \t 0.061621 \t 650.161924 s\n","725 \t 18.279224 \t 9.540631 \t8.693473 \t 0.045121 \t 650.813159 s\n","726 \t 18.422655 \t 9.816165 \t8.535670 \t 0.070820 \t 651.716108 s\n","727 \t 18.708060 \t 9.933736 \t8.725410 \t 0.048915 \t 652.391554 s\n","728 \t 18.456264 \t 9.866584 \t8.521990 \t 0.067689 \t 652.960223 s\n","729 \t 18.149976 \t 9.752617 \t8.352379 \t 0.044979 \t 653.534363 s\n","730 \t 18.750239 \t 9.949201 \t8.743049 \t 0.057990 \t 654.097183 s\n","731 \t 18.292438 \t 9.407147 \t8.840006 \t 0.045285 \t 654.673665 s\n","732 \t 18.531449 \t 9.734564 \t8.748690 \t 0.048196 \t 655.222642 s\n","733 \t 17.368091 \t 8.691009 \t8.608461 \t 0.068621 \t 655.795857 s\n","734 \t 18.387385 \t 9.892808 \t8.440341 \t 0.054236 \t 656.359169 s\n","735 \t 17.861073 \t 9.672566 \t8.134363 \t 0.054145 \t 657.068662 s\n","736 \t 18.585884 \t 10.039330 \t8.382883 \t 0.163672 \t 657.627493 s\n","737 \t 18.481950 \t 9.742929 \t8.671727 \t 0.067293 \t 658.183289 s\n","738 \t 18.191189 \t 9.719676 \t8.421094 \t 0.050420 \t 658.777393 s\n","739 \t 18.200752 \t 9.095845 \t8.975580 \t 0.129327 \t 659.341786 s\n","740 \t 18.800094 \t 9.942881 \t8.549470 \t 0.307743 \t 659.915997 s\n","741 \t 18.399786 \t 9.750798 \t8.576600 \t 0.072389 \t 660.482398 s\n","742 \t 18.077795 \t 9.742173 \t8.269728 \t 0.065894 \t 661.047903 s\n","743 \t 18.612822 \t 9.903312 \t8.655530 \t 0.053979 \t 661.616287 s\n","744 \t 17.731848 \t 9.139287 \t8.556417 \t 0.036144 \t 662.220340 s\n","745 \t 18.764234 \t 9.979419 \t8.740900 \t 0.043915 \t 663.092969 s\n","746 \t 18.686305 \t 9.912370 \t8.735723 \t 0.038212 \t 663.804709 s\n","747 \t 18.109084 \t 9.688534 \t8.370710 \t 0.049840 \t 664.565228 s\n","748 \t 18.473483 \t 9.743335 \t8.639697 \t 0.090452 \t 665.397408 s\n","749 \t 18.052231 \t 9.740899 \t8.245183 \t 0.066149 \t 665.963811 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:50<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4040\n","Hit@10: 0.4885\n","Hit@3: 0.4000\n","Hit@1: 0.3577\n","Link Prediction on Validation Set (All)\n","MRR: 0.2685\n","Hit@10: 0.3841\n","Hit@3: 0.2714\n","Hit@1: 0.2016\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2856\n","Hit@10: 0.4462\n","Hit@3: 0.2846\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4371\n","Hit@10: 0.6738\n","Hit@3: 0.5765\n","Hit@1: 0.2898\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1807\n","750 \t 18.314005 \t 9.699769 \t8.540324 \t 0.073912 \t 718.403561 s\n","751 \t 18.095257 \t 9.999050 \t8.029686 \t 0.066522 \t 718.993037 s\n","752 \t 18.631479 \t 9.998079 \t8.597200 \t 0.036199 \t 719.551044 s\n","753 \t 18.382485 \t 9.781015 \t8.540228 \t 0.061242 \t 720.124463 s\n","754 \t 18.313237 \t 10.028129 \t8.243298 \t 0.041812 \t 720.690214 s\n","755 \t 18.325823 \t 9.348948 \t8.909617 \t 0.067257 \t 721.400344 s\n","756 \t 17.682733 \t 9.402778 \t8.219231 \t 0.060724 \t 721.959461 s\n","757 \t 17.973720 \t 9.384483 \t8.409596 \t 0.179640 \t 722.541572 s\n","758 \t 18.352072 \t 9.686843 \t8.593417 \t 0.071812 \t 723.112951 s\n","759 \t 17.999482 \t 9.867657 \t8.075629 \t 0.056196 \t 723.688665 s\n","760 \t 18.204576 \t 9.879698 \t8.252182 \t 0.072697 \t 724.277620 s\n","761 \t 18.232236 \t 9.852441 \t8.295775 \t 0.084020 \t 724.913898 s\n","762 \t 17.189142 \t 8.791245 \t8.343290 \t 0.054606 \t 725.541082 s\n","763 \t 18.531966 \t 9.934903 \t8.446811 \t 0.150253 \t 726.123441 s\n","764 \t 18.534816 \t 9.712470 \t8.764153 \t 0.058194 \t 726.969569 s\n","765 \t 18.414558 \t 9.682132 \t8.654608 \t 0.077819 \t 727.549801 s\n","766 \t 18.620971 \t 9.638268 \t8.929103 \t 0.053600 \t 728.210672 s\n","767 \t 18.082820 \t 9.396715 \t8.474659 \t 0.211445 \t 729.032183 s\n","768 \t 18.306594 \t 9.606586 \t8.632339 \t 0.067669 \t 729.719740 s\n","769 \t 18.293353 \t 9.605491 \t8.356841 \t 0.331022 \t 730.401924 s\n","770 \t 18.240277 \t 9.699421 \t8.495644 \t 0.045211 \t 731.124747 s\n","771 \t 17.691913 \t 9.413696 \t8.211192 \t 0.067025 \t 731.680337 s\n","772 \t 18.284224 \t 9.811602 \t8.403414 \t 0.069207 \t 732.236750 s\n","773 \t 18.577576 \t 9.327600 \t9.038862 \t 0.211113 \t 732.788121 s\n","774 \t 18.414349 \t 9.733903 \t8.640893 \t 0.039551 \t 733.494642 s\n","775 \t 18.761579 \t 9.831896 \t8.640450 \t 0.289234 \t 734.073327 s\n","776 \t 18.284057 \t 9.740235 \t8.496665 \t 0.047156 \t 734.637009 s\n","777 \t 18.108576 \t 9.260259 \t8.782532 \t 0.065786 \t 735.188665 s\n","778 \t 18.224124 \t 9.374268 \t8.706255 \t 0.143601 \t 735.749228 s\n","779 \t 18.202148 \t 9.405724 \t8.728928 \t 0.067496 \t 736.303910 s\n","780 \t 18.850697 \t 10.135117 \t8.674507 \t 0.041072 \t 736.868309 s\n","781 \t 18.655944 \t 9.627030 \t8.958565 \t 0.070349 \t 737.419339 s\n","782 \t 18.155624 \t 9.673715 \t8.444971 \t 0.036938 \t 737.981087 s\n","783 \t 18.294904 \t 9.869571 \t8.383325 \t 0.042009 \t 738.535424 s\n","784 \t 18.426400 \t 9.555724 \t8.801696 \t 0.068980 \t 739.092551 s\n","785 \t 18.117488 \t 9.472035 \t8.564978 \t 0.080476 \t 739.820686 s\n","786 \t 18.224524 \t 9.823668 \t8.332555 \t 0.068301 \t 740.389619 s\n","787 \t 17.212325 \t 9.011027 \t7.996029 \t 0.205269 \t 740.956099 s\n","788 \t 19.168104 \t 10.212115 \t8.890672 \t 0.065317 \t 741.592432 s\n","789 \t 18.238020 \t 9.311163 \t8.885739 \t 0.041119 \t 742.250736 s\n","790 \t 18.312106 \t 9.859917 \t8.414697 \t 0.037493 \t 742.909630 s\n","791 \t 17.841546 \t 9.324721 \t8.435782 \t 0.081043 \t 743.591210 s\n","792 \t 18.325711 \t 9.577913 \t8.534387 \t 0.213411 \t 744.281616 s\n","793 \t 18.127935 \t 9.657544 \t8.392981 \t 0.077412 \t 744.828389 s\n","794 \t 18.516545 \t 9.663236 \t8.789579 \t 0.063730 \t 745.385462 s\n","795 \t 18.413498 \t 9.837014 \t8.495770 \t 0.080715 \t 745.941809 s\n","796 \t 18.139269 \t 9.616081 \t8.428986 \t 0.094202 \t 746.509665 s\n","797 \t 18.296424 \t 9.599467 \t8.655631 \t 0.041327 \t 747.236696 s\n","798 \t 18.804287 \t 10.008712 \t8.708214 \t 0.087360 \t 747.786885 s\n","799 \t 18.562840 \t 9.864131 \t8.639302 \t 0.059406 \t 748.349108 s\n","800 \t 18.176032 \t 9.308272 \t8.807297 \t 0.060463 \t 748.917748 s\n","801 \t 17.822081 \t 9.618143 \t8.140732 \t 0.063207 \t 749.535030 s\n","802 \t 18.223620 \t 9.612374 \t8.528986 \t 0.082260 \t 750.108766 s\n","803 \t 18.550939 \t 9.479693 \t8.771289 \t 0.299956 \t 750.714543 s\n","804 \t 17.896691 \t 9.489325 \t8.350945 \t 0.056421 \t 751.282825 s\n","805 \t 18.152870 \t 9.735783 \t8.219148 \t 0.197940 \t 751.843479 s\n","806 \t 17.804456 \t 9.752849 \t7.863436 \t 0.188171 \t 752.408556 s\n","807 \t 18.553519 \t 9.797670 \t8.698995 \t 0.056855 \t 752.957382 s\n","808 \t 18.649836 \t 9.745591 \t8.860577 \t 0.043668 \t 753.521731 s\n","809 \t 18.407918 \t 9.568059 \t8.800934 \t 0.038925 \t 754.091281 s\n","810 \t 18.595012 \t 9.648960 \t8.889461 \t 0.056593 \t 754.959918 s\n","811 \t 18.729853 \t 9.980804 \t8.548089 \t 0.200959 \t 755.620688 s\n","812 \t 18.112127 \t 9.483183 \t8.558245 \t 0.070699 \t 756.277536 s\n","813 \t 18.794075 \t 10.098785 \t8.638301 \t 0.056990 \t 757.010611 s\n","814 \t 18.712439 \t 9.968506 \t8.655348 \t 0.088585 \t 757.685816 s\n","815 \t 17.718859 \t 9.343325 \t8.316330 \t 0.059203 \t 758.242755 s\n","816 \t 18.479290 \t 9.826860 \t8.503567 \t 0.148862 \t 758.809875 s\n","817 \t 18.117666 \t 10.035945 \t8.026206 \t 0.055514 \t 759.367789 s\n","818 \t 18.084455 \t 9.362120 \t8.663007 \t 0.059327 \t 759.936681 s\n","819 \t 17.624335 \t 9.314967 \t8.253654 \t 0.055714 \t 760.491839 s\n","820 \t 18.165716 \t 9.422194 \t8.710319 \t 0.033203 \t 761.071067 s\n","821 \t 17.821961 \t 9.305670 \t8.351277 \t 0.165014 \t 761.621859 s\n","822 \t 18.483642 \t 9.848301 \t8.579652 \t 0.055688 \t 762.191771 s\n","823 \t 18.038662 \t 9.455327 \t8.356888 \t 0.226449 \t 762.910071 s\n","824 \t 19.218434 \t 9.718134 \t9.384150 \t 0.116152 \t 763.468593 s\n","825 \t 17.779440 \t 9.455680 \t8.247803 \t 0.075959 \t 764.038658 s\n","826 \t 18.363286 \t 9.963326 \t8.333465 \t 0.066495 \t 764.595298 s\n","827 \t 18.109891 \t 9.512020 \t8.512737 \t 0.085135 \t 765.156036 s\n","828 \t 18.815455 \t 9.792742 \t8.968635 \t 0.054077 \t 765.714003 s\n","829 \t 18.380383 \t 9.678507 \t8.640925 \t 0.060949 \t 766.282845 s\n","830 \t 18.793984 \t 9.835585 \t8.892822 \t 0.065578 \t 766.857396 s\n","831 \t 18.860829 \t 10.175025 \t8.639196 \t 0.046608 \t 767.468107 s\n","832 \t 18.174919 \t 9.508559 \t8.347317 \t 0.319043 \t 768.130763 s\n","833 \t 17.409242 \t 9.094965 \t8.285395 \t 0.028882 \t 768.792857 s\n","834 \t 18.069461 \t 9.638742 \t8.358128 \t 0.072592 \t 769.470755 s\n","835 \t 18.305266 \t 9.538588 \t8.706453 \t 0.060226 \t 770.218034 s\n","836 \t 18.684030 \t 9.434510 \t9.202940 \t 0.046581 \t 770.850159 s\n","837 \t 17.677795 \t 9.154551 \t8.459950 \t 0.063294 \t 771.571530 s\n","838 \t 18.603031 \t 9.293911 \t9.121665 \t 0.187455 \t 772.134410 s\n","839 \t 17.894826 \t 9.416061 \t8.441055 \t 0.037710 \t 772.694361 s\n","840 \t 17.297272 \t 9.225008 \t8.018095 \t 0.054169 \t 773.266102 s\n","841 \t 17.507952 \t 9.178503 \t8.270897 \t 0.058552 \t 773.838410 s\n","842 \t 18.236440 \t 9.664996 \t8.494012 \t 0.077432 \t 774.408940 s\n","843 \t 18.828087 \t 10.018288 \t8.744544 \t 0.065256 \t 774.972486 s\n","844 \t 18.266844 \t 9.714868 \t8.481978 \t 0.069997 \t 775.535675 s\n","845 \t 18.058931 \t 9.367853 \t8.624770 \t 0.066307 \t 776.089888 s\n","846 \t 18.888198 \t 9.721403 \t9.108118 \t 0.058678 \t 776.813441 s\n","847 \t 18.364490 \t 9.729314 \t8.568651 \t 0.066525 \t 777.400769 s\n","848 \t 18.283640 \t 9.520615 \t8.725965 \t 0.037060 \t 777.955515 s\n","849 \t 18.036548 \t 9.829529 \t8.159297 \t 0.047721 \t 778.514839 s\n","850 \t 17.881048 \t 9.729715 \t8.091102 \t 0.060231 \t 779.080663 s\n","851 \t 18.476602 \t 9.865263 \t8.540856 \t 0.070483 \t 779.652462 s\n","852 \t 18.223285 \t 9.440598 \t8.736594 \t 0.046092 \t 780.212119 s\n","853 \t 18.343802 \t 9.715238 \t8.308025 \t 0.320541 \t 780.815781 s\n","854 \t 17.637915 \t 9.721029 \t7.866002 \t 0.050884 \t 781.491548 s\n","855 \t 17.942677 \t 9.480517 \t8.183076 \t 0.279084 \t 782.139484 s\n","856 \t 17.802848 \t 9.238930 \t8.509019 \t 0.054898 \t 783.029190 s\n","857 \t 17.600750 \t 9.128282 \t8.405048 \t 0.067420 \t 783.715746 s\n","858 \t 18.590933 \t 9.956367 \t8.488719 \t 0.145846 \t 784.265134 s\n","859 \t 17.553258 \t 9.415119 \t8.076562 \t 0.061575 \t 784.820014 s\n","860 \t 18.558196 \t 9.639087 \t8.861145 \t 0.057963 \t 785.388104 s\n","861 \t 18.255715 \t 9.543229 \t8.661385 \t 0.051101 \t 785.946530 s\n","862 \t 18.464018 \t 9.890899 \t8.492192 \t 0.080927 \t 786.498083 s\n","863 \t 18.340569 \t 9.532490 \t8.760014 \t 0.048065 \t 787.058563 s\n","864 \t 18.233619 \t 9.735510 \t8.435854 \t 0.062255 \t 787.606798 s\n","865 \t 17.689232 \t 8.842752 \t8.779288 \t 0.067192 \t 788.174144 s\n","866 \t 17.737296 \t 9.220456 \t8.466434 \t 0.050408 \t 788.889994 s\n","867 \t 18.525969 \t 9.624626 \t8.863399 \t 0.037944 \t 789.446451 s\n","868 \t 19.065548 \t 9.790191 \t9.211389 \t 0.063969 \t 790.007104 s\n","869 \t 17.935015 \t 9.737747 \t8.149615 \t 0.047653 \t 790.560294 s\n","870 \t 18.726543 \t 10.006422 \t8.673324 \t 0.046798 \t 791.125580 s\n","871 \t 18.802197 \t 9.922667 \t8.805024 \t 0.074506 \t 791.675436 s\n","872 \t 18.356148 \t 9.573272 \t8.710883 \t 0.071992 \t 792.222121 s\n","873 \t 18.456964 \t 9.465302 \t8.821526 \t 0.170138 \t 792.773510 s\n","874 \t 18.493017 \t 9.810144 \t8.641286 \t 0.041588 \t 793.347923 s\n","875 \t 18.687064 \t 9.613401 \t9.020669 \t 0.052994 \t 794.035439 s\n","876 \t 18.181809 \t 9.491586 \t8.622190 \t 0.068032 \t 794.693647 s\n","877 \t 18.163820 \t 9.465353 \t8.521641 \t 0.176826 \t 795.535742 s\n","878 \t 17.703589 \t 9.236727 \t8.396327 \t 0.070535 \t 796.270623 s\n","879 \t 19.063140 \t 9.959557 \t8.911070 \t 0.192512 \t 796.930299 s\n","880 \t 18.441916 \t 9.715976 \t8.652308 \t 0.073631 \t 797.489283 s\n","881 \t 17.974926 \t 9.234797 \t8.685943 \t 0.054187 \t 798.056228 s\n","882 \t 18.359114 \t 9.428467 \t8.756697 \t 0.173950 \t 798.624902 s\n","883 \t 18.117256 \t 9.264821 \t8.724905 \t 0.127530 \t 799.169557 s\n","884 \t 17.920303 \t 9.528520 \t8.320660 \t 0.071123 \t 799.741195 s\n","885 \t 18.254760 \t 9.717307 \t8.244264 \t 0.293189 \t 800.310018 s\n","886 \t 18.157028 \t 9.623322 \t8.300303 \t 0.233404 \t 800.887603 s\n","887 \t 18.190572 \t 9.754354 \t8.359254 \t 0.076962 \t 801.447680 s\n","888 \t 18.083116 \t 9.912412 \t8.105062 \t 0.065642 \t 802.157061 s\n","889 \t 18.626714 \t 10.140463 \t8.361670 \t 0.124580 \t 802.710434 s\n","890 \t 17.799024 \t 9.315213 \t8.437565 \t 0.046246 \t 803.270285 s\n","891 \t 18.615855 \t 9.914886 \t8.660103 \t 0.040867 \t 803.836919 s\n","892 \t 18.102131 \t 9.756039 \t8.282813 \t 0.063280 \t 804.403740 s\n","893 \t 19.025906 \t 10.005383 \t8.991659 \t 0.028863 \t 804.947464 s\n","894 \t 17.953695 \t 9.676457 \t8.196932 \t 0.080306 \t 805.504125 s\n","895 \t 18.136682 \t 9.763130 \t8.328434 \t 0.045117 \t 806.061617 s\n","896 \t 18.661586 \t 9.879884 \t8.582388 \t 0.199314 \t 806.657376 s\n","897 \t 17.756918 \t 9.546498 \t8.158232 \t 0.052188 \t 807.320380 s\n","898 \t 17.762062 \t 9.710247 \t7.996552 \t 0.055264 \t 807.992345 s\n","899 \t 18.074601 \t 9.332100 \t8.547946 \t 0.194555 \t 808.675300 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:49<00:00,  2.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4030\n","Hit@10: 0.4962\n","Hit@3: 0.3962\n","Hit@1: 0.3577\n","Link Prediction on Validation Set (All)\n","MRR: 0.2688\n","Hit@10: 0.3968\n","Hit@3: 0.2698\n","Hit@1: 0.2016\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2876\n","Hit@10: 0.4615\n","Hit@3: 0.2769\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4874\n","Hit@10: 0.6834\n","Hit@3: 0.5754\n","Hit@1: 0.3861\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1731\n","900 \t 18.443335 \t 9.504760 \t8.750129 \t 0.188445 \t 860.163443 s\n","901 \t 18.652559 \t 9.515758 \t9.077027 \t 0.059773 \t 860.829960 s\n","902 \t 18.309853 \t 9.788122 \t8.464461 \t 0.057269 \t 861.509996 s\n","903 \t 17.753049 \t 9.845650 \t7.849461 \t 0.057936 \t 862.179781 s\n","904 \t 18.672210 \t 9.698146 \t8.912896 \t 0.061167 \t 862.736015 s\n","905 \t 18.662904 \t 10.094953 \t8.259314 \t 0.308637 \t 863.314603 s\n","906 \t 18.308348 \t 9.798960 \t8.439398 \t 0.069989 \t 863.881636 s\n","907 \t 18.140792 \t 9.421391 \t8.661871 \t 0.057530 \t 864.444109 s\n","908 \t 17.868676 \t 9.275718 \t8.517738 \t 0.075219 \t 864.999696 s\n","909 \t 17.803795 \t 8.945118 \t8.793773 \t 0.064903 \t 865.556232 s\n","910 \t 17.846138 \t 9.390984 \t8.424110 \t 0.031044 \t 866.118224 s\n","911 \t 18.053549 \t 9.756523 \t8.253300 \t 0.043725 \t 866.929086 s\n","912 \t 18.080261 \t 9.267832 \t8.758343 \t 0.054086 \t 867.529047 s\n","913 \t 18.431290 \t 9.615416 \t8.773838 \t 0.042036 \t 868.147327 s\n","914 \t 18.059803 \t 9.548930 \t8.453730 \t 0.057144 \t 868.733511 s\n","915 \t 17.727010 \t 9.648373 \t8.025173 \t 0.053464 \t 869.343509 s\n","916 \t 17.838636 \t 9.089536 \t8.698441 \t 0.050660 \t 870.071738 s\n","917 \t 18.027181 \t 9.685975 \t8.285353 \t 0.055853 \t 870.747727 s\n","918 \t 17.478673 \t 8.983259 \t8.440991 \t 0.054422 \t 871.313909 s\n","919 \t 18.023999 \t 9.499526 \t8.487811 \t 0.036662 \t 871.877447 s\n","920 \t 18.683097 \t 9.980519 \t8.652153 \t 0.050423 \t 872.549759 s\n","921 \t 17.910333 \t 9.603572 \t8.260831 \t 0.045929 \t 873.228974 s\n","922 \t 17.633971 \t 9.455233 \t7.951306 \t 0.227432 \t 874.061965 s\n","923 \t 18.037337 \t 9.502049 \t8.471367 \t 0.063922 \t 874.801430 s\n","924 \t 18.227037 \t 9.448836 \t8.728806 \t 0.049395 \t 875.450814 s\n","925 \t 17.906215 \t 9.238176 \t8.486624 \t 0.181414 \t 876.011784 s\n","926 \t 18.896627 \t 9.958968 \t8.755459 \t 0.182201 \t 876.568164 s\n","927 \t 19.227690 \t 9.932323 \t9.046661 \t 0.248707 \t 877.134488 s\n","928 \t 18.339203 \t 9.435506 \t8.832129 \t 0.071567 \t 877.699526 s\n","929 \t 18.269098 \t 9.879040 \t8.323980 \t 0.066079 \t 878.258572 s\n","930 \t 18.806601 \t 9.618433 \t8.982176 \t 0.205993 \t 878.846011 s\n","931 \t 17.712963 \t 9.338380 \t8.317815 \t 0.056768 \t 879.416276 s\n","932 \t 18.014463 \t 9.389009 \t8.579182 \t 0.046270 \t 879.962621 s\n","933 \t 18.015530 \t 9.729924 \t8.197932 \t 0.087674 \t 880.527958 s\n","934 \t 17.726902 \t 9.182188 \t8.318944 \t 0.225771 \t 881.096010 s\n","935 \t 18.508193 \t 9.446240 \t8.880657 \t 0.181296 \t 881.806195 s\n","936 \t 17.932501 \t 9.927561 \t7.945608 \t 0.059332 \t 882.351909 s\n","937 \t 17.982655 \t 9.463457 \t8.449984 \t 0.069213 \t 882.917582 s\n","938 \t 18.369281 \t 9.446733 \t8.685409 \t 0.237139 \t 883.461919 s\n","939 \t 18.129060 \t 9.545981 \t8.534478 \t 0.048601 \t 884.044276 s\n","940 \t 18.008223 \t 9.421947 \t8.502058 \t 0.084218 \t 884.623673 s\n","941 \t 17.680976 \t 9.434156 \t8.207867 \t 0.038953 \t 885.230062 s\n","942 \t 18.228727 \t 9.831456 \t8.323411 \t 0.073860 \t 885.897809 s\n","943 \t 17.822234 \t 9.351055 \t8.405581 \t 0.065598 \t 886.545463 s\n","944 \t 18.065351 \t 9.759269 \t8.253459 \t 0.052622 \t 887.251345 s\n","945 \t 17.778722 \t 9.525271 \t8.181089 \t 0.072360 \t 887.987617 s\n","946 \t 18.523185 \t 9.875832 \t8.598094 \t 0.049260 \t 888.554526 s\n","947 \t 18.700859 \t 9.964161 \t8.579760 \t 0.156938 \t 889.116855 s\n","948 \t 18.670157 \t 9.704342 \t8.828087 \t 0.137727 \t 889.851174 s\n","949 \t 18.263666 \t 9.830636 \t8.395942 \t 0.037089 \t 890.414040 s\n","950 \t 17.711013 \t 9.487648 \t8.163123 \t 0.060243 \t 890.984565 s\n","951 \t 18.576347 \t 9.608699 \t8.892536 \t 0.075113 \t 891.543712 s\n","952 \t 17.553812 \t 9.495742 \t8.015864 \t 0.042206 \t 892.111037 s\n","953 \t 18.063806 \t 9.667233 \t8.191506 \t 0.205067 \t 892.668962 s\n","954 \t 17.342075 \t 9.428854 \t7.864395 \t 0.048827 \t 893.228081 s\n","955 \t 17.883322 \t 9.269608 \t8.553438 \t 0.060276 \t 893.790627 s\n","956 \t 17.916284 \t 9.275175 \t8.596143 \t 0.044964 \t 894.345457 s\n","957 \t 18.043783 \t 9.593546 \t8.404313 \t 0.045923 \t 895.069943 s\n","958 \t 18.680762 \t 9.837118 \t8.784268 \t 0.059376 \t 895.613995 s\n","959 \t 17.980379 \t 9.738274 \t8.184413 \t 0.057692 \t 896.171060 s\n","960 \t 18.206243 \t 9.568316 \t8.602270 \t 0.035656 \t 896.723103 s\n","961 \t 18.407154 \t 9.876700 \t8.461340 \t 0.069114 \t 897.284220 s\n","962 \t 18.859518 \t 9.676733 \t9.100402 \t 0.082383 \t 897.857959 s\n","963 \t 17.462779 \t 9.445271 \t7.878358 \t 0.139149 \t 898.509627 s\n","964 \t 18.645831 \t 10.078758 \t8.506480 \t 0.060592 \t 899.162951 s\n","965 \t 18.156267 \t 9.388955 \t8.706815 \t 0.060498 \t 899.827768 s\n","966 \t 18.487079 \t 10.088787 \t8.334565 \t 0.063728 \t 900.509486 s\n","967 \t 17.696858 \t 9.795011 \t7.829150 \t 0.072698 \t 901.421804 s\n","968 \t 18.963136 \t 9.956078 \t8.959447 \t 0.047611 \t 901.986446 s\n","969 \t 17.346184 \t 9.818371 \t7.479910 \t 0.047902 \t 902.540891 s\n","970 \t 17.754261 \t 9.420190 \t8.010315 \t 0.323756 \t 903.090196 s\n","971 \t 18.905210 \t 10.068813 \t8.581857 \t 0.254541 \t 903.647119 s\n","972 \t 19.133162 \t 10.253180 \t8.755019 \t 0.124965 \t 904.201256 s\n","973 \t 17.807775 \t 8.841792 \t8.891360 \t 0.074624 \t 904.748825 s\n","974 \t 17.952087 \t 9.641036 \t8.242166 \t 0.068886 \t 905.307189 s\n","975 \t 17.869440 \t 9.286550 \t8.518000 \t 0.064891 \t 905.884528 s\n","976 \t 18.298310 \t 9.770952 \t8.383511 \t 0.143848 \t 906.439018 s\n","977 \t 17.967173 \t 9.382540 \t8.525401 \t 0.059232 \t 907.159301 s\n","978 \t 18.551798 \t 9.596118 \t8.886217 \t 0.069463 \t 907.714661 s\n","979 \t 18.057205 \t 9.435996 \t8.585145 \t 0.036065 \t 908.269114 s\n","980 \t 17.966069 \t 9.419611 \t8.308807 \t 0.237650 \t 908.834963 s\n","981 \t 18.426314 \t 9.636110 \t8.573101 \t 0.217103 \t 909.384996 s\n","982 \t 18.458535 \t 9.398177 \t9.022265 \t 0.038093 \t 909.948050 s\n","983 \t 18.377724 \t 9.974532 \t8.257679 \t 0.145514 \t 910.495255 s\n","984 \t 17.451402 \t 9.199785 \t8.197295 \t 0.054321 \t 911.079970 s\n","985 \t 18.281264 \t 9.395863 \t8.694341 \t 0.191059 \t 911.714232 s\n","986 \t 17.447915 \t 9.214965 \t8.187213 \t 0.045737 \t 912.363109 s\n","987 \t 18.177369 \t 9.853453 \t8.263709 \t 0.060207 \t 913.201226 s\n","988 \t 18.087618 \t 9.603484 \t8.419170 \t 0.064963 \t 913.872951 s\n","989 \t 18.321373 \t 9.984717 \t8.293216 \t 0.043440 \t 914.544663 s\n","990 \t 18.190076 \t 9.575535 \t8.358849 \t 0.255692 \t 915.097635 s\n","991 \t 17.776883 \t 9.141426 \t8.474518 \t 0.160940 \t 915.645490 s\n","992 \t 18.195539 \t 9.704604 \t8.362704 \t 0.128232 \t 916.185975 s\n","993 \t 18.048235 \t 9.636676 \t8.215364 \t 0.196196 \t 916.734543 s\n","994 \t 17.281599 \t 8.999525 \t8.199831 \t 0.082242 \t 917.298085 s\n","995 \t 18.255202 \t 9.506059 \t8.708222 \t 0.040921 \t 917.847736 s\n","996 \t 18.535745 \t 9.999493 \t8.484288 \t 0.051963 \t 918.557593 s\n","997 \t 17.915315 \t 9.681193 \t8.203611 \t 0.030510 \t 919.114975 s\n","998 \t 18.101486 \t 9.305277 \t8.610723 \t 0.185487 \t 919.670965 s\n","999 \t 17.638451 \t 9.462974 \t8.119594 \t 0.055883 \t 920.221490 s\n","1000 \t 18.008679 \t 9.897575 \t8.064121 \t 0.046984 \t 920.778415 s\n","1001 \t 18.501585 \t 9.859902 \t8.570340 \t 0.071342 \t 921.343811 s\n","1002 \t 18.129213 \t 9.318014 \t8.766408 \t 0.044791 \t 921.906471 s\n","1003 \t 18.023978 \t 9.222840 \t8.742225 \t 0.058913 \t 922.462590 s\n","1004 \t 17.988525 \t 9.386798 \t8.548591 \t 0.053137 \t 923.027018 s\n","1005 \t 17.785784 \t 9.459007 \t8.286682 \t 0.040094 \t 923.593256 s\n","1006 \t 18.170688 \t 9.854855 \t8.249728 \t 0.066104 \t 924.158687 s\n","1007 \t 18.044466 \t 9.751118 \t8.250242 \t 0.043105 \t 924.993090 s\n","1008 \t 17.868778 \t 9.488509 \t8.253970 \t 0.126300 \t 925.658859 s\n","1009 \t 17.630158 \t 9.643059 \t7.931652 \t 0.055447 \t 926.333494 s\n","1010 \t 18.259289 \t 9.531803 \t8.421566 \t 0.305920 \t 927.054358 s\n","1011 \t 18.300345 \t 9.482845 \t8.675039 \t 0.142462 \t 927.730235 s\n","1012 \t 18.632087 \t 9.954942 \t8.614732 \t 0.062413 \t 928.275394 s\n","1013 \t 17.565488 \t 9.562244 \t7.958247 \t 0.044996 \t 928.840471 s\n","1014 \t 18.535505 \t 9.966815 \t8.500835 \t 0.067855 \t 929.397703 s\n","1015 \t 17.988512 \t 9.316766 \t8.619246 \t 0.052501 \t 929.958714 s\n","1016 \t 18.088831 \t 9.553106 \t8.454460 \t 0.081265 \t 930.514263 s\n","1017 \t 18.060486 \t 9.625318 \t8.362941 \t 0.072228 \t 931.091570 s\n","1018 \t 18.612638 \t 9.534991 \t9.018335 \t 0.059312 \t 931.657540 s\n","1019 \t 17.488089 \t 9.457598 \t7.886599 \t 0.143890 \t 932.359175 s\n","1020 \t 18.382399 \t 9.729828 \t8.616415 \t 0.036156 \t 932.946689 s\n","1021 \t 17.844457 \t 9.393223 \t8.402325 \t 0.048909 \t 933.491264 s\n","1022 \t 17.667266 \t 9.649661 \t7.966788 \t 0.050816 \t 934.070045 s\n","1023 \t 18.011886 \t 9.607177 \t8.336049 \t 0.068660 \t 934.639518 s\n","1024 \t 18.760569 \t 9.874661 \t8.701851 \t 0.184056 \t 935.189922 s\n","1025 \t 18.070574 \t 9.755313 \t8.249446 \t 0.065816 \t 935.743682 s\n","1026 \t 17.569460 \t 9.281717 \t8.232845 \t 0.054899 \t 936.299421 s\n","1027 \t 18.309852 \t 9.900334 \t8.347237 \t 0.062280 \t 936.862062 s\n","1028 \t 18.291086 \t 9.900013 \t8.345550 \t 0.045525 \t 937.443677 s\n","1029 \t 18.318905 \t 9.756165 \t8.338775 \t 0.223965 \t 938.108888 s\n","1030 \t 18.326852 \t 10.023776 \t8.268336 \t 0.034739 \t 938.784410 s\n","1031 \t 18.370904 \t 10.065300 \t8.248256 \t 0.057347 \t 939.426537 s\n","1032 \t 18.499392 \t 9.724747 \t8.685156 \t 0.089489 \t 940.364700 s\n","1033 \t 18.062681 \t 9.607445 \t8.411304 \t 0.043933 \t 940.939628 s\n","1034 \t 18.203595 \t 9.635590 \t8.524485 \t 0.043520 \t 941.485401 s\n","1035 \t 18.827414 \t 9.534012 \t9.115896 \t 0.177506 \t 942.038205 s\n","1036 \t 17.970858 \t 9.607401 \t8.320230 \t 0.043227 \t 942.588267 s\n","1037 \t 19.006296 \t 9.896418 \t9.080531 \t 0.029348 \t 943.149110 s\n","1038 \t 17.816149 \t 9.723817 \t7.937750 \t 0.154582 \t 943.694288 s\n","1039 \t 17.841959 \t 9.481227 \t8.162560 \t 0.198172 \t 944.257828 s\n","1040 \t 17.807916 \t 9.680767 \t8.086866 \t 0.040284 \t 944.829839 s\n","1041 \t 18.392572 \t 9.317207 \t8.945750 \t 0.129615 \t 945.404865 s\n","1042 \t 18.259434 \t 9.547585 \t8.657028 \t 0.054821 \t 945.955796 s\n","1043 \t 17.304221 \t 9.255718 \t7.868255 \t 0.180249 \t 946.524664 s\n","1044 \t 17.580575 \t 9.782102 \t7.730101 \t 0.068371 \t 947.090403 s\n","1045 \t 17.903721 \t 9.558645 \t8.305354 \t 0.039721 \t 947.795511 s\n","1046 \t 18.392296 \t 9.467461 \t8.880594 \t 0.044241 \t 948.342989 s\n","1047 \t 18.022780 \t 9.820306 \t8.152605 \t 0.049870 \t 948.896691 s\n","1048 \t 18.588712 \t 9.956831 \t8.595870 \t 0.036012 \t 949.456923 s\n","1049 \t 19.051432 \t 9.909568 \t9.099782 \t 0.042082 \t 950.011351 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:50<00:00,  2.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4023\n","Hit@10: 0.5000\n","Hit@3: 0.3962\n","Hit@1: 0.3577\n","Link Prediction on Validation Set (All)\n","MRR: 0.2687\n","Hit@10: 0.3937\n","Hit@3: 0.2698\n","Hit@1: 0.2016\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2894\n","Hit@10: 0.4462\n","Hit@3: 0.3077\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4379\n","Hit@10: 0.6813\n","Hit@3: 0.5807\n","Hit@1: 0.2898\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1708\n"]}]},{"cell_type":"markdown","source":["# Test.py"],"metadata":{"id":"p0M6asfiPDtD"}},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/code/VTHNKG-CQI/checkpoint/Reproduce/VTHNKG-CQI_Retry/lr_0.0004_dim_256__1050.ckpt\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-CQI_Retry\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","def load_id_mapping(file_path):\n","    id2name = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip() == \"\" or line.startswith(\"#\"):  # 주석 또는 공백 무시\n","                continue\n","            parts = line.strip().split('\\t')\n","            if len(parts) != 2:\n","                continue\n","            name, idx = parts\n","            id2name[int(idx)] = name\n","    return id2name\n","\n","id2ent = load_id_mapping(\"entity2id.txt\")\n","id2rel = load_id_mapping(\"relation2id.txt\")\n","\n","def convert_triplet_ids_to_names(triplet, id2ent, id2rel, num_ent, num_rel):\n","    triplet_named = []\n","    for idx, val in enumerate(triplet):\n","        if idx % 2 == 0:  # entity or numeric value\n","            if val < num_ent:\n","                triplet_named.append(id2ent.get(val, f\"[ENT:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[NUM:{val - num_ent}]\")\n","        else:  # relation\n","            if val < num_rel:\n","                triplet_named.append(id2rel.get(val, f\"[REL:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[MASK_REL]\")\n","    return triplet_named\n","\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    entity_pred_log = []\n","    relation_pred_log = []\n","    numeric_pred_log = []\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    # sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    # nvp_tri_se += sq_error\n","                    # nvp_tri_se_num += 1\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","\n","                else: # qualifier\n","                  pred = score_num[0, 2, tri[ent_idx*2] - KG.num_ent]\n","                  gt = tri_num[ent_idx - 1]\n","                  sq_error = (pred - gt) ** 2\n","                  named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                  numeric_pred_log.append({\n","                      \"triplet_id\": str(tri),\n","                      \"triplet_named\": \":\".join(named_triplet),\n","                      \"position\": ent_idx,\n","                      \"type\": \"qualifier\",\n","                      \"gt\": float(gt),\n","                      \"pred\": float(pred),\n","                      \"se\": float(sq_error)\n","                  })\n","                    # sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                    topk = np.argsort(-score_ent[0,1+2*ent_idx])[:5]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    try:\n","                      topk = np.argsort(-score_ent[0,2])[:5]\n","                    except:\n","                      topk = np.argsort(-score_ent[0,2])[:]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","                topk = np.argsort(-score_rel[0,2])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"relation\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                topk = np.argsort(-score_rel[0,1])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"qualifier\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","pd.DataFrame(entity_pred_log).to_csv(\"entity_predictions.csv\", index=False)\n","pd.DataFrame(relation_pred_log).to_csv(\"relation_predictions.csv\", index=False)\n","pd.DataFrame(numeric_pred_log).to_csv(\"numeric_predictions.csv\", index=False)"],"metadata":{"id":"z1vrsHJ4J-DL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5hSy55CwMqZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K1tM08yOPFgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1vF-ijqiPHd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M-HqFVPLPJkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ChVIC_5BHELi","executionInfo":{"status":"ok","timestamp":1747357238004,"user_tz":-540,"elapsed":57544,"user":{"displayName":"URP","userId":"16515248769931109428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"518bdc7f-fd84-4494-9e98-1f3e6c987b91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 132/132 [00:51<00:00,  2.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3900\n","Hit@10: 0.4924\n","Hit@3: 0.3977\n","Hit@1: 0.3295\n","Link Prediction on Validation Set (All)\n","MRR: 0.2629\n","Hit@10: 0.3947\n","Hit@3: 0.2594\n","Hit@1: 0.1934\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3008\n","Hit@10: 0.4621\n","Hit@3: 0.3106\n","Hit@1: 0.2273\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4376\n","Hit@10: 0.6953\n","Hit@3: 0.5754\n","Hit@1: 0.2856\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1363\n"]}]}]}