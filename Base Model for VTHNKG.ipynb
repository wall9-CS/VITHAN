{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyP/rflXdk1kuepUQUVnzHnR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749796628540,"user_tz":-540,"elapsed":1179,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"034b685f-8e08-4343-a6cb-62021345e574"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n","\n","import pandas as pd"],"metadata":{"id":"xWGfSBgsm1r2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive_dir = \"/content/drive/MyDrive/code/\"\n","dataset_dir = \"VTHNKG-CQ/\"\n","dataset_name = \"VTHNKG-CQ\"\n","exp_name = \"seed0\"\n","exp_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n","test_epoch = \"1050\""],"metadata":{"id":"uUIY_MEKNGsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2h6HjbDCO-tg","executionInfo":{"status":"ok","timestamp":1749796633631,"user_tz":-540,"elapsed":70,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"7dc00de4-37b1-4cfb-b30b-956a673aabd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code\n"]}]},{"cell_type":"code","source":["# argument 정의\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default=dataset_name) # 실험 이름\n","parser.add_argument('--data', default = dataset_name + \"_\" + exp_name + \"_\" + exp_date, type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()"],"metadata":{"id":"n3IoE1gDLXVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","        self.proj_rel_vis = nn.Linear(dim_vis * 3, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)\n"],"metadata":{"id":"2CgXgeAXmg-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTMmNF8Cl5it"},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = drive_dir + dataset_dir\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        # 경로 수정 visual feature는 VTHNKG 인용\n","        if item == 'entity':\n","            vis_feats = torch.load(drive_dir + \"visual_fetures_ent_sorted\")\n","        elif item == 'relation':\n","            vis_feats = torch.load(drive_dir + 'visual_features_rel_sorted')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, drive_dir + \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, drive_dir + \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile(drive_dir + 'visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load(drive_dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(drive_dir + 'visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(drive_dir + 'visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load(drive_dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(drive_dir + 'visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(drive_dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(drive_dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    print(f\"[DEBUG] Total RP (Tri) samples collected: {len(rp_tri_list_rank)}\")\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48e36d18-b79c-413f-8111-5b9775173114","collapsed":true,"executionInfo":{"status":"ok","timestamp":1749797478949,"user_tz":-540,"elapsed":829485,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 28.025348 \t 11.246732 \t11.340196 \t 5.438420 \t 1.138465 s\n","1 \t 88.265993 \t 10.606544 \t10.687604 \t 66.971844 \t 1.665791 s\n","2 \t 27.016403 \t 10.911243 \t10.889717 \t 5.215442 \t 2.190199 s\n","3 \t 31.114573 \t 9.856365 \t11.133153 \t 10.125054 \t 2.695177 s\n","4 \t 22.378489 \t 10.774444 \t10.195858 \t 1.408189 \t 3.212057 s\n","5 \t 23.971351 \t 10.550172 \t10.370193 \t 3.050985 \t 3.722260 s\n","6 \t 21.697817 \t 10.359246 \t10.495665 \t 0.842907 \t 4.290692 s\n","7 \t 21.674289 \t 9.831825 \t10.460028 \t 1.382437 \t 4.909832 s\n","8 \t 20.755622 \t 9.931355 \t10.252053 \t 0.572215 \t 5.552029 s\n","9 \t 21.321563 \t 10.183415 \t10.288684 \t 0.849463 \t 6.343224 s\n","10 \t 20.749856 \t 10.291571 \t9.952405 \t 0.505879 \t 7.015172 s\n","11 \t 20.641596 \t 10.022360 \t10.219380 \t 0.399855 \t 7.616180 s\n","12 \t 20.731939 \t 9.881616 \t10.090373 \t 0.759951 \t 8.128546 s\n","13 \t 19.830697 \t 9.888199 \t9.675822 \t 0.266675 \t 8.628573 s\n","14 \t 19.679635 \t 9.593526 \t9.797939 \t 0.288168 \t 9.143539 s\n","15 \t 20.385738 \t 10.088924 \t10.080078 \t 0.216736 \t 9.656083 s\n","16 \t 19.778054 \t 9.596129 \t9.859025 \t 0.322900 \t 10.200284 s\n","17 \t 20.114789 \t 9.853985 \t9.900677 \t 0.360127 \t 10.701715 s\n","18 \t 20.520578 \t 10.102870 \t10.151753 \t 0.265954 \t 11.217879 s\n","19 \t 20.013637 \t 9.933432 \t9.734977 \t 0.345227 \t 11.869365 s\n","20 \t 20.239491 \t 10.067501 \t9.814135 \t 0.357855 \t 12.383745 s\n","21 \t 19.353473 \t 10.044995 \t8.989053 \t 0.319425 \t 12.882215 s\n","22 \t 20.174566 \t 9.851108 \t10.057338 \t 0.266120 \t 13.391136 s\n","23 \t 19.969594 \t 9.961585 \t9.902976 \t 0.105033 \t 13.889937 s\n","24 \t 19.705415 \t 10.121407 \t9.200256 \t 0.383753 \t 14.407263 s\n","25 \t 19.750684 \t 9.917749 \t9.689433 \t 0.143502 \t 14.907331 s\n","26 \t 20.097369 \t 9.902035 \t9.625745 \t 0.569588 \t 15.677516 s\n","27 \t 19.866258 \t 9.771368 \t9.763451 \t 0.331440 \t 16.289532 s\n","28 \t 19.806385 \t 9.976029 \t9.392745 \t 0.437610 \t 17.007091 s\n","29 \t 19.757808 \t 9.989239 \t9.523165 \t 0.245404 \t 17.949132 s\n","30 \t 19.791564 \t 9.994029 \t9.205348 \t 0.592187 \t 18.584575 s\n","31 \t 19.699404 \t 9.873528 \t9.441459 \t 0.384419 \t 19.394546 s\n","32 \t 19.293930 \t 9.768688 \t9.413412 \t 0.111832 \t 20.009876 s\n","33 \t 19.537866 \t 10.026512 \t9.364374 \t 0.146981 \t 20.705309 s\n","34 \t 19.857941 \t 9.750566 \t9.922001 \t 0.185374 \t 21.217924 s\n","35 \t 20.163404 \t 10.184237 \t9.583428 \t 0.395740 \t 21.737863 s\n","36 \t 20.106171 \t 10.110405 \t9.854832 \t 0.140934 \t 22.245412 s\n","37 \t 19.440996 \t 9.982502 \t9.303435 \t 0.155059 \t 22.763141 s\n","38 \t 20.212014 \t 9.605604 \t10.185023 \t 0.421387 \t 23.267288 s\n","39 \t 18.967601 \t 9.308069 \t9.480144 \t 0.179387 \t 23.802927 s\n","40 \t 20.123648 \t 9.891384 \t9.948038 \t 0.284226 \t 24.312306 s\n","41 \t 20.960036 \t 10.275690 \t10.121203 \t 0.563143 \t 24.986660 s\n","42 \t 20.591786 \t 10.107270 \t10.339134 \t 0.145382 \t 25.484334 s\n","43 \t 19.402786 \t 9.952123 \t9.283717 \t 0.166945 \t 25.992367 s\n","44 \t 19.327098 \t 9.963034 \t9.085788 \t 0.278276 \t 26.497577 s\n","45 \t 19.621566 \t 9.727894 \t9.741039 \t 0.152633 \t 27.013506 s\n","46 \t 20.052145 \t 9.864863 \t9.802128 \t 0.385153 \t 27.524453 s\n","47 \t 19.547986 \t 9.899553 \t9.487062 \t 0.161371 \t 28.036539 s\n","48 \t 19.833616 \t 9.970798 \t9.728844 \t 0.133975 \t 28.541967 s\n","49 \t 20.130242 \t 10.046215 \t9.711509 \t 0.372520 \t 29.058758 s\n","50 \t 19.724745 \t 9.921158 \t9.616981 \t 0.186606 \t 29.574344 s\n","51 \t 19.748549 \t 10.005009 \t9.667117 \t 0.076424 \t 30.238123 s\n","52 \t 19.520414 \t 9.762732 \t9.643397 \t 0.114285 \t 30.754466 s\n","53 \t 19.628714 \t 9.853694 \t9.613525 \t 0.161494 \t 31.401448 s\n","54 \t 19.306536 \t 9.855886 \t9.374335 \t 0.076315 \t 32.018827 s\n","55 \t 19.554627 \t 9.946044 \t9.392934 \t 0.215649 \t 32.652323 s\n","56 \t 19.795489 \t 9.983313 \t9.675158 \t 0.137018 \t 33.296712 s\n","57 \t 20.251695 \t 10.005307 \t9.987074 \t 0.259312 \t 33.953112 s\n","58 \t 19.294603 \t 9.842519 \t9.380057 \t 0.072027 \t 34.479722 s\n","59 \t 18.735639 \t 9.846225 \t8.831614 \t 0.057800 \t 34.984364 s\n","60 \t 19.326511 \t 9.913547 \t9.326779 \t 0.086187 \t 35.496143 s\n","61 \t 20.228659 \t 9.993456 \t9.929167 \t 0.306036 \t 36.003146 s\n","62 \t 19.234703 \t 9.685432 \t9.433475 \t 0.115795 \t 36.671231 s\n","63 \t 20.263594 \t 9.922735 \t10.092229 \t 0.248630 \t 37.175199 s\n","64 \t 20.811831 \t 9.730667 \t9.672112 \t 1.409051 \t 37.689229 s\n","65 \t 19.809088 \t 9.900979 \t9.673582 \t 0.234527 \t 38.183723 s\n","66 \t 19.397486 \t 9.679166 \t9.546605 \t 0.171716 \t 38.692708 s\n","67 \t 19.977161 \t 10.246027 \t9.580098 \t 0.151036 \t 39.190727 s\n","68 \t 19.743396 \t 9.860859 \t9.789475 \t 0.093061 \t 39.694076 s\n","69 \t 19.392729 \t 9.930566 \t9.117065 \t 0.345098 \t 40.197205 s\n","70 \t 19.742681 \t 9.964391 \t9.562189 \t 0.216100 \t 40.701788 s\n","71 \t 19.597788 \t 9.900329 \t9.521660 \t 0.175799 \t 41.197987 s\n","72 \t 19.483055 \t 9.910099 \t9.185518 \t 0.387438 \t 41.705058 s\n","73 \t 19.693587 \t 9.964880 \t9.357633 \t 0.371076 \t 42.202421 s\n","74 \t 19.450549 \t 9.915337 \t9.359299 \t 0.175914 \t 42.877140 s\n","75 \t 19.448149 \t 9.681231 \t9.677880 \t 0.089037 \t 43.377725 s\n","76 \t 19.203897 \t 9.679696 \t9.328152 \t 0.196050 \t 43.893091 s\n","77 \t 19.283229 \t 9.864606 \t9.251815 \t 0.166808 \t 44.498228 s\n","78 \t 19.488136 \t 10.108488 \t9.266975 \t 0.112673 \t 45.126313 s\n","79 \t 19.096987 \t 10.037232 \t8.813073 \t 0.246683 \t 45.771492 s\n","80 \t 19.378081 \t 9.583447 \t9.706072 \t 0.088563 \t 46.376799 s\n","81 \t 19.937942 \t 10.123690 \t9.733966 \t 0.080286 \t 47.059040 s\n","82 \t 19.215644 \t 9.603389 \t9.539407 \t 0.072849 \t 47.634436 s\n","83 \t 19.903522 \t 9.591628 \t9.934803 \t 0.377090 \t 48.151677 s\n","84 \t 19.070135 \t 9.735320 \t9.178900 \t 0.155915 \t 48.653685 s\n","85 \t 19.759184 \t 9.965501 \t9.679876 \t 0.113807 \t 49.168057 s\n","86 \t 19.112177 \t 9.483861 \t9.550494 \t 0.077822 \t 49.663560 s\n","87 \t 19.368508 \t 9.792264 \t9.430951 \t 0.145294 \t 50.182858 s\n","88 \t 18.862087 \t 9.475224 \t9.237544 \t 0.149319 \t 50.830992 s\n","89 \t 19.109917 \t 9.921633 \t9.074878 \t 0.113406 \t 51.338329 s\n","90 \t 19.182781 \t 9.406168 \t9.651869 \t 0.124744 \t 51.835631 s\n","91 \t 19.369340 \t 9.783054 \t9.507787 \t 0.078500 \t 52.344623 s\n","92 \t 19.602577 \t 9.694143 \t9.686540 \t 0.221895 \t 52.843213 s\n","93 \t 19.986962 \t 10.084819 \t9.805541 \t 0.096603 \t 53.348337 s\n","94 \t 18.765305 \t 9.164470 \t9.482114 \t 0.118722 \t 53.854711 s\n","95 \t 18.825811 \t 9.556073 \t9.166935 \t 0.102803 \t 54.360501 s\n","96 \t 18.640202 \t 9.236736 \t9.288474 \t 0.114991 \t 54.858639 s\n","97 \t 19.280218 \t 9.800828 \t9.399529 \t 0.079862 \t 55.516072 s\n","98 \t 19.278260 \t 9.765410 \t9.402433 \t 0.110416 \t 56.007499 s\n","99 \t 18.799964 \t 9.392607 \t9.290929 \t 0.116428 \t 56.529856 s\n","100 \t 19.988556 \t 9.954647 \t9.943685 \t 0.090224 \t 57.021356 s\n","101 \t 19.280676 \t 9.438503 \t9.712750 \t 0.129422 \t 57.593636 s\n","102 \t 19.817280 \t 9.669456 \t10.029024 \t 0.118799 \t 58.218413 s\n","103 \t 19.142184 \t 9.821906 \t9.254184 \t 0.066095 \t 58.847446 s\n","104 \t 19.605371 \t 9.940784 \t9.602931 \t 0.061656 \t 59.440679 s\n","105 \t 19.725213 \t 10.051898 \t9.615613 \t 0.057702 \t 60.053345 s\n","106 \t 19.139503 \t 9.619031 \t9.254631 \t 0.265840 \t 60.689111 s\n","107 \t 19.589494 \t 10.011570 \t9.514408 \t 0.063516 \t 61.325917 s\n","108 \t 19.649812 \t 9.842142 \t9.706453 \t 0.101218 \t 61.825625 s\n","109 \t 19.477330 \t 9.866776 \t9.527917 \t 0.082637 \t 62.323798 s\n","110 \t 19.023126 \t 9.802287 \t9.157076 \t 0.063763 \t 62.840572 s\n","111 \t 19.443665 \t 9.887558 \t9.250105 \t 0.306001 \t 63.339992 s\n","112 \t 19.176012 \t 9.467761 \t9.453853 \t 0.254398 \t 63.867780 s\n","113 \t 19.431922 \t 9.900310 \t9.446894 \t 0.084719 \t 64.367354 s\n","114 \t 19.808977 \t 9.908019 \t9.808375 \t 0.092583 \t 64.877846 s\n","115 \t 18.643408 \t 9.405995 \t9.052016 \t 0.185397 \t 65.369534 s\n","116 \t 19.213985 \t 9.652823 \t9.502031 \t 0.059131 \t 65.879723 s\n","117 \t 19.219932 \t 9.610606 \t9.493272 \t 0.116054 \t 66.531116 s\n","118 \t 19.613621 \t 9.667403 \t9.861912 \t 0.084305 \t 67.022237 s\n","119 \t 19.078555 \t 9.567897 \t9.388113 \t 0.122546 \t 67.528600 s\n","120 \t 19.714366 \t 9.978372 \t9.673117 \t 0.062876 \t 68.021063 s\n","121 \t 19.095511 \t 9.490336 \t9.555227 \t 0.049949 \t 68.529790 s\n","122 \t 18.726512 \t 9.741720 \t8.902119 \t 0.082673 \t 69.038673 s\n","123 \t 18.350222 \t 9.431314 \t8.858448 \t 0.060459 \t 69.540967 s\n","124 \t 19.218864 \t 9.763299 \t9.191287 \t 0.264279 \t 70.055225 s\n","125 \t 18.923936 \t 9.399008 \t9.474470 \t 0.050458 \t 70.574579 s\n","126 \t 19.170883 \t 9.491627 \t9.599081 \t 0.080176 \t 71.197829 s\n","127 \t 19.032811 \t 9.661427 \t9.302597 \t 0.068787 \t 71.995292 s\n","128 \t 19.712351 \t 9.915211 \t9.573446 \t 0.223694 \t 72.579427 s\n","129 \t 19.508976 \t 9.902854 \t9.542031 \t 0.064091 \t 73.206851 s\n","130 \t 18.771935 \t 9.333670 \t9.379858 \t 0.058406 \t 73.831468 s\n","131 \t 18.837644 \t 9.723639 \t9.033609 \t 0.080395 \t 74.324353 s\n","132 \t 18.914742 \t 9.460801 \t9.332474 \t 0.121468 \t 74.826304 s\n","133 \t 18.842282 \t 9.760503 \t8.921746 \t 0.160032 \t 75.345539 s\n","134 \t 18.863304 \t 9.433303 \t9.325613 \t 0.104388 \t 75.837147 s\n","135 \t 19.316607 \t 9.731946 \t9.502429 \t 0.082232 \t 76.344299 s\n","136 \t 19.345277 \t 9.792534 \t9.447677 \t 0.105066 \t 76.992843 s\n","137 \t 19.550619 \t 9.988313 \t9.460537 \t 0.101768 \t 77.487018 s\n","138 \t 18.564142 \t 9.495576 \t8.975900 \t 0.092665 \t 77.994253 s\n","139 \t 19.836915 \t 9.825097 \t9.956823 \t 0.054994 \t 78.496964 s\n","140 \t 18.529094 \t 9.061873 \t9.376037 \t 0.091184 \t 78.992349 s\n","141 \t 19.207343 \t 9.497179 \t9.542306 \t 0.167859 \t 79.490180 s\n","142 \t 19.108974 \t 9.636260 \t9.376915 \t 0.095800 \t 79.992289 s\n","143 \t 18.843777 \t 9.486117 \t9.313647 \t 0.044012 \t 80.511366 s\n","144 \t 18.372726 \t 9.046358 \t9.281914 \t 0.044455 \t 81.020450 s\n","145 \t 19.089758 \t 9.710439 \t9.284476 \t 0.094843 \t 81.533170 s\n","146 \t 18.895551 \t 9.272725 \t9.559172 \t 0.063654 \t 82.027106 s\n","147 \t 18.590737 \t 9.710663 \t8.831016 \t 0.049059 \t 82.678984 s\n","148 \t 18.517469 \t 9.358912 \t9.086815 \t 0.071741 \t 83.187814 s\n","149 \t 19.103973 \t 9.475212 \t9.579705 \t 0.049057 \t 83.698721 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/130 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 130/130 [00:33<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3573\n","Hit@10: 0.4500\n","Hit@3: 0.3692\n","Hit@1: 0.3000\n","Link Prediction on Validation Set (All)\n","MRR: 0.2478\n","Hit@10: 0.3752\n","Hit@3: 0.2351\n","Hit@1: 0.1820\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2514\n","Hit@10: 0.3923\n","Hit@3: 0.2538\n","Hit@1: 0.1769\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3242\n","Hit@10: 0.4951\n","Hit@3: 0.3586\n","Hit@1: 0.2319\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2273\n","150 \t 18.719193 \t 8.959832 \t9.670099 \t 0.089262 \t 119.049794 s\n","151 \t 19.658322 \t 9.810086 \t9.451435 \t 0.396802 \t 119.548955 s\n","152 \t 18.725944 \t 9.054060 \t9.406934 \t 0.264951 \t 120.064134 s\n","153 \t 19.182289 \t 9.656690 \t9.336597 \t 0.189003 \t 120.557709 s\n","154 \t 19.562710 \t 9.964831 \t9.487418 \t 0.110461 \t 121.072034 s\n","155 \t 19.736861 \t 9.720761 \t9.187829 \t 0.828270 \t 121.564433 s\n","156 \t 18.558561 \t 9.214783 \t9.189983 \t 0.153794 \t 122.070408 s\n","157 \t 19.407533 \t 9.884238 \t9.343157 \t 0.180138 \t 122.572630 s\n","158 \t 19.388165 \t 9.466364 \t9.792004 \t 0.129798 \t 123.077323 s\n","159 \t 19.132972 \t 9.424659 \t9.407773 \t 0.300540 \t 123.780226 s\n","160 \t 19.842394 \t 9.569897 \t9.992590 \t 0.279907 \t 124.401933 s\n","161 \t 19.688802 \t 9.919965 \t9.626483 \t 0.142353 \t 125.099148 s\n","162 \t 19.270408 \t 10.099277 \t9.097890 \t 0.073242 \t 125.758167 s\n","163 \t 19.478136 \t 9.608624 \t9.687243 \t 0.182270 \t 126.501708 s\n","164 \t 18.809299 \t 9.157832 \t9.441456 \t 0.210010 \t 127.177328 s\n","165 \t 19.751458 \t 9.785434 \t9.628994 \t 0.337029 \t 127.702530 s\n","166 \t 19.930142 \t 9.733701 \t10.123280 \t 0.073161 \t 128.259254 s\n","167 \t 19.089799 \t 9.332959 \t9.410897 \t 0.345943 \t 128.835756 s\n","168 \t 19.756863 \t 9.872618 \t9.757776 \t 0.126468 \t 129.525399 s\n","169 \t 19.708296 \t 9.934716 \t9.721997 \t 0.051582 \t 130.064237 s\n","170 \t 19.151018 \t 9.673176 \t9.339817 \t 0.138024 \t 130.611274 s\n","171 \t 18.972835 \t 9.581475 \t9.316603 \t 0.074758 \t 131.129230 s\n","172 \t 19.428031 \t 9.569579 \t9.637174 \t 0.221278 \t 131.715692 s\n","173 \t 19.131798 \t 9.897582 \t9.100946 \t 0.133270 \t 132.238635 s\n","174 \t 19.542957 \t 9.578871 \t9.857958 \t 0.106128 \t 132.733745 s\n","175 \t 18.966224 \t 9.551896 \t9.321829 \t 0.092500 \t 133.239404 s\n","176 \t 19.938525 \t 9.939857 \t9.927019 \t 0.071648 \t 133.746836 s\n","177 \t 18.744185 \t 9.166542 \t9.406751 \t 0.170894 \t 134.256695 s\n","178 \t 19.528910 \t 9.761703 \t9.678133 \t 0.089073 \t 134.895074 s\n","179 \t 19.629086 \t 9.819148 \t9.727153 \t 0.082785 \t 135.387690 s\n","180 \t 18.474589 \t 9.435426 \t8.952644 \t 0.086520 \t 135.890574 s\n","181 \t 18.997616 \t 9.728301 \t9.204067 \t 0.065247 \t 136.405838 s\n","182 \t 19.277314 \t 9.949759 \t8.963671 \t 0.363884 \t 136.956638 s\n","183 \t 19.146425 \t 9.730361 \t9.338766 \t 0.077299 \t 137.552242 s\n","184 \t 18.944978 \t 9.734186 \t9.103515 \t 0.107276 \t 138.188407 s\n","185 \t 19.344736 \t 9.714044 \t9.545416 \t 0.085277 \t 138.788853 s\n","186 \t 19.575907 \t 10.109316 \t9.367486 \t 0.099103 \t 139.407431 s\n","187 \t 18.668938 \t 9.502957 \t9.049241 \t 0.116740 \t 140.042841 s\n","188 \t 19.350649 \t 9.772995 \t9.514113 \t 0.063541 \t 140.690063 s\n","189 \t 19.467998 \t 9.739592 \t9.665055 \t 0.063352 \t 141.186992 s\n","190 \t 18.430846 \t 9.689038 \t8.645684 \t 0.096125 \t 141.691808 s\n","191 \t 19.356772 \t 9.861742 \t9.387915 \t 0.107115 \t 142.193173 s\n","192 \t 19.440738 \t 9.929049 \t9.411037 \t 0.100649 \t 142.710652 s\n","193 \t 18.849036 \t 9.706120 \t9.054765 \t 0.088152 \t 143.207153 s\n","194 \t 18.424023 \t 9.136184 \t9.241900 \t 0.045938 \t 143.719283 s\n","195 \t 18.769292 \t 9.218534 \t9.441020 \t 0.109739 \t 144.230798 s\n","196 \t 18.748386 \t 9.494216 \t9.157133 \t 0.097037 \t 144.737416 s\n","197 \t 19.797998 \t 9.776325 \t9.730686 \t 0.290987 \t 145.231581 s\n","198 \t 18.447920 \t 9.234646 \t9.095832 \t 0.117440 \t 145.883100 s\n","199 \t 19.177185 \t 9.459080 \t9.665737 \t 0.052367 \t 146.388017 s\n","200 \t 19.368125 \t 9.545667 \t9.414519 \t 0.407938 \t 146.903388 s\n","201 \t 18.321631 \t 9.473681 \t8.777845 \t 0.070105 \t 147.397700 s\n","202 \t 18.794524 \t 9.757479 \t8.827074 \t 0.209971 \t 147.913863 s\n","203 \t 18.966417 \t 9.437142 \t9.378757 \t 0.150517 \t 148.423025 s\n","204 \t 19.093052 \t 9.707108 \t9.215119 \t 0.170823 \t 148.942054 s\n","205 \t 19.218638 \t 9.638343 \t9.459099 \t 0.121196 \t 149.438257 s\n","206 \t 19.140578 \t 9.144646 \t9.443992 \t 0.551941 \t 149.960502 s\n","207 \t 18.782582 \t 9.491664 \t9.135461 \t 0.155457 \t 150.581403 s\n","208 \t 18.345439 \t 9.439476 \t8.821579 \t 0.084384 \t 151.217532 s\n","209 \t 19.264236 \t 9.714447 \t9.455809 \t 0.093981 \t 151.999798 s\n","210 \t 19.066462 \t 9.807170 \t9.081693 \t 0.177598 \t 152.629325 s\n","211 \t 19.344315 \t 10.093980 \t9.028612 \t 0.221723 \t 153.253199 s\n","212 \t 19.365788 \t 9.757697 \t9.503799 \t 0.104291 \t 153.756559 s\n","213 \t 19.186179 \t 9.443864 \t9.649908 \t 0.092407 \t 154.269719 s\n","214 \t 19.174880 \t 9.240987 \t9.771198 \t 0.162695 \t 154.778118 s\n","215 \t 18.827705 \t 9.614445 \t9.074686 \t 0.138574 \t 155.282295 s\n","216 \t 19.394119 \t 9.583332 \t9.521708 \t 0.289080 \t 155.788344 s\n","217 \t 19.834295 \t 9.979798 \t9.768772 \t 0.085725 \t 156.293033 s\n","218 \t 19.410525 \t 9.899495 \t9.348170 \t 0.162861 \t 156.806539 s\n","219 \t 18.909160 \t 9.336255 \t9.485541 \t 0.087363 \t 157.334501 s\n","220 \t 18.334467 \t 8.980233 \t9.246561 \t 0.107674 \t 157.979319 s\n","221 \t 18.685877 \t 9.443567 \t9.176305 \t 0.066005 \t 158.475902 s\n","222 \t 19.311965 \t 9.654152 \t9.522880 \t 0.134933 \t 158.976014 s\n","223 \t 19.235506 \t 9.693389 \t9.429011 \t 0.113107 \t 159.484199 s\n","224 \t 18.780103 \t 9.543674 \t9.082051 \t 0.154378 \t 159.977654 s\n","225 \t 18.859900 \t 9.711590 \t9.054707 \t 0.093603 \t 160.489789 s\n","226 \t 18.965750 \t 9.729252 \t9.071252 \t 0.165246 \t 160.983279 s\n","227 \t 19.271109 \t 9.536829 \t9.628262 \t 0.106018 \t 161.488925 s\n","228 \t 19.302212 \t 9.861472 \t9.364877 \t 0.075862 \t 161.977837 s\n","229 \t 19.720386 \t 10.044630 \t9.620409 \t 0.055346 \t 162.485420 s\n","230 \t 19.190097 \t 9.467010 \t9.570280 \t 0.152807 \t 162.982235 s\n","231 \t 18.785623 \t 9.685827 \t9.000371 \t 0.099426 \t 163.572557 s\n","232 \t 18.689593 \t 9.501937 \t9.141063 \t 0.046592 \t 164.192721 s\n","233 \t 19.078460 \t 9.563797 \t9.433348 \t 0.081315 \t 164.979725 s\n","234 \t 19.598812 \t 10.105785 \t9.406562 \t 0.086465 \t 165.582049 s\n","235 \t 19.277606 \t 9.727463 \t9.391067 \t 0.159077 \t 166.243224 s\n","236 \t 18.536922 \t 9.364417 \t8.878491 \t 0.294013 \t 166.770283 s\n","237 \t 19.911211 \t 10.061716 \t9.790959 \t 0.058536 \t 167.276476 s\n","238 \t 19.198283 \t 9.472934 \t9.665815 \t 0.059535 \t 167.774587 s\n","239 \t 18.795374 \t 9.181630 \t9.487524 \t 0.126219 \t 168.272458 s\n","240 \t 18.984113 \t 9.540132 \t9.361561 \t 0.082420 \t 168.782684 s\n","241 \t 19.658133 \t 9.953888 \t9.540573 \t 0.163671 \t 169.272875 s\n","242 \t 19.348983 \t 9.474470 \t9.800947 \t 0.073565 \t 169.780209 s\n","243 \t 19.197363 \t 9.775991 \t9.364477 \t 0.056895 \t 170.274290 s\n","244 \t 19.334062 \t 9.587383 \t9.493199 \t 0.253480 \t 170.773102 s\n","245 \t 19.399522 \t 9.874774 \t9.416094 \t 0.108654 \t 171.279580 s\n","246 \t 18.899345 \t 9.703963 \t9.149304 \t 0.046077 \t 171.943269 s\n","247 \t 19.293377 \t 9.611173 \t9.615534 \t 0.066670 \t 172.435802 s\n","248 \t 18.795362 \t 9.690722 \t9.052671 \t 0.051970 \t 172.936579 s\n","249 \t 19.384983 \t 9.471940 \t9.827199 \t 0.085845 \t 173.429971 s\n","250 \t 19.166163 \t 10.048640 \t9.063011 \t 0.054511 \t 173.926345 s\n","251 \t 19.336886 \t 9.652865 \t9.620023 \t 0.063998 \t 174.424575 s\n","252 \t 19.069585 \t 9.413779 \t9.598917 \t 0.056888 \t 174.922888 s\n","253 \t 19.109095 \t 9.578786 \t9.419315 \t 0.110993 \t 175.423882 s\n","254 \t 18.909616 \t 9.603755 \t9.259380 \t 0.046481 \t 175.911964 s\n","255 \t 18.983331 \t 9.259302 \t9.681259 \t 0.042770 \t 176.472470 s\n","256 \t 19.108868 \t 9.267532 \t9.750383 \t 0.090953 \t 177.100815 s\n","257 \t 19.364773 \t 9.841022 \t9.471619 \t 0.052131 \t 177.717509 s\n","258 \t 19.687159 \t 9.882887 \t9.718131 \t 0.086141 \t 178.510778 s\n","259 \t 18.585909 \t 9.375627 \t9.148560 \t 0.061721 \t 179.132310 s\n","260 \t 18.891228 \t 9.268306 \t9.538314 \t 0.084607 \t 179.747601 s\n","261 \t 19.077191 \t 9.529359 \t9.468808 \t 0.079024 \t 180.252363 s\n","262 \t 19.470888 \t 9.441185 \t9.834774 \t 0.194929 \t 180.754609 s\n","263 \t 18.793174 \t 9.606525 \t9.082003 \t 0.104645 \t 181.274653 s\n","264 \t 18.907203 \t 9.464919 \t9.365439 \t 0.076844 \t 181.777353 s\n","265 \t 18.910490 \t 9.133778 \t9.732030 \t 0.044681 \t 182.287354 s\n","266 \t 19.207820 \t 9.835614 \t9.277298 \t 0.094908 \t 182.783350 s\n","267 \t 18.902658 \t 9.551346 \t9.276445 \t 0.074867 \t 183.283670 s\n","268 \t 19.163154 \t 9.576536 \t9.533197 \t 0.053420 \t 183.933542 s\n","269 \t 19.435827 \t 9.840395 \t9.523460 \t 0.071972 \t 184.442670 s\n","270 \t 19.188623 \t 9.682981 \t9.429132 \t 0.076511 \t 184.940653 s\n","271 \t 18.375957 \t 9.167111 \t9.135077 \t 0.073771 \t 185.439882 s\n","272 \t 18.134295 \t 8.915375 \t9.172101 \t 0.046820 \t 185.929482 s\n","273 \t 18.795533 \t 9.519876 \t9.214972 \t 0.060686 \t 186.431775 s\n","274 \t 19.421963 \t 9.913540 \t9.203311 \t 0.305112 \t 186.924053 s\n","275 \t 18.659858 \t 9.450374 \t9.153749 \t 0.055733 \t 187.436991 s\n","276 \t 18.886375 \t 9.470625 \t9.295414 \t 0.120337 \t 187.933978 s\n","277 \t 19.855857 \t 9.635678 \t10.099223 \t 0.120955 \t 188.418667 s\n","278 \t 19.199389 \t 9.671650 \t9.481343 \t 0.046396 \t 188.929555 s\n","279 \t 18.867673 \t 9.366339 \t9.286380 \t 0.214954 \t 189.591095 s\n","280 \t 18.702178 \t 9.203494 \t9.368091 \t 0.130594 \t 190.202148 s\n","281 \t 19.295053 \t 10.082456 \t9.030506 \t 0.182092 \t 190.820974 s\n","282 \t 18.542567 \t 9.707706 \t8.780851 \t 0.054011 \t 191.416039 s\n","283 \t 18.571102 \t 9.421589 \t8.976556 \t 0.172958 \t 192.022622 s\n","284 \t 19.136003 \t 9.686352 \t9.363133 \t 0.086519 \t 192.684870 s\n","285 \t 19.395041 \t 9.826165 \t9.379801 \t 0.189074 \t 193.201144 s\n","286 \t 19.545885 \t 9.845368 \t9.623724 \t 0.076794 \t 193.705261 s\n","287 \t 18.815814 \t 8.898821 \t9.847964 \t 0.069029 \t 194.224155 s\n","288 \t 19.218446 \t 9.850485 \t9.026661 \t 0.341300 \t 194.717224 s\n","289 \t 18.890867 \t 9.338675 \t9.490895 \t 0.061297 \t 195.230793 s\n","290 \t 19.383912 \t 9.629989 \t9.429257 \t 0.324667 \t 195.724400 s\n","291 \t 19.051892 \t 9.611156 \t9.397142 \t 0.043594 \t 196.387367 s\n","292 \t 19.577070 \t 9.723931 \t9.731767 \t 0.121372 \t 196.883293 s\n","293 \t 18.449908 \t 9.669515 \t8.699189 \t 0.081205 \t 197.387627 s\n","294 \t 18.651611 \t 9.315670 \t9.267392 \t 0.068550 \t 197.890185 s\n","295 \t 18.922977 \t 9.409328 \t9.457607 \t 0.056043 \t 198.386596 s\n","296 \t 18.148983 \t 9.076991 \t9.004626 \t 0.067366 \t 198.878128 s\n","297 \t 18.361736 \t 9.105703 \t9.028037 \t 0.227997 \t 199.392442 s\n","298 \t 18.884607 \t 10.038527 \t8.800919 \t 0.045161 \t 199.882247 s\n","299 \t 19.023147 \t 9.370869 \t9.557230 \t 0.095049 \t 200.402283 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3737\n","Hit@10: 0.4577\n","Hit@3: 0.3731\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2588\n","Hit@10: 0.3994\n","Hit@3: 0.2560\n","Hit@1: 0.1900\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2830\n","Hit@10: 0.5231\n","Hit@3: 0.3000\n","Hit@1: 0.1846\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3392\n","Hit@10: 0.5461\n","Hit@3: 0.3734\n","Hit@1: 0.2401\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2437\n","300 \t 19.228020 \t 9.596145 \t9.521949 \t 0.109925 \t 236.127489 s\n","301 \t 18.883700 \t 9.670322 \t9.124179 \t 0.089198 \t 236.642203 s\n","302 \t 19.170061 \t 9.806511 \t9.297320 \t 0.066230 \t 237.139581 s\n","303 \t 19.049712 \t 9.617690 \t9.366048 \t 0.065974 \t 237.645416 s\n","304 \t 19.185981 \t 9.649585 \t9.491293 \t 0.045102 \t 238.292859 s\n","305 \t 18.828287 \t 9.385201 \t9.328310 \t 0.114776 \t 238.798278 s\n","306 \t 18.804873 \t 9.327019 \t9.436802 \t 0.041052 \t 239.295978 s\n","307 \t 19.091137 \t 9.472093 \t9.556573 \t 0.062472 \t 239.798979 s\n","308 \t 19.132509 \t 9.452821 \t9.592895 \t 0.086794 \t 240.286787 s\n","309 \t 19.084867 \t 9.629159 \t9.384279 \t 0.071430 \t 240.790196 s\n","310 \t 18.688589 \t 9.585356 \t9.035018 \t 0.068214 \t 241.290641 s\n","311 \t 19.482652 \t 9.826977 \t9.606128 \t 0.049548 \t 241.799894 s\n","312 \t 18.486949 \t 9.466020 \t8.955204 \t 0.065724 \t 242.298785 s\n","313 \t 18.863475 \t 9.427349 \t9.296864 \t 0.139264 \t 242.930475 s\n","314 \t 18.770565 \t 9.437109 \t9.252252 \t 0.081204 \t 243.575308 s\n","315 \t 18.770481 \t 9.327529 \t9.392649 \t 0.050304 \t 244.367239 s\n","316 \t 18.903579 \t 9.640271 \t9.191511 \t 0.071798 \t 245.089771 s\n","317 \t 18.369455 \t 9.073540 \t9.214835 \t 0.081081 \t 245.996575 s\n","318 \t 18.774718 \t 9.833957 \t8.895173 \t 0.045589 \t 246.517453 s\n","319 \t 19.490896 \t 9.626581 \t9.832919 \t 0.031395 \t 247.069309 s\n","320 \t 18.410193 \t 9.716079 \t8.618332 \t 0.075782 \t 247.633670 s\n","321 \t 18.575309 \t 9.550383 \t8.948776 \t 0.076150 \t 248.178331 s\n","322 \t 18.487906 \t 9.453907 \t8.974097 \t 0.059903 \t 248.722428 s\n","323 \t 18.510677 \t 9.548611 \t8.929149 \t 0.032917 \t 249.274136 s\n","324 \t 18.446016 \t 9.514573 \t8.889393 \t 0.042050 \t 249.823031 s\n","325 \t 18.726097 \t 9.615481 \t8.967060 \t 0.143556 \t 250.357665 s\n","326 \t 18.907427 \t 9.844551 \t8.742794 \t 0.320081 \t 250.900763 s\n","327 \t 18.865429 \t 9.526763 \t9.251076 \t 0.087590 \t 251.409097 s\n","328 \t 18.535232 \t 9.269462 \t9.216905 \t 0.048866 \t 251.902906 s\n","329 \t 18.697612 \t 9.362329 \t9.133138 \t 0.202145 \t 252.406568 s\n","330 \t 18.166481 \t 9.370148 \t8.737360 \t 0.058973 \t 253.051429 s\n","331 \t 18.384275 \t 9.112819 \t9.232662 \t 0.038795 \t 253.569125 s\n","332 \t 18.233074 \t 9.121483 \t8.951970 \t 0.159623 \t 254.070167 s\n","333 \t 18.468339 \t 9.261853 \t9.166857 \t 0.039629 \t 254.596896 s\n","334 \t 18.616333 \t 9.322788 \t9.230218 \t 0.063326 \t 255.098440 s\n","335 \t 19.058576 \t 9.769041 \t9.152808 \t 0.136727 \t 255.610058 s\n","336 \t 19.037809 \t 9.454531 \t9.527556 \t 0.055723 \t 256.206711 s\n","337 \t 18.718022 \t 9.380921 \t9.251226 \t 0.085875 \t 256.836721 s\n","338 \t 18.814740 \t 9.567188 \t9.189938 \t 0.057614 \t 257.471563 s\n","339 \t 18.967316 \t 9.495708 \t9.387036 \t 0.084572 \t 258.059301 s\n","340 \t 18.414729 \t 9.317823 \t9.028254 \t 0.068652 \t 258.736449 s\n","341 \t 18.420301 \t 9.363774 \t8.985981 \t 0.070546 \t 259.305432 s\n","342 \t 18.975819 \t 9.589157 \t9.240859 \t 0.145804 \t 259.811548 s\n","343 \t 18.786077 \t 9.048398 \t9.666520 \t 0.071159 \t 260.461617 s\n","344 \t 18.585155 \t 9.155212 \t9.367067 \t 0.062876 \t 260.977117 s\n","345 \t 18.658831 \t 9.393072 \t9.219038 \t 0.046721 \t 261.478040 s\n","346 \t 19.034465 \t 9.905458 \t9.053391 \t 0.075616 \t 261.985729 s\n","347 \t 18.367702 \t 9.234884 \t9.075172 \t 0.057646 \t 262.488611 s\n","348 \t 18.878934 \t 9.153377 \t9.487969 \t 0.237588 \t 262.991222 s\n","349 \t 19.061796 \t 9.661912 \t9.313960 \t 0.085924 \t 263.494831 s\n","350 \t 18.494142 \t 9.948008 \t8.435244 \t 0.110890 \t 264.035134 s\n","351 \t 19.181019 \t 9.883871 \t9.235539 \t 0.061610 \t 264.540678 s\n","352 \t 19.073853 \t 9.520408 \t9.509915 \t 0.043529 \t 265.189918 s\n","353 \t 18.288210 \t 8.899821 \t9.117477 \t 0.270912 \t 265.698422 s\n","354 \t 18.614805 \t 9.608296 \t8.955932 \t 0.050578 \t 266.218119 s\n","355 \t 18.710762 \t 9.756670 \t8.892049 \t 0.062042 \t 266.720440 s\n","356 \t 18.458616 \t 9.304152 \t9.070364 \t 0.084101 \t 267.228329 s\n","357 \t 18.334742 \t 9.093208 \t9.186831 \t 0.054703 \t 267.725790 s\n","358 \t 19.265203 \t 9.539144 \t9.443756 \t 0.282303 \t 268.247365 s\n","359 \t 18.548614 \t 9.239951 \t9.244612 \t 0.064050 \t 268.745685 s\n","360 \t 18.869112 \t 9.630182 \t9.064625 \t 0.174306 \t 269.315047 s\n","361 \t 17.827593 \t 8.783063 \t8.849217 \t 0.195312 \t 269.925970 s\n","362 \t 17.850992 \t 9.023345 \t8.783186 \t 0.044463 \t 270.721583 s\n","363 \t 18.673750 \t 9.314682 \t9.145401 \t 0.213667 \t 271.320207 s\n","364 \t 18.855851 \t 9.773698 \t8.937133 \t 0.145020 \t 271.986252 s\n","365 \t 18.539213 \t 9.223660 \t9.251721 \t 0.063832 \t 272.522767 s\n","366 \t 18.593899 \t 9.456267 \t9.086076 \t 0.051556 \t 273.018505 s\n","367 \t 19.005939 \t 9.607870 \t9.352305 \t 0.045766 \t 273.530487 s\n","368 \t 18.883335 \t 9.717455 \t9.068422 \t 0.097458 \t 274.031820 s\n","369 \t 18.969013 \t 9.378736 \t9.351196 \t 0.239082 \t 274.564734 s\n","370 \t 18.834305 \t 9.186414 \t9.511162 \t 0.136730 \t 275.059559 s\n","371 \t 18.293338 \t 9.114096 \t9.106898 \t 0.072344 \t 275.583540 s\n","372 \t 18.033197 \t 8.992186 \t8.980691 \t 0.060320 \t 276.098827 s\n","373 \t 18.679833 \t 9.374794 \t9.271236 \t 0.033803 \t 276.754998 s\n","374 \t 18.071359 \t 9.221779 \t8.739532 \t 0.110049 \t 277.254977 s\n","375 \t 19.120860 \t 9.383541 \t9.271467 \t 0.465852 \t 277.757687 s\n","376 \t 18.601973 \t 9.561377 \t8.983699 \t 0.056896 \t 278.254347 s\n","377 \t 18.564183 \t 9.514806 \t8.993183 \t 0.056195 \t 278.770748 s\n","378 \t 18.206225 \t 9.410417 \t8.764847 \t 0.030961 \t 279.260816 s\n","379 \t 18.945205 \t 9.471169 \t9.403812 \t 0.070222 \t 279.782169 s\n","380 \t 18.843734 \t 9.597773 \t9.121267 \t 0.124694 \t 280.276633 s\n","381 \t 17.558775 \t 8.962628 \t8.554867 \t 0.041280 \t 280.797269 s\n","382 \t 18.570947 \t 9.429436 \t9.024554 \t 0.116956 \t 281.292176 s\n","383 \t 17.979193 \t 9.050824 \t8.848471 \t 0.079897 \t 281.810281 s\n","384 \t 17.974245 \t 9.089970 \t8.816398 \t 0.067877 \t 282.534860 s\n","385 \t 18.184392 \t 9.215680 \t8.921971 \t 0.046741 \t 283.152305 s\n","386 \t 18.171250 \t 9.021062 \t9.072610 \t 0.077578 \t 283.768834 s\n","387 \t 18.976117 \t 9.652908 \t9.301277 \t 0.021931 \t 284.355114 s\n","388 \t 18.634759 \t 9.307892 \t9.240675 \t 0.086192 \t 284.982469 s\n","389 \t 18.418772 \t 9.471050 \t8.899437 \t 0.048284 \t 285.605083 s\n","390 \t 18.598879 \t 9.633856 \t8.906573 \t 0.058450 \t 286.107673 s\n","391 \t 18.334366 \t 9.214111 \t9.087051 \t 0.033203 \t 286.612969 s\n","392 \t 18.703503 \t 9.568320 \t9.024253 \t 0.110929 \t 287.121144 s\n","393 \t 18.665748 \t 9.763671 \t8.865671 \t 0.036406 \t 287.615623 s\n","394 \t 18.256874 \t 9.307092 \t8.878243 \t 0.071538 \t 288.133537 s\n","395 \t 19.011199 \t 9.805957 \t9.133185 \t 0.072057 \t 288.783659 s\n","396 \t 18.573524 \t 9.404819 \t9.031582 \t 0.137124 \t 289.281717 s\n","397 \t 19.150138 \t 9.311377 \t9.490126 \t 0.348635 \t 289.792312 s\n","398 \t 19.209364 \t 9.522810 \t9.406370 \t 0.280184 \t 290.287504 s\n","399 \t 18.495163 \t 9.258329 \t9.177233 \t 0.059601 \t 290.796906 s\n","400 \t 18.189835 \t 9.044055 \t9.001463 \t 0.144317 \t 291.297497 s\n","401 \t 18.187602 \t 8.767791 \t9.374111 \t 0.045700 \t 291.800982 s\n","402 \t 18.772122 \t 9.330419 \t9.380335 \t 0.061369 \t 292.306659 s\n","403 \t 18.538616 \t 9.305725 \t9.152407 \t 0.080485 \t 292.810919 s\n","404 \t 18.385413 \t 9.379086 \t8.952373 \t 0.053955 \t 293.325701 s\n","405 \t 18.460184 \t 9.262756 \t9.138681 \t 0.058746 \t 293.841946 s\n","406 \t 18.918273 \t 9.358803 \t9.005144 \t 0.554327 \t 294.370655 s\n","407 \t 18.711956 \t 9.477989 \t9.170607 \t 0.063360 \t 294.871781 s\n","408 \t 18.540154 \t 9.289449 \t8.972787 \t 0.277917 \t 295.596827 s\n","409 \t 18.762153 \t 9.482204 \t9.093588 \t 0.186360 \t 296.211022 s\n","410 \t 18.706612 \t 9.469783 \t9.202165 \t 0.034663 \t 296.853666 s\n","411 \t 18.251671 \t 9.270670 \t8.937817 \t 0.043184 \t 297.463837 s\n","412 \t 18.078745 \t 8.938314 \t9.099056 \t 0.041374 \t 298.063446 s\n","413 \t 18.866147 \t 9.448571 \t9.374093 \t 0.043484 \t 298.696651 s\n","414 \t 18.654307 \t 9.475376 \t8.681098 \t 0.497833 \t 299.201557 s\n","415 \t 18.675874 \t 9.594865 \t8.996780 \t 0.084228 \t 299.705056 s\n","416 \t 18.532755 \t 9.134789 \t9.344121 \t 0.053845 \t 300.208823 s\n","417 \t 18.800729 \t 9.199080 \t9.565637 \t 0.036011 \t 300.708632 s\n","418 \t 18.826733 \t 9.522710 \t9.206388 \t 0.097635 \t 301.209564 s\n","419 \t 18.358873 \t 9.343463 \t8.968084 \t 0.047326 \t 301.719618 s\n","420 \t 18.494425 \t 9.296256 \t8.910101 \t 0.288067 \t 302.221178 s\n","421 \t 18.694500 \t 9.410160 \t9.254523 \t 0.029817 \t 302.734214 s\n","422 \t 18.498974 \t 9.400103 \t9.036373 \t 0.062498 \t 303.385927 s\n","423 \t 18.663050 \t 9.384564 \t9.223584 \t 0.054901 \t 303.882984 s\n","424 \t 19.257134 \t 9.898664 \t9.307652 \t 0.050817 \t 304.386944 s\n","425 \t 18.556901 \t 9.390399 \t9.087387 \t 0.079114 \t 304.887080 s\n","426 \t 18.989214 \t 9.239739 \t9.668729 \t 0.080746 \t 305.390324 s\n","427 \t 18.853540 \t 9.661489 \t9.130745 \t 0.061306 \t 305.887702 s\n","428 \t 18.150181 \t 9.412088 \t8.683071 \t 0.055021 \t 306.399809 s\n","429 \t 18.772661 \t 9.366885 \t9.368932 \t 0.036843 \t 306.907435 s\n","430 \t 18.640412 \t 9.498539 \t9.073823 \t 0.068050 \t 307.398504 s\n","431 \t 18.652754 \t 9.761632 \t8.833698 \t 0.057424 \t 307.907506 s\n","432 \t 18.271338 \t 9.685994 \t8.503289 \t 0.082055 \t 308.403236 s\n","433 \t 18.033422 \t 9.237369 \t8.725287 \t 0.070766 \t 308.974729 s\n","434 \t 18.431596 \t 9.291584 \t9.096850 \t 0.043160 \t 309.614039 s\n","435 \t 19.292208 \t 9.736282 \t9.506459 \t 0.049466 \t 310.256967 s\n","436 \t 18.701181 \t 9.874930 \t8.768982 \t 0.057270 \t 311.030452 s\n","437 \t 18.839732 \t 9.729167 \t9.068994 \t 0.041572 \t 311.706637 s\n","438 \t 18.648866 \t 9.826999 \t8.777586 \t 0.044280 \t 312.259491 s\n","439 \t 18.469398 \t 9.153271 \t9.249576 \t 0.066551 \t 312.768968 s\n","440 \t 18.798210 \t 9.648410 \t9.092327 \t 0.057474 \t 313.271054 s\n","441 \t 19.318377 \t 9.629281 \t9.478938 \t 0.210158 \t 313.794690 s\n","442 \t 18.694464 \t 9.143735 \t9.344198 \t 0.206530 \t 314.296544 s\n","443 \t 18.237147 \t 9.373890 \t8.787684 \t 0.075573 \t 314.806863 s\n","444 \t 19.194715 \t 9.612315 \t9.531891 \t 0.050508 \t 315.301668 s\n","445 \t 18.562618 \t 9.065039 \t9.416183 \t 0.081397 \t 315.944404 s\n","446 \t 19.006239 \t 9.579275 \t9.387024 \t 0.039941 \t 316.451678 s\n","447 \t 18.354690 \t 9.361910 \t8.961584 \t 0.031196 \t 316.958508 s\n","448 \t 18.605977 \t 9.283664 \t9.275596 \t 0.046717 \t 317.456178 s\n","449 \t 18.170969 \t 9.299233 \t8.772819 \t 0.098917 \t 317.959646 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3718\n","Hit@10: 0.4615\n","Hit@3: 0.3923\n","Hit@1: 0.3115\n","Link Prediction on Validation Set (All)\n","MRR: 0.2601\n","Hit@10: 0.4171\n","Hit@3: 0.2593\n","Hit@1: 0.1852\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2895\n","Hit@10: 0.5692\n","Hit@3: 0.2923\n","Hit@1: 0.1846\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3730\n","Hit@10: 0.5888\n","Hit@3: 0.4194\n","Hit@1: 0.2648\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2273\n","450 \t 18.217760 \t 9.466102 \t8.717416 \t 0.034242 \t 353.903711 s\n","451 \t 18.547051 \t 9.632504 \t8.832365 \t 0.082183 \t 354.409059 s\n","452 \t 18.521582 \t 9.286170 \t9.144433 \t 0.090978 \t 354.927616 s\n","453 \t 18.947520 \t 9.256805 \t9.467185 \t 0.223530 \t 355.427811 s\n","454 \t 18.674096 \t 9.609901 \t8.980495 \t 0.083699 \t 356.139233 s\n","455 \t 19.310012 \t 9.892562 \t9.358774 \t 0.058676 \t 356.649013 s\n","456 \t 18.221824 \t 9.499935 \t8.673482 \t 0.048406 \t 357.161817 s\n","457 \t 18.957129 \t 9.633052 \t9.288760 \t 0.035316 \t 357.674666 s\n","458 \t 17.914520 \t 9.339745 \t8.492947 \t 0.081828 \t 358.177596 s\n","459 \t 19.039548 \t 9.572494 \t9.374227 \t 0.092827 \t 358.698951 s\n","460 \t 19.186883 \t 9.696253 \t9.417469 \t 0.073161 \t 359.213825 s\n","461 \t 18.533447 \t 9.358915 \t9.084795 \t 0.089738 \t 359.728777 s\n","462 \t 18.712262 \t 9.446458 \t9.223275 \t 0.042530 \t 360.240070 s\n","463 \t 19.296300 \t 9.680202 \t9.244453 \t 0.371644 \t 360.859610 s\n","464 \t 18.746112 \t 9.441484 \t9.247312 \t 0.057316 \t 361.570361 s\n","465 \t 18.642567 \t 9.431421 \t9.137263 \t 0.073884 \t 362.239990 s\n","466 \t 18.831205 \t 9.423084 \t9.024475 \t 0.383646 \t 362.930609 s\n","467 \t 18.321138 \t 9.460415 \t8.777923 \t 0.082801 \t 363.595950 s\n","468 \t 19.015346 \t 9.342167 \t9.479707 \t 0.193472 \t 364.269499 s\n","469 \t 18.785030 \t 9.589562 \t8.936582 \t 0.258888 \t 365.012882 s\n","470 \t 18.747253 \t 9.562516 \t9.065702 \t 0.119036 \t 365.622993 s\n","471 \t 19.266538 \t 9.805577 \t9.421590 \t 0.039370 \t 366.187395 s\n","472 \t 18.524827 \t 9.153243 \t9.188002 \t 0.183582 \t 366.742333 s\n","473 \t 18.129550 \t 8.968205 \t9.090383 \t 0.070962 \t 367.315044 s\n","474 \t 18.131596 \t 8.980240 \t9.082757 \t 0.068599 \t 367.821172 s\n","475 \t 18.978539 \t 9.458947 \t9.339132 \t 0.180460 \t 368.472274 s\n","476 \t 18.745311 \t 9.566442 \t9.107586 \t 0.071283 \t 368.962202 s\n","477 \t 18.735662 \t 9.377276 \t9.183844 \t 0.174541 \t 369.478790 s\n","478 \t 18.943401 \t 9.538615 \t9.277708 \t 0.127080 \t 369.986038 s\n","479 \t 19.849840 \t 9.874669 \t9.416870 \t 0.558303 \t 370.497792 s\n","480 \t 18.284944 \t 9.078902 \t8.957981 \t 0.248060 \t 370.993003 s\n","481 \t 19.220346 \t 9.250987 \t9.667068 \t 0.302291 \t 371.499719 s\n","482 \t 18.692871 \t 9.383152 \t9.235148 \t 0.074571 \t 371.995127 s\n","483 \t 19.008059 \t 9.504763 \t9.108438 \t 0.394857 \t 372.506182 s\n","484 \t 19.546863 \t 9.600920 \t9.661685 \t 0.284260 \t 373.008493 s\n","485 \t 18.423468 \t 8.908722 \t9.281515 \t 0.233231 \t 373.519803 s\n","486 \t 19.019440 \t 9.624811 \t9.268712 \t 0.125917 \t 374.023826 s\n","487 \t 19.421179 \t 9.864372 \t9.469407 \t 0.087401 \t 374.701492 s\n","488 \t 18.576846 \t 9.401802 \t9.027811 \t 0.147234 \t 375.206283 s\n","489 \t 19.063859 \t 9.790639 \t9.135010 \t 0.138211 \t 375.858948 s\n","490 \t 19.397547 \t 9.669291 \t9.477959 \t 0.250297 \t 376.514402 s\n","491 \t 19.267232 \t 9.732721 \t9.495025 \t 0.039486 \t 377.123524 s\n","492 \t 18.432694 \t 9.133896 \t8.929212 \t 0.369586 \t 377.782302 s\n","493 \t 18.601271 \t 9.227350 \t9.242439 \t 0.131483 \t 378.443901 s\n","494 \t 18.744260 \t 9.157524 \t9.361248 \t 0.225487 \t 378.976091 s\n","495 \t 18.615237 \t 9.704475 \t8.532990 \t 0.377772 \t 379.490299 s\n","496 \t 18.785332 \t 9.589097 \t9.076428 \t 0.119806 \t 380.012193 s\n","497 \t 18.360827 \t 9.213589 \t9.068735 \t 0.078503 \t 380.520713 s\n","498 \t 18.898232 \t 9.524634 \t9.301398 \t 0.072199 \t 381.042002 s\n","499 \t 18.669403 \t 9.100417 \t9.421732 \t 0.147255 \t 381.546408 s\n","500 \t 19.256402 \t 9.825952 \t9.367999 \t 0.062451 \t 382.077126 s\n","501 \t 18.678805 \t 9.192241 \t9.415705 \t 0.070859 \t 382.753607 s\n","502 \t 18.926421 \t 9.696464 \t8.983274 \t 0.246683 \t 383.258344 s\n","503 \t 18.958539 \t 9.895922 \t8.906305 \t 0.156311 \t 383.789313 s\n","504 \t 18.545348 \t 8.884439 \t8.950350 \t 0.710559 \t 384.298841 s\n","505 \t 18.809368 \t 9.169861 \t9.569531 \t 0.069976 \t 384.825754 s\n","506 \t 19.336793 \t 9.702381 \t9.410536 \t 0.223875 \t 385.333328 s\n","507 \t 19.321824 \t 9.703930 \t9.459731 \t 0.158162 \t 385.871869 s\n","508 \t 18.596277 \t 9.743940 \t8.780276 \t 0.072062 \t 386.388946 s\n","509 \t 19.045485 \t 9.654526 \t9.305852 \t 0.085106 \t 386.896298 s\n","510 \t 18.613971 \t 9.857204 \t8.675632 \t 0.081135 \t 387.555234 s\n","511 \t 18.658127 \t 9.464479 \t8.904690 \t 0.288957 \t 388.080358 s\n","512 \t 18.543758 \t 9.668365 \t8.797871 \t 0.077523 \t 388.637564 s\n","513 \t 18.789561 \t 9.681099 \t9.059220 \t 0.049242 \t 389.249066 s\n","514 \t 18.582547 \t 9.469383 \t9.042171 \t 0.070993 \t 389.877083 s\n","515 \t 18.809865 \t 9.624875 \t9.119975 \t 0.065015 \t 390.486992 s\n","516 \t 18.414993 \t 9.438577 \t8.926174 \t 0.050242 \t 391.096823 s\n","517 \t 19.254712 \t 9.840593 \t9.340609 \t 0.073511 \t 391.753102 s\n","518 \t 18.699930 \t 9.491491 \t9.032201 \t 0.176239 \t 392.265954 s\n","519 \t 18.846318 \t 9.519613 \t9.262605 \t 0.064100 \t 392.776209 s\n","520 \t 18.683182 \t 9.488448 \t9.135399 \t 0.059334 \t 393.465974 s\n","521 \t 18.510576 \t 9.670791 \t8.767681 \t 0.072105 \t 393.966579 s\n","522 \t 18.821422 \t 9.465131 \t9.280407 \t 0.075884 \t 394.490737 s\n","523 \t 18.834456 \t 9.676669 \t9.094030 \t 0.063757 \t 394.989474 s\n","524 \t 19.386691 \t 9.760046 \t9.263988 \t 0.362658 \t 395.504576 s\n","525 \t 18.802505 \t 9.100562 \t9.660982 \t 0.040960 \t 396.009207 s\n","526 \t 18.124254 \t 9.689314 \t8.386160 \t 0.048780 \t 396.521398 s\n","527 \t 18.312809 \t 9.474294 \t8.771490 \t 0.067024 \t 397.020530 s\n","528 \t 18.510613 \t 9.273974 \t9.046721 \t 0.189918 \t 397.528837 s\n","529 \t 18.541874 \t 9.642669 \t8.852225 \t 0.046982 \t 398.030697 s\n","530 \t 19.062096 \t 9.277731 \t9.737807 \t 0.046556 \t 398.548338 s\n","531 \t 19.300715 \t 9.958818 \t9.250144 \t 0.091754 \t 399.211367 s\n","532 \t 17.852782 \t 9.152301 \t8.569829 \t 0.130652 \t 399.725913 s\n","533 \t 18.894912 \t 9.666841 \t9.146311 \t 0.081761 \t 400.234012 s\n","534 \t 18.483603 \t 9.777225 \t8.668904 \t 0.037475 \t 400.750161 s\n","535 \t 18.779875 \t 9.310007 \t9.346819 \t 0.123050 \t 401.253538 s\n","536 \t 19.051560 \t 9.695215 \t9.289456 \t 0.066889 \t 401.806372 s\n","537 \t 19.544935 \t 9.790696 \t9.624651 \t 0.129587 \t 402.411313 s\n","538 \t 19.352621 \t 9.570325 \t9.589726 \t 0.192569 \t 403.050830 s\n","539 \t 18.790092 \t 9.363766 \t9.373580 \t 0.052745 \t 403.682020 s\n","540 \t 19.462690 \t 9.936456 \t9.443459 \t 0.082775 \t 404.292192 s\n","541 \t 19.516250 \t 9.994264 \t9.465529 \t 0.056456 \t 404.953384 s\n","542 \t 18.573441 \t 9.592915 \t8.666628 \t 0.313897 \t 405.605985 s\n","543 \t 19.022977 \t 9.802016 \t9.161405 \t 0.059556 \t 406.130421 s\n","544 \t 18.945595 \t 9.719818 \t9.152820 \t 0.072957 \t 406.643668 s\n","545 \t 18.262810 \t 9.395955 \t8.830194 \t 0.036662 \t 407.169749 s\n","546 \t 18.910935 \t 9.392585 \t9.450744 \t 0.067606 \t 407.676709 s\n","547 \t 18.969543 \t 9.537885 \t9.337355 \t 0.094304 \t 408.194625 s\n","548 \t 19.359596 \t 9.572726 \t9.698012 \t 0.088858 \t 408.691574 s\n","549 \t 18.366185 \t 9.527287 \t8.797470 \t 0.041427 \t 409.188921 s\n","550 \t 19.303664 \t 9.963567 \t9.229807 \t 0.110291 \t 409.693535 s\n","551 \t 18.932437 \t 9.603456 \t9.267750 \t 0.061231 \t 410.198288 s\n","552 \t 18.621243 \t 9.318964 \t9.217693 \t 0.084586 \t 410.704726 s\n","553 \t 18.939387 \t 9.663739 \t9.225767 \t 0.049881 \t 411.204931 s\n","554 \t 19.135523 \t 9.904950 \t9.100072 \t 0.130500 \t 411.694628 s\n","555 \t 18.603898 \t 9.372170 \t9.169228 \t 0.062501 \t 412.340549 s\n","556 \t 18.573850 \t 9.245302 \t9.259619 \t 0.068928 \t 412.840190 s\n","557 \t 18.529283 \t 9.174934 \t9.224006 \t 0.130343 \t 413.351886 s\n","558 \t 18.699408 \t 9.409623 \t9.220822 \t 0.068962 \t 413.853855 s\n","559 \t 18.529476 \t 9.179280 \t9.240775 \t 0.109421 \t 414.357830 s\n","560 \t 18.772674 \t 9.817732 \t8.874866 \t 0.080075 \t 414.862183 s\n","561 \t 18.818708 \t 9.397770 \t9.288788 \t 0.132150 \t 415.491091 s\n","562 \t 18.047102 \t 8.809230 \t9.173283 \t 0.064590 \t 416.124533 s\n","563 \t 17.824410 \t 9.191803 \t8.574251 \t 0.058355 \t 416.771055 s\n","564 \t 18.230058 \t 9.232051 \t8.870593 \t 0.127414 \t 417.393581 s\n","565 \t 18.757741 \t 9.489900 \t9.134484 \t 0.133357 \t 418.047299 s\n","566 \t 18.989681 \t 9.005456 \t9.423054 \t 0.561171 \t 418.659413 s\n","567 \t 19.052701 \t 9.635540 \t9.335444 \t 0.081718 \t 419.161196 s\n","568 \t 19.428751 \t 9.665356 \t9.508005 \t 0.255390 \t 419.824885 s\n","569 \t 18.846037 \t 9.530851 \t9.226310 \t 0.088875 \t 420.328512 s\n","570 \t 18.586017 \t 9.333153 \t9.204996 \t 0.047868 \t 420.842559 s\n","571 \t 18.967189 \t 9.364332 \t9.494449 \t 0.108408 \t 421.338446 s\n","572 \t 18.990682 \t 9.926084 \t8.914898 \t 0.149699 \t 421.866311 s\n","573 \t 18.384499 \t 9.289197 \t9.025318 \t 0.069985 \t 422.355004 s\n","574 \t 18.302672 \t 9.520530 \t8.730605 \t 0.051537 \t 422.883946 s\n","575 \t 19.177559 \t 9.360458 \t9.762769 \t 0.054331 \t 423.382460 s\n","576 \t 17.882810 \t 9.083636 \t8.752873 \t 0.046300 \t 423.907716 s\n","577 \t 19.001945 \t 9.711879 \t9.242589 \t 0.047477 \t 424.413594 s\n","578 \t 18.801064 \t 9.645175 \t9.056574 \t 0.099316 \t 424.944170 s\n","579 \t 18.483677 \t 9.691007 \t8.730196 \t 0.062473 \t 425.450910 s\n","580 \t 18.036639 \t 9.106853 \t8.878063 \t 0.051723 \t 425.962301 s\n","581 \t 18.374019 \t 9.700758 \t8.626258 \t 0.047004 \t 426.465855 s\n","582 \t 18.229115 \t 9.456537 \t8.670018 \t 0.102559 \t 427.113930 s\n","583 \t 18.901548 \t 9.503948 \t9.177696 \t 0.219903 \t 427.617205 s\n","584 \t 19.174221 \t 9.512197 \t9.493984 \t 0.168040 \t 428.113681 s\n","585 \t 19.143844 \t 9.900317 \t9.054431 \t 0.189096 \t 428.679942 s\n","586 \t 19.505560 \t 9.798800 \t9.440320 \t 0.266440 \t 429.308821 s\n","587 \t 18.921250 \t 9.557856 \t8.938289 \t 0.425106 \t 429.942402 s\n","588 \t 18.877716 \t 9.881228 \t8.944657 \t 0.051830 \t 430.523736 s\n","589 \t 18.789351 \t 9.456966 \t9.305479 \t 0.026906 \t 431.146133 s\n","590 \t 18.568873 \t 9.111408 \t9.414903 \t 0.042561 \t 431.791834 s\n","591 \t 18.947529 \t 9.489109 \t9.393854 \t 0.064566 \t 432.455263 s\n","592 \t 18.661823 \t 9.615786 \t8.954644 \t 0.091393 \t 432.959146 s\n","593 \t 18.976824 \t 9.426465 \t9.497459 \t 0.052901 \t 433.451699 s\n","594 \t 18.729873 \t 9.532315 \t9.152869 \t 0.044688 \t 433.954160 s\n","595 \t 18.888804 \t 9.526523 \t9.148240 \t 0.214042 \t 434.459260 s\n","596 \t 18.740177 \t 9.304502 \t9.236735 \t 0.198939 \t 434.970726 s\n","597 \t 18.520941 \t 9.349190 \t9.106952 \t 0.064799 \t 435.468994 s\n","598 \t 18.681687 \t 9.450135 \t9.051396 \t 0.180157 \t 435.968676 s\n","599 \t 18.315221 \t 9.325942 \t8.918163 \t 0.071117 \t 436.485935 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:34<00:00,  3.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3675\n","Hit@10: 0.4538\n","Hit@3: 0.3500\n","Hit@1: 0.3231\n","Link Prediction on Validation Set (All)\n","MRR: 0.2551\n","Hit@10: 0.3671\n","Hit@3: 0.2496\n","Hit@1: 0.1900\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3155\n","Hit@10: 0.5231\n","Hit@3: 0.3538\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3682\n","Hit@10: 0.5625\n","Hit@3: 0.4145\n","Hit@1: 0.2582\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2676\n","600 \t 19.186193 \t 9.563703 \t9.539282 \t 0.083209 \t 472.740137 s\n","601 \t 18.724577 \t 9.422798 \t8.982624 \t 0.319155 \t 473.250371 s\n","602 \t 18.883865 \t 9.552552 \t9.250455 \t 0.080859 \t 473.772144 s\n","603 \t 19.247406 \t 9.515641 \t9.329493 \t 0.402273 \t 474.276008 s\n","604 \t 19.180160 \t 9.569833 \t9.584562 \t 0.025764 \t 474.787812 s\n","605 \t 18.381977 \t 9.226312 \t9.090822 \t 0.064843 \t 475.293119 s\n","606 \t 18.945172 \t 9.823817 \t9.054817 \t 0.066538 \t 475.815067 s\n","607 \t 18.389501 \t 9.177319 \t9.160149 \t 0.052033 \t 476.326215 s\n","608 \t 18.321229 \t 9.369211 \t8.878531 \t 0.073487 \t 476.865586 s\n","609 \t 18.677135 \t 9.342344 \t9.295874 \t 0.038916 \t 477.365079 s\n","610 \t 18.776864 \t 9.416338 \t9.300003 \t 0.060523 \t 478.021884 s\n","611 \t 18.738348 \t 10.068862 \t8.632314 \t 0.037173 \t 478.581021 s\n","612 \t 19.092800 \t 9.882397 \t9.151714 \t 0.058689 \t 479.139784 s\n","613 \t 18.857580 \t 9.937805 \t8.797614 \t 0.122161 \t 479.662523 s\n","614 \t 18.543840 \t 9.533881 \t8.952331 \t 0.057629 \t 480.252707 s\n","615 \t 18.471881 \t 9.037414 \t9.305242 \t 0.129226 \t 480.814776 s\n","616 \t 18.818553 \t 9.844665 \t8.789770 \t 0.184120 \t 481.350042 s\n","617 \t 18.402094 \t 9.389171 \t8.883553 \t 0.129371 \t 481.986467 s\n","618 \t 18.825959 \t 9.495028 \t9.232684 \t 0.098247 \t 482.631563 s\n","619 \t 18.132530 \t 9.067499 \t8.903594 \t 0.161438 \t 483.326832 s\n","620 \t 18.532201 \t 9.432970 \t9.048201 \t 0.051030 \t 483.992094 s\n","621 \t 18.659835 \t 9.695375 \t8.914218 \t 0.050242 \t 485.004084 s\n","622 \t 18.509991 \t 9.116418 \t9.360860 \t 0.032713 \t 485.590827 s\n","623 \t 18.189464 \t 9.363440 \t8.734991 \t 0.091033 \t 486.143133 s\n","624 \t 18.216083 \t 9.667216 \t8.508254 \t 0.040613 \t 486.652358 s\n","625 \t 18.688034 \t 9.727465 \t8.903789 \t 0.056780 \t 487.162320 s\n","626 \t 18.093813 \t 9.197159 \t8.841816 \t 0.054838 \t 487.678328 s\n","627 \t 18.679554 \t 9.514747 \t9.119178 \t 0.045629 \t 488.186068 s\n","628 \t 18.461020 \t 9.634321 \t8.653378 \t 0.173322 \t 488.709808 s\n","629 \t 18.533830 \t 9.464906 \t9.043053 \t 0.025870 \t 489.214669 s\n","630 \t 18.552679 \t 9.484603 \t9.001223 \t 0.066854 \t 489.735505 s\n","631 \t 19.080562 \t 9.789590 \t9.228121 \t 0.062851 \t 490.235248 s\n","632 \t 18.168533 \t 9.428796 \t8.688626 \t 0.051112 \t 490.760019 s\n","633 \t 19.031361 \t 9.743064 \t9.190706 \t 0.097590 \t 491.441500 s\n","634 \t 19.172235 \t 9.570805 \t9.389637 \t 0.211793 \t 491.943932 s\n","635 \t 18.770977 \t 9.617187 \t9.082851 \t 0.070939 \t 492.458050 s\n","636 \t 18.576523 \t 9.474817 \t9.030201 \t 0.071504 \t 492.959363 s\n","637 \t 18.881770 \t 9.325281 \t9.526427 \t 0.030061 \t 493.477179 s\n","638 \t 18.398452 \t 9.703662 \t8.666090 \t 0.028699 \t 493.975831 s\n","639 \t 18.564124 \t 9.565185 \t8.931902 \t 0.067037 \t 494.500287 s\n","640 \t 18.243237 \t 9.193350 \t8.993502 \t 0.056384 \t 494.998602 s\n","641 \t 18.532383 \t 9.634971 \t8.844805 \t 0.052608 \t 495.614773 s\n","642 \t 18.599911 \t 9.434404 \t9.101093 \t 0.064413 \t 496.226179 s\n","643 \t 18.690936 \t 9.788668 \t8.850932 \t 0.051336 \t 496.859997 s\n","644 \t 18.048497 \t 9.248803 \t8.762846 \t 0.036849 \t 497.460067 s\n","645 \t 18.684167 \t 9.248594 \t9.360891 \t 0.074682 \t 498.139099 s\n","646 \t 18.762226 \t 9.371178 \t9.341101 \t 0.049947 \t 498.695513 s\n","647 \t 18.811908 \t 9.835764 \t8.934158 \t 0.041985 \t 499.331089 s\n","648 \t 17.911432 \t 9.387101 \t8.468858 \t 0.055473 \t 499.835603 s\n","649 \t 18.661076 \t 9.252665 \t9.256724 \t 0.151685 \t 500.351530 s\n","650 \t 18.317470 \t 9.333261 \t8.925317 \t 0.058892 \t 500.856629 s\n","651 \t 18.423979 \t 9.469443 \t8.902727 \t 0.051809 \t 501.355141 s\n","652 \t 18.358410 \t 9.745607 \t8.577372 \t 0.035431 \t 501.856924 s\n","653 \t 18.462560 \t 9.397505 \t8.997628 \t 0.067425 \t 502.354918 s\n","654 \t 18.533936 \t 9.673920 \t8.783764 \t 0.076252 \t 502.870091 s\n","655 \t 18.454843 \t 9.347040 \t9.058578 \t 0.049225 \t 503.365495 s\n","656 \t 19.041843 \t 9.690241 \t9.292746 \t 0.058857 \t 504.021914 s\n","657 \t 18.337013 \t 9.314523 \t8.872164 \t 0.150327 \t 504.535290 s\n","658 \t 18.574283 \t 9.488350 \t9.050798 \t 0.035135 \t 505.039529 s\n","659 \t 18.346495 \t 9.750157 \t8.523311 \t 0.073027 \t 505.542700 s\n","660 \t 18.249903 \t 9.225533 \t8.947557 \t 0.076812 \t 506.044153 s\n","661 \t 19.061428 \t 9.466094 \t9.288171 \t 0.307163 \t 506.539537 s\n","662 \t 19.244924 \t 9.705420 \t9.455382 \t 0.084122 \t 507.064513 s\n","663 \t 18.292024 \t 9.626432 \t8.616188 \t 0.049404 \t 507.566160 s\n","664 \t 19.232296 \t 9.750718 \t9.408512 \t 0.073066 \t 508.067706 s\n","665 \t 19.056632 \t 9.860997 \t9.065715 \t 0.129920 \t 508.631303 s\n","666 \t 18.421102 \t 9.536576 \t8.841706 \t 0.042820 \t 509.434514 s\n","667 \t 18.055022 \t 9.229225 \t8.759419 \t 0.066377 \t 510.052366 s\n","668 \t 19.278084 \t 9.673764 \t9.545652 \t 0.058668 \t 510.665202 s\n","669 \t 18.416020 \t 9.665865 \t8.680518 \t 0.069639 \t 511.361439 s\n","670 \t 18.776895 \t 9.405869 \t9.310920 \t 0.060105 \t 511.901605 s\n","671 \t 18.281207 \t 9.362738 \t8.798204 \t 0.120264 \t 512.404190 s\n","672 \t 19.096952 \t 9.320309 \t9.687047 \t 0.089597 \t 512.899831 s\n","673 \t 18.450773 \t 9.607622 \t8.779193 \t 0.063959 \t 513.407274 s\n","674 \t 18.916858 \t 9.276711 \t9.549574 \t 0.090573 \t 513.907974 s\n","675 \t 18.113911 \t 9.460618 \t8.627994 \t 0.025300 \t 514.433020 s\n","676 \t 18.087927 \t 9.305807 \t8.721764 \t 0.060356 \t 514.928994 s\n","677 \t 17.955003 \t 9.512364 \t8.396636 \t 0.046002 \t 515.566510 s\n","678 \t 17.840788 \t 9.312981 \t8.448240 \t 0.079568 \t 516.065787 s\n","679 \t 18.799985 \t 9.676168 \t9.077391 \t 0.046426 \t 516.568898 s\n","680 \t 18.615255 \t 9.385337 \t8.892661 \t 0.337257 \t 517.073005 s\n","681 \t 18.660560 \t 9.551086 \t8.944962 \t 0.164511 \t 517.571081 s\n","682 \t 18.416332 \t 9.324808 \t8.881912 \t 0.209612 \t 518.068067 s\n","683 \t 18.112006 \t 9.415222 \t8.506120 \t 0.190665 \t 518.569517 s\n","684 \t 18.987061 \t 9.576826 \t9.365745 \t 0.044490 \t 519.069839 s\n","685 \t 18.886918 \t 9.651864 \t9.172891 \t 0.062163 \t 519.575934 s\n","686 \t 18.277276 \t 9.273074 \t8.952794 \t 0.051408 \t 520.079820 s\n","687 \t 18.370628 \t 9.285523 \t9.043205 \t 0.041900 \t 520.595345 s\n","688 \t 18.942483 \t 9.947331 \t8.926311 \t 0.068841 \t 521.233816 s\n","689 \t 17.905367 \t 9.621104 \t8.219784 \t 0.064479 \t 521.778672 s\n","690 \t 17.962604 \t 9.445242 \t8.470318 \t 0.047043 \t 522.374117 s\n","691 \t 18.236076 \t 9.801890 \t8.396627 \t 0.037559 \t 523.018082 s\n","692 \t 18.219578 \t 9.417653 \t8.714199 \t 0.087727 \t 523.606572 s\n","693 \t 19.175635 \t 9.490476 \t9.605784 \t 0.079375 \t 524.270041 s\n","694 \t 19.103672 \t 9.928701 \t9.122550 \t 0.052422 \t 524.887852 s\n","695 \t 18.480475 \t 9.514621 \t8.789836 \t 0.176018 \t 525.386446 s\n","696 \t 19.645931 \t 10.104586 \t9.457943 \t 0.083401 \t 525.899575 s\n","697 \t 17.920725 \t 9.293502 \t8.574970 \t 0.052252 \t 526.402821 s\n","698 \t 18.607119 \t 9.390753 \t9.134666 \t 0.081698 \t 526.906457 s\n","699 \t 19.013633 \t 9.466188 \t9.502025 \t 0.045419 \t 527.397167 s\n","700 \t 18.892951 \t 9.371224 \t9.426041 \t 0.095686 \t 527.900104 s\n","701 \t 18.381789 \t 9.103187 \t9.225549 \t 0.053054 \t 528.540929 s\n","702 \t 17.989622 \t 8.793512 \t9.070752 \t 0.125357 \t 529.041417 s\n","703 \t 18.514188 \t 9.468461 \t8.996778 \t 0.048948 \t 529.540915 s\n","704 \t 18.365602 \t 9.306205 \t8.758471 \t 0.300928 \t 530.040198 s\n","705 \t 18.444744 \t 9.723315 \t8.646233 \t 0.075198 \t 530.538264 s\n","706 \t 18.810282 \t 9.667316 \t9.090070 \t 0.052895 \t 531.036212 s\n","707 \t 18.568095 \t 9.336535 \t9.191916 \t 0.039644 \t 531.539445 s\n","708 \t 18.610477 \t 9.160339 \t9.387908 \t 0.062231 \t 532.041351 s\n","709 \t 18.885044 \t 9.811705 \t8.854506 \t 0.218833 \t 532.537574 s\n","710 \t 18.438213 \t 9.488482 \t8.878572 \t 0.071159 \t 533.048712 s\n","711 \t 18.744453 \t 9.720652 \t8.978231 \t 0.045570 \t 533.547959 s\n","712 \t 18.278038 \t 9.091054 \t9.119485 \t 0.067499 \t 534.050837 s\n","713 \t 18.810931 \t 9.570554 \t9.199558 \t 0.040819 \t 534.561955 s\n","714 \t 18.605373 \t 9.537515 \t8.985051 \t 0.082809 \t 535.326339 s\n","715 \t 19.224211 \t 9.972437 \t9.216921 \t 0.034853 \t 535.930109 s\n","716 \t 18.276525 \t 9.353358 \t8.876845 \t 0.046321 \t 536.530325 s\n","717 \t 17.746129 \t 9.153707 \t8.525747 \t 0.066675 \t 537.127074 s\n","718 \t 17.892548 \t 9.119764 \t8.751417 \t 0.021367 \t 537.791146 s\n","719 \t 18.526378 \t 9.963458 \t8.504732 \t 0.058189 \t 538.354672 s\n","720 \t 19.220557 \t 9.543129 \t8.683751 \t 0.993677 \t 538.849973 s\n","721 \t 18.007681 \t 9.629227 \t8.332058 \t 0.046395 \t 539.352116 s\n","722 \t 18.557218 \t 9.690818 \t8.831964 \t 0.034435 \t 539.843602 s\n","723 \t 18.009652 \t 9.480193 \t8.361781 \t 0.167678 \t 540.343339 s\n","724 \t 18.533739 \t 9.399567 \t9.078526 \t 0.055646 \t 540.852717 s\n","725 \t 18.431753 \t 9.650509 \t8.581455 \t 0.199789 \t 541.361494 s\n","726 \t 18.224847 \t 9.436139 \t8.727529 \t 0.061179 \t 541.862709 s\n","727 \t 18.066214 \t 9.068680 \t8.941377 \t 0.056156 \t 542.509423 s\n","728 \t 18.937209 \t 9.816756 \t9.060466 \t 0.059987 \t 542.997689 s\n","729 \t 17.927867 \t 9.315613 \t8.561335 \t 0.050920 \t 543.514004 s\n","730 \t 17.958296 \t 9.534028 \t8.335057 \t 0.089212 \t 544.010993 s\n","731 \t 18.388171 \t 9.434573 \t8.896073 \t 0.057524 \t 544.542171 s\n","732 \t 18.647058 \t 9.900786 \t8.692730 \t 0.053543 \t 545.040667 s\n","733 \t 18.016375 \t 9.584708 \t8.383642 \t 0.048025 \t 545.557261 s\n","734 \t 18.124045 \t 9.251827 \t8.833707 \t 0.038511 \t 546.059030 s\n","735 \t 18.247802 \t 9.384559 \t8.817053 \t 0.046189 \t 546.565412 s\n","736 \t 18.450722 \t 9.696280 \t8.617805 \t 0.136636 \t 547.066999 s\n","737 \t 17.984951 \t 9.201919 \t8.729913 \t 0.053118 \t 547.578888 s\n","738 \t 18.878928 \t 9.566575 \t9.251841 \t 0.060512 \t 548.134397 s\n","739 \t 19.129821 \t 9.649164 \t9.459565 \t 0.021092 \t 548.749719 s\n","740 \t 18.620366 \t 9.656805 \t8.913036 \t 0.050524 \t 549.568403 s\n","741 \t 18.430130 \t 9.095136 \t9.162403 \t 0.172591 \t 550.186137 s\n","742 \t 18.865407 \t 9.555258 \t9.282876 \t 0.027273 \t 550.828905 s\n","743 \t 18.243589 \t 9.607332 \t8.608542 \t 0.027715 \t 551.433417 s\n","744 \t 17.897210 \t 9.072796 \t8.750269 \t 0.074145 \t 551.939960 s\n","745 \t 18.323401 \t 9.200495 \t9.048094 \t 0.074811 \t 552.432600 s\n","746 \t 18.663270 \t 9.376767 \t9.256934 \t 0.029568 \t 552.947021 s\n","747 \t 18.517199 \t 9.463159 \t8.957855 \t 0.096185 \t 553.453332 s\n","748 \t 18.788279 \t 9.629187 \t9.093138 \t 0.065954 \t 553.956837 s\n","749 \t 18.263185 \t 9.006143 \t9.001895 \t 0.255147 \t 554.467172 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:33<00:00,  3.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3649\n","Hit@10: 0.4731\n","Hit@3: 0.3846\n","Hit@1: 0.2962\n","Link Prediction on Validation Set (All)\n","MRR: 0.2506\n","Hit@10: 0.3833\n","Hit@3: 0.2528\n","Hit@1: 0.1787\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3408\n","Hit@10: 0.5538\n","Hit@3: 0.3538\n","Hit@1: 0.2385\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3824\n","Hit@10: 0.6036\n","Hit@3: 0.4079\n","Hit@1: 0.2812\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2438\n","750 \t 19.013442 \t 9.647407 \t9.284558 \t 0.081478 \t 589.579550 s\n","751 \t 17.873907 \t 8.916856 \t8.910672 \t 0.046378 \t 590.170979 s\n","752 \t 18.488403 \t 9.549500 \t8.837941 \t 0.100962 \t 590.835234 s\n","753 \t 18.039191 \t 9.232878 \t8.742461 \t 0.063852 \t 591.542033 s\n","754 \t 18.636224 \t 9.662199 \t8.903226 \t 0.070799 \t 592.037920 s\n","755 \t 18.592090 \t 9.590590 \t8.915599 \t 0.085899 \t 592.547940 s\n","756 \t 18.991920 \t 9.758020 \t9.174139 \t 0.059760 \t 593.042471 s\n","757 \t 18.526918 \t 9.622197 \t8.790946 \t 0.113776 \t 593.548589 s\n","758 \t 18.887484 \t 9.591558 \t9.250076 \t 0.045849 \t 594.050451 s\n","759 \t 18.987659 \t 9.500254 \t9.276807 \t 0.210596 \t 594.564525 s\n","760 \t 18.189603 \t 9.388117 \t8.738925 \t 0.062562 \t 595.075058 s\n","761 \t 18.234104 \t 9.554740 \t8.568788 \t 0.110576 \t 595.575598 s\n","762 \t 18.769698 \t 9.495308 \t9.226621 \t 0.047768 \t 596.088977 s\n","763 \t 17.743752 \t 9.296218 \t8.410087 \t 0.037448 \t 596.592370 s\n","764 \t 18.777944 \t 9.570188 \t9.034493 \t 0.173262 \t 597.108287 s\n","765 \t 18.068170 \t 9.357135 \t8.632732 \t 0.078302 \t 597.611701 s\n","766 \t 18.789968 \t 9.724575 \t9.031173 \t 0.034221 \t 598.278517 s\n","767 \t 18.254671 \t 9.434542 \t8.740372 \t 0.079757 \t 598.865943 s\n","768 \t 18.284390 \t 9.734324 \t8.516807 \t 0.033259 \t 599.414801 s\n","769 \t 19.045666 \t 9.796891 \t9.119945 \t 0.128830 \t 600.021845 s\n","770 \t 18.584502 \t 9.659050 \t8.800900 \t 0.124552 \t 600.553859 s\n","771 \t 18.762120 \t 9.277201 \t9.170593 \t 0.314326 \t 601.175711 s\n","772 \t 18.214264 \t 9.303919 \t8.859584 \t 0.050761 \t 601.877960 s\n","773 \t 19.227041 \t 9.942773 \t9.218458 \t 0.065810 \t 602.639228 s\n","774 \t 18.575930 \t 9.575197 \t8.953873 \t 0.046859 \t 603.325043 s\n","775 \t 18.676577 \t 9.349496 \t9.270074 \t 0.057007 \t 604.267752 s\n","776 \t 18.156479 \t 9.267809 \t8.647563 \t 0.241107 \t 604.812684 s\n","777 \t 19.229527 \t 9.868767 \t9.309257 \t 0.051505 \t 605.307668 s\n","778 \t 18.630016 \t 9.459727 \t9.082725 \t 0.087565 \t 605.804482 s\n","779 \t 18.173820 \t 9.495465 \t8.443700 \t 0.234656 \t 606.306548 s\n","780 \t 18.710592 \t 9.222828 \t9.440316 \t 0.047448 \t 606.823024 s\n","781 \t 18.432489 \t 9.214480 \t9.161619 \t 0.056389 \t 607.322793 s\n","782 \t 18.199859 \t 9.505118 \t8.555171 \t 0.139570 \t 607.824795 s\n","783 \t 18.391894 \t 9.814875 \t8.538670 \t 0.038348 \t 608.324151 s\n","784 \t 18.783927 \t 9.777676 \t8.909751 \t 0.096499 \t 608.821293 s\n","785 \t 18.102427 \t 9.012435 \t9.053157 \t 0.036836 \t 609.470562 s\n","786 \t 18.438684 \t 9.828657 \t8.527273 \t 0.082753 \t 609.960151 s\n","787 \t 18.679472 \t 9.571503 \t9.070353 \t 0.037616 \t 610.473130 s\n","788 \t 18.884508 \t 9.611084 \t9.173194 \t 0.100230 \t 610.964590 s\n","789 \t 18.085454 \t 9.138274 \t8.890867 \t 0.056314 \t 611.465495 s\n","790 \t 18.639556 \t 9.764414 \t8.832160 \t 0.042983 \t 611.964406 s\n","791 \t 18.014468 \t 9.258197 \t8.701964 \t 0.054307 \t 612.461686 s\n","792 \t 18.897229 \t 9.530816 \t9.219459 \t 0.146956 \t 612.957095 s\n","793 \t 17.872069 \t 9.478544 \t8.366986 \t 0.026540 \t 613.462551 s\n","794 \t 17.910306 \t 9.421436 \t8.434711 \t 0.054159 \t 613.963653 s\n","795 \t 18.952736 \t 9.709398 \t9.167316 \t 0.076021 \t 614.523210 s\n","796 \t 18.548645 \t 9.563508 \t8.896586 \t 0.088551 \t 615.115722 s\n","797 \t 17.965853 \t 8.998186 \t8.929897 \t 0.037770 \t 615.911895 s\n","798 \t 18.582989 \t 9.546159 \t8.982069 \t 0.054760 \t 616.525546 s\n","799 \t 17.873795 \t 9.183558 \t8.562600 \t 0.127637 \t 617.145098 s\n","800 \t 18.699458 \t 9.427356 \t9.217059 \t 0.055043 \t 617.766667 s\n","801 \t 17.552159 \t 9.129897 \t8.377656 \t 0.044606 \t 618.261452 s\n","802 \t 18.934879 \t 9.380336 \t9.374718 \t 0.179826 \t 618.762055 s\n","803 \t 18.457065 \t 9.584640 \t8.831261 \t 0.041164 \t 619.253517 s\n","804 \t 18.184640 \t 9.167075 \t8.966943 \t 0.050622 \t 619.764484 s\n","805 \t 18.174828 \t 9.541184 \t8.594693 \t 0.038952 \t 620.264634 s\n","806 \t 18.118747 \t 9.307326 \t8.757659 \t 0.053761 \t 620.781779 s\n","807 \t 18.765779 \t 9.443875 \t9.232095 \t 0.089808 \t 621.282352 s\n","808 \t 18.170927 \t 9.187153 \t8.943374 \t 0.040400 \t 621.780156 s\n","809 \t 18.950705 \t 9.521129 \t9.368059 \t 0.061518 \t 622.269425 s\n","810 \t 17.692881 \t 9.103532 \t8.548512 \t 0.040837 \t 622.916785 s\n","811 \t 18.498215 \t 9.479637 \t8.959225 \t 0.059352 \t 623.415555 s\n","812 \t 18.489800 \t 9.492485 \t8.957197 \t 0.040119 \t 623.918887 s\n","813 \t 18.036414 \t 9.288188 \t8.710423 \t 0.037802 \t 624.413966 s\n","814 \t 18.325831 \t 9.667585 \t8.490866 \t 0.167381 \t 624.915854 s\n","815 \t 18.520929 \t 9.184839 \t9.289374 \t 0.046715 \t 625.409956 s\n","816 \t 18.644049 \t 9.331098 \t9.282741 \t 0.030209 \t 625.917883 s\n","817 \t 18.517841 \t 9.311383 \t9.149899 \t 0.056559 \t 626.421547 s\n","818 \t 18.752116 \t 9.531617 \t8.949278 \t 0.271221 \t 626.923347 s\n","819 \t 18.496366 \t 9.441613 \t9.014722 \t 0.040031 \t 627.418018 s\n","820 \t 18.783937 \t 9.558232 \t9.175986 \t 0.049719 \t 627.997221 s\n","821 \t 18.261258 \t 9.578964 \t8.642549 \t 0.039746 \t 628.624587 s\n","822 \t 18.469635 \t 9.494021 \t8.743782 \t 0.231832 \t 629.307461 s\n","823 \t 18.171030 \t 9.067224 \t9.047482 \t 0.056325 \t 630.119385 s\n","824 \t 18.307964 \t 9.218150 \t9.037930 \t 0.051885 \t 630.771508 s\n","825 \t 18.799139 \t 9.584548 \t9.055964 \t 0.158627 \t 631.274450 s\n","826 \t 18.595829 \t 9.399309 \t9.148311 \t 0.048209 \t 631.769822 s\n","827 \t 18.319291 \t 9.504867 \t8.702273 \t 0.112151 \t 632.276679 s\n","828 \t 17.875589 \t 8.857115 \t8.808762 \t 0.209714 \t 632.777837 s\n","829 \t 18.203176 \t 9.680244 \t8.461614 \t 0.061318 \t 633.277639 s\n","830 \t 18.979850 \t 9.526624 \t9.254988 \t 0.198238 \t 633.790634 s\n","831 \t 18.339293 \t 9.570631 \t8.711034 \t 0.057627 \t 634.303071 s\n","832 \t 17.969400 \t 9.319511 \t8.558771 \t 0.091119 \t 634.946934 s\n","833 \t 18.732021 \t 9.452024 \t9.024875 \t 0.255122 \t 635.447294 s\n","834 \t 18.558330 \t 9.530301 \t8.974905 \t 0.053123 \t 635.945577 s\n","835 \t 18.637521 \t 9.266945 \t9.330883 \t 0.039691 \t 636.452447 s\n","836 \t 18.519383 \t 9.595394 \t8.901524 \t 0.022466 \t 636.946977 s\n","837 \t 18.488978 \t 9.477731 \t8.947894 \t 0.063353 \t 637.440182 s\n","838 \t 18.770635 \t 9.611812 \t9.101759 \t 0.057064 \t 637.934320 s\n","839 \t 17.583924 \t 8.737384 \t8.758480 \t 0.088060 \t 638.426116 s\n","840 \t 18.865375 \t 9.693629 \t9.136724 \t 0.035021 \t 638.923460 s\n","841 \t 17.849387 \t 9.002736 \t8.783358 \t 0.063294 \t 639.435699 s\n","842 \t 18.738744 \t 9.578058 \t9.116570 \t 0.044116 \t 640.081038 s\n","843 \t 18.157841 \t 9.287221 \t8.773201 \t 0.097419 \t 640.592969 s\n","844 \t 17.987031 \t 9.129336 \t8.744482 \t 0.113213 \t 641.138855 s\n","845 \t 18.328128 \t 9.148960 \t9.140687 \t 0.038481 \t 641.737817 s\n","846 \t 18.642601 \t 9.932852 \t8.657849 \t 0.051900 \t 642.380344 s\n","847 \t 17.842894 \t 9.507904 \t8.279499 \t 0.055490 \t 642.979897 s\n","848 \t 19.211221 \t 9.694357 \t9.285692 \t 0.231172 \t 643.600231 s\n","849 \t 18.072790 \t 9.452615 \t8.558365 \t 0.061810 \t 644.217534 s\n","850 \t 18.443119 \t 9.114058 \t9.288967 \t 0.040093 \t 644.713801 s\n","851 \t 18.431808 \t 9.472188 \t8.897634 \t 0.061987 \t 645.218910 s\n","852 \t 18.263961 \t 9.641774 \t8.506478 \t 0.115708 \t 645.871570 s\n","853 \t 18.683777 \t 9.626778 \t8.997215 \t 0.059784 \t 646.361937 s\n","854 \t 17.718304 \t 8.872990 \t8.797176 \t 0.048138 \t 646.860033 s\n","855 \t 18.577881 \t 9.667212 \t8.875546 \t 0.035124 \t 647.352702 s\n","856 \t 18.407995 \t 9.657045 \t8.690853 \t 0.060096 \t 647.851873 s\n","857 \t 17.838743 \t 9.179451 \t8.595049 \t 0.064243 \t 648.345725 s\n","858 \t 18.305116 \t 9.571344 \t8.673666 \t 0.060105 \t 648.854480 s\n","859 \t 18.975187 \t 9.847869 \t9.094827 \t 0.032491 \t 649.355957 s\n","860 \t 18.388613 \t 9.258007 \t9.081989 \t 0.048618 \t 649.859543 s\n","861 \t 18.715349 \t 9.634616 \t9.011767 \t 0.068968 \t 650.353894 s\n","862 \t 17.653826 \t 8.793838 \t8.826962 \t 0.033026 \t 650.995239 s\n","863 \t 18.229433 \t 9.103444 \t9.050025 \t 0.075964 \t 651.495821 s\n","864 \t 18.348441 \t 9.368234 \t8.924836 \t 0.055370 \t 651.992844 s\n","865 \t 19.209060 \t 10.021401 \t9.143578 \t 0.044080 \t 652.493002 s\n","866 \t 18.455315 \t 9.534800 \t8.876736 \t 0.043778 \t 652.988144 s\n","867 \t 18.023937 \t 9.147596 \t8.831316 \t 0.045025 \t 653.489936 s\n","868 \t 18.402801 \t 9.516258 \t8.775485 \t 0.111057 \t 653.991148 s\n","869 \t 18.221979 \t 9.218116 \t8.954105 \t 0.049758 \t 654.606611 s\n","870 \t 18.480106 \t 9.372678 \t9.025876 \t 0.081553 \t 655.228593 s\n","871 \t 18.394039 \t 9.617200 \t8.734836 \t 0.042002 \t 655.842523 s\n","872 \t 17.803323 \t 9.354327 \t8.429944 \t 0.019051 \t 656.463609 s\n","873 \t 18.413403 \t 9.657425 \t8.609650 \t 0.146328 \t 657.318829 s\n","874 \t 18.078326 \t 9.434588 \t8.576996 \t 0.066742 \t 657.816643 s\n","875 \t 18.786311 \t 9.684490 \t9.055110 \t 0.046711 \t 658.317236 s\n","876 \t 18.525971 \t 9.562922 \t8.901597 \t 0.061453 \t 658.819951 s\n","877 \t 17.904272 \t 9.470163 \t8.405734 \t 0.028374 \t 659.318658 s\n","878 \t 18.002243 \t 9.322606 \t8.635750 \t 0.043887 \t 659.820991 s\n","879 \t 17.630715 \t 9.169210 \t8.393866 \t 0.067640 \t 660.331092 s\n","880 \t 18.418730 \t 9.372386 \t9.010373 \t 0.035972 \t 660.833223 s\n","881 \t 18.493016 \t 9.704138 \t8.732378 \t 0.056500 \t 661.329781 s\n","882 \t 18.282213 \t 9.502755 \t8.753831 \t 0.025627 \t 661.829064 s\n","883 \t 17.999356 \t 9.194631 \t8.764680 \t 0.040046 \t 662.324365 s\n","884 \t 18.113738 \t 9.325018 \t8.628333 \t 0.160386 \t 662.963216 s\n","885 \t 18.491327 \t 9.843009 \t8.597601 \t 0.050717 \t 663.462886 s\n","886 \t 18.354945 \t 9.526120 \t8.796767 \t 0.032058 \t 663.958959 s\n","887 \t 18.187801 \t 9.289729 \t8.865614 \t 0.032457 \t 664.465834 s\n","888 \t 17.963170 \t 9.290214 \t8.591715 \t 0.081241 \t 664.959322 s\n","889 \t 19.342473 \t 9.533498 \t9.752000 \t 0.056975 \t 665.457340 s\n","890 \t 17.782926 \t 9.079325 \t8.642310 \t 0.061289 \t 665.950302 s\n","891 \t 18.542227 \t 9.765903 \t8.733313 \t 0.043012 \t 666.461508 s\n","892 \t 18.119820 \t 9.457913 \t8.629118 \t 0.032789 \t 666.959591 s\n","893 \t 17.764902 \t 9.162025 \t8.542490 \t 0.060387 \t 667.519178 s\n","894 \t 18.013309 \t 9.411530 \t8.542820 \t 0.058960 \t 668.105698 s\n","895 \t 17.906682 \t 9.395535 \t8.465315 \t 0.045832 \t 668.751950 s\n","896 \t 18.427418 \t 9.580761 \t8.829444 \t 0.017212 \t 669.339239 s\n","897 \t 18.589374 \t 9.595080 \t8.652075 \t 0.342217 \t 670.143064 s\n","898 \t 18.549438 \t 9.168165 \t9.318341 \t 0.062932 \t 670.765558 s\n","899 \t 18.524572 \t 9.643629 \t8.833720 \t 0.047223 \t 671.263577 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:33<00:00,  3.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3611\n","Hit@10: 0.4808\n","Hit@3: 0.3731\n","Hit@1: 0.2923\n","Link Prediction on Validation Set (All)\n","MRR: 0.2522\n","Hit@10: 0.3929\n","Hit@3: 0.2480\n","Hit@1: 0.1771\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3560\n","Hit@10: 0.5846\n","Hit@3: 0.3923\n","Hit@1: 0.2462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4095\n","Hit@10: 0.6250\n","Hit@3: 0.4556\n","Hit@1: 0.2961\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2402\n","900 \t 17.650500 \t 9.025431 \t8.598647 \t 0.026423 \t 705.885455 s\n","901 \t 18.558482 \t 9.597274 \t8.901073 \t 0.060135 \t 706.401187 s\n","902 \t 18.791925 \t 9.353449 \t9.380088 \t 0.058389 \t 706.918180 s\n","903 \t 18.189613 \t 9.633461 \t8.511989 \t 0.044163 \t 707.548122 s\n","904 \t 18.369925 \t 9.001728 \t9.321778 \t 0.046419 \t 708.165205 s\n","905 \t 18.738357 \t 9.586560 \t8.979769 \t 0.172029 \t 708.766003 s\n","906 \t 17.955893 \t 9.327909 \t8.583574 \t 0.044410 \t 709.382209 s\n","907 \t 18.147368 \t 9.393928 \t8.600919 \t 0.152522 \t 710.017189 s\n","908 \t 18.364797 \t 9.774053 \t8.539124 \t 0.051619 \t 710.670597 s\n","909 \t 18.341078 \t 9.611231 \t8.651340 \t 0.078506 \t 711.175318 s\n","910 \t 18.078506 \t 9.362444 \t8.674771 \t 0.041291 \t 711.661443 s\n","911 \t 18.689438 \t 9.968077 \t8.689012 \t 0.032350 \t 712.245946 s\n","912 \t 18.383814 \t 9.530858 \t8.828554 \t 0.024401 \t 712.792856 s\n","913 \t 17.634683 \t 9.042054 \t8.554777 \t 0.037851 \t 713.359629 s\n","914 \t 18.643539 \t 9.869987 \t8.715135 \t 0.058417 \t 713.951040 s\n","915 \t 17.963403 \t 9.448733 \t8.475997 \t 0.038672 \t 714.478405 s\n","916 \t 18.400128 \t 9.524238 \t8.757139 \t 0.118753 \t 715.022473 s\n","917 \t 18.259317 \t 9.237084 \t8.993240 \t 0.028994 \t 715.543915 s\n","918 \t 18.030956 \t 9.383770 \t8.323679 \t 0.323507 \t 716.105242 s\n","919 \t 18.600143 \t 9.219421 \t9.342706 \t 0.038015 \t 716.674624 s\n","920 \t 18.599063 \t 9.653736 \t8.911983 \t 0.033344 \t 717.204651 s\n","921 \t 18.364378 \t 9.504346 \t8.825885 \t 0.034148 \t 717.879095 s\n","922 \t 18.584507 \t 9.725383 \t8.803046 \t 0.056077 \t 718.371991 s\n","923 \t 18.785584 \t 9.569169 \t8.919315 \t 0.297101 \t 718.888290 s\n","924 \t 18.569869 \t 9.520444 \t9.004322 \t 0.045103 \t 719.391557 s\n","925 \t 18.125835 \t 9.190935 \t8.896421 \t 0.038479 \t 719.899440 s\n","926 \t 17.657302 \t 8.893023 \t8.694000 \t 0.070278 \t 720.481647 s\n","927 \t 18.255183 \t 9.601682 \t8.601075 \t 0.052426 \t 721.105008 s\n","928 \t 18.417664 \t 9.516164 \t8.833067 \t 0.068432 \t 721.743198 s\n","929 \t 18.068200 \t 9.637461 \t8.410552 \t 0.020187 \t 722.330117 s\n","930 \t 18.557369 \t 9.940277 \t8.480024 \t 0.137070 \t 722.994466 s\n","931 \t 18.461667 \t 9.437421 \t8.936559 \t 0.087688 \t 723.560624 s\n","932 \t 18.189865 \t 9.742020 \t8.414529 \t 0.033316 \t 724.081287 s\n","933 \t 18.708680 \t 9.747165 \t8.930116 \t 0.031399 \t 724.585972 s\n","934 \t 18.380185 \t 9.654919 \t8.685200 \t 0.040066 \t 725.249912 s\n","935 \t 18.113595 \t 9.193311 \t8.880365 \t 0.039917 \t 725.747666 s\n","936 \t 18.581563 \t 9.556848 \t8.962800 \t 0.061916 \t 726.267576 s\n","937 \t 18.793161 \t 9.498461 \t9.252388 \t 0.042314 \t 726.780303 s\n","938 \t 17.222706 \t 8.960222 \t8.170875 \t 0.091608 \t 727.296930 s\n","939 \t 17.926599 \t 9.367469 \t8.515148 \t 0.043980 \t 727.793681 s\n","940 \t 18.301969 \t 9.503271 \t8.752829 \t 0.045869 \t 728.315958 s\n","941 \t 18.627532 \t 9.539607 \t9.052337 \t 0.035588 \t 728.814667 s\n","942 \t 18.242296 \t 9.293293 \t8.730351 \t 0.218650 \t 729.326957 s\n","943 \t 17.971471 \t 9.340620 \t8.599663 \t 0.031187 \t 730.023180 s\n","944 \t 18.117685 \t 9.247319 \t8.841099 \t 0.029266 \t 730.521166 s\n","945 \t 18.420671 \t 9.706448 \t8.660360 \t 0.053862 \t 731.033422 s\n","946 \t 19.026723 \t 9.634820 \t8.748144 \t 0.643760 \t 731.535654 s\n","947 \t 18.721839 \t 9.706263 \t8.655258 \t 0.360318 \t 732.034048 s\n","948 \t 18.246367 \t 9.620591 \t8.582753 \t 0.043022 \t 732.533220 s\n","949 \t 18.590313 \t 9.562298 \t8.935148 \t 0.092868 \t 733.030332 s\n","950 \t 17.820706 \t 8.979347 \t8.800964 \t 0.040395 \t 733.588269 s\n","951 \t 18.177381 \t 9.331545 \t8.805439 \t 0.040397 \t 734.223341 s\n","952 \t 18.388905 \t 9.473295 \t8.840169 \t 0.075440 \t 734.857546 s\n","953 \t 18.374707 \t 9.282590 \t9.016695 \t 0.075422 \t 735.646390 s\n","954 \t 17.699817 \t 8.926768 \t8.642432 \t 0.130616 \t 736.316695 s\n","955 \t 18.315894 \t 9.601600 \t8.665552 \t 0.048742 \t 736.829453 s\n","956 \t 18.534213 \t 9.672215 \t8.837556 \t 0.024444 \t 737.340654 s\n","957 \t 17.439029 \t 8.882148 \t8.448376 \t 0.108505 \t 737.842916 s\n","958 \t 18.326497 \t 9.497334 \t8.759410 \t 0.069752 \t 738.348071 s\n","959 \t 18.410441 \t 9.506568 \t8.845903 \t 0.057970 \t 738.849257 s\n","960 \t 17.879896 \t 9.576831 \t8.264718 \t 0.038347 \t 739.350116 s\n","961 \t 18.547581 \t 9.833417 \t8.656191 \t 0.057973 \t 739.849452 s\n","962 \t 18.216460 \t 9.337735 \t8.638621 \t 0.240105 \t 740.351565 s\n","963 \t 18.225140 \t 9.463935 \t8.716657 \t 0.044548 \t 740.852037 s\n","964 \t 18.464713 \t 9.841568 \t8.565222 \t 0.057924 \t 741.495322 s\n","965 \t 18.191494 \t 9.543391 \t8.429078 \t 0.219024 \t 741.989712 s\n","966 \t 17.953746 \t 9.363424 \t8.521201 \t 0.069121 \t 742.510802 s\n","967 \t 18.139623 \t 9.446973 \t8.653205 \t 0.039445 \t 742.995518 s\n","968 \t 18.415787 \t 9.434155 \t8.933695 \t 0.047937 \t 743.513962 s\n","969 \t 18.376331 \t 9.467147 \t8.883406 \t 0.025777 \t 744.007238 s\n","970 \t 18.468799 \t 8.976347 \t9.265215 \t 0.227237 \t 744.536144 s\n","971 \t 18.637919 \t 9.639668 \t8.912457 \t 0.085794 \t 745.025210 s\n","972 \t 17.982783 \t 9.300462 \t8.640786 \t 0.041536 \t 745.533825 s\n","973 \t 18.134521 \t 9.133229 \t8.957114 \t 0.044176 \t 746.031189 s\n","974 \t 18.319576 \t 9.735636 \t8.515178 \t 0.068763 \t 746.592931 s\n","975 \t 18.021681 \t 9.477005 \t8.509051 \t 0.035624 \t 747.369491 s\n","976 \t 18.732136 \t 9.476236 \t9.217855 \t 0.038044 \t 747.996066 s\n","977 \t 18.913852 \t 9.717383 \t9.155416 \t 0.041053 \t 748.585930 s\n","978 \t 18.622906 \t 9.512630 \t8.955334 \t 0.154942 \t 749.255660 s\n","979 \t 18.706555 \t 9.368895 \t9.276187 \t 0.061474 \t 749.857539 s\n","980 \t 18.498509 \t 9.229722 \t9.224754 \t 0.044034 \t 750.341879 s\n","981 \t 18.501719 \t 9.700236 \t8.738406 \t 0.063077 \t 750.849540 s\n","982 \t 18.279201 \t 9.389411 \t8.846685 \t 0.043104 \t 751.349756 s\n","983 \t 18.221827 \t 9.419542 \t8.756958 \t 0.045327 \t 751.874269 s\n","984 \t 18.191532 \t 9.283644 \t8.838628 \t 0.069262 \t 752.366490 s\n","985 \t 18.317890 \t 9.481074 \t8.800716 \t 0.036099 \t 752.878702 s\n","986 \t 18.230820 \t 9.257213 \t8.931079 \t 0.042529 \t 753.377574 s\n","987 \t 18.651127 \t 9.725457 \t8.872855 \t 0.052816 \t 754.017669 s\n","988 \t 18.534225 \t 9.195712 \t9.261930 \t 0.076585 \t 754.544209 s\n","989 \t 18.211300 \t 9.341327 \t8.827161 \t 0.042812 \t 755.047611 s\n","990 \t 18.419902 \t 9.478367 \t8.903527 \t 0.038008 \t 755.552800 s\n","991 \t 18.764031 \t 9.439332 \t9.282733 \t 0.041967 \t 756.056309 s\n","992 \t 18.401025 \t 9.287058 \t9.065551 \t 0.048415 \t 756.569137 s\n","993 \t 17.350893 \t 9.158041 \t8.127005 \t 0.065845 \t 757.076157 s\n","994 \t 18.161639 \t 9.110919 \t9.016499 \t 0.034221 \t 757.575810 s\n","995 \t 17.924485 \t 9.509089 \t8.376858 \t 0.038539 \t 758.088520 s\n","996 \t 18.152766 \t 9.280807 \t8.819613 \t 0.052346 \t 758.591229 s\n","997 \t 18.525063 \t 9.422406 \t9.065049 \t 0.037608 \t 759.097312 s\n","998 \t 17.176696 \t 9.128854 \t7.991807 \t 0.056034 \t 759.649026 s\n","999 \t 18.703525 \t 9.792484 \t8.795923 \t 0.115117 \t 760.259065 s\n","1000 \t 18.084913 \t 9.400185 \t8.621065 \t 0.063664 \t 761.050263 s\n","1001 \t 18.063310 \t 9.185382 \t8.830850 \t 0.047078 \t 761.678943 s\n","1002 \t 18.076590 \t 9.288552 \t8.745468 \t 0.042570 \t 762.299988 s\n","1003 \t 17.944590 \t 9.451200 \t8.449017 \t 0.044373 \t 762.918029 s\n","1004 \t 17.756530 \t 9.282932 \t8.431915 \t 0.041684 \t 763.426499 s\n","1005 \t 17.677732 \t 8.791825 \t8.836749 \t 0.049159 \t 763.920284 s\n","1006 \t 18.320118 \t 9.494823 \t8.389781 \t 0.435512 \t 764.427306 s\n","1007 \t 18.593167 \t 9.414423 \t9.137768 \t 0.040977 \t 764.927796 s\n","1008 \t 18.309860 \t 9.347713 \t8.932451 \t 0.029696 \t 765.444176 s\n","1009 \t 18.163716 \t 9.352551 \t8.775828 \t 0.035338 \t 765.934329 s\n","1010 \t 18.567359 \t 9.353642 \t9.165592 \t 0.048125 \t 766.438185 s\n","1011 \t 18.431208 \t 9.678230 \t8.678878 \t 0.074099 \t 766.927990 s\n","1012 \t 18.115386 \t 9.470694 \t8.560082 \t 0.084609 \t 767.433552 s\n","1013 \t 18.295094 \t 9.162626 \t9.033113 \t 0.099356 \t 768.073464 s\n","1014 \t 18.212331 \t 9.482622 \t8.692300 \t 0.037409 \t 768.587696 s\n","1015 \t 17.833102 \t 9.412803 \t8.359803 \t 0.060497 \t 769.090038 s\n","1016 \t 18.375585 \t 9.369921 \t8.964364 \t 0.041300 \t 769.594516 s\n","1017 \t 17.990448 \t 9.328904 \t8.629165 \t 0.032380 \t 770.090051 s\n","1018 \t 19.125536 \t 9.825233 \t9.271767 \t 0.028537 \t 770.608995 s\n","1019 \t 18.166208 \t 9.317425 \t8.591231 \t 0.257553 \t 771.103128 s\n","1020 \t 18.687929 \t 9.676222 \t8.964104 \t 0.047603 \t 771.614319 s\n","1021 \t 18.925230 \t 9.593814 \t9.157204 \t 0.174212 \t 772.102569 s\n","1022 \t 18.180841 \t 9.765960 \t8.356291 \t 0.058590 \t 772.768464 s\n","1023 \t 18.200431 \t 9.226766 \t8.919684 \t 0.053981 \t 773.373528 s\n","1024 \t 18.195341 \t 9.480878 \t8.678263 \t 0.036200 \t 773.993174 s\n","1025 \t 18.737732 \t 9.546999 \t8.929234 \t 0.261498 \t 774.596971 s\n","1026 \t 17.133234 \t 9.057499 \t8.030905 \t 0.044830 \t 775.231169 s\n","1027 \t 17.479874 \t 9.119774 \t8.341927 \t 0.018173 \t 775.859061 s\n","1028 \t 18.536419 \t 9.419069 \t9.094614 \t 0.022737 \t 776.354549 s\n","1029 \t 17.846225 \t 9.363202 \t8.440992 \t 0.042031 \t 776.860812 s\n","1030 \t 18.184914 \t 9.348479 \t8.777266 \t 0.059168 \t 777.351955 s\n","1031 \t 18.527620 \t 9.454744 \t9.044525 \t 0.028353 \t 777.861120 s\n","1032 \t 18.282166 \t 9.266171 \t8.863478 \t 0.152519 \t 778.500832 s\n","1033 \t 18.449956 \t 9.525200 \t8.809347 \t 0.115410 \t 779.002681 s\n","1034 \t 18.885192 \t 9.579945 \t9.021542 \t 0.283705 \t 779.507353 s\n","1035 \t 19.228656 \t 9.609389 \t9.583226 \t 0.036041 \t 780.006503 s\n","1036 \t 18.346685 \t 9.242277 \t8.994566 \t 0.109842 \t 780.499454 s\n","1037 \t 18.576941 \t 9.191671 \t9.353334 \t 0.031937 \t 781.009601 s\n","1038 \t 18.386314 \t 9.544818 \t8.807541 \t 0.033954 \t 781.511266 s\n","1039 \t 18.402131 \t 9.423093 \t8.932127 \t 0.046910 \t 782.019299 s\n","1040 \t 18.643979 \t 9.568158 \t9.014899 \t 0.060921 \t 782.559269 s\n","1041 \t 18.810525 \t 9.723047 \t9.009028 \t 0.078449 \t 783.063708 s\n","1042 \t 17.831586 \t 9.230074 \t8.543888 \t 0.057624 \t 783.719861 s\n","1043 \t 17.917313 \t 9.388245 \t8.489131 \t 0.039937 \t 784.224244 s\n","1044 \t 17.568958 \t 8.959634 \t8.583745 \t 0.025579 \t 784.722128 s\n","1045 \t 18.846175 \t 9.508115 \t9.296303 \t 0.041757 \t 785.225003 s\n","1046 \t 18.941667 \t 9.713986 \t9.092535 \t 0.135144 \t 785.719100 s\n","1047 \t 18.545201 \t 9.414292 \t9.086152 \t 0.044757 \t 786.367601 s\n","1048 \t 18.213284 \t 9.125844 \t9.032956 \t 0.054486 \t 786.963695 s\n","1049 \t 17.946367 \t 9.414030 \t8.479424 \t 0.052913 \t 787.575310 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:33<00:00,  3.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3709\n","Hit@10: 0.4962\n","Hit@3: 0.3962\n","Hit@1: 0.3038\n","Link Prediction on Validation Set (All)\n","MRR: 0.2567\n","Hit@10: 0.3994\n","Hit@3: 0.2576\n","Hit@1: 0.1820\n","[DEBUG] Total RP (Tri) samples collected: 130\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3560\n","Hit@10: 0.5923\n","Hit@3: 0.4154\n","Hit@1: 0.2462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3995\n","Hit@10: 0.6250\n","Hit@3: 0.4589\n","Hit@1: 0.2796\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2276\n"]}]},{"cell_type":"markdown","source":["# Test.py"],"metadata":{"id":"p0M6asfiPDtD"}},{"cell_type":"code","source":["model_path = f\"./checkpoint/{file_format}_{test_epoch}.ckpt\"\n","\n","def load_id_mapping(file_path):\n","    id2name = {}\n","    with open(drive_dir + dataset_dir + file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip() == \"\" or line.startswith(\"#\"):  # 주석 또는 공백 무시\n","                continue\n","            parts = line.strip().split('\\t')\n","            if len(parts) != 2:\n","                continue\n","            name, idx = parts\n","            id2name[int(idx)] = name\n","    return id2name\n","\n","id2ent = load_id_mapping(\"entity2id.txt\")\n","id2rel = load_id_mapping(\"relation2id.txt\")\n","\n","def convert_triplet_ids_to_names(triplet, id2ent, id2rel, num_ent, num_rel):\n","    triplet_named = []\n","    for idx, val in enumerate(triplet):\n","        if idx % 2 == 0:  # entity or numeric value\n","            if val < num_ent:\n","                triplet_named.append(id2ent.get(val, f\"[ENT:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[NUM:{val - num_ent}]\")\n","        else:  # relation\n","            if val < num_rel:\n","                triplet_named.append(id2rel.get(val, f\"[REL:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[MASK_REL]\")\n","    return triplet_named\n","\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    entity_pred_log = []\n","    relation_pred_log = []\n","    numeric_pred_log = []\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    # sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    # nvp_tri_se += sq_error\n","                    # nvp_tri_se_num += 1\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    numeric_pred_log.append({\n","                      \"triplet_id\": str(tri),\n","                      \"triplet_named\": \":\".join(named_triplet),\n","                      \"position\": ent_idx,\n","                      \"type\": \"triplet\",\n","                      \"gt\": float(gt),\n","                      \"pred\": float(pred),\n","                      \"se\": float(sq_error)\n","                    })\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","\n","                else: # qualifier\n","                  pred = score_num[0, 2, tri[ent_idx*2] - KG.num_ent]\n","                  gt = tri_num[ent_idx - 1]\n","                  sq_error = (pred - gt) ** 2\n","                  named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                  numeric_pred_log.append({\n","                      \"triplet_id\": str(tri),\n","                      \"triplet_named\": \":\".join(named_triplet),\n","                      \"position\": ent_idx,\n","                      \"type\": \"qualifier\",\n","                      \"gt\": float(gt),\n","                      \"pred\": float(pred),\n","                      \"se\": float(sq_error)\n","                  })\n","                    # sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                    topk = np.argsort(-score_ent[0,1+2*ent_idx])[:5]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    try:\n","                      topk = np.argsort(-score_ent[0,2])[:5]\n","                    except:\n","                      topk = np.argsort(-score_ent[0,2])[:]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","                topk = np.argsort(-score_rel[0,2])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"relation\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                topk = np.argsort(-score_rel[0,1])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"qualifier\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","os.makedirs(f\"./visualization/{file_format}_{test_epoch}\", exist_ok=True)\n","pd.DataFrame(entity_pred_log).to_csv(f\"./visualization/{file_format}_{test_epoch}/entity_predictions.csv\", index=False)\n","pd.DataFrame(relation_pred_log).to_csv(f\"./visualization/{file_format}_{test_epoch}/relation_predictions.csv\", index=False)\n","pd.DataFrame(numeric_pred_log).to_csv(f\"./visualization/{file_format}_{test_epoch}/numeric_predictions.csv\", index=False)"],"metadata":{"id":"z1vrsHJ4J-DL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749797519488,"user_tz":-540,"elapsed":40521,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"645ffa81-c148-4df5-8117-dcd3a875cb81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 132/132 [00:36<00:00,  3.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3963\n","Hit@10: 0.5076\n","Hit@3: 0.4015\n","Hit@1: 0.3409\n","Link Prediction on Validation Set (All)\n","MRR: 0.2605\n","Hit@10: 0.3840\n","Hit@3: 0.2618\n","Hit@1: 0.1944\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2769\n","Hit@10: 0.5455\n","Hit@3: 0.3106\n","Hit@1: 0.1742\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3921\n","Hit@10: 0.6292\n","Hit@3: 0.4358\n","Hit@1: 0.2821\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2276\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}