{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","toc_visible":true,"machine_shape":"hm","mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyMY01swHrVvYs6ydAl6xN9P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749785758073,"user_tz":-540,"elapsed":11549,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"65d7e09a-0bb0-4391-ccc2-ae2721594626"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1749785762015,"user_tz":-540,"elapsed":3941,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1749785857079,"user_tz":-540,"elapsed":3,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0: # numeric triplet 처리\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","        else:\n","            self.proj_ent_vis = nn.Identity()\n","            self.proj_rel_vis = nn.Identity()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1749785833969,"user_tz":-540,"elapsed":26,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1749785799703,"user_tz":-540,"elapsed":34,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-O/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile(self.dir + 'visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(self.dir + 'visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-O/\"\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1749785786155,"user_tz":-540,"elapsed":1221,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"46679f92-d964-4e5d-f3a9-6193d98a3e01"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-O\n"," checkpoint\t\t    result\n"," entities.txt\t\t    test.txt\n"," entity2id.txt\t\t    textual_features_ent.pt\n"," entity2textlong.txt\t    textual_features_rel.pt\n"," entity2text.txt\t    train.txt\n"," entity_predictions.csv     triplets.txt.gdoc\n","'Model based on VTHNKG-O'   triplets_with_qualifiers_filtered.txt\n"," numeric_predictions.csv   'triplets_with_qualifiers(gpt-4o-mini sample).txt'\n"," relation2id.txt\t   'triplets_with_qualifiers(gpt4o sample).txt'\n"," relation2textlong.txt\t    triplets_with_qualifiers_raw.txt\n"," relation2text.txt\t    valid.txt\n"," relation_predictions.csv   visual_features_ent_sorted.pt\n"," relations.txt\t\t    visual_features_rel_sorted.pt\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-O_seed0\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749782808901,"user_tz":-540,"elapsed":558139,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"de3fa229-dcdc-4809-ecaa-d14db7f8addc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 23.405706 \t 11.335860 \t12.069847 \t 0.000000 \t 1.590422 s\n","1 \t 21.082318 \t 10.207479 \t10.874838 \t 0.000000 \t 1.940223 s\n","2 \t 21.084627 \t 10.327359 \t10.757268 \t 0.000000 \t 2.358384 s\n","3 \t 21.110727 \t 10.461330 \t10.649398 \t 0.000000 \t 2.767040 s\n","4 \t 20.851122 \t 10.367454 \t10.483668 \t 0.000000 \t 3.123178 s\n","5 \t 20.353591 \t 10.134416 \t10.219175 \t 0.000000 \t 3.466437 s\n","6 \t 19.928020 \t 9.798110 \t10.129910 \t 0.000000 \t 3.806269 s\n","7 \t 20.493776 \t 10.325273 \t10.168503 \t 0.000000 \t 4.166617 s\n","8 \t 20.068845 \t 9.726798 \t10.342047 \t 0.000000 \t 4.504529 s\n","9 \t 20.321253 \t 10.074993 \t10.246260 \t 0.000000 \t 4.848426 s\n","10 \t 20.093095 \t 9.802274 \t10.290821 \t 0.000000 \t 5.318156 s\n","11 \t 19.596725 \t 9.541965 \t10.054760 \t 0.000000 \t 5.667176 s\n","12 \t 19.869281 \t 9.892040 \t9.977241 \t 0.000000 \t 6.009386 s\n","13 \t 19.589150 \t 9.214624 \t10.374526 \t 0.000000 \t 6.352415 s\n","14 \t 19.855475 \t 9.811874 \t10.043601 \t 0.000000 \t 6.705014 s\n","15 \t 19.881133 \t 9.663560 \t10.217574 \t 0.000000 \t 7.050829 s\n","16 \t 19.826864 \t 9.730139 \t10.096725 \t 0.000000 \t 7.393454 s\n","17 \t 19.935289 \t 9.777036 \t10.158253 \t 0.000000 \t 7.735984 s\n","18 \t 19.667317 \t 9.605472 \t10.061846 \t 0.000000 \t 8.077631 s\n","19 \t 19.888535 \t 9.747783 \t10.140753 \t 0.000000 \t 8.420238 s\n","20 \t 19.764479 \t 9.617407 \t10.147071 \t 0.000000 \t 8.758355 s\n","21 \t 19.538242 \t 9.553725 \t9.984517 \t 0.000000 \t 9.109622 s\n","22 \t 19.575134 \t 9.895415 \t9.679718 \t 0.000000 \t 9.573535 s\n","23 \t 19.582513 \t 9.503087 \t10.079426 \t 0.000000 \t 9.911600 s\n","24 \t 19.401719 \t 9.371298 \t10.030421 \t 0.000000 \t 10.290170 s\n","25 \t 20.281921 \t 10.016877 \t10.265043 \t 0.000000 \t 10.629622 s\n","26 \t 19.370058 \t 9.251440 \t10.118618 \t 0.000000 \t 10.969402 s\n","27 \t 19.143352 \t 9.391610 \t9.751742 \t 0.000000 \t 11.309954 s\n","28 \t 19.361217 \t 9.611413 \t9.749805 \t 0.000000 \t 11.658330 s\n","29 \t 19.772984 \t 9.553718 \t10.219265 \t 0.000000 \t 12.000457 s\n","30 \t 19.782563 \t 9.969680 \t9.812883 \t 0.000000 \t 12.377213 s\n","31 \t 20.009911 \t 9.874192 \t10.135718 \t 0.000000 \t 12.752506 s\n","32 \t 19.753515 \t 9.872402 \t9.881113 \t 0.000000 \t 13.103452 s\n","33 \t 19.910789 \t 9.792571 \t10.118219 \t 0.000000 \t 13.506708 s\n","34 \t 19.405563 \t 9.475621 \t9.929942 \t 0.000000 \t 13.970525 s\n","35 \t 19.358976 \t 9.477497 \t9.881479 \t 0.000000 \t 14.316076 s\n","36 \t 19.871327 \t 9.996055 \t9.875273 \t 0.000000 \t 14.654079 s\n","37 \t 19.857851 \t 9.976151 \t9.881701 \t 0.000000 \t 14.992233 s\n","38 \t 19.139910 \t 9.495768 \t9.644142 \t 0.000000 \t 15.332276 s\n","39 \t 19.367741 \t 9.597932 \t9.769810 \t 0.000000 \t 15.672683 s\n","40 \t 19.175223 \t 9.510515 \t9.664708 \t 0.000000 \t 16.015655 s\n","41 \t 19.626431 \t 9.864077 \t9.762354 \t 0.000000 \t 16.370649 s\n","42 \t 19.427251 \t 9.876149 \t9.551103 \t 0.000000 \t 16.731084 s\n","43 \t 19.425478 \t 9.632820 \t9.792659 \t 0.000000 \t 17.144247 s\n","44 \t 18.952227 \t 9.714159 \t9.238068 \t 0.000000 \t 17.491186 s\n","45 \t 19.444044 \t 9.527775 \t9.916268 \t 0.000000 \t 17.833552 s\n","46 \t 18.634561 \t 9.000879 \t9.633682 \t 0.000000 \t 18.311181 s\n","47 \t 19.131572 \t 9.269624 \t9.861948 \t 0.000000 \t 18.664335 s\n","48 \t 19.490972 \t 9.744143 \t9.746828 \t 0.000000 \t 19.011920 s\n","49 \t 18.940428 \t 9.265686 \t9.674741 \t 0.000000 \t 19.353073 s\n","50 \t 18.371966 \t 9.652788 \t8.719178 \t 0.000000 \t 19.692808 s\n","51 \t 18.787756 \t 9.476592 \t9.311164 \t 0.000000 \t 20.039937 s\n","52 \t 19.408678 \t 9.800679 \t9.607999 \t 0.000000 \t 20.383736 s\n","53 \t 19.204340 \t 9.638531 \t9.565809 \t 0.000000 \t 20.723669 s\n","54 \t 19.106618 \t 9.475343 \t9.631274 \t 0.000000 \t 21.071686 s\n","55 \t 19.167551 \t 9.672790 \t9.494761 \t 0.000000 \t 21.414959 s\n","56 \t 19.409273 \t 9.622235 \t9.787038 \t 0.000000 \t 21.755738 s\n","57 \t 18.745939 \t 9.511533 \t9.234406 \t 0.000000 \t 22.097940 s\n","58 \t 18.691895 \t 9.252800 \t9.439095 \t 0.000000 \t 22.564929 s\n","59 \t 18.895492 \t 9.475063 \t9.420429 \t 0.000000 \t 22.903440 s\n","60 \t 19.313132 \t 9.537456 \t9.775677 \t 0.000000 \t 23.244593 s\n","61 \t 19.324768 \t 9.668804 \t9.655965 \t 0.000000 \t 23.582778 s\n","62 \t 18.186707 \t 9.318555 \t8.868151 \t 0.000000 \t 23.932012 s\n","63 \t 19.393552 \t 9.855394 \t9.538158 \t 0.000000 \t 24.282206 s\n","64 \t 18.691688 \t 9.112761 \t9.578928 \t 0.000000 \t 24.623614 s\n","65 \t 18.631785 \t 9.228740 \t9.403045 \t 0.000000 \t 24.974495 s\n","66 \t 19.223108 \t 9.660779 \t9.562330 \t 0.000000 \t 25.318199 s\n","67 \t 18.146716 \t 9.280570 \t8.866146 \t 0.000000 \t 25.660401 s\n","68 \t 18.468588 \t 9.215354 \t9.253234 \t 0.000000 \t 26.015841 s\n","69 \t 18.955979 \t 9.323218 \t9.632761 \t 0.000000 \t 26.354791 s\n","70 \t 18.673759 \t 9.378819 \t9.294940 \t 0.000000 \t 26.823947 s\n","71 \t 18.210413 \t 9.413224 \t8.797189 \t 0.000000 \t 27.190192 s\n","72 \t 18.459609 \t 9.312188 \t9.147421 \t 0.000000 \t 27.527331 s\n","73 \t 17.717621 \t 8.637492 \t9.080129 \t 0.000000 \t 27.866129 s\n","74 \t 18.608396 \t 9.529220 \t9.079175 \t 0.000000 \t 28.229780 s\n","75 \t 19.254731 \t 9.577376 \t9.677355 \t 0.000000 \t 28.625711 s\n","76 \t 18.509032 \t 9.565557 \t8.943476 \t 0.000000 \t 28.977222 s\n","77 \t 18.758372 \t 9.687007 \t9.071365 \t 0.000000 \t 29.320875 s\n","78 \t 18.738714 \t 9.777071 \t8.961643 \t 0.000000 \t 29.662066 s\n","79 \t 18.945926 \t 9.732749 \t9.213176 \t 0.000000 \t 30.126007 s\n","80 \t 18.378638 \t 9.345634 \t9.033005 \t 0.000000 \t 30.466273 s\n","81 \t 18.608224 \t 9.299951 \t9.308273 \t 0.000000 \t 30.830455 s\n","82 \t 18.712851 \t 9.571556 \t9.141294 \t 0.000000 \t 31.172128 s\n","83 \t 18.080838 \t 8.914740 \t9.166098 \t 0.000000 \t 31.509025 s\n","84 \t 18.741473 \t 9.472028 \t9.269445 \t 0.000000 \t 31.846581 s\n","85 \t 18.299683 \t 9.245306 \t9.054375 \t 0.000000 \t 32.191085 s\n","86 \t 18.013521 \t 9.151531 \t8.861990 \t 0.000000 \t 32.529585 s\n","87 \t 18.104248 \t 9.174570 \t8.929678 \t 0.000000 \t 32.868639 s\n","88 \t 18.604092 \t 9.237576 \t9.366515 \t 0.000000 \t 33.337732 s\n","89 \t 18.777061 \t 9.175183 \t9.601877 \t 0.000000 \t 33.676593 s\n","90 \t 18.131271 \t 9.355768 \t8.775503 \t 0.000000 \t 34.026283 s\n","91 \t 18.215061 \t 9.390952 \t8.824109 \t 0.000000 \t 34.400341 s\n","92 \t 18.013465 \t 9.150119 \t8.863346 \t 0.000000 \t 34.737643 s\n","93 \t 18.584034 \t 9.214713 \t9.369320 \t 0.000000 \t 35.121318 s\n","94 \t 18.079264 \t 9.129948 \t8.949316 \t 0.000000 \t 35.460414 s\n","95 \t 18.021845 \t 9.185006 \t8.836840 \t 0.000000 \t 35.796816 s\n","96 \t 18.603803 \t 9.362313 \t9.241490 \t 0.000000 \t 36.165506 s\n","97 \t 18.120078 \t 8.994215 \t9.125862 \t 0.000000 \t 36.636047 s\n","98 \t 17.908988 \t 9.164300 \t8.744688 \t 0.000000 \t 36.970984 s\n","99 \t 17.466777 \t 9.177170 \t8.289607 \t 0.000000 \t 37.329757 s\n","100 \t 18.210597 \t 9.129661 \t9.080935 \t 0.000000 \t 37.670209 s\n","101 \t 17.781613 \t 8.734364 \t9.047249 \t 0.000000 \t 38.012186 s\n","102 \t 17.694595 \t 9.024347 \t8.670249 \t 0.000000 \t 38.357663 s\n","103 \t 18.730065 \t 8.962285 \t9.767781 \t 0.000000 \t 38.699637 s\n","104 \t 17.783934 \t 9.063920 \t8.720013 \t 0.000000 \t 39.038888 s\n","105 \t 18.375724 \t 9.671822 \t8.703902 \t 0.000000 \t 39.376514 s\n","106 \t 17.800321 \t 9.094282 \t8.706038 \t 0.000000 \t 39.717620 s\n","107 \t 18.149200 \t 8.878758 \t9.270443 \t 0.000000 \t 40.192335 s\n","108 \t 18.375854 \t 9.472202 \t8.903651 \t 0.000000 \t 40.611404 s\n","109 \t 18.092516 \t 9.495946 \t8.596570 \t 0.000000 \t 40.954738 s\n","110 \t 18.343627 \t 9.442023 \t8.901604 \t 0.000000 \t 41.297683 s\n","111 \t 17.845444 \t 9.018563 \t8.826880 \t 0.000000 \t 41.639518 s\n","112 \t 17.851202 \t 8.958088 \t8.893114 \t 0.000000 \t 41.981957 s\n","113 \t 18.496582 \t 9.262803 \t9.233778 \t 0.000000 \t 42.326962 s\n","114 \t 17.629150 \t 8.938305 \t8.690845 \t 0.000000 \t 42.674777 s\n","115 \t 17.850117 \t 8.960733 \t8.889384 \t 0.000000 \t 43.034296 s\n","116 \t 18.012280 \t 9.009758 \t9.002522 \t 0.000000 \t 43.568792 s\n","117 \t 18.105378 \t 9.126331 \t8.979048 \t 0.000000 \t 43.907476 s\n","118 \t 18.356599 \t 9.512884 \t8.843715 \t 0.000000 \t 44.248963 s\n","119 \t 18.268549 \t 9.135725 \t9.132824 \t 0.000000 \t 44.613904 s\n","120 \t 17.459049 \t 8.760368 \t8.698680 \t 0.000000 \t 44.956112 s\n","121 \t 18.103476 \t 8.989442 \t9.114033 \t 0.000000 \t 45.300091 s\n","122 \t 17.998830 \t 9.094662 \t8.904168 \t 0.000000 \t 45.643396 s\n","123 \t 17.483514 \t 8.928060 \t8.555455 \t 0.000000 \t 45.994095 s\n","124 \t 17.596636 \t 9.053100 \t8.543536 \t 0.000000 \t 46.334033 s\n","125 \t 17.144953 \t 8.684633 \t8.460320 \t 0.000000 \t 46.672830 s\n","126 \t 17.752265 \t 9.122735 \t8.629530 \t 0.000000 \t 47.143914 s\n","127 \t 17.577972 \t 8.821744 \t8.756228 \t 0.000000 \t 47.485640 s\n","128 \t 18.045391 \t 8.967969 \t9.077423 \t 0.000000 \t 47.827474 s\n","129 \t 17.913297 \t 8.894842 \t9.018455 \t 0.000000 \t 48.168727 s\n","130 \t 18.241049 \t 9.058010 \t9.183039 \t 0.000000 \t 48.506279 s\n","131 \t 18.322420 \t 9.193039 \t9.129381 \t 0.000000 \t 48.843289 s\n","132 \t 17.612500 \t 8.610002 \t9.002499 \t 0.000000 \t 49.186647 s\n","133 \t 17.726863 \t 8.784360 \t8.942503 \t 0.000000 \t 49.524951 s\n","134 \t 17.989077 \t 9.034092 \t8.954985 \t 0.000000 \t 49.861496 s\n","135 \t 17.712904 \t 8.770550 \t8.942354 \t 0.000000 \t 50.206825 s\n","136 \t 17.597383 \t 8.624172 \t8.973210 \t 0.000000 \t 50.546945 s\n","137 \t 18.448120 \t 9.216808 \t9.231311 \t 0.000000 \t 51.026623 s\n","138 \t 17.606835 \t 8.772587 \t8.834249 \t 0.000000 \t 51.374888 s\n","139 \t 18.439438 \t 9.322177 \t9.117260 \t 0.000000 \t 51.716485 s\n","140 \t 17.615999 \t 9.105034 \t8.510966 \t 0.000000 \t 52.068274 s\n","141 \t 17.740149 \t 8.556297 \t9.183852 \t 0.000000 \t 52.408733 s\n","142 \t 18.117756 \t 8.966591 \t9.151165 \t 0.000000 \t 52.747937 s\n","143 \t 17.882767 \t 8.846335 \t9.036432 \t 0.000000 \t 53.103679 s\n","144 \t 17.442741 \t 8.943612 \t8.499129 \t 0.000000 \t 53.493339 s\n","145 \t 17.236326 \t 8.534509 \t8.701818 \t 0.000000 \t 53.847983 s\n","146 \t 17.663998 \t 8.796699 \t8.867298 \t 0.000000 \t 54.211155 s\n","147 \t 18.104355 \t 9.135676 \t8.968678 \t 0.000000 \t 54.554774 s\n","148 \t 17.252325 \t 8.871179 \t8.381146 \t 0.000000 \t 55.044873 s\n","149 \t 17.762033 \t 9.096898 \t8.665134 \t 0.000000 \t 55.384439 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/130 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 130/130 [00:22<00:00,  5.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3849\n","Hit@10: 0.5692\n","Hit@3: 0.3846\n","Hit@1: 0.3038\n","Link Prediction on Validation Set (All)\n","MRR: 0.3070\n","Hit@10: 0.5395\n","Hit@3: 0.3301\n","Hit@1: 0.1981\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3999\n","Hit@10: 0.6000\n","Hit@3: 0.4231\n","Hit@1: 0.3000\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3772\n","Hit@10: 0.6253\n","Hit@3: 0.4420\n","Hit@1: 0.2383\n","150 \t 17.443054 \t 8.669126 \t8.773929 \t 0.000000 \t 79.300481 s\n","151 \t 18.525624 \t 9.201228 \t9.324396 \t 0.000000 \t 79.641093 s\n","152 \t 18.553095 \t 9.376560 \t9.176535 \t 0.000000 \t 79.988293 s\n","153 \t 18.604739 \t 9.289247 \t9.315492 \t 0.000000 \t 80.324256 s\n","154 \t 19.272310 \t 9.512460 \t9.759850 \t 0.000000 \t 80.664608 s\n","155 \t 18.463501 \t 9.369877 \t9.093623 \t 0.000000 \t 81.019978 s\n","156 \t 18.262294 \t 9.258794 \t9.003500 \t 0.000000 \t 81.357837 s\n","157 \t 18.365532 \t 9.161488 \t9.204044 \t 0.000000 \t 81.842771 s\n","158 \t 18.750087 \t 9.357046 \t9.393040 \t 0.000000 \t 82.189594 s\n","159 \t 18.479414 \t 9.153296 \t9.326118 \t 0.000000 \t 82.527610 s\n","160 \t 18.247577 \t 9.014104 \t9.233473 \t 0.000000 \t 82.868262 s\n","161 \t 17.750581 \t 8.962893 \t8.787688 \t 0.000000 \t 83.224337 s\n","162 \t 18.249660 \t 9.122786 \t9.126875 \t 0.000000 \t 83.561810 s\n","163 \t 18.158643 \t 8.919047 \t9.239596 \t 0.000000 \t 83.902368 s\n","164 \t 18.441808 \t 9.237657 \t9.204150 \t 0.000000 \t 84.272072 s\n","165 \t 17.346816 \t 8.841564 \t8.505252 \t 0.000000 \t 84.685248 s\n","166 \t 17.573851 \t 8.838557 \t8.735293 \t 0.000000 \t 85.164426 s\n","167 \t 17.347700 \t 8.545631 \t8.802070 \t 0.000000 \t 85.502943 s\n","168 \t 17.797445 \t 8.957382 \t8.840063 \t 0.000000 \t 85.855379 s\n","169 \t 17.650726 \t 8.855175 \t8.795552 \t 0.000000 \t 86.220883 s\n","170 \t 17.980572 \t 9.125725 \t8.854847 \t 0.000000 \t 86.581542 s\n","171 \t 17.615959 \t 8.608801 \t9.007159 \t 0.000000 \t 86.938455 s\n","172 \t 17.490249 \t 8.497416 \t8.992833 \t 0.000000 \t 87.303598 s\n","173 \t 17.766014 \t 9.115440 \t8.650575 \t 0.000000 \t 87.658173 s\n","174 \t 17.789459 \t 8.777285 \t9.012175 \t 0.000000 \t 88.017357 s\n","175 \t 17.756392 \t 9.006060 \t8.750331 \t 0.000000 \t 88.449570 s\n","176 \t 17.764515 \t 8.881803 \t8.882712 \t 0.000000 \t 88.932825 s\n","177 \t 17.605617 \t 8.889443 \t8.716173 \t 0.000000 \t 89.290036 s\n","178 \t 17.188196 \t 8.579436 \t8.608760 \t 0.000000 \t 89.640435 s\n","179 \t 16.946234 \t 8.468830 \t8.477404 \t 0.000000 \t 90.026986 s\n","180 \t 17.409403 \t 8.714320 \t8.695083 \t 0.000000 \t 90.404449 s\n","181 \t 17.428349 \t 8.541279 \t8.887069 \t 0.000000 \t 90.751545 s\n","182 \t 17.734190 \t 8.751538 \t8.982652 \t 0.000000 \t 91.104324 s\n","183 \t 17.675902 \t 8.988503 \t8.687399 \t 0.000000 \t 91.445396 s\n","184 \t 17.396965 \t 8.725022 \t8.671943 \t 0.000000 \t 91.786935 s\n","185 \t 17.537791 \t 8.582617 \t8.955175 \t 0.000000 \t 92.170806 s\n","186 \t 17.767920 \t 8.847633 \t8.920286 \t 0.000000 \t 92.700717 s\n","187 \t 16.935987 \t 8.488815 \t8.447173 \t 0.000000 \t 93.069461 s\n","188 \t 16.972898 \t 8.648149 \t8.324750 \t 0.000000 \t 93.408533 s\n","189 \t 17.873053 \t 8.853673 \t9.019379 \t 0.000000 \t 93.747349 s\n","190 \t 17.205344 \t 8.464257 \t8.741087 \t 0.000000 \t 94.110893 s\n","191 \t 17.703401 \t 8.840194 \t8.863206 \t 0.000000 \t 94.460882 s\n","192 \t 16.897986 \t 8.458690 \t8.439297 \t 0.000000 \t 94.808494 s\n","193 \t 16.943368 \t 8.385082 \t8.558287 \t 0.000000 \t 95.156334 s\n","194 \t 17.063829 \t 8.652016 \t8.411813 \t 0.000000 \t 95.494936 s\n","195 \t 17.393492 \t 8.748952 \t8.644539 \t 0.000000 \t 95.831002 s\n","196 \t 17.238732 \t 8.504276 \t8.734456 \t 0.000000 \t 96.307159 s\n","197 \t 16.986249 \t 8.511896 \t8.474354 \t 0.000000 \t 96.645937 s\n","198 \t 17.372431 \t 8.490663 \t8.881767 \t 0.000000 \t 96.993391 s\n","199 \t 16.824076 \t 8.540355 \t8.283720 \t 0.000000 \t 97.335936 s\n","200 \t 17.391676 \t 8.673154 \t8.718522 \t 0.000000 \t 97.673077 s\n","201 \t 16.970172 \t 8.437716 \t8.532455 \t 0.000000 \t 98.013001 s\n","202 \t 16.660885 \t 8.206219 \t8.454667 \t 0.000000 \t 98.351656 s\n","203 \t 17.293202 \t 8.615682 \t8.677520 \t 0.000000 \t 98.688606 s\n","204 \t 17.232912 \t 8.276022 \t8.956890 \t 0.000000 \t 99.032640 s\n","205 \t 16.541831 \t 8.337554 \t8.204278 \t 0.000000 \t 99.372920 s\n","206 \t 16.959562 \t 8.566407 \t8.393157 \t 0.000000 \t 99.850523 s\n","207 \t 17.332605 \t 8.873108 \t8.459497 \t 0.000000 \t 100.251731 s\n","208 \t 16.956907 \t 8.541353 \t8.415555 \t 0.000000 \t 100.589012 s\n","209 \t 16.920611 \t 8.297193 \t8.623419 \t 0.000000 \t 100.926870 s\n","210 \t 16.625580 \t 8.583357 \t8.042223 \t 0.000000 \t 101.270284 s\n","211 \t 16.808594 \t 8.055925 \t8.752669 \t 0.000000 \t 101.610634 s\n","212 \t 16.626604 \t 8.175288 \t8.451316 \t 0.000000 \t 101.953911 s\n","213 \t 16.773762 \t 8.185116 \t8.588645 \t 0.000000 \t 102.296814 s\n","214 \t 16.804008 \t 8.496919 \t8.307090 \t 0.000000 \t 102.642727 s\n","215 \t 17.538809 \t 8.594659 \t8.944150 \t 0.000000 \t 102.991791 s\n","216 \t 17.007521 \t 8.443377 \t8.564144 \t 0.000000 \t 103.331234 s\n","217 \t 17.036742 \t 8.310230 \t8.726512 \t 0.000000 \t 103.806118 s\n","218 \t 16.967075 \t 8.373432 \t8.593643 \t 0.000000 \t 104.204364 s\n","219 \t 16.375162 \t 8.251011 \t8.124150 \t 0.000000 \t 104.549687 s\n","220 \t 16.498475 \t 8.256485 \t8.241991 \t 0.000000 \t 104.896824 s\n","221 \t 16.412148 \t 8.564824 \t7.847323 \t 0.000000 \t 105.237176 s\n","222 \t 16.349953 \t 7.861690 \t8.488263 \t 0.000000 \t 105.589962 s\n","223 \t 17.158616 \t 8.558489 \t8.600128 \t 0.000000 \t 105.934074 s\n","224 \t 16.492970 \t 8.085968 \t8.407002 \t 0.000000 \t 106.370443 s\n","225 \t 16.827452 \t 8.156377 \t8.671074 \t 0.000000 \t 106.716769 s\n","226 \t 16.623655 \t 8.003791 \t8.619864 \t 0.000000 \t 107.140615 s\n","227 \t 16.326460 \t 8.424816 \t7.901644 \t 0.000000 \t 107.483411 s\n","228 \t 16.239487 \t 7.916196 \t8.323292 \t 0.000000 \t 107.946795 s\n","229 \t 16.701825 \t 8.158789 \t8.543036 \t 0.000000 \t 108.353121 s\n","230 \t 16.218025 \t 7.973746 \t8.244280 \t 0.000000 \t 108.690943 s\n","231 \t 16.395091 \t 8.153827 \t8.241264 \t 0.000000 \t 109.031823 s\n","232 \t 16.619620 \t 8.258131 \t8.361490 \t 0.000000 \t 109.384288 s\n","233 \t 16.382235 \t 7.886695 \t8.495540 \t 0.000000 \t 109.784985 s\n","234 \t 16.243865 \t 8.090689 \t8.153176 \t 0.000000 \t 110.138386 s\n","235 \t 16.138381 \t 7.973843 \t8.164538 \t 0.000000 \t 110.480118 s\n","236 \t 16.219789 \t 8.028332 \t8.191458 \t 0.000000 \t 110.828375 s\n","237 \t 15.738418 \t 7.717965 \t8.020453 \t 0.000000 \t 111.234758 s\n","238 \t 16.457238 \t 7.962884 \t8.494355 \t 0.000000 \t 111.576597 s\n","239 \t 15.984518 \t 7.799316 \t8.185202 \t 0.000000 \t 111.914113 s\n","240 \t 16.276410 \t 8.306014 \t7.970396 \t 0.000000 \t 112.427778 s\n","241 \t 16.248575 \t 7.931929 \t8.316647 \t 0.000000 \t 112.772667 s\n","242 \t 15.834586 \t 8.055442 \t7.779144 \t 0.000000 \t 113.113124 s\n","243 \t 16.006129 \t 7.794631 \t8.211498 \t 0.000000 \t 113.465760 s\n","244 \t 16.193334 \t 7.980721 \t8.212612 \t 0.000000 \t 113.811800 s\n","245 \t 16.440962 \t 8.001945 \t8.439017 \t 0.000000 \t 114.155757 s\n","246 \t 16.207563 \t 8.163304 \t8.044260 \t 0.000000 \t 114.507904 s\n","247 \t 15.874923 \t 7.964156 \t7.910766 \t 0.000000 \t 114.869263 s\n","248 \t 15.981218 \t 7.698456 \t8.282761 \t 0.000000 \t 115.213418 s\n","249 \t 16.016947 \t 7.771199 \t8.245748 \t 0.000000 \t 115.550440 s\n","250 \t 16.340885 \t 7.904504 \t8.436382 \t 0.000000 \t 115.889883 s\n","251 \t 15.845088 \t 7.679923 \t8.165165 \t 0.000000 \t 116.294822 s\n","252 \t 16.533895 \t 7.809362 \t8.724532 \t 0.000000 \t 116.755573 s\n","253 \t 15.643884 \t 7.833501 \t7.810382 \t 0.000000 \t 117.093096 s\n","254 \t 16.017672 \t 7.722567 \t8.295105 \t 0.000000 \t 117.434680 s\n","255 \t 16.465451 \t 8.133171 \t8.332281 \t 0.000000 \t 117.774233 s\n","256 \t 16.311555 \t 7.843402 \t8.468152 \t 0.000000 \t 118.116711 s\n","257 \t 15.990033 \t 8.236923 \t7.753110 \t 0.000000 \t 118.454604 s\n","258 \t 15.008175 \t 7.384341 \t7.623834 \t 0.000000 \t 118.792850 s\n","259 \t 16.109222 \t 7.721703 \t8.387519 \t 0.000000 \t 119.138923 s\n","260 \t 15.606206 \t 7.604833 \t8.001373 \t 0.000000 \t 119.477982 s\n","261 \t 15.995499 \t 7.948121 \t8.047379 \t 0.000000 \t 119.943722 s\n","262 \t 15.806990 \t 7.774073 \t8.032917 \t 0.000000 \t 120.307118 s\n","263 \t 15.993880 \t 7.485646 \t8.508234 \t 0.000000 \t 120.646026 s\n","264 \t 15.670114 \t 7.581055 \t8.089059 \t 0.000000 \t 120.989592 s\n","265 \t 16.157651 \t 7.827801 \t8.329850 \t 0.000000 \t 121.342976 s\n","266 \t 15.385128 \t 7.593967 \t7.791161 \t 0.000000 \t 121.694544 s\n","267 \t 15.922984 \t 8.022403 \t7.900581 \t 0.000000 \t 122.036382 s\n","268 \t 16.103005 \t 7.740440 \t8.362566 \t 0.000000 \t 122.458937 s\n","269 \t 15.448142 \t 7.627474 \t7.820668 \t 0.000000 \t 122.803061 s\n","270 \t 15.889518 \t 7.962388 \t7.927130 \t 0.000000 \t 123.261902 s\n","271 \t 15.757626 \t 7.853927 \t7.903699 \t 0.000000 \t 123.598163 s\n","272 \t 15.863606 \t 7.740783 \t8.122823 \t 0.000000 \t 123.935047 s\n","273 \t 15.851737 \t 7.842113 \t8.009624 \t 0.000000 \t 124.304338 s\n","274 \t 16.047831 \t 7.769988 \t8.277842 \t 0.000000 \t 124.642742 s\n","275 \t 16.436719 \t 7.993531 \t8.443189 \t 0.000000 \t 124.989109 s\n","276 \t 15.504112 \t 7.635773 \t7.868338 \t 0.000000 \t 125.378957 s\n","277 \t 15.751373 \t 7.762335 \t7.989038 \t 0.000000 \t 125.719895 s\n","278 \t 15.513083 \t 7.483418 \t8.029665 \t 0.000000 \t 126.063746 s\n","279 \t 15.809103 \t 7.713534 \t8.095569 \t 0.000000 \t 126.532166 s\n","280 \t 15.660482 \t 7.854024 \t7.806459 \t 0.000000 \t 126.877244 s\n","281 \t 15.533071 \t 7.702690 \t7.830381 \t 0.000000 \t 127.242642 s\n","282 \t 15.067874 \t 7.399627 \t7.668247 \t 0.000000 \t 127.584374 s\n","283 \t 15.524420 \t 7.605756 \t7.918664 \t 0.000000 \t 127.925578 s\n","284 \t 15.284534 \t 7.468239 \t7.816295 \t 0.000000 \t 128.264668 s\n","285 \t 15.308167 \t 7.333072 \t7.975095 \t 0.000000 \t 128.608407 s\n","286 \t 15.349505 \t 7.720848 \t7.628657 \t 0.000000 \t 128.951528 s\n","287 \t 15.241107 \t 7.479807 \t7.761301 \t 0.000000 \t 129.308845 s\n","288 \t 14.882034 \t 7.235193 \t7.646842 \t 0.000000 \t 129.650624 s\n","289 \t 15.036311 \t 7.416901 \t7.619410 \t 0.000000 \t 130.169718 s\n","290 \t 15.717700 \t 7.672729 \t8.044971 \t 0.000000 \t 130.507793 s\n","291 \t 15.241261 \t 7.411586 \t7.829676 \t 0.000000 \t 130.846913 s\n","292 \t 14.867500 \t 7.482424 \t7.385077 \t 0.000000 \t 131.242878 s\n","293 \t 14.844969 \t 7.242904 \t7.602066 \t 0.000000 \t 131.582152 s\n","294 \t 15.372925 \t 7.595750 \t7.777175 \t 0.000000 \t 131.918950 s\n","295 \t 14.856896 \t 7.318410 \t7.538486 \t 0.000000 \t 132.268962 s\n","296 \t 15.933219 \t 7.772776 \t8.160443 \t 0.000000 \t 132.608648 s\n","297 \t 15.637131 \t 7.671613 \t7.965518 \t 0.000000 \t 132.944647 s\n","298 \t 15.210409 \t 7.579313 \t7.631097 \t 0.000000 \t 133.289786 s\n","299 \t 15.019353 \t 7.434856 \t7.584497 \t 0.000000 \t 133.752292 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:23<00:00,  5.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5435\n","Hit@10: 0.7231\n","Hit@3: 0.5769\n","Hit@1: 0.4538\n","Link Prediction on Validation Set (All)\n","MRR: 0.4972\n","Hit@10: 0.7568\n","Hit@3: 0.5604\n","Hit@1: 0.3704\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5569\n","Hit@10: 0.7231\n","Hit@3: 0.6462\n","Hit@1: 0.4462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4968\n","Hit@10: 0.7556\n","Hit@3: 0.5866\n","Hit@1: 0.3585\n","300 \t 15.170491 \t 7.190419 \t7.980072 \t 0.000000 \t 157.820607 s\n","301 \t 15.419949 \t 7.213667 \t8.206282 \t 0.000000 \t 158.162282 s\n","302 \t 15.234905 \t 7.488492 \t7.746413 \t 0.000000 \t 158.499632 s\n","303 \t 15.168640 \t 7.786925 \t7.381715 \t 0.000000 \t 158.846572 s\n","304 \t 15.181688 \t 7.522752 \t7.658937 \t 0.000000 \t 159.227707 s\n","305 \t 15.006493 \t 7.284083 \t7.722410 \t 0.000000 \t 159.567891 s\n","306 \t 15.343390 \t 7.559266 \t7.784124 \t 0.000000 \t 159.913388 s\n","307 \t 14.273146 \t 7.024796 \t7.248350 \t 0.000000 \t 160.270056 s\n","308 \t 14.358039 \t 6.901426 \t7.456613 \t 0.000000 \t 160.609937 s\n","309 \t 14.729483 \t 7.340747 \t7.388736 \t 0.000000 \t 161.079257 s\n","310 \t 15.265453 \t 7.511925 \t7.753528 \t 0.000000 \t 161.486773 s\n","311 \t 15.447955 \t 7.620707 \t7.827248 \t 0.000000 \t 161.830243 s\n","312 \t 14.856548 \t 7.341519 \t7.515028 \t 0.000000 \t 162.187099 s\n","313 \t 15.021957 \t 7.219601 \t7.802356 \t 0.000000 \t 162.533507 s\n","314 \t 14.992784 \t 7.342698 \t7.650086 \t 0.000000 \t 162.882636 s\n","315 \t 14.715568 \t 7.561515 \t7.154053 \t 0.000000 \t 163.244107 s\n","316 \t 15.016963 \t 7.299089 \t7.717874 \t 0.000000 \t 163.594579 s\n","317 \t 14.651418 \t 7.422530 \t7.228888 \t 0.000000 \t 164.071227 s\n","318 \t 14.957761 \t 7.384550 \t7.573211 \t 0.000000 \t 164.485942 s\n","319 \t 14.516367 \t 7.110533 \t7.405835 \t 0.000000 \t 164.999407 s\n","320 \t 15.290692 \t 7.547805 \t7.742887 \t 0.000000 \t 165.377208 s\n","321 \t 14.834736 \t 7.221419 \t7.613317 \t 0.000000 \t 165.803995 s\n","322 \t 15.206669 \t 7.381016 \t7.825654 \t 0.000000 \t 166.250363 s\n","323 \t 14.878445 \t 7.281103 \t7.597342 \t 0.000000 \t 166.589695 s\n","324 \t 14.489358 \t 7.211724 \t7.277634 \t 0.000000 \t 166.928749 s\n","325 \t 14.578589 \t 7.011638 \t7.566951 \t 0.000000 \t 167.285568 s\n","326 \t 14.859480 \t 7.247347 \t7.612133 \t 0.000000 \t 167.700811 s\n","327 \t 15.239083 \t 7.382353 \t7.856731 \t 0.000000 \t 168.049375 s\n","328 \t 14.752094 \t 7.398718 \t7.353376 \t 0.000000 \t 168.388031 s\n","329 \t 14.677952 \t 7.066740 \t7.611212 \t 0.000000 \t 168.725286 s\n","330 \t 14.734547 \t 7.185096 \t7.549451 \t 0.000000 \t 169.187326 s\n","331 \t 14.131965 \t 7.084970 \t7.046994 \t 0.000000 \t 169.535261 s\n","332 \t 14.178953 \t 6.960638 \t7.218315 \t 0.000000 \t 169.875602 s\n","333 \t 14.596276 \t 6.919123 \t7.677153 \t 0.000000 \t 170.251073 s\n","334 \t 14.297985 \t 6.829287 \t7.468699 \t 0.000000 \t 170.588328 s\n","335 \t 14.513906 \t 7.085783 \t7.428122 \t 0.000000 \t 170.928831 s\n","336 \t 15.102806 \t 7.101831 \t8.000975 \t 0.000000 \t 171.294391 s\n","337 \t 14.856865 \t 7.044282 \t7.812582 \t 0.000000 \t 171.643008 s\n","338 \t 15.114779 \t 7.035654 \t8.079126 \t 0.000000 \t 171.982474 s\n","339 \t 14.888371 \t 6.988526 \t7.899845 \t 0.000000 \t 172.321507 s\n","340 \t 14.354191 \t 7.054529 \t7.299662 \t 0.000000 \t 172.661532 s\n","341 \t 14.532578 \t 7.124727 \t7.407851 \t 0.000000 \t 173.147538 s\n","342 \t 14.838736 \t 7.183300 \t7.655437 \t 0.000000 \t 173.499681 s\n","343 \t 14.582639 \t 6.765379 \t7.817260 \t 0.000000 \t 173.844778 s\n","344 \t 14.678346 \t 7.130756 \t7.547590 \t 0.000000 \t 174.201032 s\n","345 \t 14.703063 \t 7.125815 \t7.577247 \t 0.000000 \t 174.542373 s\n","346 \t 14.600831 \t 7.174287 \t7.426544 \t 0.000000 \t 174.883171 s\n","347 \t 14.901359 \t 7.320655 \t7.580703 \t 0.000000 \t 175.240433 s\n","348 \t 14.349378 \t 7.064868 \t7.284509 \t 0.000000 \t 175.582892 s\n","349 \t 15.233207 \t 7.575300 \t7.657907 \t 0.000000 \t 175.925341 s\n","350 \t 14.357590 \t 6.988490 \t7.369100 \t 0.000000 \t 176.287560 s\n","351 \t 14.730344 \t 7.189797 \t7.540547 \t 0.000000 \t 176.642453 s\n","352 \t 14.111923 \t 6.788068 \t7.323854 \t 0.000000 \t 177.122083 s\n","353 \t 14.513178 \t 7.185973 \t7.327205 \t 0.000000 \t 177.476113 s\n","354 \t 14.243726 \t 7.008183 \t7.235543 \t 0.000000 \t 177.814059 s\n","355 \t 15.054562 \t 7.390906 \t7.663656 \t 0.000000 \t 178.153161 s\n","356 \t 14.420436 \t 7.007740 \t7.412696 \t 0.000000 \t 178.493056 s\n","357 \t 14.547528 \t 7.182692 \t7.364837 \t 0.000000 \t 178.830342 s\n","358 \t 14.798344 \t 7.117216 \t7.681128 \t 0.000000 \t 179.187924 s\n","359 \t 14.466219 \t 6.933888 \t7.532331 \t 0.000000 \t 179.534369 s\n","360 \t 14.583707 \t 7.098227 \t7.485480 \t 0.000000 \t 179.872044 s\n","361 \t 14.584058 \t 7.265992 \t7.318066 \t 0.000000 \t 180.210810 s\n","362 \t 13.849080 \t 6.652048 \t7.197032 \t 0.000000 \t 180.553193 s\n","363 \t 14.379846 \t 7.072716 \t7.307130 \t 0.000000 \t 181.045993 s\n","364 \t 14.128375 \t 6.802243 \t7.326132 \t 0.000000 \t 181.392873 s\n","365 \t 14.489671 \t 7.035454 \t7.454217 \t 0.000000 \t 181.731504 s\n","366 \t 14.255110 \t 6.925285 \t7.329824 \t 0.000000 \t 182.093461 s\n","367 \t 14.535911 \t 7.370980 \t7.164930 \t 0.000000 \t 182.435103 s\n","368 \t 14.321816 \t 6.986706 \t7.335110 \t 0.000000 \t 182.777796 s\n","369 \t 14.568155 \t 7.282720 \t7.285435 \t 0.000000 \t 183.116764 s\n","370 \t 14.668677 \t 6.968273 \t7.700404 \t 0.000000 \t 183.468839 s\n","371 \t 14.556628 \t 7.182584 \t7.374044 \t 0.000000 \t 183.812030 s\n","372 \t 14.367813 \t 7.141615 \t7.226197 \t 0.000000 \t 184.337806 s\n","373 \t 14.058164 \t 6.715792 \t7.342372 \t 0.000000 \t 184.759475 s\n","374 \t 14.209046 \t 6.961523 \t7.247523 \t 0.000000 \t 185.104205 s\n","375 \t 14.278986 \t 7.009773 \t7.269212 \t 0.000000 \t 185.529828 s\n","376 \t 14.625543 \t 7.034250 \t7.591292 \t 0.000000 \t 185.887785 s\n","377 \t 14.079170 \t 7.062931 \t7.016239 \t 0.000000 \t 186.237017 s\n","378 \t 14.409386 \t 7.238637 \t7.170748 \t 0.000000 \t 186.581325 s\n","379 \t 14.228458 \t 7.025139 \t7.203319 \t 0.000000 \t 186.924690 s\n","380 \t 13.494073 \t 6.664816 \t6.829257 \t 0.000000 \t 187.402581 s\n","381 \t 14.096270 \t 7.060914 \t7.035355 \t 0.000000 \t 187.741329 s\n","382 \t 14.006758 \t 6.794910 \t7.211848 \t 0.000000 \t 188.105247 s\n","383 \t 14.443910 \t 6.755295 \t7.688614 \t 0.000000 \t 188.510108 s\n","384 \t 14.415462 \t 7.151258 \t7.264204 \t 0.000000 \t 188.849870 s\n","385 \t 14.729738 \t 7.156897 \t7.572842 \t 0.000000 \t 189.189461 s\n","386 \t 14.044947 \t 6.885421 \t7.159526 \t 0.000000 \t 189.531213 s\n","387 \t 14.086060 \t 6.846440 \t7.239619 \t 0.000000 \t 189.879296 s\n","388 \t 13.818209 \t 6.822104 \t6.996104 \t 0.000000 \t 190.288021 s\n","389 \t 14.407790 \t 7.011668 \t7.396122 \t 0.000000 \t 190.629320 s\n","390 \t 14.051878 \t 6.961453 \t7.090425 \t 0.000000 \t 191.105679 s\n","391 \t 13.970222 \t 6.796436 \t7.173785 \t 0.000000 \t 191.462392 s\n","392 \t 14.094340 \t 6.730985 \t7.363355 \t 0.000000 \t 191.801972 s\n","393 \t 14.583364 \t 6.595035 \t7.988328 \t 0.000000 \t 192.145275 s\n","394 \t 14.606153 \t 7.146810 \t7.459342 \t 0.000000 \t 192.487457 s\n","395 \t 14.239704 \t 7.061064 \t7.178640 \t 0.000000 \t 192.824447 s\n","396 \t 14.138258 \t 6.818425 \t7.319832 \t 0.000000 \t 193.164044 s\n","397 \t 14.316810 \t 7.183681 \t7.133129 \t 0.000000 \t 193.501034 s\n","398 \t 14.045514 \t 6.815598 \t7.229915 \t 0.000000 \t 193.855877 s\n","399 \t 13.827264 \t 6.931140 \t6.896124 \t 0.000000 \t 194.201195 s\n","400 \t 13.933500 \t 6.998168 \t6.935332 \t 0.000000 \t 194.664580 s\n","401 \t 13.765798 \t 6.964143 \t6.801655 \t 0.000000 \t 195.006762 s\n","402 \t 14.333674 \t 6.755086 \t7.578588 \t 0.000000 \t 195.350377 s\n","403 \t 14.154945 \t 6.904876 \t7.250070 \t 0.000000 \t 195.689998 s\n","404 \t 14.103995 \t 6.663374 \t7.440621 \t 0.000000 \t 196.054733 s\n","405 \t 14.255227 \t 6.842825 \t7.412402 \t 0.000000 \t 196.422649 s\n","406 \t 14.522996 \t 6.987965 \t7.535031 \t 0.000000 \t 196.772350 s\n","407 \t 14.302775 \t 7.054393 \t7.248382 \t 0.000000 \t 197.120595 s\n","408 \t 14.617507 \t 6.981337 \t7.636170 \t 0.000000 \t 197.545777 s\n","409 \t 14.385855 \t 6.790188 \t7.595667 \t 0.000000 \t 197.898793 s\n","410 \t 14.402482 \t 7.031296 \t7.371186 \t 0.000000 \t 198.246396 s\n","411 \t 13.899109 \t 6.763088 \t7.136021 \t 0.000000 \t 198.717519 s\n","412 \t 13.697076 \t 6.561432 \t7.135644 \t 0.000000 \t 199.066929 s\n","413 \t 14.255280 \t 6.904585 \t7.350694 \t 0.000000 \t 199.414952 s\n","414 \t 14.073617 \t 6.800420 \t7.273197 \t 0.000000 \t 199.752905 s\n","415 \t 14.418849 \t 6.835038 \t7.583811 \t 0.000000 \t 200.106145 s\n","416 \t 14.196860 \t 7.043419 \t7.153441 \t 0.000000 \t 200.465263 s\n","417 \t 13.962415 \t 6.827399 \t7.135015 \t 0.000000 \t 200.802008 s\n","418 \t 14.195625 \t 6.879679 \t7.315946 \t 0.000000 \t 201.151583 s\n","419 \t 14.356598 \t 7.063877 \t7.292721 \t 0.000000 \t 201.487589 s\n","420 \t 14.045602 \t 6.847299 \t7.198303 \t 0.000000 \t 201.825962 s\n","421 \t 14.055959 \t 6.616553 \t7.439406 \t 0.000000 \t 202.166674 s\n","422 \t 14.235540 \t 7.053584 \t7.181956 \t 0.000000 \t 202.628903 s\n","423 \t 13.867416 \t 6.883791 \t6.983625 \t 0.000000 \t 202.971470 s\n","424 \t 14.280491 \t 6.902045 \t7.378445 \t 0.000000 \t 203.313295 s\n","425 \t 13.829357 \t 6.775312 \t7.054045 \t 0.000000 \t 203.655593 s\n","426 \t 14.112230 \t 6.793552 \t7.318678 \t 0.000000 \t 203.994197 s\n","427 \t 13.852010 \t 6.907547 \t6.944462 \t 0.000000 \t 204.330043 s\n","428 \t 14.529171 \t 7.254091 \t7.275080 \t 0.000000 \t 204.672697 s\n","429 \t 14.026187 \t 6.968536 \t7.057651 \t 0.000000 \t 205.017367 s\n","430 \t 14.263576 \t 6.811111 \t7.452465 \t 0.000000 \t 205.370892 s\n","431 \t 13.986302 \t 6.825677 \t7.160625 \t 0.000000 \t 205.709914 s\n","432 \t 13.760146 \t 6.726132 \t7.034013 \t 0.000000 \t 206.058289 s\n","433 \t 13.923759 \t 6.883415 \t7.040344 \t 0.000000 \t 206.524121 s\n","434 \t 13.531284 \t 6.700644 \t6.830639 \t 0.000000 \t 206.863956 s\n","435 \t 14.144287 \t 6.732367 \t7.411920 \t 0.000000 \t 207.249212 s\n","436 \t 14.730330 \t 6.941606 \t7.788724 \t 0.000000 \t 207.590352 s\n","437 \t 13.793613 \t 6.890638 \t6.902976 \t 0.000000 \t 207.926587 s\n","438 \t 14.335333 \t 6.826505 \t7.508828 \t 0.000000 \t 208.262322 s\n","439 \t 13.944325 \t 6.840262 \t7.104063 \t 0.000000 \t 208.598572 s\n","440 \t 14.445355 \t 7.242626 \t7.202729 \t 0.000000 \t 208.939485 s\n","441 \t 13.808531 \t 6.640329 \t7.168202 \t 0.000000 \t 209.287610 s\n","442 \t 13.710291 \t 6.663661 \t7.046630 \t 0.000000 \t 209.646711 s\n","443 \t 14.083848 \t 6.865411 \t7.218437 \t 0.000000 \t 209.995642 s\n","444 \t 13.898533 \t 6.874555 \t7.023978 \t 0.000000 \t 210.468759 s\n","445 \t 13.920884 \t 6.907249 \t7.013635 \t 0.000000 \t 210.817401 s\n","446 \t 14.388461 \t 6.702679 \t7.685782 \t 0.000000 \t 211.170604 s\n","447 \t 14.229266 \t 7.054413 \t7.174853 \t 0.000000 \t 211.513344 s\n","448 \t 14.337008 \t 6.945772 \t7.391236 \t 0.000000 \t 211.849378 s\n","449 \t 14.022914 \t 6.778077 \t7.244837 \t 0.000000 \t 212.191687 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:23<00:00,  5.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5764\n","Hit@10: 0.7385\n","Hit@3: 0.6154\n","Hit@1: 0.4846\n","Link Prediction on Validation Set (All)\n","MRR: 0.5342\n","Hit@10: 0.7729\n","Hit@3: 0.6055\n","Hit@1: 0.4090\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.6119\n","Hit@10: 0.7615\n","Hit@3: 0.6462\n","Hit@1: 0.5385\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5411\n","Hit@10: 0.7984\n","Hit@3: 0.6090\n","Hit@1: 0.4155\n","450 \t 14.536702 \t 6.831454 \t7.705248 \t 0.000000 \t 236.492944 s\n","451 \t 14.308956 \t 6.953726 \t7.355229 \t 0.000000 \t 236.828343 s\n","452 \t 14.708283 \t 7.117689 \t7.590594 \t 0.000000 \t 237.169585 s\n","453 \t 14.735021 \t 7.425055 \t7.309966 \t 0.000000 \t 237.506336 s\n","454 \t 14.264230 \t 7.067711 \t7.196519 \t 0.000000 \t 237.972520 s\n","455 \t 14.700572 \t 7.201447 \t7.499125 \t 0.000000 \t 238.312267 s\n","456 \t 14.574665 \t 6.917660 \t7.657005 \t 0.000000 \t 238.649892 s\n","457 \t 14.827155 \t 7.315910 \t7.511245 \t 0.000000 \t 238.992746 s\n","458 \t 15.198186 \t 7.671898 \t7.526288 \t 0.000000 \t 239.337083 s\n","459 \t 15.044864 \t 7.382967 \t7.661896 \t 0.000000 \t 239.675799 s\n","460 \t 14.859927 \t 7.085361 \t7.774566 \t 0.000000 \t 240.012618 s\n","461 \t 14.889058 \t 7.089999 \t7.799059 \t 0.000000 \t 240.349067 s\n","462 \t 14.802452 \t 7.146980 \t7.655472 \t 0.000000 \t 240.686840 s\n","463 \t 15.293852 \t 7.727389 \t7.566463 \t 0.000000 \t 241.027175 s\n","464 \t 14.388886 \t 6.922879 \t7.466007 \t 0.000000 \t 241.490584 s\n","465 \t 14.376938 \t 7.029261 \t7.347677 \t 0.000000 \t 241.830336 s\n","466 \t 14.985892 \t 7.101003 \t7.884890 \t 0.000000 \t 242.200550 s\n","467 \t 14.808181 \t 7.109111 \t7.699070 \t 0.000000 \t 242.675420 s\n","468 \t 14.700479 \t 7.018095 \t7.682384 \t 0.000000 \t 243.167806 s\n","469 \t 14.503102 \t 7.016635 \t7.486468 \t 0.000000 \t 243.626156 s\n","470 \t 14.994016 \t 7.402976 \t7.591039 \t 0.000000 \t 244.071321 s\n","471 \t 14.491855 \t 7.102199 \t7.389657 \t 0.000000 \t 244.423843 s\n","472 \t 14.804323 \t 7.393544 \t7.410779 \t 0.000000 \t 244.763246 s\n","473 \t 14.809254 \t 7.078536 \t7.730718 \t 0.000000 \t 245.107819 s\n","474 \t 14.372973 \t 7.009843 \t7.363130 \t 0.000000 \t 245.589424 s\n","475 \t 14.850727 \t 7.328996 \t7.521731 \t 0.000000 \t 245.930043 s\n","476 \t 15.341498 \t 7.671350 \t7.670148 \t 0.000000 \t 246.316830 s\n","477 \t 14.797666 \t 7.333415 \t7.464251 \t 0.000000 \t 246.680639 s\n","478 \t 14.489045 \t 6.994263 \t7.494782 \t 0.000000 \t 247.020950 s\n","479 \t 15.030495 \t 6.956557 \t8.073938 \t 0.000000 \t 247.364602 s\n","480 \t 14.636545 \t 7.380014 \t7.256531 \t 0.000000 \t 247.706950 s\n","481 \t 14.492118 \t 7.194530 \t7.297588 \t 0.000000 \t 248.045710 s\n","482 \t 14.601689 \t 7.324289 \t7.277401 \t 0.000000 \t 248.413367 s\n","483 \t 14.482172 \t 7.133378 \t7.348794 \t 0.000000 \t 248.749405 s\n","484 \t 14.328072 \t 6.976554 \t7.351518 \t 0.000000 \t 249.245664 s\n","485 \t 13.962607 \t 6.812261 \t7.150346 \t 0.000000 \t 249.582907 s\n","486 \t 14.694527 \t 7.154157 \t7.540370 \t 0.000000 \t 249.920371 s\n","487 \t 14.477311 \t 7.180469 \t7.296842 \t 0.000000 \t 250.269996 s\n","488 \t 14.452966 \t 7.058280 \t7.394686 \t 0.000000 \t 250.606882 s\n","489 \t 14.570778 \t 7.204826 \t7.365952 \t 0.000000 \t 250.958805 s\n","490 \t 14.977885 \t 7.230641 \t7.747244 \t 0.000000 \t 251.361966 s\n","491 \t 14.399827 \t 6.850709 \t7.549117 \t 0.000000 \t 251.775586 s\n","492 \t 15.158178 \t 7.202185 \t7.955994 \t 0.000000 \t 252.120144 s\n","493 \t 14.056126 \t 6.902228 \t7.153898 \t 0.000000 \t 252.462396 s\n","494 \t 14.182397 \t 6.978423 \t7.203974 \t 0.000000 \t 252.928936 s\n","495 \t 14.652387 \t 7.006701 \t7.645685 \t 0.000000 \t 253.268032 s\n","496 \t 14.456625 \t 7.105074 \t7.351551 \t 0.000000 \t 253.606429 s\n","497 \t 14.117331 \t 6.986840 \t7.130491 \t 0.000000 \t 253.943353 s\n","498 \t 14.338533 \t 6.931902 \t7.406631 \t 0.000000 \t 254.287913 s\n","499 \t 14.547476 \t 7.104361 \t7.443115 \t 0.000000 \t 254.626966 s\n","500 \t 14.815829 \t 7.069270 \t7.746558 \t 0.000000 \t 254.964774 s\n","501 \t 14.691522 \t 7.011378 \t7.680143 \t 0.000000 \t 255.350199 s\n","502 \t 13.961982 \t 6.748872 \t7.213110 \t 0.000000 \t 255.690834 s\n","503 \t 14.040499 \t 6.761529 \t7.278970 \t 0.000000 \t 256.028666 s\n","504 \t 14.445595 \t 7.328091 \t7.117503 \t 0.000000 \t 256.367408 s\n","505 \t 14.133828 \t 7.109737 \t7.024091 \t 0.000000 \t 256.832971 s\n","506 \t 14.774147 \t 7.297416 \t7.476730 \t 0.000000 \t 257.180972 s\n","507 \t 15.053137 \t 7.291746 \t7.761392 \t 0.000000 \t 257.523379 s\n","508 \t 14.202652 \t 6.890509 \t7.312142 \t 0.000000 \t 257.874242 s\n","509 \t 14.762300 \t 7.368931 \t7.393369 \t 0.000000 \t 258.243791 s\n","510 \t 14.201345 \t 6.810700 \t7.390645 \t 0.000000 \t 258.619058 s\n","511 \t 14.541812 \t 7.157317 \t7.384495 \t 0.000000 \t 258.957997 s\n","512 \t 14.597396 \t 6.823118 \t7.774278 \t 0.000000 \t 259.305558 s\n","513 \t 14.125257 \t 7.099437 \t7.025821 \t 0.000000 \t 259.650928 s\n","514 \t 14.058895 \t 6.977211 \t7.081683 \t 0.000000 \t 259.992871 s\n","515 \t 13.846974 \t 6.721560 \t7.125415 \t 0.000000 \t 260.342168 s\n","516 \t 13.996477 \t 6.785188 \t7.211289 \t 0.000000 \t 260.813118 s\n","517 \t 14.212197 \t 6.912020 \t7.300177 \t 0.000000 \t 261.156465 s\n","518 \t 14.218632 \t 7.180691 \t7.037941 \t 0.000000 \t 261.498767 s\n","519 \t 14.573065 \t 7.088788 \t7.484277 \t 0.000000 \t 261.838024 s\n","520 \t 14.277567 \t 6.987848 \t7.289720 \t 0.000000 \t 262.201040 s\n","521 \t 13.769183 \t 6.873336 \t6.895847 \t 0.000000 \t 262.537451 s\n","522 \t 13.997035 \t 6.809180 \t7.187854 \t 0.000000 \t 262.875649 s\n","523 \t 14.268139 \t 6.886972 \t7.381167 \t 0.000000 \t 263.213018 s\n","524 \t 13.932780 \t 7.113983 \t6.818797 \t 0.000000 \t 263.548182 s\n","525 \t 13.785349 \t 6.764839 \t7.020510 \t 0.000000 \t 263.895900 s\n","526 \t 14.006221 \t 6.992128 \t7.014093 \t 0.000000 \t 264.273775 s\n","527 \t 14.017863 \t 6.938787 \t7.079076 \t 0.000000 \t 264.609766 s\n","528 \t 13.796078 \t 6.462999 \t7.333080 \t 0.000000 \t 265.071865 s\n","529 \t 14.149487 \t 6.753507 \t7.395980 \t 0.000000 \t 265.412100 s\n","530 \t 13.880090 \t 6.670033 \t7.210057 \t 0.000000 \t 265.749917 s\n","531 \t 13.901263 \t 6.794303 \t7.106960 \t 0.000000 \t 266.095981 s\n","532 \t 14.095516 \t 7.079544 \t7.015972 \t 0.000000 \t 266.432352 s\n","533 \t 14.129744 \t 6.759710 \t7.370033 \t 0.000000 \t 266.772119 s\n","534 \t 14.279926 \t 6.910536 \t7.369390 \t 0.000000 \t 267.109228 s\n","535 \t 14.183597 \t 6.721593 \t7.462005 \t 0.000000 \t 267.452586 s\n","536 \t 13.728349 \t 6.915732 \t6.812617 \t 0.000000 \t 267.794224 s\n","537 \t 13.688787 \t 6.678556 \t7.010231 \t 0.000000 \t 268.263956 s\n","538 \t 13.849942 \t 6.863555 \t6.986387 \t 0.000000 \t 268.603840 s\n","539 \t 13.379784 \t 6.585591 \t6.794193 \t 0.000000 \t 268.956991 s\n","540 \t 14.012992 \t 6.979125 \t7.033867 \t 0.000000 \t 269.321985 s\n","541 \t 14.487205 \t 6.930885 \t7.556319 \t 0.000000 \t 269.668478 s\n","542 \t 14.278883 \t 6.989182 \t7.289700 \t 0.000000 \t 270.015048 s\n","543 \t 14.262777 \t 7.047842 \t7.214935 \t 0.000000 \t 270.356027 s\n","544 \t 13.821799 \t 6.622434 \t7.199365 \t 0.000000 \t 270.699456 s\n","545 \t 13.900786 \t 6.794498 \t7.106289 \t 0.000000 \t 271.196389 s\n","546 \t 13.899292 \t 6.835569 \t7.063723 \t 0.000000 \t 271.544179 s\n","547 \t 13.894401 \t 6.800792 \t7.093609 \t 0.000000 \t 271.884326 s\n","548 \t 14.121599 \t 6.858783 \t7.262815 \t 0.000000 \t 272.260327 s\n","549 \t 14.070089 \t 6.846736 \t7.223353 \t 0.000000 \t 272.598874 s\n","550 \t 14.153523 \t 6.859389 \t7.294135 \t 0.000000 \t 272.955299 s\n","551 \t 14.293237 \t 7.032897 \t7.260340 \t 0.000000 \t 273.351829 s\n","552 \t 14.206289 \t 7.065993 \t7.140296 \t 0.000000 \t 273.701895 s\n","553 \t 14.154376 \t 7.060785 \t7.093590 \t 0.000000 \t 274.038107 s\n","554 \t 14.369022 \t 6.864095 \t7.504927 \t 0.000000 \t 274.508562 s\n","555 \t 13.972264 \t 6.782909 \t7.189356 \t 0.000000 \t 274.847712 s\n","556 \t 13.754239 \t 6.664891 \t7.089347 \t 0.000000 \t 275.197320 s\n","557 \t 14.007599 \t 6.819904 \t7.187696 \t 0.000000 \t 275.541463 s\n","558 \t 13.944704 \t 6.573257 \t7.371446 \t 0.000000 \t 275.878771 s\n","559 \t 14.180359 \t 6.603812 \t7.576547 \t 0.000000 \t 276.288000 s\n","560 \t 13.952605 \t 6.642652 \t7.309954 \t 0.000000 \t 276.631280 s\n","561 \t 13.252453 \t 6.566728 \t6.685724 \t 0.000000 \t 276.974807 s\n","562 \t 14.007618 \t 6.873855 \t7.133763 \t 0.000000 \t 277.315606 s\n","563 \t 13.970201 \t 6.996208 \t6.973993 \t 0.000000 \t 277.780640 s\n","564 \t 13.813311 \t 6.432298 \t7.381013 \t 0.000000 \t 278.118300 s\n","565 \t 13.683172 \t 6.734294 \t6.948878 \t 0.000000 \t 278.460464 s\n","566 \t 13.877382 \t 6.886911 \t6.990471 \t 0.000000 \t 278.797526 s\n","567 \t 13.451390 \t 6.576061 \t6.875329 \t 0.000000 \t 279.206805 s\n","568 \t 13.520227 \t 6.732624 \t6.787604 \t 0.000000 \t 279.542468 s\n","569 \t 14.068816 \t 6.654538 \t7.414279 \t 0.000000 \t 279.881358 s\n","570 \t 13.725863 \t 6.755646 \t6.970217 \t 0.000000 \t 280.261258 s\n","571 \t 13.634107 \t 6.616741 \t7.017366 \t 0.000000 \t 280.619509 s\n","572 \t 13.280046 \t 6.348592 \t6.931454 \t 0.000000 \t 280.957898 s\n","573 \t 13.680082 \t 6.773520 \t6.906562 \t 0.000000 \t 281.428398 s\n","574 \t 13.487353 \t 6.480575 \t7.006778 \t 0.000000 \t 281.774018 s\n","575 \t 13.426159 \t 6.691505 \t6.734653 \t 0.000000 \t 282.114472 s\n","576 \t 13.875448 \t 6.711013 \t7.164435 \t 0.000000 \t 282.459238 s\n","577 \t 13.646070 \t 6.809489 \t6.836581 \t 0.000000 \t 282.798821 s\n","578 \t 13.630206 \t 6.631306 \t6.998900 \t 0.000000 \t 283.167814 s\n","579 \t 14.070135 \t 6.881737 \t7.188398 \t 0.000000 \t 283.515422 s\n","580 \t 13.252739 \t 6.355078 \t6.897661 \t 0.000000 \t 283.854482 s\n","581 \t 14.228855 \t 6.889832 \t7.339022 \t 0.000000 \t 284.202352 s\n","582 \t 13.554761 \t 6.602442 \t6.952319 \t 0.000000 \t 284.542636 s\n","583 \t 13.540904 \t 6.690653 \t6.850250 \t 0.000000 \t 285.011636 s\n","584 \t 13.788902 \t 6.738705 \t7.050197 \t 0.000000 \t 285.417775 s\n","585 \t 13.294159 \t 6.547127 \t6.747032 \t 0.000000 \t 285.752581 s\n","586 \t 13.558758 \t 6.616455 \t6.942303 \t 0.000000 \t 286.099668 s\n","587 \t 13.525925 \t 6.634538 \t6.891386 \t 0.000000 \t 286.444683 s\n","588 \t 13.514441 \t 6.600442 \t6.913999 \t 0.000000 \t 286.778993 s\n","589 \t 13.843610 \t 6.950501 \t6.893109 \t 0.000000 \t 287.117444 s\n","590 \t 13.351065 \t 6.519762 \t6.831303 \t 0.000000 \t 287.460582 s\n","591 \t 13.604477 \t 6.734238 \t6.870239 \t 0.000000 \t 287.800972 s\n","592 \t 13.761788 \t 6.505479 \t7.256309 \t 0.000000 \t 288.142076 s\n","593 \t 13.734104 \t 6.775911 \t6.958193 \t 0.000000 \t 288.485937 s\n","594 \t 13.681128 \t 6.590000 \t7.091127 \t 0.000000 \t 288.952500 s\n","595 \t 13.452672 \t 6.558388 \t6.894284 \t 0.000000 \t 289.318954 s\n","596 \t 13.661186 \t 6.620832 \t7.040353 \t 0.000000 \t 289.745809 s\n","597 \t 13.521125 \t 6.538798 \t6.982327 \t 0.000000 \t 290.098411 s\n","598 \t 13.677713 \t 6.778089 \t6.899625 \t 0.000000 \t 290.485965 s\n","599 \t 13.335313 \t 6.587831 \t6.747483 \t 0.000000 \t 290.827510 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:22<00:00,  5.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5814\n","Hit@10: 0.7769\n","Hit@3: 0.6269\n","Hit@1: 0.4846\n","Link Prediction on Validation Set (All)\n","MRR: 0.5384\n","Hit@10: 0.7762\n","Hit@3: 0.6023\n","Hit@1: 0.4203\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5924\n","Hit@10: 0.7231\n","Hit@3: 0.6308\n","Hit@1: 0.5154\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5318\n","Hit@10: 0.7760\n","Hit@3: 0.6029\n","Hit@1: 0.4094\n","600 \t 14.027103 \t 6.883921 \t7.143183 \t 0.000000 \t 314.710979 s\n","601 \t 13.230220 \t 6.751508 \t6.478713 \t 0.000000 \t 315.047693 s\n","602 \t 13.607718 \t 6.648317 \t6.959400 \t 0.000000 \t 315.385875 s\n","603 \t 13.104549 \t 6.548117 \t6.556433 \t 0.000000 \t 315.723030 s\n","604 \t 13.853741 \t 6.788648 \t7.065093 \t 0.000000 \t 316.076419 s\n","605 \t 13.365980 \t 6.728124 \t6.637856 \t 0.000000 \t 316.537531 s\n","606 \t 13.722540 \t 6.705957 \t7.016582 \t 0.000000 \t 316.875214 s\n","607 \t 13.172394 \t 6.586227 \t6.586166 \t 0.000000 \t 317.224793 s\n","608 \t 13.448809 \t 6.680891 \t6.767919 \t 0.000000 \t 317.636863 s\n","609 \t 13.121859 \t 6.402444 \t6.719414 \t 0.000000 \t 317.978824 s\n","610 \t 13.685896 \t 6.825845 \t6.860052 \t 0.000000 \t 318.318562 s\n","611 \t 13.420175 \t 6.718115 \t6.702060 \t 0.000000 \t 318.669926 s\n","612 \t 13.419669 \t 6.591537 \t6.828132 \t 0.000000 \t 319.010761 s\n","613 \t 13.562489 \t 6.751984 \t6.810505 \t 0.000000 \t 319.364735 s\n","614 \t 13.357201 \t 6.535012 \t6.822189 \t 0.000000 \t 319.719686 s\n","615 \t 13.255719 \t 6.582702 \t6.673016 \t 0.000000 \t 320.062591 s\n","616 \t 13.203874 \t 6.339036 \t6.864838 \t 0.000000 \t 320.547065 s\n","617 \t 13.202015 \t 6.541349 \t6.660666 \t 0.000000 \t 320.894781 s\n","618 \t 13.368844 \t 6.705564 \t6.663280 \t 0.000000 \t 321.278895 s\n","619 \t 13.157449 \t 6.396405 \t6.761045 \t 0.000000 \t 321.646840 s\n","620 \t 13.309733 \t 6.636747 \t6.672987 \t 0.000000 \t 322.005662 s\n","621 \t 13.697955 \t 6.511703 \t7.186252 \t 0.000000 \t 322.352821 s\n","622 \t 14.051658 \t 6.805917 \t7.245741 \t 0.000000 \t 322.691236 s\n","623 \t 13.445309 \t 6.446034 \t6.999274 \t 0.000000 \t 323.062357 s\n","624 \t 13.197284 \t 6.434194 \t6.763090 \t 0.000000 \t 323.420651 s\n","625 \t 13.476414 \t 6.605653 \t6.870761 \t 0.000000 \t 323.762420 s\n","626 \t 13.463836 \t 6.653797 \t6.810039 \t 0.000000 \t 324.107258 s\n","627 \t 13.391662 \t 6.631412 \t6.760250 \t 0.000000 \t 324.579161 s\n","628 \t 13.138676 \t 6.299158 \t6.839518 \t 0.000000 \t 324.919491 s\n","629 \t 13.068526 \t 6.423813 \t6.644714 \t 0.000000 \t 325.262029 s\n","630 \t 13.298074 \t 6.380137 \t6.917938 \t 0.000000 \t 325.602216 s\n","631 \t 13.261832 \t 6.541561 \t6.720271 \t 0.000000 \t 325.945738 s\n","632 \t 13.023715 \t 6.347149 \t6.676566 \t 0.000000 \t 326.367354 s\n","633 \t 13.097184 \t 6.275312 \t6.821872 \t 0.000000 \t 326.705279 s\n","634 \t 12.775144 \t 6.269693 \t6.505450 \t 0.000000 \t 327.058917 s\n","635 \t 13.085322 \t 6.400217 \t6.685105 \t 0.000000 \t 327.413137 s\n","636 \t 13.284142 \t 6.449944 \t6.834198 \t 0.000000 \t 327.750561 s\n","637 \t 12.981215 \t 6.297630 \t6.683584 \t 0.000000 \t 328.099032 s\n","638 \t 13.046730 \t 6.345811 \t6.700919 \t 0.000000 \t 328.560498 s\n","639 \t 13.093953 \t 6.373399 \t6.720553 \t 0.000000 \t 328.900831 s\n","640 \t 13.093788 \t 6.579081 \t6.514708 \t 0.000000 \t 329.251759 s\n","641 \t 13.238541 \t 6.528360 \t6.710181 \t 0.000000 \t 329.591231 s\n","642 \t 13.255731 \t 6.679769 \t6.575962 \t 0.000000 \t 329.948969 s\n","643 \t 13.605244 \t 6.559282 \t7.045962 \t 0.000000 \t 330.352878 s\n","644 \t 13.076979 \t 6.333621 \t6.743358 \t 0.000000 \t 330.708553 s\n","645 \t 13.104182 \t 6.365825 \t6.738357 \t 0.000000 \t 331.051795 s\n","646 \t 12.972337 \t 6.348163 \t6.624174 \t 0.000000 \t 331.400066 s\n","647 \t 13.024391 \t 6.331578 \t6.692813 \t 0.000000 \t 331.766422 s\n","648 \t 13.207539 \t 6.685972 \t6.521567 \t 0.000000 \t 332.123837 s\n","649 \t 13.048926 \t 6.265930 \t6.782997 \t 0.000000 \t 332.599513 s\n","650 \t 12.796424 \t 6.207018 \t6.589406 \t 0.000000 \t 332.937921 s\n","651 \t 13.058194 \t 6.482081 \t6.576113 \t 0.000000 \t 333.300301 s\n","652 \t 13.153016 \t 6.344552 \t6.808464 \t 0.000000 \t 333.642699 s\n","653 \t 13.125578 \t 6.473784 \t6.651794 \t 0.000000 \t 333.985654 s\n","654 \t 12.952723 \t 6.201713 \t6.751010 \t 0.000000 \t 334.323823 s\n","655 \t 13.231506 \t 6.751410 \t6.480096 \t 0.000000 \t 334.667252 s\n","656 \t 13.117763 \t 6.396567 \t6.721196 \t 0.000000 \t 335.006088 s\n","657 \t 12.972912 \t 6.371412 \t6.601500 \t 0.000000 \t 335.351935 s\n","658 \t 12.939133 \t 6.425350 \t6.513783 \t 0.000000 \t 335.695986 s\n","659 \t 13.049892 \t 6.436211 \t6.613681 \t 0.000000 \t 336.054938 s\n","660 \t 13.285608 \t 6.321748 \t6.963860 \t 0.000000 \t 336.534039 s\n","661 \t 12.805961 \t 6.240031 \t6.565930 \t 0.000000 \t 336.874175 s\n","662 \t 13.192092 \t 6.407139 \t6.784953 \t 0.000000 \t 337.213658 s\n","663 \t 12.850027 \t 6.445805 \t6.404222 \t 0.000000 \t 337.551630 s\n","664 \t 13.105666 \t 6.364985 \t6.740681 \t 0.000000 \t 337.900809 s\n","665 \t 13.170251 \t 6.533178 \t6.637073 \t 0.000000 \t 338.240740 s\n","666 \t 12.878768 \t 6.276315 \t6.602452 \t 0.000000 \t 338.576754 s\n","667 \t 13.117144 \t 6.520166 \t6.596978 \t 0.000000 \t 338.915338 s\n","668 \t 13.121236 \t 6.377953 \t6.743284 \t 0.000000 \t 339.277246 s\n","669 \t 13.491419 \t 6.615000 \t6.876419 \t 0.000000 \t 339.617697 s\n","670 \t 13.270355 \t 6.499660 \t6.770694 \t 0.000000 \t 339.958408 s\n","671 \t 12.890584 \t 6.389279 \t6.501305 \t 0.000000 \t 340.424749 s\n","672 \t 12.857726 \t 6.388984 \t6.468741 \t 0.000000 \t 340.761992 s\n","673 \t 13.016623 \t 6.487447 \t6.529176 \t 0.000000 \t 341.108340 s\n","674 \t 12.823495 \t 6.322464 \t6.501031 \t 0.000000 \t 341.457519 s\n","675 \t 13.025981 \t 6.324874 \t6.701108 \t 0.000000 \t 341.801895 s\n","676 \t 13.031801 \t 6.217601 \t6.814200 \t 0.000000 \t 342.194824 s\n","677 \t 13.147264 \t 6.654220 \t6.493045 \t 0.000000 \t 342.541432 s\n","678 \t 13.080192 \t 6.355549 \t6.724643 \t 0.000000 \t 342.885322 s\n","679 \t 13.026962 \t 6.559917 \t6.467046 \t 0.000000 \t 343.226751 s\n","680 \t 13.067820 \t 6.230057 \t6.837762 \t 0.000000 \t 343.575838 s\n","681 \t 12.865314 \t 6.259849 \t6.605465 \t 0.000000 \t 343.937664 s\n","682 \t 12.800623 \t 6.306120 \t6.494503 \t 0.000000 \t 344.334416 s\n","683 \t 12.861660 \t 6.193228 \t6.668432 \t 0.000000 \t 344.801664 s\n","684 \t 12.967071 \t 6.202050 \t6.765021 \t 0.000000 \t 345.152785 s\n","685 \t 13.264436 \t 6.383326 \t6.881110 \t 0.000000 \t 345.589761 s\n","686 \t 12.789444 \t 6.454287 \t6.335157 \t 0.000000 \t 345.934396 s\n","687 \t 12.865803 \t 6.351172 \t6.514631 \t 0.000000 \t 346.292831 s\n","688 \t 12.702879 \t 6.226866 \t6.476013 \t 0.000000 \t 346.632468 s\n","689 \t 13.004582 \t 6.407114 \t6.597468 \t 0.000000 \t 346.978991 s\n","690 \t 12.871765 \t 6.416048 \t6.455717 \t 0.000000 \t 347.332632 s\n","691 \t 13.310307 \t 6.498635 \t6.811671 \t 0.000000 \t 347.673432 s\n","692 \t 13.203766 \t 6.539978 \t6.663788 \t 0.000000 \t 348.029691 s\n","693 \t 13.001008 \t 6.344390 \t6.656618 \t 0.000000 \t 348.369028 s\n","694 \t 12.737386 \t 6.255505 \t6.481881 \t 0.000000 \t 348.706205 s\n","695 \t 12.956078 \t 6.387681 \t6.568398 \t 0.000000 \t 349.172483 s\n","696 \t 12.869205 \t 6.316184 \t6.553021 \t 0.000000 \t 349.509794 s\n","697 \t 12.927395 \t 6.435538 \t6.491857 \t 0.000000 \t 349.854831 s\n","698 \t 12.626218 \t 6.248464 \t6.377754 \t 0.000000 \t 350.249249 s\n","699 \t 12.987688 \t 6.323421 \t6.664266 \t 0.000000 \t 350.589569 s\n","700 \t 13.184545 \t 6.417432 \t6.767114 \t 0.000000 \t 350.926082 s\n","701 \t 12.862866 \t 6.333639 \t6.529227 \t 0.000000 \t 351.265907 s\n","702 \t 12.658461 \t 6.242887 \t6.415573 \t 0.000000 \t 351.606036 s\n","703 \t 12.586621 \t 6.072285 \t6.514336 \t 0.000000 \t 351.943328 s\n","704 \t 12.877026 \t 6.534324 \t6.342701 \t 0.000000 \t 352.283373 s\n","705 \t 12.728373 \t 6.271760 \t6.456613 \t 0.000000 \t 352.623614 s\n","706 \t 12.698933 \t 6.272694 \t6.426238 \t 0.000000 \t 352.974633 s\n","707 \t 12.645861 \t 6.351645 \t6.294215 \t 0.000000 \t 353.451573 s\n","708 \t 12.673293 \t 6.193598 \t6.479696 \t 0.000000 \t 353.787935 s\n","709 \t 12.574319 \t 6.124496 \t6.449823 \t 0.000000 \t 354.150774 s\n","710 \t 12.933803 \t 6.423832 \t6.509970 \t 0.000000 \t 354.493794 s\n","711 \t 12.656505 \t 6.190407 \t6.466099 \t 0.000000 \t 354.856509 s\n","712 \t 12.842751 \t 6.378072 \t6.464678 \t 0.000000 \t 355.203385 s\n","713 \t 12.812302 \t 6.344410 \t6.467891 \t 0.000000 \t 355.545548 s\n","714 \t 12.647927 \t 6.218162 \t6.429765 \t 0.000000 \t 355.889707 s\n","715 \t 12.761778 \t 6.450332 \t6.311445 \t 0.000000 \t 356.238014 s\n","716 \t 13.006866 \t 6.521082 \t6.485784 \t 0.000000 \t 356.576251 s\n","717 \t 12.611097 \t 6.272135 \t6.338962 \t 0.000000 \t 356.914611 s\n","718 \t 12.599157 \t 6.169491 \t6.429665 \t 0.000000 \t 357.253108 s\n","719 \t 12.950618 \t 6.317025 \t6.633593 \t 0.000000 \t 357.715382 s\n","720 \t 12.624681 \t 6.251088 \t6.373593 \t 0.000000 \t 358.068345 s\n","721 \t 12.469174 \t 6.048535 \t6.420639 \t 0.000000 \t 358.408741 s\n","722 \t 12.710765 \t 6.193688 \t6.517077 \t 0.000000 \t 358.744994 s\n","723 \t 13.043124 \t 6.355057 \t6.688068 \t 0.000000 \t 359.082300 s\n","724 \t 12.735318 \t 6.235348 \t6.499969 \t 0.000000 \t 359.422573 s\n","725 \t 12.843106 \t 6.211964 \t6.631142 \t 0.000000 \t 359.759582 s\n","726 \t 12.644340 \t 6.231696 \t6.412644 \t 0.000000 \t 360.125602 s\n","727 \t 12.792562 \t 6.431952 \t6.360610 \t 0.000000 \t 360.465694 s\n","728 \t 12.434540 \t 6.119738 \t6.314802 \t 0.000000 \t 360.802095 s\n","729 \t 12.531200 \t 6.190451 \t6.340750 \t 0.000000 \t 361.141222 s\n","730 \t 12.508517 \t 6.074444 \t6.434074 \t 0.000000 \t 361.480207 s\n","731 \t 12.428523 \t 6.062212 \t6.366311 \t 0.000000 \t 361.944849 s\n","732 \t 12.283870 \t 5.995013 \t6.288857 \t 0.000000 \t 362.287861 s\n","733 \t 12.353108 \t 5.999427 \t6.353680 \t 0.000000 \t 362.627031 s\n","734 \t 12.708845 \t 6.156994 \t6.551850 \t 0.000000 \t 362.968006 s\n","735 \t 12.391242 \t 6.194413 \t6.196829 \t 0.000000 \t 363.306017 s\n","736 \t 12.521438 \t 6.173132 \t6.348305 \t 0.000000 \t 363.642371 s\n","737 \t 12.452864 \t 6.218570 \t6.234294 \t 0.000000 \t 364.004869 s\n","738 \t 12.591853 \t 6.283970 \t6.307882 \t 0.000000 \t 364.393260 s\n","739 \t 12.650347 \t 6.130124 \t6.520223 \t 0.000000 \t 364.729793 s\n","740 \t 12.383393 \t 6.166134 \t6.217259 \t 0.000000 \t 365.067344 s\n","741 \t 12.485916 \t 6.259486 \t6.226430 \t 0.000000 \t 365.407881 s\n","742 \t 12.418505 \t 6.131089 \t6.287416 \t 0.000000 \t 365.745974 s\n","743 \t 12.428972 \t 6.102179 \t6.326794 \t 0.000000 \t 366.236789 s\n","744 \t 12.295105 \t 6.086674 \t6.208431 \t 0.000000 \t 366.584250 s\n","745 \t 12.430891 \t 6.174107 \t6.256783 \t 0.000000 \t 366.927590 s\n","746 \t 12.579844 \t 6.334076 \t6.245769 \t 0.000000 \t 367.292085 s\n","747 \t 12.606333 \t 6.173534 \t6.432799 \t 0.000000 \t 367.642652 s\n","748 \t 12.544070 \t 6.132126 \t6.411944 \t 0.000000 \t 368.004438 s\n","749 \t 12.494241 \t 6.051862 \t6.442379 \t 0.000000 \t 368.350483 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:22<00:00,  5.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5705\n","Hit@10: 0.7538\n","Hit@3: 0.6154\n","Hit@1: 0.4808\n","Link Prediction on Validation Set (All)\n","MRR: 0.5385\n","Hit@10: 0.7601\n","Hit@3: 0.5974\n","Hit@1: 0.4267\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5973\n","Hit@10: 0.7231\n","Hit@3: 0.6385\n","Hit@1: 0.5231\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5283\n","Hit@10: 0.7556\n","Hit@3: 0.6029\n","Hit@1: 0.4073\n","750 \t 12.635502 \t 6.108088 \t6.527414 \t 0.000000 \t 392.179097 s\n","751 \t 12.444000 \t 6.085471 \t6.358529 \t 0.000000 \t 392.523083 s\n","752 \t 12.423491 \t 6.143224 \t6.280267 \t 0.000000 \t 392.860587 s\n","753 \t 12.568114 \t 6.100080 \t6.468034 \t 0.000000 \t 393.204097 s\n","754 \t 12.362986 \t 6.145586 \t6.217399 \t 0.000000 \t 393.540516 s\n","755 \t 12.664730 \t 6.296403 \t6.368327 \t 0.000000 \t 394.004036 s\n","756 \t 12.264993 \t 6.048401 \t6.216591 \t 0.000000 \t 394.341133 s\n","757 \t 12.291893 \t 6.127954 \t6.163939 \t 0.000000 \t 394.682588 s\n","758 \t 12.459971 \t 6.072571 \t6.387400 \t 0.000000 \t 395.023647 s\n","759 \t 12.638936 \t 6.302610 \t6.336326 \t 0.000000 \t 395.420383 s\n","760 \t 12.266763 \t 6.117956 \t6.148806 \t 0.000000 \t 395.753411 s\n","761 \t 12.175950 \t 5.972552 \t6.203399 \t 0.000000 \t 396.088819 s\n","762 \t 12.240012 \t 6.142584 \t6.097427 \t 0.000000 \t 396.430771 s\n","763 \t 12.226592 \t 6.055553 \t6.171039 \t 0.000000 \t 396.771390 s\n","764 \t 12.455684 \t 6.220639 \t6.235044 \t 0.000000 \t 397.113317 s\n","765 \t 12.575705 \t 6.231868 \t6.343836 \t 0.000000 \t 397.457844 s\n","766 \t 12.541272 \t 6.261292 \t6.279980 \t 0.000000 \t 397.863432 s\n","767 \t 12.487843 \t 6.172673 \t6.315169 \t 0.000000 \t 398.372458 s\n","768 \t 12.420681 \t 6.071126 \t6.349555 \t 0.000000 \t 398.711457 s\n","769 \t 12.130235 \t 6.030511 \t6.099723 \t 0.000000 \t 399.057832 s\n","770 \t 12.458350 \t 6.083548 \t6.374802 \t 0.000000 \t 399.399783 s\n","771 \t 12.201385 \t 6.001452 \t6.199933 \t 0.000000 \t 399.753674 s\n","772 \t 12.364763 \t 6.067888 \t6.296875 \t 0.000000 \t 400.110541 s\n","773 \t 12.321575 \t 6.219103 \t6.102472 \t 0.000000 \t 400.453706 s\n","774 \t 12.133228 \t 6.059000 \t6.074227 \t 0.000000 \t 400.790680 s\n","775 \t 12.551277 \t 6.177602 \t6.373674 \t 0.000000 \t 401.196828 s\n","776 \t 12.312636 \t 6.095854 \t6.216782 \t 0.000000 \t 401.538302 s\n","777 \t 12.234500 \t 5.996071 \t6.238430 \t 0.000000 \t 401.877023 s\n","778 \t 12.435806 \t 6.298491 \t6.137315 \t 0.000000 \t 402.224771 s\n","779 \t 12.419070 \t 6.121140 \t6.297929 \t 0.000000 \t 402.701379 s\n","780 \t 12.698798 \t 6.323837 \t6.374961 \t 0.000000 \t 403.057624 s\n","781 \t 12.364764 \t 6.085146 \t6.279618 \t 0.000000 \t 403.397595 s\n","782 \t 12.358092 \t 6.203937 \t6.154155 \t 0.000000 \t 403.743414 s\n","783 \t 12.328197 \t 6.024057 \t6.304140 \t 0.000000 \t 404.108908 s\n","784 \t 12.389828 \t 6.080144 \t6.309684 \t 0.000000 \t 404.450474 s\n","785 \t 12.122282 \t 5.935481 \t6.186801 \t 0.000000 \t 404.828496 s\n","786 \t 12.327200 \t 6.055134 \t6.272066 \t 0.000000 \t 405.204507 s\n","787 \t 12.350808 \t 6.103856 \t6.246952 \t 0.000000 \t 405.621207 s\n","788 \t 12.281798 \t 6.181799 \t6.099998 \t 0.000000 \t 405.964751 s\n","789 \t 12.098564 \t 5.995422 \t6.103142 \t 0.000000 \t 406.314043 s\n","790 \t 12.208858 \t 6.034793 \t6.174066 \t 0.000000 \t 406.663580 s\n","791 \t 12.304510 \t 6.148873 \t6.155638 \t 0.000000 \t 407.127124 s\n","792 \t 12.388660 \t 6.178183 \t6.210478 \t 0.000000 \t 407.489479 s\n","793 \t 12.367421 \t 6.102830 \t6.264591 \t 0.000000 \t 407.839236 s\n","794 \t 12.263415 \t 6.101094 \t6.162321 \t 0.000000 \t 408.206254 s\n","795 \t 12.225928 \t 5.969090 \t6.256838 \t 0.000000 \t 408.546246 s\n","796 \t 12.095756 \t 6.030260 \t6.065495 \t 0.000000 \t 408.882620 s\n","797 \t 12.333772 \t 6.172564 \t6.161208 \t 0.000000 \t 409.223051 s\n","798 \t 12.196708 \t 6.056178 \t6.140530 \t 0.000000 \t 409.560744 s\n","799 \t 12.388173 \t 6.044755 \t6.343418 \t 0.000000 \t 409.897243 s\n","800 \t 12.230237 \t 5.992785 \t6.237452 \t 0.000000 \t 410.273772 s\n","801 \t 12.291707 \t 6.084786 \t6.206921 \t 0.000000 \t 410.615945 s\n","802 \t 12.451045 \t 6.153597 \t6.297447 \t 0.000000 \t 410.956038 s\n","803 \t 12.414808 \t 6.038732 \t6.376076 \t 0.000000 \t 411.422791 s\n","804 \t 12.485835 \t 6.146427 \t6.339408 \t 0.000000 \t 411.761088 s\n","805 \t 12.246960 \t 6.078461 \t6.168499 \t 0.000000 \t 412.114991 s\n","806 \t 12.189472 \t 6.006897 \t6.182575 \t 0.000000 \t 412.495642 s\n","807 \t 12.370033 \t 6.062946 \t6.307088 \t 0.000000 \t 412.842321 s\n","808 \t 12.032087 \t 5.925663 \t6.106424 \t 0.000000 \t 413.231945 s\n","809 \t 12.109812 \t 5.970415 \t6.139397 \t 0.000000 \t 413.569521 s\n","810 \t 12.171004 \t 5.966450 \t6.204554 \t 0.000000 \t 413.917018 s\n","811 \t 12.072774 \t 5.964976 \t6.107799 \t 0.000000 \t 414.313459 s\n","812 \t 12.208398 \t 6.089229 \t6.119169 \t 0.000000 \t 414.657099 s\n","813 \t 12.053993 \t 5.949190 \t6.104803 \t 0.000000 \t 415.005956 s\n","814 \t 12.123922 \t 6.025953 \t6.097970 \t 0.000000 \t 415.357698 s\n","815 \t 12.199836 \t 6.064442 \t6.135393 \t 0.000000 \t 415.842573 s\n","816 \t 12.148368 \t 5.956887 \t6.191481 \t 0.000000 \t 416.197237 s\n","817 \t 12.150297 \t 5.973130 \t6.177167 \t 0.000000 \t 416.606429 s\n","818 \t 12.169618 \t 6.080526 \t6.089092 \t 0.000000 \t 416.940482 s\n","819 \t 12.112708 \t 5.955000 \t6.157708 \t 0.000000 \t 417.322096 s\n","820 \t 11.867723 \t 5.858383 \t6.009341 \t 0.000000 \t 417.661454 s\n","821 \t 12.112484 \t 5.993058 \t6.119426 \t 0.000000 \t 418.004958 s\n","822 \t 12.237947 \t 6.090922 \t6.147026 \t 0.000000 \t 418.344490 s\n","823 \t 12.071955 \t 6.010741 \t6.061213 \t 0.000000 \t 418.683919 s\n","824 \t 12.154246 \t 6.069951 \t6.084296 \t 0.000000 \t 419.022382 s\n","825 \t 12.316514 \t 6.097178 \t6.219336 \t 0.000000 \t 419.359391 s\n","826 \t 12.218378 \t 6.011873 \t6.206505 \t 0.000000 \t 419.703790 s\n","827 \t 12.102098 \t 5.996874 \t6.105223 \t 0.000000 \t 420.176233 s\n","828 \t 12.414957 \t 6.050680 \t6.364276 \t 0.000000 \t 420.514337 s\n","829 \t 12.177701 \t 6.090162 \t6.087539 \t 0.000000 \t 420.861033 s\n","830 \t 12.138449 \t 6.040922 \t6.097527 \t 0.000000 \t 421.207506 s\n","831 \t 12.314994 \t 5.996904 \t6.318090 \t 0.000000 \t 421.546659 s\n","832 \t 12.338340 \t 6.201793 \t6.136547 \t 0.000000 \t 421.882886 s\n","833 \t 12.180542 \t 5.964949 \t6.215593 \t 0.000000 \t 422.226721 s\n","834 \t 12.322515 \t 6.148726 \t6.173789 \t 0.000000 \t 422.566406 s\n","835 \t 12.209353 \t 6.093767 \t6.115587 \t 0.000000 \t 422.916758 s\n","836 \t 12.131032 \t 6.092534 \t6.038498 \t 0.000000 \t 423.259020 s\n","837 \t 11.995605 \t 5.950797 \t6.044808 \t 0.000000 \t 423.595945 s\n","838 \t 11.994598 \t 5.992914 \t6.001684 \t 0.000000 \t 423.947978 s\n","839 \t 12.171937 \t 6.023397 \t6.148539 \t 0.000000 \t 424.493337 s\n","840 \t 11.951823 \t 5.901038 \t6.050785 \t 0.000000 \t 424.832247 s\n","841 \t 12.001087 \t 5.963956 \t6.037131 \t 0.000000 \t 425.173080 s\n","842 \t 11.952895 \t 5.915047 \t6.037848 \t 0.000000 \t 425.513420 s\n","843 \t 12.311227 \t 6.088338 \t6.222888 \t 0.000000 \t 425.867096 s\n","844 \t 12.013959 \t 5.959874 \t6.054086 \t 0.000000 \t 426.245075 s\n","845 \t 12.235041 \t 6.033068 \t6.201973 \t 0.000000 \t 426.584637 s\n","846 \t 12.124736 \t 6.047925 \t6.076811 \t 0.000000 \t 426.929875 s\n","847 \t 12.038717 \t 5.997203 \t6.041515 \t 0.000000 \t 427.272389 s\n","848 \t 12.211310 \t 6.127815 \t6.083495 \t 0.000000 \t 427.736960 s\n","849 \t 12.076431 \t 6.084678 \t5.991753 \t 0.000000 \t 428.098971 s\n","850 \t 12.048209 \t 5.921988 \t6.126222 \t 0.000000 \t 428.446963 s\n","851 \t 11.938161 \t 5.888684 \t6.049477 \t 0.000000 \t 428.786103 s\n","852 \t 12.130253 \t 6.037206 \t6.093047 \t 0.000000 \t 429.175521 s\n","853 \t 12.094025 \t 5.958711 \t6.135314 \t 0.000000 \t 429.513028 s\n","854 \t 12.080794 \t 5.992609 \t6.088185 \t 0.000000 \t 429.855794 s\n","855 \t 12.128401 \t 5.994142 \t6.134260 \t 0.000000 \t 430.208725 s\n","856 \t 11.979020 \t 5.879254 \t6.099766 \t 0.000000 \t 430.546920 s\n","857 \t 12.054232 \t 5.978145 \t6.076087 \t 0.000000 \t 430.881328 s\n","858 \t 12.302055 \t 5.978739 \t6.323315 \t 0.000000 \t 431.344573 s\n","859 \t 12.185081 \t 5.958940 \t6.226142 \t 0.000000 \t 431.685760 s\n","860 \t 11.996294 \t 5.925821 \t6.070473 \t 0.000000 \t 432.024441 s\n","861 \t 11.988533 \t 5.929027 \t6.059506 \t 0.000000 \t 432.373027 s\n","862 \t 11.885201 \t 5.914467 \t5.970734 \t 0.000000 \t 432.712119 s\n","863 \t 12.004759 \t 5.948940 \t6.055820 \t 0.000000 \t 433.050664 s\n","864 \t 11.874122 \t 5.859292 \t6.014830 \t 0.000000 \t 433.434847 s\n","865 \t 12.070088 \t 5.948644 \t6.121444 \t 0.000000 \t 433.772519 s\n","866 \t 12.196002 \t 5.959762 \t6.236240 \t 0.000000 \t 434.108768 s\n","867 \t 12.083733 \t 5.999912 \t6.083821 \t 0.000000 \t 434.453129 s\n","868 \t 11.934314 \t 5.916130 \t6.018184 \t 0.000000 \t 434.913118 s\n","869 \t 11.985109 \t 5.915932 \t6.069177 \t 0.000000 \t 435.248997 s\n","870 \t 11.946293 \t 5.850210 \t6.096082 \t 0.000000 \t 435.590141 s\n","871 \t 12.039183 \t 5.968928 \t6.070255 \t 0.000000 \t 435.930619 s\n","872 \t 11.951724 \t 5.887132 \t6.064592 \t 0.000000 \t 436.322496 s\n","873 \t 11.849091 \t 5.865592 \t5.983500 \t 0.000000 \t 436.660205 s\n","874 \t 11.880085 \t 5.839022 \t6.041063 \t 0.000000 \t 436.998722 s\n","875 \t 11.866744 \t 5.905531 \t5.961213 \t 0.000000 \t 437.340933 s\n","876 \t 12.051069 \t 6.007313 \t6.043756 \t 0.000000 \t 437.679427 s\n","877 \t 12.065616 \t 5.928343 \t6.137273 \t 0.000000 \t 438.060200 s\n","878 \t 11.978244 \t 5.924021 \t6.054223 \t 0.000000 \t 438.476091 s\n","879 \t 11.862563 \t 5.855216 \t6.007346 \t 0.000000 \t 438.814882 s\n","880 \t 12.011165 \t 5.810970 \t6.200195 \t 0.000000 \t 439.293248 s\n","881 \t 11.949593 \t 5.889974 \t6.059619 \t 0.000000 \t 439.632627 s\n","882 \t 11.895489 \t 5.841464 \t6.054025 \t 0.000000 \t 439.996963 s\n","883 \t 12.061105 \t 5.997327 \t6.063778 \t 0.000000 \t 440.333224 s\n","884 \t 11.981630 \t 5.945315 \t6.036315 \t 0.000000 \t 440.680752 s\n","885 \t 11.998628 \t 5.965639 \t6.032989 \t 0.000000 \t 441.026012 s\n","886 \t 11.981437 \t 5.890331 \t6.091106 \t 0.000000 \t 441.364123 s\n","887 \t 11.997067 \t 5.908586 \t6.088481 \t 0.000000 \t 441.703001 s\n","888 \t 12.267235 \t 5.959419 \t6.307815 \t 0.000000 \t 442.046176 s\n","889 \t 11.939554 \t 5.881290 \t6.058264 \t 0.000000 \t 442.384115 s\n","890 \t 11.922146 \t 5.878226 \t6.043920 \t 0.000000 \t 442.720847 s\n","891 \t 11.989128 \t 5.909684 \t6.079444 \t 0.000000 \t 443.056772 s\n","892 \t 11.939986 \t 5.867135 \t6.072851 \t 0.000000 \t 443.519083 s\n","893 \t 11.949471 \t 5.909188 \t6.040282 \t 0.000000 \t 443.859327 s\n","894 \t 11.870171 \t 5.847612 \t6.022558 \t 0.000000 \t 444.297845 s\n","895 \t 11.926687 \t 5.844547 \t6.082140 \t 0.000000 \t 444.667079 s\n","896 \t 11.992302 \t 5.883945 \t6.108356 \t 0.000000 \t 445.004178 s\n","897 \t 12.113860 \t 5.959012 \t6.154849 \t 0.000000 \t 445.343658 s\n","898 \t 11.932858 \t 5.915185 \t6.017673 \t 0.000000 \t 445.681618 s\n","899 \t 11.917261 \t 5.939240 \t5.978021 \t 0.000000 \t 446.032415 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:22<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5786\n","Hit@10: 0.7231\n","Hit@3: 0.6192\n","Hit@1: 0.5000\n","Link Prediction on Validation Set (All)\n","MRR: 0.5544\n","Hit@10: 0.7472\n","Hit@3: 0.6087\n","Hit@1: 0.4557\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5965\n","Hit@10: 0.7538\n","Hit@3: 0.6308\n","Hit@1: 0.5231\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5262\n","Hit@10: 0.7454\n","Hit@3: 0.5866\n","Hit@1: 0.4114\n","900 \t 11.958819 \t 5.950665 \t6.008153 \t 0.000000 \t 469.989818 s\n","901 \t 11.984024 \t 5.981930 \t6.002094 \t 0.000000 \t 470.331371 s\n","902 \t 11.876820 \t 5.825910 \t6.050909 \t 0.000000 \t 470.670986 s\n","903 \t 11.965214 \t 5.982921 \t5.982292 \t 0.000000 \t 471.013816 s\n","904 \t 11.998681 \t 5.916055 \t6.082626 \t 0.000000 \t 471.528299 s\n","905 \t 11.955268 \t 5.984123 \t5.971145 \t 0.000000 \t 471.864318 s\n","906 \t 11.797127 \t 5.818449 \t5.978678 \t 0.000000 \t 472.206872 s\n","907 \t 11.899876 \t 5.859970 \t6.039906 \t 0.000000 \t 472.548077 s\n","908 \t 11.946872 \t 5.923333 \t6.023539 \t 0.000000 \t 472.884851 s\n","909 \t 12.031983 \t 6.027381 \t6.004602 \t 0.000000 \t 473.224272 s\n","910 \t 12.008717 \t 5.990810 \t6.017907 \t 0.000000 \t 473.571784 s\n","911 \t 11.951641 \t 5.889502 \t6.062139 \t 0.000000 \t 473.908092 s\n","912 \t 11.789474 \t 5.842844 \t5.946630 \t 0.000000 \t 474.269151 s\n","913 \t 11.825774 \t 5.852716 \t5.973058 \t 0.000000 \t 474.613467 s\n","914 \t 11.969074 \t 5.962844 \t6.006229 \t 0.000000 \t 475.082648 s\n","915 \t 11.832568 \t 5.850042 \t5.982526 \t 0.000000 \t 475.425996 s\n","916 \t 11.923176 \t 5.857205 \t6.065971 \t 0.000000 \t 475.769346 s\n","917 \t 12.056911 \t 6.016280 \t6.040632 \t 0.000000 \t 476.232646 s\n","918 \t 11.907107 \t 5.889386 \t6.017721 \t 0.000000 \t 476.684320 s\n","919 \t 12.114151 \t 6.065583 \t6.048568 \t 0.000000 \t 477.092388 s\n","920 \t 11.867262 \t 5.841600 \t6.025662 \t 0.000000 \t 477.447451 s\n","921 \t 11.968567 \t 5.928706 \t6.039861 \t 0.000000 \t 477.816128 s\n","922 \t 12.051723 \t 6.028286 \t6.023436 \t 0.000000 \t 478.186005 s\n","923 \t 11.832689 \t 5.773344 \t6.059345 \t 0.000000 \t 478.541911 s\n","924 \t 11.977189 \t 5.911534 \t6.065655 \t 0.000000 \t 478.878469 s\n","925 \t 11.832826 \t 5.876098 \t5.956728 \t 0.000000 \t 479.220221 s\n","926 \t 11.891853 \t 5.809375 \t6.082478 \t 0.000000 \t 479.698782 s\n","927 \t 11.993498 \t 5.954107 \t6.039391 \t 0.000000 \t 480.046810 s\n","928 \t 11.944951 \t 5.970937 \t5.974014 \t 0.000000 \t 480.441964 s\n","929 \t 11.919789 \t 5.915362 \t6.004427 \t 0.000000 \t 480.851815 s\n","930 \t 11.995642 \t 5.916328 \t6.079314 \t 0.000000 \t 481.205884 s\n","931 \t 11.837238 \t 5.800042 \t6.037196 \t 0.000000 \t 481.546614 s\n","932 \t 11.854937 \t 5.889055 \t5.965883 \t 0.000000 \t 481.890642 s\n","933 \t 11.870221 \t 5.844519 \t6.025702 \t 0.000000 \t 482.266834 s\n","934 \t 11.944839 \t 5.865536 \t6.079303 \t 0.000000 \t 482.602957 s\n","935 \t 11.795584 \t 5.837726 \t5.957858 \t 0.000000 \t 482.940366 s\n","936 \t 12.002459 \t 5.929764 \t6.072694 \t 0.000000 \t 483.303829 s\n","937 \t 11.938733 \t 5.929330 \t6.009403 \t 0.000000 \t 483.646358 s\n","938 \t 11.774384 \t 5.854852 \t5.919533 \t 0.000000 \t 484.121713 s\n","939 \t 11.841511 \t 5.859464 \t5.982048 \t 0.000000 \t 484.459631 s\n","940 \t 11.896676 \t 5.910745 \t5.985931 \t 0.000000 \t 484.862824 s\n","941 \t 11.850120 \t 5.877200 \t5.972920 \t 0.000000 \t 485.216239 s\n","942 \t 12.051276 \t 5.939592 \t6.111684 \t 0.000000 \t 485.555978 s\n","943 \t 11.881385 \t 5.850535 \t6.030850 \t 0.000000 \t 485.892633 s\n","944 \t 11.759784 \t 5.805910 \t5.953874 \t 0.000000 \t 486.236016 s\n","945 \t 11.983921 \t 5.952344 \t6.031576 \t 0.000000 \t 486.580864 s\n","946 \t 11.868266 \t 5.914137 \t5.954129 \t 0.000000 \t 486.927812 s\n","947 \t 11.949261 \t 5.968163 \t5.981098 \t 0.000000 \t 487.274288 s\n","948 \t 12.036475 \t 5.987394 \t6.049081 \t 0.000000 \t 487.615796 s\n","949 \t 11.870321 \t 5.847172 \t6.023150 \t 0.000000 \t 487.954710 s\n","950 \t 11.826344 \t 5.883524 \t5.942820 \t 0.000000 \t 488.432161 s\n","951 \t 11.861630 \t 5.869984 \t5.991646 \t 0.000000 \t 488.770766 s\n","952 \t 11.989214 \t 5.884450 \t6.104764 \t 0.000000 \t 489.110579 s\n","953 \t 11.886926 \t 5.963787 \t5.923139 \t 0.000000 \t 489.450735 s\n","954 \t 12.113142 \t 6.093641 \t6.019500 \t 0.000000 \t 489.794289 s\n","955 \t 11.801434 \t 5.833805 \t5.967629 \t 0.000000 \t 490.132187 s\n","956 \t 11.851619 \t 5.892258 \t5.959361 \t 0.000000 \t 490.469684 s\n","957 \t 11.854787 \t 5.892725 \t5.962062 \t 0.000000 \t 490.808614 s\n","958 \t 12.078962 \t 6.038739 \t6.040223 \t 0.000000 \t 491.157234 s\n","959 \t 11.919207 \t 5.919609 \t5.999598 \t 0.000000 \t 491.506720 s\n","960 \t 11.869576 \t 5.852749 \t6.016828 \t 0.000000 \t 491.846444 s\n","961 \t 11.823318 \t 5.844600 \t5.978718 \t 0.000000 \t 492.190929 s\n","962 \t 11.785482 \t 5.803754 \t5.981729 \t 0.000000 \t 492.653387 s\n","963 \t 11.961745 \t 5.975531 \t5.986214 \t 0.000000 \t 492.990897 s\n","964 \t 11.886799 \t 5.851722 \t6.035077 \t 0.000000 \t 493.332025 s\n","965 \t 11.983768 \t 6.033487 \t5.950280 \t 0.000000 \t 493.668826 s\n","966 \t 12.023306 \t 5.885019 \t6.138287 \t 0.000000 \t 494.022426 s\n","967 \t 11.916032 \t 5.962747 \t5.953286 \t 0.000000 \t 494.359004 s\n","968 \t 11.792367 \t 5.896441 \t5.895926 \t 0.000000 \t 494.700600 s\n","969 \t 11.872768 \t 5.854596 \t6.018172 \t 0.000000 \t 495.047664 s\n","970 \t 11.896062 \t 5.902323 \t5.993740 \t 0.000000 \t 495.386107 s\n","971 \t 11.995147 \t 5.997325 \t5.997821 \t 0.000000 \t 495.724916 s\n","972 \t 11.890504 \t 5.893256 \t5.997248 \t 0.000000 \t 496.087113 s\n","973 \t 11.826866 \t 5.819960 \t6.006905 \t 0.000000 \t 496.458474 s\n","974 \t 11.845981 \t 5.920509 \t5.925472 \t 0.000000 \t 496.925234 s\n","975 \t 11.859491 \t 5.897890 \t5.961601 \t 0.000000 \t 497.274299 s\n","976 \t 12.037326 \t 6.101017 \t5.936310 \t 0.000000 \t 497.614379 s\n","977 \t 11.837937 \t 5.863910 \t5.974028 \t 0.000000 \t 497.953264 s\n","978 \t 11.811646 \t 5.820238 \t5.991409 \t 0.000000 \t 498.326988 s\n","979 \t 12.034050 \t 5.998841 \t6.035209 \t 0.000000 \t 498.673918 s\n","980 \t 11.853646 \t 5.876062 \t5.977584 \t 0.000000 \t 499.021900 s\n","981 \t 11.815579 \t 5.867523 \t5.948056 \t 0.000000 \t 499.364115 s\n","982 \t 11.722256 \t 5.786595 \t5.935661 \t 0.000000 \t 499.703152 s\n","983 \t 11.811671 \t 5.857616 \t5.954055 \t 0.000000 \t 500.183149 s\n","984 \t 11.727026 \t 5.817531 \t5.909495 \t 0.000000 \t 500.537275 s\n","985 \t 11.859156 \t 5.926909 \t5.932247 \t 0.000000 \t 500.874346 s\n","986 \t 11.819888 \t 5.904193 \t5.915696 \t 0.000000 \t 501.257121 s\n","987 \t 11.769240 \t 5.781096 \t5.988144 \t 0.000000 \t 501.597813 s\n","988 \t 11.868515 \t 5.929814 \t5.938701 \t 0.000000 \t 501.934465 s\n","989 \t 11.814611 \t 5.912601 \t5.902010 \t 0.000000 \t 502.291909 s\n","990 \t 11.875213 \t 5.846844 \t6.028369 \t 0.000000 \t 502.628952 s\n","991 \t 11.901947 \t 5.959997 \t5.941951 \t 0.000000 \t 503.096338 s\n","992 \t 11.762563 \t 5.779386 \t5.983177 \t 0.000000 \t 503.433020 s\n","993 \t 11.941599 \t 5.775701 \t6.165897 \t 0.000000 \t 503.775768 s\n","994 \t 11.684632 \t 5.783209 \t5.901423 \t 0.000000 \t 504.142893 s\n","995 \t 11.872476 \t 5.907569 \t5.964906 \t 0.000000 \t 504.485006 s\n","996 \t 11.915049 \t 5.962249 \t5.952800 \t 0.000000 \t 504.823579 s\n","997 \t 11.721593 \t 5.762896 \t5.958697 \t 0.000000 \t 505.196118 s\n","998 \t 11.825230 \t 5.869796 \t5.955433 \t 0.000000 \t 505.547449 s\n","999 \t 11.826332 \t 5.805895 \t6.020437 \t 0.000000 \t 505.903117 s\n","1000 \t 11.864935 \t 5.925399 \t5.939536 \t 0.000000 \t 506.385190 s\n","1001 \t 11.877370 \t 5.890786 \t5.986583 \t 0.000000 \t 506.726675 s\n","1002 \t 11.855531 \t 5.908966 \t5.946565 \t 0.000000 \t 507.061280 s\n","1003 \t 12.088383 \t 6.096366 \t5.992017 \t 0.000000 \t 507.420221 s\n","1004 \t 11.683979 \t 5.782105 \t5.901874 \t 0.000000 \t 507.759225 s\n","1005 \t 11.869670 \t 5.881115 \t5.988555 \t 0.000000 \t 508.097777 s\n","1006 \t 11.852326 \t 5.905864 \t5.946462 \t 0.000000 \t 508.446484 s\n","1007 \t 11.923502 \t 6.023925 \t5.899577 \t 0.000000 \t 508.800038 s\n","1008 \t 11.823194 \t 5.838247 \t5.984946 \t 0.000000 \t 509.166776 s\n","1009 \t 11.826468 \t 5.900387 \t5.926081 \t 0.000000 \t 509.643867 s\n","1010 \t 11.933959 \t 5.906977 \t6.026982 \t 0.000000 \t 509.978348 s\n","1011 \t 11.837744 \t 5.862399 \t5.975345 \t 0.000000 \t 510.343985 s\n","1012 \t 11.799212 \t 5.777040 \t6.022172 \t 0.000000 \t 510.686375 s\n","1013 \t 11.882284 \t 5.909176 \t5.973109 \t 0.000000 \t 511.027612 s\n","1014 \t 11.815493 \t 5.792674 \t6.022819 \t 0.000000 \t 511.369762 s\n","1015 \t 11.978658 \t 5.897838 \t6.080821 \t 0.000000 \t 511.712029 s\n","1016 \t 11.800041 \t 5.862131 \t5.937909 \t 0.000000 \t 512.094130 s\n","1017 \t 11.811602 \t 5.844757 \t5.966845 \t 0.000000 \t 512.442161 s\n","1018 \t 11.924781 \t 5.890928 \t6.033854 \t 0.000000 \t 512.902090 s\n","1019 \t 11.829623 \t 5.853733 \t5.975890 \t 0.000000 \t 513.241328 s\n","1020 \t 11.780428 \t 5.879895 \t5.900533 \t 0.000000 \t 513.580197 s\n","1021 \t 11.865260 \t 5.814096 \t6.051165 \t 0.000000 \t 513.918997 s\n","1022 \t 11.879349 \t 5.878772 \t6.000577 \t 0.000000 \t 514.297329 s\n","1023 \t 11.842319 \t 5.907185 \t5.935135 \t 0.000000 \t 514.638609 s\n","1024 \t 11.689262 \t 5.772364 \t5.916898 \t 0.000000 \t 514.990647 s\n","1025 \t 11.994905 \t 5.996083 \t5.998823 \t 0.000000 \t 515.330739 s\n","1026 \t 11.748389 \t 5.781817 \t5.966573 \t 0.000000 \t 515.672256 s\n","1027 \t 11.822354 \t 5.891327 \t5.931027 \t 0.000000 \t 516.157581 s\n","1028 \t 11.785957 \t 5.800331 \t5.985626 \t 0.000000 \t 516.496508 s\n","1029 \t 11.926316 \t 5.883173 \t6.043143 \t 0.000000 \t 516.835075 s\n","1030 \t 11.762600 \t 5.812373 \t5.950227 \t 0.000000 \t 517.171913 s\n","1031 \t 11.780879 \t 5.837138 \t5.943741 \t 0.000000 \t 517.509793 s\n","1032 \t 11.809424 \t 5.825583 \t5.983841 \t 0.000000 \t 517.846007 s\n","1033 \t 11.952116 \t 5.944275 \t6.007841 \t 0.000000 \t 518.211554 s\n","1034 \t 11.810360 \t 5.898344 \t5.912016 \t 0.000000 \t 518.548969 s\n","1035 \t 11.848952 \t 5.881036 \t5.967916 \t 0.000000 \t 519.007916 s\n","1036 \t 11.876199 \t 5.894348 \t5.981851 \t 0.000000 \t 519.341665 s\n","1037 \t 11.904957 \t 5.776332 \t6.128625 \t 0.000000 \t 519.681246 s\n","1038 \t 11.823149 \t 5.923282 \t5.899866 \t 0.000000 \t 520.029111 s\n","1039 \t 11.851305 \t 5.866820 \t5.984484 \t 0.000000 \t 520.369335 s\n","1040 \t 11.865960 \t 5.892169 \t5.973790 \t 0.000000 \t 520.708335 s\n","1041 \t 11.797324 \t 5.875076 \t5.922248 \t 0.000000 \t 521.050480 s\n","1042 \t 11.809630 \t 5.871291 \t5.938339 \t 0.000000 \t 521.400764 s\n","1043 \t 11.814918 \t 5.903911 \t5.911007 \t 0.000000 \t 521.739945 s\n","1044 \t 11.891602 \t 5.911177 \t5.980425 \t 0.000000 \t 522.220692 s\n","1045 \t 11.923967 \t 5.873788 \t6.050179 \t 0.000000 \t 522.654006 s\n","1046 \t 11.988638 \t 6.012095 \t5.976543 \t 0.000000 \t 523.051277 s\n","1047 \t 11.893793 \t 5.873671 \t6.020123 \t 0.000000 \t 523.398780 s\n","1048 \t 11.872142 \t 5.928762 \t5.943380 \t 0.000000 \t 523.740159 s\n","1049 \t 11.908285 \t 5.881662 \t6.026622 \t 0.000000 \t 524.104154 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:22<00:00,  5.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5874\n","Hit@10: 0.7231\n","Hit@3: 0.6308\n","Hit@1: 0.5115\n","Link Prediction on Validation Set (All)\n","MRR: 0.5571\n","Hit@10: 0.7407\n","Hit@3: 0.6039\n","Hit@1: 0.4638\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5895\n","Hit@10: 0.7538\n","Hit@3: 0.6308\n","Hit@1: 0.5077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5215\n","Hit@10: 0.7454\n","Hit@3: 0.5845\n","Hit@1: 0.4033\n"]}]},{"cell_type":"markdown","source":["# Test.py\n"],"metadata":{"id":"y5QjwDOdXRnt"}},{"cell_type":"code","source":["import pandas as pd\n","model_path = \"/content/drive/MyDrive/code/VTHNKG-O/checkpoint/Reproduce/VTHNKG-O_seed0/lr_0.0004_dim_256__1050.ckpt\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-O_seed0\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","def load_id_mapping(file_path):\n","    id2name = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip() == \"\" or line.startswith(\"#\"):  # 주석 또는 공백 무시\n","                continue\n","            parts = line.strip().split('\\t')\n","            if len(parts) != 2:\n","                continue\n","            name, idx = parts\n","            id2name[int(idx)] = name\n","    return id2name\n","\n","id2ent = load_id_mapping(\"entity2id.txt\")\n","id2rel = load_id_mapping(\"relation2id.txt\")\n","\n","def convert_triplet_ids_to_names(triplet, id2ent, id2rel, num_ent, num_rel):\n","    triplet_named = []\n","    for idx, val in enumerate(triplet):\n","        if idx % 2 == 0:  # entity or numeric value\n","            if val < num_ent:\n","                triplet_named.append(id2ent.get(val, f\"[ENT:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[NUM:{val - num_ent}]\")\n","        else:  # relation\n","            if val < num_rel:\n","                triplet_named.append(id2rel.get(val, f\"[REL:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[MASK_REL]\")\n","    return triplet_named\n","\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    entity_pred_log = []\n","    relation_pred_log = []\n","    numeric_pred_log = []\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    # sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    # nvp_tri_se += sq_error\n","                    # nvp_tri_se_num += 1\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","\n","                else: # qualifier\n","                  pred = score_num[0, 2, tri[ent_idx*2] - KG.num_ent]\n","                  gt = tri_num[ent_idx - 1]\n","                  sq_error = (pred - gt) ** 2\n","                  named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                  numeric_pred_log.append({\n","                      \"triplet_id\": str(tri),\n","                      \"triplet_named\": \":\".join(named_triplet),\n","                      \"position\": ent_idx,\n","                      \"type\": \"qualifier\",\n","                      \"gt\": float(gt),\n","                      \"pred\": float(pred),\n","                      \"se\": float(sq_error)\n","                  })\n","                    # sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                    topk = np.argsort(-score_ent[0,1+2*ent_idx])[:5]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    try:\n","                      topk = np.argsort(-score_ent[0,2])[:5]\n","                    except:\n","                      topk = np.argsort(-score_ent[0,2])[:]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","                topk = np.argsort(-score_rel[0,2])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"relation\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                topk = np.argsort(-score_rel[0,1])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"qualifier\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","pd.DataFrame(entity_pred_log).to_csv(\"entity_predictions.csv\", index=False)\n","pd.DataFrame(relation_pred_log).to_csv(\"relation_predictions.csv\", index=False)\n","pd.DataFrame(numeric_pred_log).to_csv(\"numeric_predictions.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChVIC_5BHELi","executionInfo":{"status":"ok","timestamp":1749785885903,"user_tz":-540,"elapsed":26349,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"957f322f-0e8a-4d26-83ac-934b5631212c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 132/132 [00:23<00:00,  5.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.6240\n","Hit@10: 0.7917\n","Hit@3: 0.6705\n","Hit@1: 0.5417\n","Link Prediction on Validation Set (All)\n","MRR: 0.5671\n","Hit@10: 0.7680\n","Hit@3: 0.6332\n","Hit@1: 0.4608\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.5530\n","Hit@10: 0.7121\n","Hit@3: 0.5606\n","Hit@1: 0.4848\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5130\n","Hit@10: 0.7154\n","Hit@3: 0.5751\n","Hit@1: 0.4091\n"]}]}]}