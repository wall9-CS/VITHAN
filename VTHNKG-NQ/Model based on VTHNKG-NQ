{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyMMrzpQ/vHizgxsZgNeCSNb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746165183273,"user_tz":-540,"elapsed":22382,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"f033883f-a9ed-4b14-e36e-e4839cb2740f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1746165189865,"user_tz":-540,"elapsed":4545,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1746165192387,"user_tz":-540,"elapsed":25,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","        self.proj_rel_vis = nn.Linear(dim_vis * 3, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)\n"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1746165192461,"user_tz":-540,"elapsed":71,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1746165605363,"user_tz":-540,"elapsed":18,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-NQ/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        # 경로 수정 visual feature는 VTHNKG 인용\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + \"visual_fetures_ent_sorted\")\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel_sorted')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile(self.dir + 'visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(self.dir + 'visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-NQ/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1746165616964,"user_tz":-540,"elapsed":122,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"d59118d4-ddca-41b5-9571-0165ad24df43"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-NQ\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHN\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746166331787,"user_tz":-540,"elapsed":708706,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"3f9cc0d3-7b27-4ef7-ac36-8135548d11e4"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 43.471053 \t 11.314445 \t11.034001 \t 21.122606 \t 2.250802 s\n","1 \t 32.988660 \t 10.466745 \t10.028197 \t 12.493719 \t 2.944814 s\n","2 \t 24.992743 \t 10.077811 \t11.092454 \t 3.822479 \t 3.437781 s\n","3 \t 20.211907 \t 9.627444 \t8.938756 \t 1.645708 \t 3.903898 s\n","4 \t 22.149066 \t 10.568012 \t10.222840 \t 1.358214 \t 4.375798 s\n","5 \t 19.596506 \t 9.711812 \t9.327833 \t 0.556862 \t 4.854475 s\n","6 \t 19.490508 \t 9.630080 \t9.089796 \t 0.770631 \t 5.342437 s\n","7 \t 18.691816 \t 9.319491 \t9.205493 \t 0.166831 \t 5.813843 s\n","8 \t 18.251637 \t 9.512522 \t8.379911 \t 0.359203 \t 6.288635 s\n","9 \t 18.556598 \t 9.568786 \t8.798585 \t 0.189226 \t 6.760201 s\n","10 \t 17.229228 \t 8.489210 \t8.614429 \t 0.125589 \t 7.234899 s\n","11 \t 18.468242 \t 9.049683 \t9.027719 \t 0.390840 \t 7.712448 s\n","12 \t 18.170797 \t 9.078333 \t8.867821 \t 0.224644 \t 8.358764 s\n","13 \t 18.213411 \t 9.083211 \t8.965619 \t 0.164582 \t 8.830083 s\n","14 \t 18.010270 \t 9.162821 \t8.258768 \t 0.588682 \t 9.303557 s\n","15 \t 17.468559 \t 8.642383 \t8.520671 \t 0.305505 \t 9.821845 s\n","16 \t 18.435597 \t 9.475566 \t8.776972 \t 0.183058 \t 10.414119 s\n","17 \t 18.725445 \t 9.538591 \t8.895078 \t 0.291776 \t 10.984071 s\n","18 \t 18.548992 \t 9.526523 \t8.597068 \t 0.425402 \t 11.565341 s\n","19 \t 19.133612 \t 9.407320 \t9.403245 \t 0.323046 \t 12.153053 s\n","20 \t 17.606854 \t 9.063705 \t8.379245 \t 0.163904 \t 12.771606 s\n","21 \t 18.099423 \t 9.147380 \t8.514906 \t 0.437137 \t 13.231769 s\n","22 \t 19.045094 \t 9.454762 \t9.308498 \t 0.281836 \t 13.707372 s\n","23 \t 18.189543 \t 9.445565 \t8.474764 \t 0.269213 \t 14.173863 s\n","24 \t 18.186071 \t 9.406347 \t8.226820 \t 0.552905 \t 14.652308 s\n","25 \t 18.719526 \t 9.435558 \t8.974490 \t 0.309478 \t 15.275593 s\n","26 \t 16.727562 \t 8.643165 \t7.916910 \t 0.167487 \t 15.751811 s\n","27 \t 18.577414 \t 9.549850 \t8.807387 \t 0.220177 \t 16.222494 s\n","28 \t 18.143315 \t 8.968554 \t8.712279 \t 0.462481 \t 16.708500 s\n","29 \t 17.948334 \t 9.290396 \t8.478224 \t 0.179714 \t 17.185198 s\n","30 \t 17.503478 \t 9.323450 \t8.084183 \t 0.095846 \t 17.662652 s\n","31 \t 18.102150 \t 9.397695 \t8.492502 \t 0.211952 \t 18.151287 s\n","32 \t 17.301514 \t 8.710251 \t8.468484 \t 0.122778 \t 18.620623 s\n","33 \t 18.468837 \t 9.454868 \t8.930776 \t 0.083193 \t 19.103890 s\n","34 \t 17.322794 \t 9.135207 \t7.997046 \t 0.190541 \t 19.566874 s\n","35 \t 17.305657 \t 9.108017 \t8.103427 \t 0.094213 \t 20.049810 s\n","36 \t 18.463943 \t 9.501637 \t8.894743 \t 0.067562 \t 20.516559 s\n","37 \t 17.244205 \t 9.411097 \t7.719883 \t 0.113225 \t 20.990206 s\n","38 \t 17.520677 \t 9.129696 \t8.315435 \t 0.075545 \t 21.448647 s\n","39 \t 18.337649 \t 9.261724 \t8.847295 \t 0.228631 \t 22.086307 s\n","40 \t 18.023018 \t 9.343797 \t8.608904 \t 0.070317 \t 22.545400 s\n","41 \t 17.084468 \t 8.931620 \t7.986627 \t 0.166221 \t 23.075246 s\n","42 \t 17.033713 \t 8.906808 \t7.969133 \t 0.157772 \t 23.646267 s\n","43 \t 18.082911 \t 9.239175 \t8.619633 \t 0.224104 \t 24.220992 s\n","44 \t 17.145521 \t 8.458368 \t8.523147 \t 0.164007 \t 24.784352 s\n","45 \t 18.825799 \t 9.371137 \t9.218908 \t 0.235755 \t 25.443533 s\n","46 \t 17.286079 \t 9.236514 \t7.880836 \t 0.168729 \t 26.036729 s\n","47 \t 17.088537 \t 8.236738 \t8.502150 \t 0.349650 \t 26.500279 s\n","48 \t 17.305017 \t 9.013216 \t8.158468 \t 0.133332 \t 26.971362 s\n","49 \t 16.900805 \t 8.949956 \t7.823507 \t 0.127342 \t 27.619625 s\n","50 \t 18.735326 \t 9.444443 \t9.044718 \t 0.246165 \t 28.098284 s\n","51 \t 17.382642 \t 9.016089 \t8.224089 \t 0.142463 \t 28.560150 s\n","52 \t 17.282819 \t 8.493077 \t8.660180 \t 0.129562 \t 29.035713 s\n","53 \t 17.218349 \t 8.656249 \t8.438412 \t 0.123687 \t 29.520353 s\n","54 \t 17.508480 \t 9.157265 \t8.270913 \t 0.080303 \t 29.991493 s\n","55 \t 16.795349 \t 8.840971 \t7.785587 \t 0.168790 \t 30.469853 s\n","56 \t 16.663240 \t 8.365911 \t8.188609 \t 0.108721 \t 30.961654 s\n","57 \t 17.591752 \t 8.695655 \t8.575066 \t 0.321031 \t 31.434710 s\n","58 \t 17.106717 \t 8.793118 \t8.110539 \t 0.203060 \t 31.916166 s\n","59 \t 17.175331 \t 9.231509 \t7.848392 \t 0.095430 \t 32.384544 s\n","60 \t 16.724090 \t 8.409476 \t8.117558 \t 0.197055 \t 33.029624 s\n","61 \t 17.583218 \t 8.610481 \t8.791263 \t 0.181473 \t 33.501151 s\n","62 \t 17.975812 \t 9.620407 \t8.241211 \t 0.114194 \t 33.980815 s\n","63 \t 16.743557 \t 8.345910 \t8.202106 \t 0.195540 \t 34.450806 s\n","64 \t 17.143710 \t 8.811886 \t8.229502 \t 0.102322 \t 34.920880 s\n","65 \t 16.601530 \t 8.506257 \t7.930723 \t 0.164549 \t 35.432204 s\n","66 \t 16.891768 \t 9.008165 \t7.793813 \t 0.089789 \t 35.943031 s\n","67 \t 17.814160 \t 9.093547 \t8.622553 \t 0.098061 \t 36.570882 s\n","68 \t 17.615005 \t 8.940133 \t8.540662 \t 0.134210 \t 37.136918 s\n","69 \t 17.915856 \t 9.197618 \t8.648426 \t 0.069812 \t 37.735318 s\n","70 \t 17.899090 \t 9.102619 \t8.677450 \t 0.119021 \t 38.322336 s\n","71 \t 17.069033 \t 8.740016 \t8.241559 \t 0.087458 \t 38.945097 s\n","72 \t 17.003376 \t 8.901562 \t8.035915 \t 0.065898 \t 39.584520 s\n","73 \t 16.944242 \t 8.738113 \t8.052896 \t 0.153234 \t 40.052237 s\n","74 \t 16.706194 \t 8.571933 \t7.878824 \t 0.255437 \t 40.524608 s\n","75 \t 17.153704 \t 8.841035 \t8.208683 \t 0.103985 \t 41.042958 s\n","76 \t 17.240536 \t 9.258204 \t7.918114 \t 0.064219 \t 41.513288 s\n","77 \t 17.615415 \t 9.249962 \t8.191538 \t 0.173915 \t 42.003578 s\n","78 \t 17.197107 \t 8.949349 \t8.151313 \t 0.096445 \t 42.468845 s\n","79 \t 17.455113 \t 9.067474 \t8.301999 \t 0.085640 \t 42.948796 s\n","80 \t 16.954439 \t 8.854287 \t8.020883 \t 0.079270 \t 43.416878 s\n","81 \t 17.342525 \t 9.263861 \t8.007099 \t 0.071566 \t 43.890874 s\n","82 \t 17.461864 \t 9.072755 \t8.288527 \t 0.100582 \t 44.410844 s\n","83 \t 16.452763 \t 8.501238 \t7.689744 \t 0.261780 \t 44.881243 s\n","84 \t 16.588923 \t 8.796041 \t7.730293 \t 0.062588 \t 45.349527 s\n","85 \t 16.372547 \t 8.663614 \t7.645016 \t 0.063916 \t 45.993764 s\n","86 \t 16.970316 \t 8.703030 \t8.195343 \t 0.071942 \t 46.461152 s\n","87 \t 17.350917 \t 9.409752 \t7.711483 \t 0.229682 \t 46.940877 s\n","88 \t 17.628567 \t 9.493955 \t8.077620 \t 0.056991 \t 47.413549 s\n","89 \t 16.818870 \t 9.222988 \t7.512680 \t 0.083202 \t 47.897817 s\n","90 \t 16.739754 \t 8.804155 \t7.795556 \t 0.140041 \t 48.393720 s\n","91 \t 17.321644 \t 9.352307 \t7.860460 \t 0.108876 \t 48.910473 s\n","92 \t 16.431721 \t 8.627792 \t7.674604 \t 0.129325 \t 49.520718 s\n","93 \t 17.291078 \t 9.531118 \t7.681949 \t 0.078011 \t 50.095294 s\n","94 \t 16.804835 \t 8.861389 \t7.838514 \t 0.104932 \t 50.672382 s\n","95 \t 17.659612 \t 9.171739 \t8.210026 \t 0.277847 \t 51.258770 s\n","96 \t 17.925276 \t 9.543022 \t8.313459 \t 0.068794 \t 51.884761 s\n","97 \t 17.836046 \t 9.074143 \t8.525024 \t 0.236877 \t 52.363261 s\n","98 \t 16.937676 \t 8.717542 \t8.156462 \t 0.063672 \t 53.012791 s\n","99 \t 17.805865 \t 9.152626 \t8.584995 \t 0.068245 \t 53.497786 s\n","100 \t 16.664968 \t 9.049482 \t7.513680 \t 0.101805 \t 53.975631 s\n","101 \t 16.846539 \t 8.832000 \t7.894952 \t 0.119587 \t 54.459372 s\n","102 \t 16.883762 \t 8.864778 \t7.962445 \t 0.056539 \t 54.936593 s\n","103 \t 16.570289 \t 8.955652 \t7.539515 \t 0.075122 \t 55.423710 s\n","104 \t 17.702741 \t 9.252719 \t8.350925 \t 0.099096 \t 55.895650 s\n","105 \t 16.158908 \t 8.442585 \t7.630768 \t 0.085555 \t 56.363735 s\n","106 \t 16.823524 \t 8.923953 \t7.837425 \t 0.062145 \t 56.842700 s\n","107 \t 16.308818 \t 8.794761 \t7.409164 \t 0.104893 \t 57.322815 s\n","108 \t 17.156516 \t 8.958655 \t8.100147 \t 0.097714 \t 57.802227 s\n","109 \t 16.835474 \t 9.059578 \t7.691801 \t 0.084096 \t 58.284367 s\n","110 \t 16.727789 \t 8.808902 \t7.862040 \t 0.056848 \t 58.766655 s\n","111 \t 17.825191 \t 9.275236 \t8.475312 \t 0.074643 \t 59.238254 s\n","112 \t 16.842525 \t 8.830244 \t7.951012 \t 0.061270 \t 59.722567 s\n","113 \t 17.685142 \t 9.379973 \t8.259953 \t 0.045216 \t 60.363732 s\n","114 \t 17.175797 \t 8.687497 \t8.374030 \t 0.114271 \t 60.843457 s\n","115 \t 16.668592 \t 8.616160 \t7.955398 \t 0.097035 \t 61.320671 s\n","116 \t 17.070285 \t 8.973743 \t7.921165 \t 0.175378 \t 61.797599 s\n","117 \t 16.079639 \t 8.619210 \t7.319797 \t 0.140632 \t 62.364006 s\n","118 \t 17.096725 \t 9.002828 \t8.021252 \t 0.072646 \t 62.961102 s\n","119 \t 16.647825 \t 8.352430 \t8.243210 \t 0.052185 \t 63.533613 s\n","120 \t 17.433684 \t 9.136265 \t8.180320 \t 0.117100 \t 64.116431 s\n","121 \t 16.374308 \t 8.637123 \t7.582118 \t 0.155069 \t 64.769371 s\n","122 \t 17.272655 \t 9.323907 \t7.862476 \t 0.086271 \t 65.327545 s\n","123 \t 16.810423 \t 8.472545 \t8.279103 \t 0.058775 \t 65.993479 s\n","124 \t 17.482386 \t 9.011585 \t8.438465 \t 0.032335 \t 66.464436 s\n","125 \t 17.516189 \t 8.925226 \t8.531035 \t 0.059927 \t 66.960454 s\n","126 \t 16.723657 \t 8.652542 \t7.969301 \t 0.101813 \t 67.434028 s\n","127 \t 16.444175 \t 8.818142 \t7.576705 \t 0.049327 \t 67.918704 s\n","128 \t 15.921293 \t 7.976507 \t7.881961 \t 0.062824 \t 68.391081 s\n","129 \t 17.578803 \t 9.241371 \t8.273935 \t 0.063497 \t 68.875084 s\n","130 \t 16.734683 \t 8.678977 \t7.997579 \t 0.058128 \t 69.359941 s\n","131 \t 17.102103 \t 8.707503 \t8.331776 \t 0.062824 \t 69.849422 s\n","132 \t 16.466292 \t 8.741855 \t7.667407 \t 0.057031 \t 70.323781 s\n","133 \t 16.298173 \t 8.508063 \t7.707873 \t 0.082237 \t 70.814148 s\n","134 \t 17.274186 \t 8.728155 \t8.496114 \t 0.049917 \t 71.460070 s\n","135 \t 17.180158 \t 9.304314 \t7.790373 \t 0.085470 \t 71.935449 s\n","136 \t 17.349196 \t 8.943269 \t8.327446 \t 0.078481 \t 72.403417 s\n","137 \t 17.278633 \t 9.222907 \t7.881764 \t 0.173962 \t 72.891802 s\n","138 \t 16.488492 \t 8.656516 \t7.787600 \t 0.044375 \t 73.652587 s\n","139 \t 16.482817 \t 8.608237 \t7.819662 \t 0.054917 \t 74.141337 s\n","140 \t 17.473203 \t 8.516956 \t8.881371 \t 0.074875 \t 74.615323 s\n","141 \t 17.224771 \t 8.726818 \t8.401804 \t 0.096150 \t 75.144263 s\n","142 \t 15.421147 \t 8.087733 \t7.277170 \t 0.056245 \t 75.733510 s\n","143 \t 16.991606 \t 9.002377 \t7.807945 \t 0.181284 \t 76.309838 s\n","144 \t 16.729802 \t 9.063897 \t7.599696 \t 0.066209 \t 76.887546 s\n","145 \t 16.925057 \t 8.846291 \t8.011712 \t 0.067056 \t 77.735526 s\n","146 \t 16.703710 \t 8.926692 \t7.704147 \t 0.072871 \t 78.337951 s\n","147 \t 16.582769 \t 8.742849 \t7.796143 \t 0.043777 \t 78.811258 s\n","148 \t 16.542730 \t 8.940290 \t7.542175 \t 0.060265 \t 79.294312 s\n","149 \t 16.547664 \t 8.748661 \t7.703792 \t 0.095212 \t 79.780619 s\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/130 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 130/130 [00:15<00:00,  8.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4255\n","Hit@10: 0.5500\n","Hit@3: 0.4192\n","Hit@1: 0.3692\n","Link Prediction on Validation Set (All)\n","MRR: 0.4255\n","Hit@10: 0.5500\n","Hit@3: 0.4192\n","Hit@1: 0.3692\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2269\n","Hit@10: 0.4462\n","Hit@3: 0.2231\n","Hit@1: 0.1385\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5833\n","Hit@10: 0.7143\n","Hit@3: 0.5992\n","Hit@1: 0.5238\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1437\n","150 \t 17.559045 \t 8.901073 \t8.411470 \t 0.246501 \t 97.298716 s\n","151 \t 17.901066 \t 9.308833 \t7.655844 \t 0.936388 \t 97.793468 s\n","152 \t 17.900250 \t 8.928730 \t8.814549 \t 0.156971 \t 98.264526 s\n","153 \t 16.939334 \t 8.666158 \t8.085456 \t 0.187720 \t 98.748679 s\n","154 \t 17.773264 \t 9.137096 \t8.478784 \t 0.157383 \t 99.229490 s\n","155 \t 17.231368 \t 9.126711 \t7.906194 \t 0.198463 \t 99.710243 s\n","156 \t 19.199726 \t 9.311880 \t9.105621 \t 0.782226 \t 100.204072 s\n","157 \t 17.690230 \t 9.094787 \t8.384827 \t 0.210617 \t 100.680917 s\n","158 \t 18.658957 \t 9.352628 \t9.153317 \t 0.153011 \t 101.388446 s\n","159 \t 17.685015 \t 9.350394 \t8.069124 \t 0.265497 \t 101.981943 s\n","160 \t 16.731356 \t 8.944538 \t7.637148 \t 0.149670 \t 102.543241 s\n","161 \t 17.084479 \t 9.173450 \t7.540266 \t 0.370764 \t 103.216725 s\n","162 \t 17.034540 \t 8.598197 \t8.085046 \t 0.351297 \t 104.216795 s\n","163 \t 17.519558 \t 9.234505 \t8.207156 \t 0.077896 \t 105.273200 s\n","164 \t 16.399004 \t 8.747938 \t7.554383 \t 0.096683 \t 106.381232 s\n","165 \t 17.248915 \t 8.869585 \t8.196515 \t 0.182815 \t 107.182929 s\n","166 \t 16.826142 \t 8.975210 \t7.590068 \t 0.260865 \t 107.692849 s\n","167 \t 17.423884 \t 9.305974 \t7.998867 \t 0.119044 \t 108.214860 s\n","168 \t 16.177702 \t 8.597392 \t7.324549 \t 0.255762 \t 108.791030 s\n","169 \t 17.332578 \t 8.943139 \t8.275857 \t 0.113582 \t 109.308755 s\n","170 \t 18.172444 \t 9.211173 \t8.744953 \t 0.216318 \t 109.818297 s\n","171 \t 16.617460 \t 9.028216 \t7.508369 \t 0.080876 \t 110.290006 s\n","172 \t 17.474794 \t 9.384704 \t7.919456 \t 0.170635 \t 110.772835 s\n","173 \t 15.763871 \t 8.133728 \t7.540060 \t 0.090083 \t 111.425896 s\n","174 \t 16.655445 \t 8.533225 \t8.017225 \t 0.104995 \t 111.900978 s\n","175 \t 16.947847 \t 8.820076 \t8.068867 \t 0.058905 \t 112.377407 s\n","176 \t 16.330895 \t 8.916805 \t7.303797 \t 0.110293 \t 112.861170 s\n","177 \t 17.648594 \t 9.012104 \t8.553431 \t 0.083058 \t 113.333041 s\n","178 \t 17.438911 \t 8.973392 \t8.357342 \t 0.108177 \t 113.824733 s\n","179 \t 16.066341 \t 8.746410 \t7.251333 \t 0.068597 \t 114.304179 s\n","180 \t 16.684243 \t 8.680914 \t7.882878 \t 0.120451 \t 114.933300 s\n","181 \t 17.558475 \t 9.319332 \t8.144948 \t 0.094194 \t 115.507332 s\n","182 \t 17.492346 \t 8.831501 \t8.566187 \t 0.094659 \t 116.086879 s\n","183 \t 15.862350 \t 8.304337 \t7.404405 \t 0.153608 \t 116.689212 s\n","184 \t 17.999540 \t 9.007864 \t8.853697 \t 0.137980 \t 117.337979 s\n","185 \t 17.796384 \t 9.112874 \t8.585715 \t 0.097796 \t 117.821866 s\n","186 \t 17.107652 \t 9.058310 \t7.914102 \t 0.135240 \t 118.298205 s\n","187 \t 17.170932 \t 9.566822 \t7.480450 \t 0.123660 \t 118.789752 s\n","188 \t 17.493085 \t 9.054555 \t8.267783 \t 0.170746 \t 119.436729 s\n","189 \t 17.496665 \t 8.709027 \t8.705493 \t 0.082145 \t 119.917167 s\n","190 \t 17.175202 \t 8.715042 \t8.282546 \t 0.177615 \t 120.389061 s\n","191 \t 18.071518 \t 8.918724 \t9.001781 \t 0.151014 \t 120.873505 s\n","192 \t 17.508797 \t 9.129395 \t8.072633 \t 0.306768 \t 121.346066 s\n","193 \t 17.395606 \t 9.027286 \t8.133952 \t 0.234368 \t 121.836536 s\n","194 \t 17.633362 \t 9.301345 \t8.132869 \t 0.199147 \t 122.304847 s\n","195 \t 17.537564 \t 9.303085 \t8.084235 \t 0.150244 \t 122.801917 s\n","196 \t 17.333535 \t 8.837007 \t8.412532 \t 0.083997 \t 123.285950 s\n","197 \t 16.917665 \t 8.692147 \t8.043375 \t 0.182145 \t 123.763355 s\n","198 \t 17.185491 \t 8.911018 \t8.191826 \t 0.082646 \t 124.414386 s\n","199 \t 16.944948 \t 9.021519 \t7.832610 \t 0.090819 \t 124.906355 s\n","200 \t 17.951183 \t 9.327493 \t8.524656 \t 0.099034 \t 125.385767 s\n","201 \t 16.924538 \t 8.933451 \t7.894142 \t 0.096944 \t 125.864445 s\n","202 \t 17.261730 \t 9.235460 \t7.911940 \t 0.114329 \t 126.352129 s\n","203 \t 17.019595 \t 8.887684 \t8.055028 \t 0.076884 \t 126.830708 s\n","204 \t 16.850530 \t 8.682921 \t8.039171 \t 0.128437 \t 127.322360 s\n","205 \t 17.934298 \t 9.073227 \t8.758245 \t 0.102825 \t 127.920880 s\n","206 \t 18.561716 \t 9.262543 \t9.103538 \t 0.195634 \t 128.523809 s\n","207 \t 17.849696 \t 9.360112 \t8.394768 \t 0.094818 \t 129.117291 s\n","208 \t 17.055781 \t 8.938408 \t7.824070 \t 0.293304 \t 129.721019 s\n","209 \t 16.998279 \t 9.058027 \t7.855114 \t 0.085137 \t 130.529560 s\n","210 \t 17.189168 \t 8.808993 \t8.294476 \t 0.085698 \t 131.002658 s\n","211 \t 17.114056 \t 9.127267 \t7.931625 \t 0.055163 \t 131.481659 s\n","212 \t 16.813817 \t 9.092084 \t7.609954 \t 0.111779 \t 131.960539 s\n","213 \t 16.568436 \t 9.199678 \t7.284780 \t 0.083978 \t 132.438266 s\n","214 \t 17.188095 \t 9.039382 \t8.097967 \t 0.050747 \t 132.918635 s\n","215 \t 16.426421 \t 8.529939 \t7.747907 \t 0.148575 \t 133.391588 s\n","216 \t 16.710947 \t 8.635448 \t8.000607 \t 0.074891 \t 133.868156 s\n","217 \t 16.834827 \t 9.237659 \t7.457994 \t 0.139175 \t 134.351319 s\n","218 \t 16.629623 \t 8.798786 \t7.749382 \t 0.081455 \t 134.832316 s\n","219 \t 17.295357 \t 9.276605 \t7.911883 \t 0.106868 \t 135.320080 s\n","220 \t 16.730761 \t 8.419122 \t8.117481 \t 0.194159 \t 135.970806 s\n","221 \t 17.220007 \t 9.067410 \t8.039098 \t 0.113499 \t 136.447715 s\n","222 \t 16.984303 \t 8.739580 \t8.145358 \t 0.099364 \t 136.926065 s\n","223 \t 17.726662 \t 8.813823 \t8.716784 \t 0.196054 \t 137.414272 s\n","224 \t 16.998427 \t 8.750903 \t7.915986 \t 0.331539 \t 137.879998 s\n","225 \t 16.317794 \t 8.491440 \t7.753968 \t 0.072386 \t 138.347368 s\n","226 \t 18.386799 \t 9.454254 \t8.781598 \t 0.150947 \t 138.864384 s\n","227 \t 17.313827 \t 9.204202 \t7.963083 \t 0.146543 \t 139.341687 s\n","228 \t 16.579986 \t 8.909197 \t7.621305 \t 0.049483 \t 139.838584 s\n","229 \t 17.239453 \t 9.222730 \t7.914454 \t 0.102269 \t 140.308946 s\n","230 \t 17.063077 \t 8.939101 \t8.051647 \t 0.072329 \t 140.874748 s\n","231 \t 17.050394 \t 8.858299 \t8.129743 \t 0.062352 \t 141.454195 s\n","232 \t 16.897283 \t 9.168203 \t7.653198 \t 0.075881 \t 142.042919 s\n","233 \t 17.617481 \t 9.335877 \t8.221018 \t 0.060586 \t 143.149219 s\n","234 \t 17.713994 \t 9.379159 \t8.180453 \t 0.154382 \t 143.839695 s\n","235 \t 17.446190 \t 9.284078 \t8.098554 \t 0.063559 \t 144.313920 s\n","236 \t 17.124179 \t 9.022777 \t8.045941 \t 0.055460 \t 144.803771 s\n","237 \t 17.295781 \t 8.795009 \t8.407032 \t 0.093740 \t 145.289188 s\n","238 \t 16.513284 \t 8.511098 \t7.965426 \t 0.036760 \t 145.778786 s\n","239 \t 16.612676 \t 8.301848 \t8.272249 \t 0.038579 \t 146.251720 s\n","240 \t 16.943871 \t 8.916668 \t7.976772 \t 0.050431 \t 146.730088 s\n","241 \t 16.889249 \t 8.285010 \t8.501451 \t 0.102788 \t 147.223033 s\n","242 \t 16.828707 \t 9.031266 \t7.739379 \t 0.058062 \t 147.694252 s\n","243 \t 17.137757 \t 8.867267 \t8.225431 \t 0.045059 \t 148.186377 s\n","244 \t 16.598482 \t 8.460828 \t8.048132 \t 0.089521 \t 148.676223 s\n","245 \t 16.181380 \t 9.036844 \t7.075726 \t 0.068811 \t 149.148476 s\n","246 \t 17.407653 \t 9.131412 \t8.172390 \t 0.103852 \t 149.794470 s\n","247 \t 17.503276 \t 9.403481 \t8.046904 \t 0.052890 \t 150.285424 s\n","248 \t 16.848194 \t 8.629035 \t8.093973 \t 0.125186 \t 150.761665 s\n","249 \t 17.259738 \t 9.530965 \t7.683651 \t 0.045122 \t 151.241998 s\n","250 \t 16.571984 \t 8.442335 \t8.064446 \t 0.065203 \t 151.714000 s\n","251 \t 16.736446 \t 8.501444 \t8.110260 \t 0.124742 \t 152.184730 s\n","252 \t 16.849055 \t 9.044258 \t7.647786 \t 0.157011 \t 152.660335 s\n","253 \t 16.850727 \t 8.898458 \t7.877478 \t 0.074792 \t 153.137483 s\n","254 \t 17.568396 \t 9.057854 \t8.428815 \t 0.081727 \t 153.613413 s\n","255 \t 17.282345 \t 8.953259 \t8.067003 \t 0.262083 \t 154.187540 s\n","256 \t 16.615375 \t 8.735660 \t7.826466 \t 0.053250 \t 154.756033 s\n","257 \t 16.667615 \t 8.777572 \t7.840238 \t 0.049806 \t 155.331369 s\n","258 \t 16.989348 \t 9.147740 \t7.799780 \t 0.041829 \t 155.905607 s\n","259 \t 16.730973 \t 9.028735 \t7.645823 \t 0.056415 \t 156.766600 s\n","260 \t 16.583581 \t 8.666307 \t7.857257 \t 0.060017 \t 157.251500 s\n","261 \t 16.953168 \t 8.483964 \t8.356502 \t 0.112702 \t 157.729022 s\n","262 \t 17.090130 \t 8.897009 \t8.147605 \t 0.045516 \t 158.207530 s\n","263 \t 17.487450 \t 9.312389 \t8.124052 \t 0.051008 \t 158.709265 s\n","264 \t 16.818504 \t 8.429413 \t8.330125 \t 0.058966 \t 159.182550 s\n","265 \t 17.014668 \t 8.833456 \t8.134192 \t 0.047021 \t 159.677064 s\n","266 \t 17.146558 \t 8.466740 \t8.645210 \t 0.034609 \t 160.149799 s\n","267 \t 16.655280 \t 9.011860 \t7.598067 \t 0.045353 \t 160.632011 s\n","268 \t 16.390011 \t 8.063201 \t8.096699 \t 0.230111 \t 161.108699 s\n","269 \t 17.278377 \t 9.299541 \t7.948350 \t 0.030485 \t 161.588151 s\n","270 \t 16.496535 \t 8.785666 \t7.658192 \t 0.052676 \t 162.067456 s\n","271 \t 16.524606 \t 8.848624 \t7.622753 \t 0.053229 \t 162.541728 s\n","272 \t 17.697798 \t 9.668519 \t7.989404 \t 0.039874 \t 163.181997 s\n","273 \t 17.094147 \t 9.068012 \t7.983679 \t 0.042454 \t 163.663711 s\n","274 \t 16.981182 \t 8.796023 \t8.076556 \t 0.108603 \t 164.145536 s\n","275 \t 17.388921 \t 9.281975 \t8.056958 \t 0.049989 \t 164.624530 s\n","276 \t 17.624071 \t 9.038248 \t8.313039 \t 0.272784 \t 165.105556 s\n","277 \t 17.365889 \t 8.958304 \t8.310492 \t 0.097092 \t 165.589556 s\n","278 \t 16.366716 \t 8.587319 \t7.667061 \t 0.112336 \t 166.073510 s\n","279 \t 16.966527 \t 8.923103 \t7.857885 \t 0.185537 \t 166.555438 s\n","280 \t 17.110316 \t 8.705191 \t8.370576 \t 0.034550 \t 167.091008 s\n","281 \t 17.156967 \t 8.669765 \t8.349627 \t 0.137575 \t 167.674820 s\n","282 \t 16.455134 \t 8.755714 \t7.666217 \t 0.033204 \t 168.239706 s\n","283 \t 17.003772 \t 8.988354 \t7.940918 \t 0.074499 \t 168.816788 s\n","284 \t 16.967276 \t 9.156054 \t7.776649 \t 0.034572 \t 169.446685 s\n","285 \t 17.627476 \t 8.820805 \t8.751184 \t 0.055487 \t 170.033793 s\n","286 \t 16.688460 \t 8.745841 \t7.884157 \t 0.058462 \t 170.511556 s\n","287 \t 17.604440 \t 9.324912 \t8.241806 \t 0.037723 \t 171.169273 s\n","288 \t 16.828514 \t 8.782066 \t7.942242 \t 0.104205 \t 171.637676 s\n","289 \t 15.951374 \t 8.285919 \t7.623095 \t 0.042359 \t 172.114171 s\n","290 \t 17.492033 \t 9.455477 \t7.951857 \t 0.084699 \t 172.587512 s\n","291 \t 16.848590 \t 8.519530 \t8.220567 \t 0.108493 \t 173.060337 s\n","292 \t 16.605386 \t 8.217527 \t8.323344 \t 0.064515 \t 173.536791 s\n","293 \t 16.728787 \t 8.494801 \t8.186707 \t 0.047279 \t 174.021486 s\n","294 \t 16.614652 \t 8.734855 \t7.816243 \t 0.063554 \t 174.497055 s\n","295 \t 16.800334 \t 8.797297 \t7.946803 \t 0.056234 \t 174.989899 s\n","296 \t 17.018376 \t 8.913701 \t8.062636 \t 0.042038 \t 175.462968 s\n","297 \t 17.050160 \t 8.862480 \t8.132413 \t 0.055267 \t 175.939615 s\n","298 \t 17.611233 \t 8.959625 \t8.593654 \t 0.057955 \t 176.412507 s\n","299 \t 17.305049 \t 8.953804 \t8.295877 \t 0.055369 \t 176.886695 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:15<00:00,  8.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4079\n","Hit@10: 0.4962\n","Hit@3: 0.4154\n","Hit@1: 0.3500\n","Link Prediction on Validation Set (All)\n","MRR: 0.4079\n","Hit@10: 0.4962\n","Hit@3: 0.4154\n","Hit@1: 0.3500\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2427\n","Hit@10: 0.4154\n","Hit@3: 0.2615\n","Hit@1: 0.1538\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5915\n","Hit@10: 0.6984\n","Hit@3: 0.6190\n","Hit@1: 0.5317\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1271\n","300 \t 17.274943 \t 9.179157 \t7.816541 \t 0.279246 \t 194.262016 s\n","301 \t 17.121184 \t 8.653563 \t8.418044 \t 0.049577 \t 195.061397 s\n","302 \t 17.211324 \t 8.930025 \t8.182328 \t 0.098971 \t 195.656695 s\n","303 \t 17.889968 \t 9.464264 \t8.379545 \t 0.046158 \t 196.457436 s\n","304 \t 16.755225 \t 8.911452 \t7.802346 \t 0.041425 \t 197.094724 s\n","305 \t 16.550257 \t 8.685169 \t7.734344 \t 0.130744 \t 197.601038 s\n","306 \t 16.295455 \t 8.730092 \t7.539400 \t 0.025963 \t 198.278395 s\n","307 \t 16.873244 \t 8.553433 \t8.278972 \t 0.040840 \t 198.767326 s\n","308 \t 17.183207 \t 8.873253 \t8.162292 \t 0.147662 \t 199.244577 s\n","309 \t 16.681289 \t 8.432051 \t8.185675 \t 0.063563 \t 199.756771 s\n","310 \t 16.557710 \t 8.354559 \t8.125032 \t 0.078119 \t 200.232172 s\n","311 \t 17.231430 \t 8.982602 \t8.202862 \t 0.045966 \t 200.819079 s\n","312 \t 16.471009 \t 8.589807 \t7.789681 \t 0.091521 \t 201.333572 s\n","313 \t 17.202191 \t 8.948912 \t8.216086 \t 0.037192 \t 201.892434 s\n","314 \t 17.291046 \t 8.908381 \t8.328169 \t 0.054496 \t 202.396708 s\n","315 \t 17.835105 \t 8.939631 \t8.803536 \t 0.091938 \t 202.913544 s\n","316 \t 17.144668 \t 9.191107 \t7.875779 \t 0.077782 \t 203.595330 s\n","317 \t 17.079302 \t 9.147453 \t7.886586 \t 0.045263 \t 204.104535 s\n","318 \t 17.186648 \t 8.780776 \t8.355311 \t 0.050561 \t 204.676034 s\n","319 \t 16.101024 \t 8.514013 \t7.535239 \t 0.051773 \t 205.183268 s\n","320 \t 16.917389 \t 8.459783 \t8.412212 \t 0.045394 \t 205.715295 s\n","321 \t 17.655029 \t 9.346484 \t8.250926 \t 0.057620 \t 206.242767 s\n","322 \t 16.169652 \t 8.698844 \t7.443638 \t 0.027170 \t 206.922579 s\n","323 \t 16.691764 \t 8.496198 \t8.093545 \t 0.102021 \t 207.553496 s\n","324 \t 16.436289 \t 8.668993 \t7.727079 \t 0.040216 \t 208.140928 s\n","325 \t 17.122581 \t 9.017563 \t8.060647 \t 0.044371 \t 208.758210 s\n","326 \t 17.181456 \t 8.793295 \t8.321500 \t 0.066661 \t 209.355386 s\n","327 \t 17.231357 \t 8.927196 \t8.256819 \t 0.047343 \t 210.017579 s\n","328 \t 16.032214 \t 8.520422 \t7.295182 \t 0.216610 \t 210.501522 s\n","329 \t 16.795712 \t 8.296685 \t8.428333 \t 0.070695 \t 210.973832 s\n","330 \t 16.228594 \t 8.665071 \t7.520863 \t 0.042659 \t 211.450679 s\n","331 \t 17.648356 \t 9.090451 \t8.140172 \t 0.417733 \t 211.930372 s\n","332 \t 15.904724 \t 8.276085 \t7.365150 \t 0.263490 \t 212.405487 s\n","333 \t 17.166025 \t 9.031862 \t8.086996 \t 0.047166 \t 212.892979 s\n","334 \t 17.803631 \t 9.102917 \t8.655079 \t 0.045634 \t 213.379744 s\n","335 \t 16.645606 \t 8.744289 \t7.863624 \t 0.037695 \t 213.874843 s\n","336 \t 16.628152 \t 8.885566 \t7.509221 \t 0.233365 \t 214.364592 s\n","337 \t 17.264257 \t 9.122965 \t8.113062 \t 0.028231 \t 214.858099 s\n","338 \t 17.047136 \t 9.186265 \t7.786927 \t 0.073945 \t 215.337972 s\n","339 \t 16.431726 \t 8.787875 \t7.607267 \t 0.036584 \t 216.001105 s\n","340 \t 16.376458 \t 8.773436 \t7.576104 \t 0.026919 \t 216.471590 s\n","341 \t 15.907264 \t 8.425595 \t7.427654 \t 0.054015 \t 216.958173 s\n","342 \t 16.405378 \t 8.762836 \t7.611761 \t 0.030782 \t 217.439862 s\n","343 \t 16.090854 \t 8.448051 \t7.406012 \t 0.236791 \t 217.950954 s\n","344 \t 16.987483 \t 8.723316 \t8.229981 \t 0.034185 \t 218.422620 s\n","345 \t 16.632782 \t 8.551836 \t7.944351 \t 0.136595 \t 218.899640 s\n","346 \t 16.504000 \t 8.693691 \t7.751624 \t 0.058683 \t 219.413448 s\n","347 \t 17.111799 \t 8.607430 \t8.447055 \t 0.057314 \t 220.004642 s\n","348 \t 16.555367 \t 8.633026 \t7.854899 \t 0.067442 \t 220.572050 s\n","349 \t 16.924409 \t 8.956272 \t7.903195 \t 0.064944 \t 221.167578 s\n","350 \t 17.167511 \t 8.613936 \t8.471001 \t 0.082573 \t 221.763973 s\n","351 \t 16.832095 \t 8.563473 \t8.233864 \t 0.034759 \t 222.376845 s\n","352 \t 16.439375 \t 8.906472 \t7.510613 \t 0.022290 \t 222.853612 s\n","353 \t 17.743291 \t 9.087454 \t8.586320 \t 0.069516 \t 223.517597 s\n","354 \t 16.346182 \t 8.486729 \t7.726933 \t 0.132520 \t 224.004294 s\n","355 \t 17.202907 \t 8.633028 \t8.512834 \t 0.057044 \t 224.482956 s\n","356 \t 16.307281 \t 8.525962 \t7.709255 \t 0.072064 \t 224.967841 s\n","357 \t 15.886139 \t 8.288719 \t7.534281 \t 0.063139 \t 225.438317 s\n","358 \t 16.470070 \t 8.470693 \t7.907739 \t 0.091638 \t 225.914888 s\n","359 \t 17.019091 \t 8.855805 \t8.098338 \t 0.064947 \t 226.406888 s\n","360 \t 16.311906 \t 8.832321 \t7.439553 \t 0.040032 \t 226.875811 s\n","361 \t 16.442311 \t 8.362843 \t8.028069 \t 0.051400 \t 227.356597 s\n","362 \t 16.411503 \t 8.578835 \t7.795957 \t 0.036710 \t 227.835670 s\n","363 \t 17.049744 \t 8.408330 \t8.600946 \t 0.040468 \t 228.306604 s\n","364 \t 16.328380 \t 8.913770 \t7.378492 \t 0.036118 \t 228.787179 s\n","365 \t 16.960467 \t 8.755700 \t8.166115 \t 0.038651 \t 229.261471 s\n","366 \t 16.425164 \t 8.058609 \t8.316008 \t 0.050547 \t 229.754481 s\n","367 \t 17.267813 \t 8.875954 \t8.347059 \t 0.044798 \t 230.245482 s\n","368 \t 16.433483 \t 8.727242 \t7.674447 \t 0.031794 \t 230.885273 s\n","369 \t 17.080806 \t 9.097444 \t7.953706 \t 0.029655 \t 231.360266 s\n","370 \t 16.675838 \t 8.400020 \t8.222931 \t 0.052887 \t 231.843269 s\n","371 \t 15.880245 \t 8.465242 \t7.376125 \t 0.038878 \t 232.334557 s\n","372 \t 17.453638 \t 8.908098 \t8.326147 \t 0.219394 \t 232.927939 s\n","373 \t 17.373448 \t 8.840884 \t8.503816 \t 0.028748 \t 233.499039 s\n","374 \t 16.387113 \t 8.985735 \t7.371789 \t 0.029590 \t 234.070917 s\n","375 \t 16.715319 \t 8.371376 \t8.305082 \t 0.038859 \t 234.678380 s\n","376 \t 16.857707 \t 8.714818 \t8.114358 \t 0.028531 \t 235.306499 s\n","377 \t 15.982687 \t 8.315532 \t7.612868 \t 0.054286 \t 235.801295 s\n","378 \t 16.665515 \t 8.617541 \t7.928824 \t 0.119150 \t 236.274798 s\n","379 \t 16.657869 \t 8.863254 \t7.774793 \t 0.019822 \t 236.763221 s\n","380 \t 16.891227 \t 8.823309 \t7.963947 \t 0.103970 \t 237.236865 s\n","381 \t 16.402566 \t 8.674974 \t7.698176 \t 0.029415 \t 237.714540 s\n","382 \t 16.945629 \t 8.486464 \t8.435378 \t 0.023788 \t 238.182489 s\n","383 \t 16.328330 \t 8.203090 \t8.079886 \t 0.045354 \t 238.847933 s\n","384 \t 17.353138 \t 8.716640 \t8.603234 \t 0.033264 \t 239.316267 s\n","385 \t 16.823616 \t 9.004501 \t7.755207 \t 0.063908 \t 239.803062 s\n","386 \t 16.846716 \t 8.495044 \t8.308377 \t 0.043295 \t 240.282417 s\n","387 \t 15.648149 \t 8.097389 \t7.509075 \t 0.041684 \t 240.774209 s\n","388 \t 16.470724 \t 8.473089 \t7.959548 \t 0.038086 \t 241.254936 s\n","389 \t 17.136528 \t 8.810224 \t8.097166 \t 0.229139 \t 241.737727 s\n","390 \t 16.390778 \t 8.757892 \t7.522310 \t 0.110575 \t 242.212276 s\n","391 \t 16.770640 \t 8.607639 \t8.121425 \t 0.041578 \t 242.697580 s\n","392 \t 16.682586 \t 8.682560 \t7.902415 \t 0.097611 \t 243.180211 s\n","393 \t 17.106420 \t 8.915766 \t8.060323 \t 0.130330 \t 243.827198 s\n","394 \t 16.186425 \t 8.149194 \t8.007271 \t 0.029960 \t 244.308360 s\n","395 \t 17.071634 \t 9.219869 \t7.580028 \t 0.271737 \t 244.799257 s\n","396 \t 16.314272 \t 8.261097 \t8.015444 \t 0.037730 \t 245.300039 s\n","397 \t 16.391450 \t 8.453606 \t7.883947 \t 0.053897 \t 245.915778 s\n","398 \t 16.412345 \t 8.654523 \t7.693445 \t 0.064376 \t 246.484124 s\n","399 \t 16.564016 \t 8.804464 \t7.742975 \t 0.016577 \t 247.082205 s\n","400 \t 16.887933 \t 8.998909 \t7.848410 \t 0.040614 \t 247.677671 s\n","401 \t 16.461226 \t 9.088162 \t7.326518 \t 0.046545 \t 248.309045 s\n","402 \t 15.923470 \t 8.163861 \t7.716785 \t 0.042824 \t 248.785534 s\n","403 \t 16.022716 \t 8.352487 \t7.580832 \t 0.089397 \t 249.263276 s\n","404 \t 17.206050 \t 8.785407 \t8.380618 \t 0.040024 \t 249.910663 s\n","405 \t 17.197034 \t 8.713051 \t8.364132 \t 0.119851 \t 250.399021 s\n","406 \t 17.177926 \t 8.713051 \t8.417065 \t 0.047810 \t 250.873536 s\n","407 \t 16.529651 \t 8.630863 \t7.816122 \t 0.082667 \t 251.351479 s\n","408 \t 15.444245 \t 8.155627 \t7.215817 \t 0.072801 \t 251.826089 s\n","409 \t 16.221452 \t 8.161197 \t7.938803 \t 0.121452 \t 252.302836 s\n","410 \t 16.702743 \t 8.873442 \t7.801545 \t 0.027755 \t 252.781035 s\n","411 \t 16.952734 \t 8.701866 \t8.194316 \t 0.056552 \t 253.256468 s\n","412 \t 16.749695 \t 8.858891 \t7.865512 \t 0.025292 \t 253.730340 s\n","413 \t 16.583200 \t 8.644054 \t7.912057 \t 0.027088 \t 254.214787 s\n","414 \t 15.686560 \t 8.111582 \t7.538843 \t 0.036134 \t 254.687024 s\n","415 \t 16.914231 \t 9.118707 \t7.764492 \t 0.031032 \t 255.330179 s\n","416 \t 16.369678 \t 8.927015 \t7.419772 \t 0.022890 \t 255.805593 s\n","417 \t 16.592033 \t 8.746904 \t7.770814 \t 0.074316 \t 256.293071 s\n","418 \t 16.998344 \t 8.804440 \t8.143800 \t 0.050105 \t 256.767391 s\n","419 \t 16.490976 \t 8.579581 \t7.695534 \t 0.215861 \t 257.239815 s\n","420 \t 15.967120 \t 8.503527 \t7.420373 \t 0.043220 \t 257.716536 s\n","421 \t 15.999757 \t 8.199732 \t7.767122 \t 0.032903 \t 258.184497 s\n","422 \t 16.565083 \t 8.777560 \t7.756488 \t 0.031035 \t 258.767000 s\n","423 \t 17.052330 \t 9.130066 \t7.547705 \t 0.374559 \t 259.334903 s\n","424 \t 17.095467 \t 8.908040 \t8.078497 \t 0.108929 \t 259.917597 s\n","425 \t 16.374714 \t 8.602749 \t7.743698 \t 0.028266 \t 260.517742 s\n","426 \t 16.764654 \t 8.357962 \t8.228460 \t 0.178233 \t 261.148411 s\n","427 \t 16.811516 \t 8.995449 \t7.777846 \t 0.038221 \t 261.871561 s\n","428 \t 16.461079 \t 8.503558 \t7.851564 \t 0.105956 \t 262.340222 s\n","429 \t 17.406859 \t 9.482859 \t7.902016 \t 0.021985 \t 262.827944 s\n","430 \t 15.341386 \t 8.226329 \t7.081286 \t 0.033771 \t 263.296987 s\n","431 \t 15.812929 \t 8.440084 \t7.333914 \t 0.038930 \t 263.774680 s\n","432 \t 16.176511 \t 8.171498 \t7.969871 \t 0.035143 \t 264.249710 s\n","433 \t 16.477112 \t 8.797593 \t7.618156 \t 0.061364 \t 264.730975 s\n","434 \t 17.602315 \t 9.339527 \t8.206598 \t 0.056189 \t 265.206345 s\n","435 \t 15.851978 \t 8.281477 \t7.513610 \t 0.056891 \t 265.689666 s\n","436 \t 15.881038 \t 8.692605 \t7.155190 \t 0.033243 \t 266.161592 s\n","437 \t 16.234650 \t 8.513830 \t7.686308 \t 0.034512 \t 266.644428 s\n","438 \t 16.506901 \t 8.721337 \t7.721983 \t 0.063582 \t 267.112605 s\n","439 \t 17.137593 \t 8.870113 \t8.203393 \t 0.064088 \t 267.590575 s\n","440 \t 17.218742 \t 8.734277 \t8.432413 \t 0.052052 \t 268.240264 s\n","441 \t 16.773978 \t 8.894212 \t7.835212 \t 0.044554 \t 268.725616 s\n","442 \t 16.650109 \t 8.467963 \t7.881890 \t 0.300255 \t 269.198216 s\n","443 \t 16.449883 \t 8.934912 \t7.439961 \t 0.075010 \t 269.672410 s\n","444 \t 16.615515 \t 8.599605 \t7.983406 \t 0.032504 \t 270.156569 s\n","445 \t 16.531186 \t 8.559137 \t7.925668 \t 0.046381 \t 270.628569 s\n","446 \t 16.217348 \t 8.328161 \t7.799492 \t 0.089695 \t 271.116901 s\n","447 \t 17.049400 \t 8.622637 \t8.307940 \t 0.118823 \t 271.638379 s\n","448 \t 15.980571 \t 8.300369 \t7.642247 \t 0.037956 \t 272.219782 s\n","449 \t 15.987239 \t 8.893369 \t7.062632 \t 0.031238 \t 272.790359 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:15<00:00,  8.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4017\n","Hit@10: 0.4923\n","Hit@3: 0.3923\n","Hit@1: 0.3500\n","Link Prediction on Validation Set (All)\n","MRR: 0.4017\n","Hit@10: 0.4923\n","Hit@3: 0.3923\n","Hit@1: 0.3500\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2344\n","Hit@10: 0.4077\n","Hit@3: 0.2385\n","Hit@1: 0.1462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5882\n","Hit@10: 0.6944\n","Hit@3: 0.6032\n","Hit@1: 0.5278\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1259\n","450 \t 16.303867 \t 8.500293 \t7.665576 \t 0.137997 \t 290.003168 s\n","451 \t 16.440114 \t 8.665544 \t7.713035 \t 0.061535 \t 290.490363 s\n","452 \t 17.213303 \t 9.034992 \t8.123611 \t 0.054701 \t 290.967094 s\n","453 \t 16.789967 \t 8.910983 \t7.795224 \t 0.083759 \t 291.434759 s\n","454 \t 17.155017 \t 9.306614 \t7.781013 \t 0.067390 \t 292.085871 s\n","455 \t 17.885924 \t 9.218573 \t8.593612 \t 0.073739 \t 292.552200 s\n","456 \t 17.604850 \t 9.182772 \t8.312761 \t 0.109317 \t 293.037588 s\n","457 \t 17.697212 \t 9.204261 \t8.427766 \t 0.065185 \t 293.507590 s\n","458 \t 16.215285 \t 8.445603 \t7.717564 \t 0.052119 \t 293.993151 s\n","459 \t 17.227121 \t 8.917769 \t8.265063 \t 0.044289 \t 294.465034 s\n","460 \t 17.005923 \t 8.815219 \t8.013589 \t 0.177116 \t 294.947244 s\n","461 \t 17.354963 \t 8.829843 \t8.456347 \t 0.068773 \t 295.419277 s\n","462 \t 16.826247 \t 8.556086 \t8.229048 \t 0.041113 \t 295.904599 s\n","463 \t 17.035064 \t 9.241429 \t7.743305 \t 0.050331 \t 296.429899 s\n","464 \t 17.215432 \t 9.231183 \t7.915644 \t 0.068605 \t 296.963057 s\n","465 \t 17.158184 \t 9.152148 \t7.929114 \t 0.076922 \t 297.491050 s\n","466 \t 16.824736 \t 8.878872 \t7.906147 \t 0.039717 \t 298.154204 s\n","467 \t 17.420478 \t 8.832624 \t8.525203 \t 0.062650 \t 298.766634 s\n","468 \t 16.268219 \t 8.725733 \t7.501195 \t 0.041290 \t 299.675546 s\n","469 \t 16.620928 \t 8.477028 \t8.104987 \t 0.038912 \t 300.417916 s\n","470 \t 16.963755 \t 8.991206 \t7.947469 \t 0.025080 \t 301.026153 s\n","471 \t 17.122882 \t 9.155204 \t7.918451 \t 0.049227 \t 301.598105 s\n","472 \t 16.516078 \t 8.731499 \t7.728735 \t 0.055844 \t 302.120961 s\n","473 \t 16.624121 \t 8.632806 \t7.947638 \t 0.043677 \t 302.650737 s\n","474 \t 16.361077 \t 8.809841 \t7.504469 \t 0.046768 \t 303.125312 s\n","475 \t 16.841968 \t 8.757843 \t7.894464 \t 0.189662 \t 303.618454 s\n","476 \t 16.778466 \t 8.380834 \t8.306369 \t 0.091264 \t 304.096875 s\n","477 \t 16.214836 \t 8.413388 \t7.758120 \t 0.043329 \t 304.579554 s\n","478 \t 16.590121 \t 9.006020 \t7.554401 \t 0.029701 \t 305.065666 s\n","479 \t 16.567844 \t 8.469284 \t8.047791 \t 0.050770 \t 305.549842 s\n","480 \t 16.138419 \t 8.633054 \t7.435625 \t 0.069741 \t 306.037302 s\n","481 \t 16.857708 \t 8.300769 \t8.511582 \t 0.045358 \t 306.510442 s\n","482 \t 16.783069 \t 8.918555 \t7.797247 \t 0.067265 \t 306.985785 s\n","483 \t 15.328944 \t 8.401154 \t6.876206 \t 0.051585 \t 307.639570 s\n","484 \t 17.815145 \t 9.260774 \t8.503648 \t 0.050724 \t 308.113210 s\n","485 \t 17.653229 \t 9.102948 \t8.472887 \t 0.077393 \t 308.598399 s\n","486 \t 16.718219 \t 8.923872 \t7.742456 \t 0.051892 \t 309.081041 s\n","487 \t 16.093409 \t 8.511834 \t7.461461 \t 0.120114 \t 309.557614 s\n","488 \t 16.792382 \t 8.437679 \t8.309568 \t 0.045135 \t 310.036922 s\n","489 \t 16.695285 \t 8.628585 \t8.029574 \t 0.037125 \t 310.522012 s\n","490 \t 17.242029 \t 9.189206 \t7.998958 \t 0.053866 \t 311.085073 s\n","491 \t 16.327637 \t 8.758453 \t7.521780 \t 0.047404 \t 311.678971 s\n","492 \t 16.742159 \t 8.789905 \t7.780895 \t 0.171359 \t 312.243011 s\n","493 \t 17.065716 \t 8.697231 \t8.154833 \t 0.213652 \t 313.070681 s\n","494 \t 17.062229 \t 8.914327 \t8.056884 \t 0.091018 \t 313.705821 s\n","495 \t 17.159603 \t 8.812878 \t8.301278 \t 0.045448 \t 314.184570 s\n","496 \t 16.474092 \t 8.734313 \t7.679859 \t 0.059920 \t 314.667549 s\n","497 \t 17.290447 \t 8.984454 \t8.255893 \t 0.050099 \t 315.143209 s\n","498 \t 16.598928 \t 8.398025 \t7.979295 \t 0.221609 \t 315.623557 s\n","499 \t 16.708869 \t 8.545821 \t8.070491 \t 0.092557 \t 316.112765 s\n","500 \t 17.191228 \t 9.117760 \t7.995552 \t 0.077918 \t 316.586395 s\n","501 \t 16.662531 \t 8.871387 \t7.736018 \t 0.055126 \t 317.082343 s\n","502 \t 16.882076 \t 9.102260 \t7.725253 \t 0.054565 \t 317.561560 s\n","503 \t 16.070789 \t 8.577245 \t7.413659 \t 0.079885 \t 318.039786 s\n","504 \t 15.694002 \t 8.089121 \t7.539071 \t 0.065810 \t 318.676938 s\n","505 \t 17.282844 \t 9.075927 \t8.133043 \t 0.073873 \t 319.161439 s\n","506 \t 17.854312 \t 9.567437 \t8.237189 \t 0.049686 \t 319.634110 s\n","507 \t 17.488552 \t 9.139699 \t8.295386 \t 0.053467 \t 320.111495 s\n","508 \t 16.927977 \t 8.488908 \t8.377163 \t 0.061905 \t 320.602180 s\n","509 \t 17.341408 \t 8.615910 \t8.631919 \t 0.093579 \t 321.083562 s\n","510 \t 16.594357 \t 8.686501 \t7.882297 \t 0.025559 \t 321.556412 s\n","511 \t 17.548916 \t 9.103808 \t8.287854 \t 0.157253 \t 322.037277 s\n","512 \t 16.965285 \t 9.038756 \t7.853542 \t 0.072987 \t 322.505687 s\n","513 \t 16.422668 \t 8.583373 \t7.788885 \t 0.050409 \t 322.990021 s\n","514 \t 16.755309 \t 8.539270 \t8.172695 \t 0.043344 \t 323.458737 s\n","515 \t 16.041652 \t 8.667990 \t7.207201 \t 0.166462 \t 324.203077 s\n","516 \t 16.641412 \t 8.459572 \t8.128160 \t 0.053679 \t 324.772953 s\n","517 \t 16.477494 \t 8.830863 \t7.585359 \t 0.061272 \t 325.366744 s\n","518 \t 15.906129 \t 8.358035 \t7.516673 \t 0.031421 \t 325.964083 s\n","519 \t 16.999676 \t 9.107806 \t7.792217 \t 0.099653 \t 326.612523 s\n","520 \t 17.286888 \t 8.907731 \t8.180694 \t 0.198463 \t 327.106561 s\n","521 \t 16.830010 \t 8.484431 \t8.285677 \t 0.059901 \t 327.598246 s\n","522 \t 17.154712 \t 8.361809 \t8.739214 \t 0.053688 \t 328.074763 s\n","523 \t 16.773772 \t 8.537573 \t8.043571 \t 0.192629 \t 328.574228 s\n","524 \t 16.328799 \t 8.700430 \t7.580591 \t 0.047778 \t 329.054729 s\n","525 \t 17.176821 \t 8.879494 \t8.260258 \t 0.037069 \t 329.530205 s\n","526 \t 16.028846 \t 8.545776 \t7.424572 \t 0.058497 \t 330.174070 s\n","527 \t 17.114465 \t 8.725679 \t8.343435 \t 0.045351 \t 330.667236 s\n","528 \t 16.576025 \t 9.165358 \t7.343882 \t 0.066785 \t 331.138965 s\n","529 \t 17.396161 \t 9.025248 \t8.258919 \t 0.111994 \t 331.620882 s\n","530 \t 16.340587 \t 8.917159 \t7.386727 \t 0.036701 \t 332.098852 s\n","531 \t 16.983499 \t 9.053676 \t7.842825 \t 0.086997 \t 332.578201 s\n","532 \t 16.898582 \t 9.264211 \t7.508436 \t 0.125934 \t 333.054314 s\n","533 \t 17.081562 \t 8.830369 \t8.219254 \t 0.031939 \t 333.532162 s\n","534 \t 16.242395 \t 8.814439 \t7.374774 \t 0.053182 \t 334.010457 s\n","535 \t 17.102714 \t 8.956650 \t8.090858 \t 0.055206 \t 334.493780 s\n","536 \t 16.896692 \t 8.921589 \t7.938894 \t 0.036209 \t 334.968022 s\n","537 \t 16.273651 \t 8.400858 \t7.795394 \t 0.077399 \t 335.623090 s\n","538 \t 17.232574 \t 8.919386 \t8.249449 \t 0.063738 \t 336.099814 s\n","539 \t 16.739281 \t 8.708023 \t7.986048 \t 0.045211 \t 336.579198 s\n","540 \t 17.273523 \t 9.247124 \t7.973463 \t 0.052936 \t 337.142667 s\n","541 \t 17.446049 \t 9.404196 \t7.984069 \t 0.057783 \t 337.725164 s\n","542 \t 16.446124 \t 8.759260 \t7.630474 \t 0.056389 \t 338.281780 s\n","543 \t 16.541908 \t 8.452218 \t8.018801 \t 0.070888 \t 338.872149 s\n","544 \t 16.868841 \t 8.469970 \t8.330651 \t 0.068221 \t 339.495165 s\n","545 \t 16.841223 \t 8.586162 \t8.146654 \t 0.108407 \t 340.067775 s\n","546 \t 16.837628 \t 8.724703 \t8.064525 \t 0.048401 \t 340.540550 s\n","547 \t 16.710098 \t 9.085842 \t7.578028 \t 0.046229 \t 341.034332 s\n","548 \t 17.324187 \t 9.329896 \t7.835227 \t 0.159065 \t 341.502949 s\n","549 \t 16.385943 \t 8.691163 \t7.641677 \t 0.053103 \t 341.984093 s\n","550 \t 16.945841 \t 8.858305 \t8.022796 \t 0.064741 \t 342.617778 s\n","551 \t 16.575518 \t 8.345971 \t8.188451 \t 0.041096 \t 343.110620 s\n","552 \t 17.799460 \t 9.249437 \t8.516806 \t 0.033217 \t 343.582353 s\n","553 \t 17.183003 \t 9.141496 \t7.855450 \t 0.186057 \t 344.064859 s\n","554 \t 16.672788 \t 8.735988 \t7.888062 \t 0.048739 \t 344.534048 s\n","555 \t 16.651296 \t 8.852257 \t7.758574 \t 0.040466 \t 345.016765 s\n","556 \t 16.297486 \t 8.634053 \t7.606778 \t 0.056655 \t 345.493659 s\n","557 \t 17.019915 \t 8.674711 \t8.024564 \t 0.320640 \t 345.968508 s\n","558 \t 16.955125 \t 9.086749 \t7.805238 \t 0.063138 \t 346.455877 s\n","559 \t 16.907785 \t 8.935165 \t7.889259 \t 0.083361 \t 346.934760 s\n","560 \t 16.615508 \t 8.890810 \t7.679216 \t 0.045482 \t 347.413261 s\n","561 \t 16.621907 \t 8.682540 \t7.888498 \t 0.050869 \t 347.898662 s\n","562 \t 16.409720 \t 8.426075 \t7.946760 \t 0.036884 \t 348.369171 s\n","563 \t 16.780017 \t 8.991047 \t7.616459 \t 0.172509 \t 348.849740 s\n","564 \t 16.579943 \t 8.903725 \t7.564576 \t 0.111642 \t 349.506596 s\n","565 \t 16.519495 \t 8.750396 \t7.737505 \t 0.031594 \t 350.037560 s\n","566 \t 16.523345 \t 8.303798 \t8.168860 \t 0.050687 \t 350.611575 s\n","567 \t 16.916034 \t 8.967349 \t7.894191 \t 0.054494 \t 351.231066 s\n","568 \t 16.368918 \t 8.606632 \t7.681804 \t 0.080482 \t 351.799331 s\n","569 \t 16.173789 \t 8.532328 \t7.595289 \t 0.046172 \t 352.426710 s\n","570 \t 16.063064 \t 8.893557 \t7.132200 \t 0.037308 \t 353.007290 s\n","571 \t 16.765139 \t 9.149822 \t7.566820 \t 0.048497 \t 353.487276 s\n","572 \t 17.040827 \t 8.882382 \t8.111177 \t 0.047267 \t 353.961959 s\n","573 \t 16.915669 \t 8.654988 \t8.225039 \t 0.035642 \t 354.437417 s\n","574 \t 16.231443 \t 8.630168 \t7.521935 \t 0.079341 \t 354.916743 s\n","575 \t 17.346550 \t 8.890412 \t8.431938 \t 0.024200 \t 355.402436 s\n","576 \t 17.391067 \t 8.609423 \t8.739445 \t 0.042199 \t 355.877551 s\n","577 \t 16.201359 \t 8.730433 \t7.310718 \t 0.160209 \t 356.363689 s\n","578 \t 16.925533 \t 8.944949 \t7.891942 \t 0.088643 \t 356.841026 s\n","579 \t 16.975742 \t 8.719838 \t8.189279 \t 0.066626 \t 357.494095 s\n","580 \t 16.648962 \t 9.044051 \t7.487515 \t 0.117396 \t 357.972554 s\n","581 \t 16.603425 \t 8.533293 \t7.889218 \t 0.180914 \t 358.451857 s\n","582 \t 16.111857 \t 8.614907 \t7.358559 \t 0.138391 \t 358.935333 s\n","583 \t 16.825253 \t 8.603660 \t8.044534 \t 0.177058 \t 359.418151 s\n","584 \t 16.324291 \t 8.535601 \t7.737501 \t 0.051190 \t 359.895503 s\n","585 \t 16.959818 \t 9.016461 \t7.869549 \t 0.073809 \t 360.371880 s\n","586 \t 16.571739 \t 8.471421 \t8.046493 \t 0.053825 \t 360.863020 s\n","587 \t 16.476318 \t 8.511434 \t7.925212 \t 0.039673 \t 361.332576 s\n","588 \t 16.331920 \t 8.763700 \t7.393471 \t 0.174749 \t 361.829612 s\n","589 \t 16.436598 \t 8.871568 \t7.454929 \t 0.110101 \t 362.475497 s\n","590 \t 16.195057 \t 8.745141 \t7.392481 \t 0.057436 \t 363.005017 s\n","591 \t 17.128513 \t 8.600820 \t8.395926 \t 0.131768 \t 363.615884 s\n","592 \t 17.125726 \t 8.488245 \t8.566180 \t 0.071301 \t 364.197760 s\n","593 \t 16.465716 \t 8.679492 \t7.674747 \t 0.111478 \t 364.793742 s\n","594 \t 16.320238 \t 8.711567 \t7.565023 \t 0.043648 \t 365.424793 s\n","595 \t 15.596685 \t 8.193149 \t7.360723 \t 0.042813 \t 366.018742 s\n","596 \t 16.814836 \t 8.790587 \t7.793252 \t 0.230997 \t 366.495759 s\n","597 \t 16.628978 \t 8.952665 \t7.595638 \t 0.080675 \t 366.968357 s\n","598 \t 16.206371 \t 8.856179 \t7.268698 \t 0.081495 \t 367.446171 s\n","599 \t 17.547659 \t 9.316615 \t8.148012 \t 0.083032 \t 367.933978 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:15<00:00,  8.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3961\n","Hit@10: 0.4731\n","Hit@3: 0.4000\n","Hit@1: 0.3385\n","Link Prediction on Validation Set (All)\n","MRR: 0.3961\n","Hit@10: 0.4731\n","Hit@3: 0.4000\n","Hit@1: 0.3385\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.1833\n","Hit@10: 0.4308\n","Hit@3: 0.1846\n","Hit@1: 0.0692\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5589\n","Hit@10: 0.7024\n","Hit@3: 0.5675\n","Hit@1: 0.4881\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1508\n","600 \t 17.246496 \t 9.054268 \t8.082296 \t 0.109932 \t 384.982603 s\n","601 \t 16.617410 \t 8.261250 \t8.089510 \t 0.266650 \t 385.455419 s\n","602 \t 16.395774 \t 8.447553 \t7.869721 \t 0.078500 \t 385.941234 s\n","603 \t 17.061948 \t 9.028089 \t7.971241 \t 0.062619 \t 386.408561 s\n","604 \t 16.217394 \t 7.998155 \t8.027382 \t 0.191857 \t 386.893255 s\n","605 \t 16.851535 \t 8.496015 \t8.291095 \t 0.064426 \t 387.362772 s\n","606 \t 16.570564 \t 9.159517 \t7.330527 \t 0.080520 \t 387.839488 s\n","607 \t 16.799441 \t 8.416594 \t8.128736 \t 0.254112 \t 388.308635 s\n","608 \t 16.472505 \t 8.868072 \t7.517837 \t 0.086595 \t 388.787290 s\n","609 \t 17.182556 \t 9.024696 \t8.055455 \t 0.102404 \t 389.362716 s\n","610 \t 16.924350 \t 8.793130 \t8.051905 \t 0.079314 \t 390.146553 s\n","611 \t 16.597247 \t 8.575028 \t7.950773 \t 0.071446 \t 390.759602 s\n","612 \t 16.189489 \t 8.531115 \t7.579145 \t 0.079230 \t 391.365074 s\n","613 \t 16.497632 \t 8.853807 \t7.597662 \t 0.046162 \t 392.100708 s\n","614 \t 16.429572 \t 8.289110 \t8.060756 \t 0.079706 \t 392.630387 s\n","615 \t 15.881976 \t 8.684183 \t7.163492 \t 0.034301 \t 393.197335 s\n","616 \t 16.140387 \t 8.529614 \t7.562892 \t 0.047881 \t 393.700082 s\n","617 \t 16.653948 \t 8.631991 \t7.982459 \t 0.039498 \t 394.242663 s\n","618 \t 16.330502 \t 8.388976 \t7.901141 \t 0.040384 \t 394.751034 s\n","619 \t 16.222779 \t 8.592140 \t7.586458 \t 0.044182 \t 395.343384 s\n","620 \t 16.104769 \t 8.621041 \t7.447502 \t 0.036225 \t 395.875041 s\n","621 \t 16.423981 \t 8.631664 \t7.758865 \t 0.033451 \t 396.400324 s\n","622 \t 16.755409 \t 8.457827 \t8.251616 \t 0.045967 \t 397.082626 s\n","623 \t 16.920140 \t 8.423176 \t8.455727 \t 0.041238 \t 397.557482 s\n","624 \t 16.555405 \t 8.481865 \t8.014590 \t 0.058950 \t 398.049095 s\n","625 \t 17.378620 \t 8.976731 \t8.345395 \t 0.056494 \t 398.548210 s\n","626 \t 15.811712 \t 8.140626 \t7.513225 \t 0.157861 \t 399.033248 s\n","627 \t 16.194114 \t 8.307952 \t7.824643 \t 0.061518 \t 399.510976 s\n","628 \t 16.649993 \t 8.286554 \t8.317666 \t 0.045774 \t 399.986648 s\n","629 \t 15.678730 \t 8.305212 \t7.340220 \t 0.033299 \t 400.471202 s\n","630 \t 17.346306 \t 8.920018 \t8.391184 \t 0.035104 \t 400.945821 s\n","631 \t 16.582829 \t 9.062849 \t7.478553 \t 0.041427 \t 401.417231 s\n","632 \t 16.445044 \t 8.559541 \t7.832184 \t 0.053318 \t 401.887965 s\n","633 \t 16.642032 \t 8.529660 \t8.084178 \t 0.028193 \t 402.481531 s\n","634 \t 16.887784 \t 8.979796 \t7.868604 \t 0.039384 \t 403.044161 s\n","635 \t 16.517966 \t 8.775887 \t7.669386 \t 0.072694 \t 403.856176 s\n","636 \t 16.193159 \t 8.258904 \t7.794795 \t 0.139461 \t 404.461292 s\n","637 \t 16.122205 \t 8.542158 \t7.546452 \t 0.033595 \t 405.048760 s\n","638 \t 16.580806 \t 9.158469 \t7.371760 \t 0.050577 \t 405.526955 s\n","639 \t 16.579596 \t 8.650347 \t7.873772 \t 0.055477 \t 406.012731 s\n","640 \t 16.330387 \t 8.408290 \t7.888765 \t 0.033332 \t 406.493916 s\n","641 \t 16.567215 \t 8.612526 \t7.916113 \t 0.038575 \t 406.968457 s\n","642 \t 16.331438 \t 8.419209 \t7.858094 \t 0.054135 \t 407.448319 s\n","643 \t 16.711542 \t 8.906800 \t7.745177 \t 0.059565 \t 407.917922 s\n","644 \t 16.596374 \t 8.592961 \t7.937795 \t 0.065618 \t 408.386220 s\n","645 \t 16.022406 \t 8.772371 \t7.217820 \t 0.032215 \t 408.879015 s\n","646 \t 15.390812 \t 7.894862 \t7.463040 \t 0.032909 \t 409.347739 s\n","647 \t 16.120952 \t 8.305846 \t7.754787 \t 0.060319 \t 409.835699 s\n","648 \t 16.812627 \t 8.628195 \t8.137277 \t 0.047154 \t 410.305869 s\n","649 \t 16.942064 \t 9.096324 \t7.740050 \t 0.105691 \t 410.959079 s\n","650 \t 16.214336 \t 8.327754 \t7.858747 \t 0.027834 \t 411.450809 s\n","651 \t 16.939668 \t 8.576069 \t8.243192 \t 0.120407 \t 411.945362 s\n","652 \t 16.109309 \t 8.567170 \t7.500410 \t 0.041730 \t 412.426150 s\n","653 \t 16.112103 \t 8.461650 \t7.620594 \t 0.029860 \t 412.900888 s\n","654 \t 16.196128 \t 8.395164 \t7.769132 \t 0.031831 \t 413.373488 s\n","655 \t 17.098185 \t 8.509556 \t8.519591 \t 0.069037 \t 413.854672 s\n","656 \t 16.314404 \t 8.094881 \t8.189258 \t 0.030265 \t 414.331048 s\n","657 \t 16.576723 \t 8.365748 \t8.122931 \t 0.088044 \t 414.816066 s\n","658 \t 16.194477 \t 8.747031 \t7.406425 \t 0.041021 \t 415.392207 s\n","659 \t 15.744653 \t 8.471276 \t7.233415 \t 0.039962 \t 415.967174 s\n","660 \t 16.492631 \t 8.749338 \t7.594154 \t 0.149138 \t 416.542436 s\n","661 \t 16.823812 \t 8.967540 \t7.829054 \t 0.027219 \t 417.138367 s\n","662 \t 16.043061 \t 8.158449 \t7.841910 \t 0.042704 \t 417.784902 s\n","663 \t 15.789683 \t 8.097039 \t7.649633 \t 0.043012 \t 418.318921 s\n","664 \t 15.848539 \t 8.417819 \t7.268156 \t 0.162565 \t 418.974310 s\n","665 \t 16.389064 \t 8.880006 \t7.476659 \t 0.032399 \t 419.446481 s\n","666 \t 15.707504 \t 7.909344 \t7.778367 \t 0.019794 \t 419.931752 s\n","667 \t 16.657335 \t 8.809021 \t7.820350 \t 0.027963 \t 420.408822 s\n","668 \t 16.126740 \t 8.261277 \t7.830977 \t 0.034486 \t 420.885175 s\n","669 \t 16.565275 \t 8.539298 \t7.985704 \t 0.040274 \t 421.372546 s\n","670 \t 16.261081 \t 8.291074 \t7.924390 \t 0.045616 \t 421.847116 s\n","671 \t 16.995736 \t 8.633169 \t8.323873 \t 0.038695 \t 422.319355 s\n","672 \t 16.044983 \t 8.464395 \t7.547602 \t 0.032987 \t 422.803695 s\n","673 \t 16.271604 \t 7.953815 \t8.265194 \t 0.052594 \t 423.289315 s\n","674 \t 16.612761 \t 8.590817 \t7.992785 \t 0.029159 \t 423.769379 s\n","675 \t 15.669394 \t 8.178021 \t7.463551 \t 0.027822 \t 424.249807 s\n","676 \t 16.524067 \t 8.688912 \t7.751381 \t 0.083773 \t 424.738716 s\n","677 \t 16.526246 \t 8.255615 \t8.236248 \t 0.034383 \t 425.225729 s\n","678 \t 16.297988 \t 8.226523 \t8.045553 \t 0.025912 \t 425.703321 s\n","679 \t 16.373087 \t 8.694393 \t7.643204 \t 0.035488 \t 426.346841 s\n","680 \t 16.676127 \t 9.041576 \t7.597445 \t 0.037106 \t 426.828898 s\n","681 \t 16.062371 \t 8.180886 \t7.771439 \t 0.110047 \t 427.304731 s\n","682 \t 16.783971 \t 8.467746 \t8.281426 \t 0.034799 \t 427.787445 s\n","683 \t 16.013367 \t 8.226840 \t7.743132 \t 0.043395 \t 428.344509 s\n","684 \t 16.192638 \t 8.766487 \t7.368584 \t 0.057568 \t 428.925588 s\n","685 \t 16.380535 \t 8.881112 \t7.465958 \t 0.033464 \t 429.497095 s\n","686 \t 16.023111 \t 8.222217 \t7.738518 \t 0.062378 \t 430.080675 s\n","687 \t 15.878846 \t 8.089638 \t7.738101 \t 0.051108 \t 430.732277 s\n","688 \t 15.321828 \t 8.291467 \t6.980619 \t 0.049742 \t 431.286639 s\n","689 \t 16.096125 \t 8.716290 \t7.346296 \t 0.033539 \t 431.952152 s\n","690 \t 15.286070 \t 8.012375 \t7.238308 \t 0.035387 \t 432.439161 s\n","691 \t 16.123998 \t 8.670573 \t7.420879 \t 0.032546 \t 432.911561 s\n","692 \t 16.129206 \t 8.569501 \t7.480099 \t 0.079605 \t 433.380043 s\n","693 \t 16.726193 \t 8.436790 \t8.247681 \t 0.041722 \t 433.880369 s\n","694 \t 16.687618 \t 8.710182 \t7.913054 \t 0.064380 \t 434.347114 s\n","695 \t 17.327025 \t 8.880266 \t8.299250 \t 0.147510 \t 434.843004 s\n","696 \t 16.178009 \t 8.760370 \t7.389600 \t 0.028039 \t 435.316404 s\n","697 \t 16.039140 \t 8.441993 \t7.566289 \t 0.030857 \t 435.800142 s\n","698 \t 16.243060 \t 8.080274 \t8.134110 \t 0.028676 \t 436.275514 s\n","699 \t 16.172861 \t 9.020141 \t7.102360 \t 0.050360 \t 436.755519 s\n","700 \t 15.626549 \t 8.259758 \t7.330566 \t 0.036225 \t 437.388668 s\n","701 \t 16.032842 \t 8.364372 \t7.615975 \t 0.052495 \t 437.871764 s\n","702 \t 16.074395 \t 8.420259 \t7.614324 \t 0.039812 \t 438.342083 s\n","703 \t 16.260383 \t 8.224432 \t8.015877 \t 0.020073 \t 438.822989 s\n","704 \t 16.440992 \t 8.510291 \t7.906341 \t 0.024361 \t 439.292807 s\n","705 \t 16.316842 \t 8.505678 \t7.775843 \t 0.035321 \t 439.780363 s\n","706 \t 15.721597 \t 8.552972 \t7.149105 \t 0.019520 \t 440.248577 s\n","707 \t 16.359145 \t 8.545550 \t7.779573 \t 0.034023 \t 440.738834 s\n","708 \t 15.714348 \t 8.176700 \t7.507860 \t 0.029787 \t 441.265805 s\n","709 \t 15.884677 \t 8.049844 \t7.795056 \t 0.039778 \t 441.884731 s\n","710 \t 16.198219 \t 8.379715 \t7.796726 \t 0.021778 \t 442.443360 s\n","711 \t 15.649527 \t 8.332800 \t7.290348 \t 0.026380 \t 443.255955 s\n","712 \t 16.451013 \t 8.449978 \t7.956370 \t 0.044666 \t 443.907181 s\n","713 \t 15.751653 \t 8.251327 \t7.461023 \t 0.039303 \t 444.412680 s\n","714 \t 15.812750 \t 8.526498 \t7.255475 \t 0.030777 \t 444.890708 s\n","715 \t 15.333146 \t 7.912896 \t7.405570 \t 0.014681 \t 445.389934 s\n","716 \t 15.024755 \t 7.740422 \t7.253577 \t 0.030756 \t 445.869609 s\n","717 \t 16.771512 \t 8.588158 \t8.141109 \t 0.042245 \t 446.357501 s\n","718 \t 16.281775 \t 8.260329 \t7.994066 \t 0.027380 \t 446.840124 s\n","719 \t 16.085121 \t 8.363837 \t7.598985 \t 0.122298 \t 447.322123 s\n","720 \t 16.242167 \t 8.448662 \t7.717675 \t 0.075829 \t 447.797899 s\n","721 \t 16.118752 \t 8.526016 \t7.566181 \t 0.026555 \t 448.273694 s\n","722 \t 16.289321 \t 8.514641 \t7.727485 \t 0.047195 \t 448.748523 s\n","723 \t 16.501017 \t 8.706998 \t7.765340 \t 0.028678 \t 449.227613 s\n","724 \t 15.572908 \t 8.137655 \t7.377105 \t 0.058148 \t 449.874397 s\n","725 \t 16.364414 \t 8.651799 \t7.165023 \t 0.547592 \t 450.346997 s\n","726 \t 15.488397 \t 8.531584 \t6.898126 \t 0.058687 \t 450.831101 s\n","727 \t 16.551133 \t 8.675374 \t7.829630 \t 0.046129 \t 451.314206 s\n","728 \t 16.332199 \t 8.396393 \t7.870429 \t 0.065377 \t 451.789845 s\n","729 \t 16.285567 \t 8.552258 \t7.565752 \t 0.167558 \t 452.267755 s\n","730 \t 16.094602 \t 8.382541 \t7.657072 \t 0.054989 \t 452.746586 s\n","731 \t 15.833992 \t 8.057289 \t7.719857 \t 0.056845 \t 453.232962 s\n","732 \t 16.547495 \t 8.664026 \t7.822480 \t 0.060989 \t 453.709049 s\n","733 \t 16.098414 \t 8.622507 \t7.462761 \t 0.013146 \t 454.239698 s\n","734 \t 16.486176 \t 8.421442 \t7.952931 \t 0.111804 \t 454.833282 s\n","735 \t 16.760736 \t 8.512362 \t8.186995 \t 0.061381 \t 455.416584 s\n","736 \t 15.375322 \t 8.118225 \t7.219811 \t 0.037286 \t 455.994339 s\n","737 \t 15.732588 \t 8.261742 \t7.413635 \t 0.057211 \t 456.606453 s\n","738 \t 16.337745 \t 8.589968 \t7.723794 \t 0.023982 \t 457.405407 s\n","739 \t 16.292599 \t 8.735578 \t7.522698 \t 0.034323 \t 457.897295 s\n","740 \t 16.186385 \t 8.304814 \t7.864278 \t 0.017293 \t 458.376085 s\n","741 \t 15.884076 \t 8.234640 \t7.553814 \t 0.095621 \t 458.871062 s\n","742 \t 16.649698 \t 8.681480 \t7.925259 \t 0.042959 \t 459.341782 s\n","743 \t 16.200712 \t 8.008731 \t8.168719 \t 0.023261 \t 459.820771 s\n","744 \t 16.565517 \t 8.438405 \t8.061322 \t 0.065790 \t 460.289606 s\n","745 \t 15.670012 \t 8.353445 \t7.282340 \t 0.034228 \t 460.774602 s\n","746 \t 16.814159 \t 8.910485 \t7.861915 \t 0.041759 \t 461.242308 s\n","747 \t 15.673120 \t 8.278793 \t7.318454 \t 0.075872 \t 461.721026 s\n","748 \t 15.715825 \t 8.320467 \t7.364385 \t 0.030973 \t 462.192885 s\n","749 \t 16.368112 \t 8.614521 \t7.715477 \t 0.038113 \t 462.668236 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:15<00:00,  8.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4167\n","Hit@10: 0.5538\n","Hit@3: 0.4423\n","Hit@1: 0.3462\n","Link Prediction on Validation Set (All)\n","MRR: 0.4167\n","Hit@10: 0.5538\n","Hit@3: 0.4423\n","Hit@1: 0.3462\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2814\n","Hit@10: 0.4923\n","Hit@3: 0.2769\n","Hit@1: 0.1923\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6128\n","Hit@10: 0.7381\n","Hit@3: 0.6270\n","Hit@1: 0.5516\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1206\n","750 \t 16.226499 \t 8.306543 \t7.794176 \t 0.125780 \t 479.662929 s\n","751 \t 15.750314 \t 8.498041 \t7.204597 \t 0.047675 \t 480.174541 s\n","752 \t 15.803443 \t 8.655883 \t7.106487 \t 0.041073 \t 480.975763 s\n","753 \t 15.872729 \t 8.326741 \t7.503145 \t 0.042844 \t 481.547302 s\n","754 \t 15.541469 \t 7.814865 \t7.698781 \t 0.027823 \t 482.119767 s\n","755 \t 16.078615 \t 8.304534 \t7.717682 \t 0.056398 \t 482.751873 s\n","756 \t 16.286815 \t 8.746888 \t7.507154 \t 0.032773 \t 483.351479 s\n","757 \t 16.542823 \t 8.475926 \t8.002618 \t 0.064279 \t 483.845403 s\n","758 \t 15.156513 \t 8.430868 \t6.674648 \t 0.050997 \t 484.319961 s\n","759 \t 16.225502 \t 8.544698 \t7.630521 \t 0.050283 \t 484.808649 s\n","760 \t 15.851625 \t 8.610495 \t7.117851 \t 0.123280 \t 485.294018 s\n","761 \t 16.705123 \t 8.638114 \t8.032984 \t 0.034025 \t 485.779103 s\n","762 \t 16.509007 \t 8.709497 \t7.769106 \t 0.030404 \t 486.255426 s\n","763 \t 15.776655 \t 8.525099 \t7.206892 \t 0.044665 \t 486.746447 s\n","764 \t 15.715234 \t 7.944890 \t7.735620 \t 0.034725 \t 487.233724 s\n","765 \t 16.374771 \t 8.809260 \t7.524415 \t 0.041096 \t 487.821953 s\n","766 \t 16.430646 \t 8.685720 \t7.707590 \t 0.037337 \t 488.353320 s\n","767 \t 15.850177 \t 8.203926 \t7.623063 \t 0.023189 \t 489.079277 s\n","768 \t 16.248379 \t 8.726006 \t7.501464 \t 0.020909 \t 489.556221 s\n","769 \t 16.477878 \t 8.741758 \t7.695433 \t 0.040685 \t 490.147851 s\n","770 \t 16.059071 \t 8.731814 \t7.296180 \t 0.031076 \t 490.664485 s\n","771 \t 15.829580 \t 8.152865 \t7.622301 \t 0.054415 \t 491.199717 s\n","772 \t 17.298054 \t 9.039698 \t8.116950 \t 0.141407 \t 491.692581 s\n","773 \t 15.523532 \t 8.039985 \t7.448062 \t 0.035484 \t 492.264822 s\n","774 \t 15.828241 \t 8.349146 \t7.422508 \t 0.056587 \t 492.786371 s\n","775 \t 16.309875 \t 8.509036 \t7.747659 \t 0.053181 \t 493.338250 s\n","776 \t 15.246874 \t 8.191471 \t7.031263 \t 0.024141 \t 493.934899 s\n","777 \t 15.658047 \t 8.446297 \t7.175710 \t 0.036040 \t 494.497309 s\n","778 \t 15.837003 \t 7.996155 \t7.649330 \t 0.191517 \t 495.103181 s\n","779 \t 16.679915 \t 8.479097 \t8.111355 \t 0.089462 \t 495.711917 s\n","780 \t 16.280735 \t 8.550003 \t7.690256 \t 0.040476 \t 496.301230 s\n","781 \t 15.699190 \t 8.374703 \t7.278836 \t 0.045651 \t 496.777796 s\n","782 \t 16.215898 \t 8.437538 \t7.742697 \t 0.035663 \t 497.425029 s\n","783 \t 16.174377 \t 8.617178 \t7.538359 \t 0.018841 \t 497.908809 s\n","784 \t 15.608737 \t 8.155256 \t7.412139 \t 0.041343 \t 498.388496 s\n","785 \t 16.078517 \t 8.108313 \t7.938406 \t 0.031798 \t 498.870276 s\n","786 \t 15.441785 \t 8.295151 \t7.121813 \t 0.024820 \t 499.342354 s\n","787 \t 16.099103 \t 8.282529 \t7.781910 \t 0.034664 \t 499.823299 s\n","788 \t 16.000679 \t 8.242265 \t7.721177 \t 0.037237 \t 500.299560 s\n","789 \t 16.704010 \t 8.405470 \t8.256722 \t 0.041817 \t 500.780311 s\n","790 \t 15.816927 \t 8.383337 \t7.406949 \t 0.026642 \t 501.262572 s\n","791 \t 16.064720 \t 8.357388 \t7.669523 \t 0.037808 \t 501.741645 s\n","792 \t 15.035454 \t 7.834216 \t7.143461 \t 0.057777 \t 502.383147 s\n","793 \t 15.515461 \t 8.147515 \t7.339160 \t 0.028786 \t 502.861472 s\n","794 \t 16.516930 \t 8.491694 \t7.990784 \t 0.034451 \t 503.340617 s\n","795 \t 15.722307 \t 8.720489 \t6.908427 \t 0.093390 \t 503.820729 s\n","796 \t 16.199386 \t 8.777518 \t7.260200 \t 0.161669 \t 504.302074 s\n","797 \t 16.758628 \t 8.697539 \t8.014921 \t 0.046169 \t 504.777194 s\n","798 \t 16.251710 \t 8.224500 \t7.980872 \t 0.046339 \t 505.262598 s\n","799 \t 16.306997 \t 8.694130 \t7.558454 \t 0.054413 \t 505.758077 s\n","800 \t 15.955688 \t 8.421156 \t7.513287 \t 0.021244 \t 506.273116 s\n","801 \t 16.403633 \t 8.973193 \t7.393263 \t 0.037178 \t 506.885704 s\n","802 \t 15.385061 \t 8.236818 \t7.121544 \t 0.026699 \t 507.459806 s\n","803 \t 15.944619 \t 8.370977 \t7.546256 \t 0.027386 \t 508.239992 s\n","804 \t 16.107318 \t 8.694767 \t7.355516 \t 0.057036 \t 508.889413 s\n","805 \t 15.643306 \t 8.117865 \t7.486962 \t 0.038479 \t 509.456614 s\n","806 \t 15.701217 \t 8.365105 \t7.316834 \t 0.019278 \t 509.946461 s\n","807 \t 16.143499 \t 8.563828 \t7.556221 \t 0.023450 \t 510.418201 s\n","808 \t 15.642648 \t 7.960162 \t7.649019 \t 0.033467 \t 510.904206 s\n","809 \t 16.348000 \t 8.574410 \t7.646735 \t 0.126854 \t 511.377445 s\n","810 \t 16.133342 \t 8.294281 \t7.719005 \t 0.120056 \t 511.861582 s\n","811 \t 15.718708 \t 8.317985 \t7.364471 \t 0.036252 \t 512.331825 s\n","812 \t 15.636904 \t 8.385506 \t7.207463 \t 0.043935 \t 512.814605 s\n","813 \t 16.630038 \t 8.815407 \t7.783794 \t 0.030837 \t 513.283541 s\n","814 \t 15.850190 \t 8.420480 \t7.411339 \t 0.018371 \t 513.937937 s\n","815 \t 15.796816 \t 8.264889 \t7.455074 \t 0.076853 \t 514.406897 s\n","816 \t 16.429053 \t 8.282973 \t8.104996 \t 0.041083 \t 514.883667 s\n","817 \t 16.193226 \t 8.210572 \t7.949878 \t 0.032777 \t 515.359373 s\n","818 \t 15.316522 \t 8.050744 \t7.230378 \t 0.035401 \t 515.845233 s\n","819 \t 15.518192 \t 8.329545 \t6.982096 \t 0.206550 \t 516.319168 s\n","820 \t 16.353638 \t 8.352850 \t7.970349 \t 0.030440 \t 516.805319 s\n","821 \t 15.946392 \t 8.094817 \t7.768702 \t 0.082874 \t 517.289685 s\n","822 \t 15.796857 \t 8.492832 \t7.245833 \t 0.058192 \t 517.763866 s\n","823 \t 16.221120 \t 8.265689 \t7.913642 \t 0.041789 \t 518.259604 s\n","824 \t 16.379030 \t 8.627047 \t7.704662 \t 0.047321 \t 518.742769 s\n","825 \t 15.677331 \t 8.279667 \t7.369380 \t 0.028284 \t 519.279193 s\n","826 \t 16.051019 \t 8.579618 \t7.411581 \t 0.059821 \t 519.865304 s\n","827 \t 15.932696 \t 8.117782 \t7.759296 \t 0.055618 \t 520.656732 s\n","828 \t 16.052286 \t 8.240561 \t7.777923 \t 0.033803 \t 521.251385 s\n","829 \t 16.057723 \t 8.219896 \t7.807550 \t 0.030277 \t 521.881102 s\n","830 \t 16.536177 \t 8.514438 \t7.874918 \t 0.146822 \t 522.455916 s\n","831 \t 16.467262 \t 8.561658 \t7.876909 \t 0.028695 \t 522.927501 s\n","832 \t 16.006830 \t 8.412284 \t7.568559 \t 0.025986 \t 523.401451 s\n","833 \t 16.057854 \t 8.108427 \t7.928330 \t 0.021097 \t 523.876419 s\n","834 \t 15.771138 \t 8.226472 \t7.510086 \t 0.034580 \t 524.355341 s\n","835 \t 15.720258 \t 7.913830 \t7.773934 \t 0.032493 \t 524.831903 s\n","836 \t 16.173162 \t 8.898011 \t7.241259 \t 0.033893 \t 525.310194 s\n","837 \t 15.534321 \t 8.528347 \t6.967579 \t 0.038395 \t 525.794019 s\n","838 \t 15.537795 \t 8.340866 \t7.073848 \t 0.123080 \t 526.287499 s\n","839 \t 16.187965 \t 8.410166 \t7.738933 \t 0.038866 \t 526.765985 s\n","840 \t 16.274045 \t 8.542670 \t7.704878 \t 0.026497 \t 527.250766 s\n","841 \t 15.210447 \t 8.236300 \t6.950759 \t 0.023389 \t 527.883573 s\n","842 \t 16.168437 \t 8.854880 \t7.224597 \t 0.088960 \t 528.381631 s\n","843 \t 16.503850 \t 8.302435 \t8.180719 \t 0.020695 \t 528.860357 s\n","844 \t 16.086786 \t 8.645051 \t7.415427 \t 0.026307 \t 529.350796 s\n","845 \t 15.521187 \t 8.385532 \t7.072896 \t 0.062759 \t 529.822764 s\n","846 \t 16.418796 \t 8.658161 \t7.728781 \t 0.031854 \t 530.295042 s\n","847 \t 15.600231 \t 8.394642 \t7.162464 \t 0.043125 \t 530.784596 s\n","848 \t 16.082830 \t 8.135098 \t7.918489 \t 0.029244 \t 531.254220 s\n","849 \t 15.820113 \t 7.972695 \t7.835168 \t 0.012250 \t 531.746034 s\n","850 \t 16.501945 \t 8.311524 \t8.153842 \t 0.036579 \t 532.254966 s\n","851 \t 16.031809 \t 8.874911 \t7.135244 \t 0.021654 \t 532.882052 s\n","852 \t 16.464362 \t 8.193422 \t8.245089 \t 0.025851 \t 533.453814 s\n","853 \t 16.461326 \t 8.761812 \t7.668928 \t 0.030585 \t 534.072400 s\n","854 \t 15.679116 \t 7.901409 \t7.741991 \t 0.035717 \t 534.705175 s\n","855 \t 16.465290 \t 8.290037 \t8.106682 \t 0.068573 \t 535.320324 s\n","856 \t 16.197865 \t 8.437130 \t7.728731 \t 0.032004 \t 535.973071 s\n","857 \t 16.232857 \t 8.644446 \t7.558903 \t 0.029508 \t 536.443207 s\n","858 \t 15.133413 \t 7.898449 \t7.206695 \t 0.028270 \t 536.917078 s\n","859 \t 15.537093 \t 8.034815 \t7.475789 \t 0.026489 \t 537.394230 s\n","860 \t 15.936864 \t 8.223696 \t7.627109 \t 0.086058 \t 537.869426 s\n","861 \t 15.868712 \t 8.198046 \t7.601639 \t 0.069026 \t 538.349540 s\n","862 \t 15.551184 \t 7.784386 \t7.736869 \t 0.029930 \t 538.829599 s\n","863 \t 16.121325 \t 8.099397 \t7.978292 \t 0.043635 \t 539.305023 s\n","864 \t 15.164131 \t 7.898150 \t7.232175 \t 0.033805 \t 539.788733 s\n","865 \t 15.913550 \t 8.426341 \t7.362067 \t 0.125142 \t 540.262570 s\n","866 \t 16.138036 \t 8.387694 \t7.709595 \t 0.040747 \t 540.900695 s\n","867 \t 16.487035 \t 8.556474 \t7.847548 \t 0.083012 \t 541.371793 s\n","868 \t 15.397017 \t 8.045240 \t7.135857 \t 0.215920 \t 541.854577 s\n","869 \t 15.031858 \t 7.706222 \t7.277755 \t 0.047882 \t 542.329304 s\n","870 \t 14.925889 \t 7.254180 \t7.643229 \t 0.028479 \t 542.814359 s\n","871 \t 15.794079 \t 8.429995 \t7.331408 \t 0.032676 \t 543.293112 s\n","872 \t 15.309799 \t 8.039843 \t7.247920 \t 0.022036 \t 543.766417 s\n","873 \t 15.303931 \t 8.065926 \t7.207623 \t 0.030381 \t 544.271344 s\n","874 \t 16.086628 \t 8.411508 \t7.632145 \t 0.042975 \t 544.751742 s\n","875 \t 15.888268 \t 8.260370 \t7.589836 \t 0.038063 \t 545.249319 s\n","876 \t 15.486828 \t 8.118084 \t7.332237 \t 0.036506 \t 545.865992 s\n","877 \t 15.724679 \t 8.323237 \t7.362408 \t 0.039035 \t 546.682154 s\n","878 \t 15.956419 \t 8.096331 \t7.805161 \t 0.054927 \t 547.264216 s\n","879 \t 15.913774 \t 7.996268 \t7.863327 \t 0.054178 \t 547.867837 s\n","880 \t 15.654168 \t 7.942038 \t7.644392 \t 0.067738 \t 548.472026 s\n","881 \t 15.588566 \t 7.911829 \t7.616394 \t 0.060344 \t 548.946619 s\n","882 \t 15.946527 \t 8.065553 \t7.864008 \t 0.016967 \t 549.432532 s\n","883 \t 15.707463 \t 7.952354 \t7.713994 \t 0.041115 \t 549.901043 s\n","884 \t 16.055520 \t 8.360200 \t7.669988 \t 0.025332 \t 550.379749 s\n","885 \t 15.963256 \t 8.047418 \t7.876650 \t 0.039188 \t 550.851643 s\n","886 \t 16.594269 \t 8.667647 \t7.903634 \t 0.022988 \t 551.325321 s\n","887 \t 16.185629 \t 7.840154 \t8.133249 \t 0.212227 \t 551.804488 s\n","888 \t 15.352326 \t 7.942654 \t7.347371 \t 0.062301 \t 552.454462 s\n","889 \t 15.870013 \t 7.956746 \t7.803175 \t 0.110091 \t 552.922455 s\n","890 \t 15.654138 \t 7.986032 \t7.644076 \t 0.024029 \t 553.398891 s\n","891 \t 16.289133 \t 8.476730 \t7.786996 \t 0.025407 \t 553.868217 s\n","892 \t 16.451788 \t 8.325714 \t8.098358 \t 0.027715 \t 554.353038 s\n","893 \t 16.166407 \t 8.416483 \t7.637851 \t 0.112072 \t 554.841786 s\n","894 \t 16.207760 \t 8.024710 \t8.166677 \t 0.016373 \t 555.326293 s\n","895 \t 15.567791 \t 7.998837 \t7.535525 \t 0.033428 \t 555.803492 s\n","896 \t 15.578831 \t 8.283352 \t7.260744 \t 0.034735 \t 556.276878 s\n","897 \t 15.621888 \t 8.251145 \t7.333495 \t 0.037247 \t 556.766599 s\n","898 \t 15.202933 \t 7.855247 \t7.219110 \t 0.128576 \t 557.243623 s\n","899 \t 15.869837 \t 8.140482 \t7.689183 \t 0.040172 \t 557.895320 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:15<00:00,  8.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4359\n","Hit@10: 0.5577\n","Hit@3: 0.4423\n","Hit@1: 0.3692\n","Link Prediction on Validation Set (All)\n","MRR: 0.4359\n","Hit@10: 0.5577\n","Hit@3: 0.4423\n","Hit@1: 0.3692\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2871\n","Hit@10: 0.4846\n","Hit@3: 0.2769\n","Hit@1: 0.2000\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6124\n","Hit@10: 0.7302\n","Hit@3: 0.6151\n","Hit@1: 0.5556\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1172\n","900 \t 15.589457 \t 8.156695 \t7.397437 \t 0.035324 \t 575.398032 s\n","901 \t 16.367513 \t 8.333501 \t7.996220 \t 0.037792 \t 575.872097 s\n","902 \t 15.488149 \t 8.086823 \t7.381001 \t 0.020325 \t 576.355580 s\n","903 \t 15.517688 \t 7.912743 \t7.560014 \t 0.044931 \t 576.833919 s\n","904 \t 14.790725 \t 7.828558 \t6.920550 \t 0.041616 \t 577.327035 s\n","905 \t 15.899399 \t 8.215899 \t7.619202 \t 0.064298 \t 577.808200 s\n","906 \t 16.311191 \t 8.210766 \t8.069485 \t 0.030940 \t 578.280727 s\n","907 \t 15.551608 \t 8.067011 \t7.307934 \t 0.176664 \t 578.773989 s\n","908 \t 15.525878 \t 8.259394 \t7.239838 \t 0.026646 \t 579.257729 s\n","909 \t 15.983255 \t 8.127591 \t7.693253 \t 0.162412 \t 579.751326 s\n","910 \t 15.754681 \t 8.275895 \t7.440705 \t 0.038082 \t 580.397666 s\n","911 \t 16.376054 \t 8.212024 \t8.136750 \t 0.027281 \t 580.870956 s\n","912 \t 15.134707 \t 7.646859 \t7.350741 \t 0.137106 \t 581.338485 s\n","913 \t 15.790244 \t 8.263106 \t7.465392 \t 0.061745 \t 581.862068 s\n","914 \t 15.300337 \t 7.884142 \t7.248485 \t 0.167710 \t 582.357589 s\n","915 \t 16.048834 \t 8.412213 \t7.598697 \t 0.037924 \t 582.887511 s\n","916 \t 16.167776 \t 8.353617 \t7.759710 \t 0.054450 \t 583.366445 s\n","917 \t 15.644271 \t 8.165308 \t7.426811 \t 0.052152 \t 583.930617 s\n","918 \t 15.904796 \t 8.287489 \t7.593203 \t 0.024104 \t 584.442459 s\n","919 \t 15.966334 \t 8.115744 \t7.474645 \t 0.375946 \t 585.006615 s\n","920 \t 16.004319 \t 8.321333 \t7.647965 \t 0.035021 \t 585.683880 s\n","921 \t 15.774751 \t 8.217242 \t7.526560 \t 0.030949 \t 586.619572 s\n","922 \t 15.013132 \t 7.634973 \t7.309667 \t 0.068491 \t 587.296396 s\n","923 \t 15.355891 \t 7.909772 \t7.408299 \t 0.037820 \t 587.990577 s\n","924 \t 16.611011 \t 8.230419 \t8.229382 \t 0.151209 \t 588.499202 s\n","925 \t 16.240481 \t 8.534538 \t7.663588 \t 0.042355 \t 588.993297 s\n","926 \t 15.008956 \t 7.537928 \t7.393482 \t 0.077546 \t 589.466826 s\n","927 \t 14.970284 \t 7.996338 \t6.931349 \t 0.042597 \t 589.948328 s\n","928 \t 15.441788 \t 7.930464 \t7.490935 \t 0.020390 \t 590.420360 s\n","929 \t 14.966143 \t 7.789737 \t7.129607 \t 0.046799 \t 590.897897 s\n","930 \t 15.645631 \t 8.338518 \t7.258807 \t 0.048306 \t 591.375394 s\n","931 \t 14.804051 \t 8.052153 \t6.716817 \t 0.035080 \t 591.853641 s\n","932 \t 16.347418 \t 8.677774 \t7.641099 \t 0.028544 \t 592.333115 s\n","933 \t 14.794218 \t 7.724739 \t7.048968 \t 0.020511 \t 592.807917 s\n","934 \t 15.628830 \t 8.263019 \t7.325304 \t 0.040508 \t 593.472976 s\n","935 \t 15.333559 \t 8.148226 \t7.152122 \t 0.033210 \t 593.953547 s\n","936 \t 14.871000 \t 7.710956 \t7.035691 \t 0.124354 \t 594.423018 s\n","937 \t 15.781281 \t 8.202288 \t7.551514 \t 0.027479 \t 594.897343 s\n","938 \t 15.504372 \t 7.995123 \t7.481231 \t 0.028017 \t 595.395266 s\n","939 \t 15.848746 \t 8.098502 \t7.685471 \t 0.064774 \t 595.872664 s\n","940 \t 15.176717 \t 8.002791 \t7.157461 \t 0.016463 \t 596.361970 s\n","941 \t 16.335846 \t 8.541249 \t7.746824 \t 0.047773 \t 596.835316 s\n","942 \t 15.148325 \t 7.856043 \t7.252224 \t 0.040058 \t 597.316247 s\n","943 \t 15.228739 \t 7.798211 \t7.403295 \t 0.027234 \t 597.790238 s\n","944 \t 15.066159 \t 7.378847 \t7.664558 \t 0.022754 \t 598.370596 s\n","945 \t 15.346607 \t 8.092573 \t7.218516 \t 0.035518 \t 598.947341 s\n","946 \t 15.870924 \t 8.128354 \t7.697580 \t 0.044991 \t 599.529315 s\n","947 \t 15.225850 \t 7.195516 \t7.971697 \t 0.058636 \t 600.121123 s\n","948 \t 15.639618 \t 8.030641 \t7.591871 \t 0.017106 \t 600.960106 s\n","949 \t 15.582219 \t 8.026516 \t7.530347 \t 0.025355 \t 601.439560 s\n","950 \t 15.082847 \t 7.661118 \t7.389316 \t 0.032413 \t 601.910574 s\n","951 \t 14.900752 \t 8.022408 \t6.846589 \t 0.031755 \t 602.388864 s\n","952 \t 16.229681 \t 8.345400 \t7.691900 \t 0.192381 \t 602.869799 s\n","953 \t 15.568771 \t 7.839210 \t7.680959 \t 0.048602 \t 603.359143 s\n","954 \t 14.974194 \t 7.836898 \t7.069987 \t 0.067310 \t 603.837256 s\n","955 \t 15.087029 \t 7.699860 \t7.349145 \t 0.038024 \t 604.309217 s\n","956 \t 16.230257 \t 8.137327 \t8.055519 \t 0.037410 \t 604.808553 s\n","957 \t 15.432653 \t 8.380152 \t7.016437 \t 0.036065 \t 605.297210 s\n","958 \t 16.352083 \t 8.000638 \t8.321016 \t 0.030428 \t 605.795578 s\n","959 \t 15.447859 \t 8.107162 \t7.134194 \t 0.206504 \t 606.278621 s\n","960 \t 14.887219 \t 7.794624 \t7.063785 \t 0.028810 \t 606.769393 s\n","961 \t 15.154121 \t 8.098221 \t7.037232 \t 0.018667 \t 607.243018 s\n","962 \t 14.898345 \t 7.884552 \t6.973399 \t 0.040394 \t 607.889496 s\n","963 \t 15.342845 \t 8.167415 \t7.132509 \t 0.042921 \t 608.371536 s\n","964 \t 15.236974 \t 7.827650 \t7.353902 \t 0.055422 \t 608.868536 s\n","965 \t 15.212816 \t 7.629675 \t7.557683 \t 0.025457 \t 609.355098 s\n","966 \t 14.923835 \t 7.677388 \t7.208393 \t 0.038053 \t 609.835844 s\n","967 \t 15.713518 \t 7.817405 \t7.872491 \t 0.023623 \t 610.311510 s\n","968 \t 15.365819 \t 8.096087 \t7.247301 \t 0.022431 \t 610.796383 s\n","969 \t 15.676492 \t 7.914498 \t7.741967 \t 0.020026 \t 611.376882 s\n","970 \t 15.553749 \t 8.159453 \t7.365634 \t 0.028661 \t 611.961795 s\n","971 \t 15.036591 \t 7.898653 \t7.113201 \t 0.024737 \t 612.522237 s\n","972 \t 15.782547 \t 8.045145 \t7.691790 \t 0.045614 \t 613.119471 s\n","973 \t 15.787516 \t 7.779654 \t7.975421 \t 0.032441 \t 613.770524 s\n","974 \t 15.396917 \t 7.874888 \t7.496023 \t 0.026005 \t 614.289522 s\n","975 \t 15.100157 \t 7.868429 \t7.197495 \t 0.034233 \t 614.782702 s\n","976 \t 15.645871 \t 8.419191 \t7.203190 \t 0.023490 \t 615.257587 s\n","977 \t 15.172247 \t 7.754168 \t7.369670 \t 0.048410 \t 615.910689 s\n","978 \t 16.157209 \t 8.098876 \t8.029194 \t 0.029139 \t 616.389339 s\n","979 \t 15.598815 \t 8.187166 \t7.138413 \t 0.273236 \t 616.874467 s\n","980 \t 15.728411 \t 7.969327 \t7.737583 \t 0.021501 \t 617.350439 s\n","981 \t 15.761818 \t 8.203901 \t7.493277 \t 0.064641 \t 617.834682 s\n","982 \t 15.769086 \t 8.034734 \t7.674627 \t 0.059725 \t 618.306118 s\n","983 \t 15.445703 \t 7.888811 \t7.534471 \t 0.022421 \t 618.786556 s\n","984 \t 15.616965 \t 7.826930 \t7.753098 \t 0.036937 \t 619.274679 s\n","985 \t 15.465425 \t 7.884009 \t7.549376 \t 0.032040 \t 619.753838 s\n","986 \t 15.750963 \t 8.041506 \t7.658374 \t 0.051083 \t 620.236310 s\n","987 \t 15.577400 \t 7.601168 \t7.950573 \t 0.025659 \t 620.713949 s\n","988 \t 14.651631 \t 7.591572 \t7.031300 \t 0.028760 \t 621.190898 s\n","989 \t 14.551783 \t 7.075175 \t7.373817 \t 0.102791 \t 621.667856 s\n","990 \t 15.017065 \t 7.908560 \t7.086854 \t 0.021652 \t 622.141187 s\n","991 \t 15.376639 \t 7.955438 \t7.358976 \t 0.062225 \t 622.617336 s\n","992 \t 14.928863 \t 7.280366 \t7.614613 \t 0.033884 \t 623.258316 s\n","993 \t 15.935486 \t 8.207099 \t7.698755 \t 0.029631 \t 623.765268 s\n","994 \t 15.126419 \t 7.859962 \t7.230480 \t 0.035977 \t 624.308752 s\n","995 \t 16.062188 \t 8.411034 \t7.629157 \t 0.021997 \t 624.891715 s\n","996 \t 15.645837 \t 7.947291 \t7.657822 \t 0.040724 \t 625.482428 s\n","997 \t 15.690902 \t 7.916830 \t7.727816 \t 0.046256 \t 626.052089 s\n","998 \t 15.209242 \t 7.653850 \t7.529498 \t 0.025893 \t 626.723911 s\n","999 \t 15.667437 \t 8.168324 \t7.479641 \t 0.019472 \t 627.310718 s\n","1000 \t 15.251391 \t 7.643196 \t7.577967 \t 0.030229 \t 627.788198 s\n","1001 \t 16.411638 \t 8.595987 \t7.785975 \t 0.029676 \t 628.262359 s\n","1002 \t 15.392221 \t 7.931943 \t7.424419 \t 0.035860 \t 628.918637 s\n","1003 \t 15.493050 \t 8.076320 \t7.383709 \t 0.033021 \t 629.416793 s\n","1004 \t 15.560256 \t 8.132689 \t7.388006 \t 0.039560 \t 629.889484 s\n","1005 \t 15.184632 \t 7.302330 \t7.844168 \t 0.038135 \t 630.368691 s\n","1006 \t 15.590034 \t 7.869184 \t7.588017 \t 0.132833 \t 630.846783 s\n","1007 \t 15.398829 \t 8.268718 \t7.098475 \t 0.031635 \t 631.323091 s\n","1008 \t 15.916977 \t 8.091644 \t7.807448 \t 0.017885 \t 631.818712 s\n","1009 \t 15.088898 \t 7.765137 \t7.285752 \t 0.038009 \t 632.297210 s\n","1010 \t 16.150779 \t 8.266474 \t7.833096 \t 0.051209 \t 632.787539 s\n","1011 \t 14.994824 \t 7.652645 \t7.319676 \t 0.022503 \t 633.261646 s\n","1012 \t 15.951322 \t 8.078514 \t7.847739 \t 0.025069 \t 633.746253 s\n","1013 \t 16.208220 \t 8.250163 \t7.908509 \t 0.049549 \t 634.395043 s\n","1014 \t 14.878136 \t 7.620931 \t7.236181 \t 0.021024 \t 634.887414 s\n","1015 \t 15.426955 \t 8.440829 \t6.955585 \t 0.030540 \t 635.359085 s\n","1016 \t 16.048118 \t 8.337287 \t7.679421 \t 0.031409 \t 635.844105 s\n","1017 \t 15.791715 \t 8.377163 \t7.371564 \t 0.042988 \t 636.331554 s\n","1018 \t 15.917843 \t 7.922351 \t7.904588 \t 0.090903 \t 636.816343 s\n","1019 \t 16.035541 \t 8.471772 \t7.538413 \t 0.025356 \t 637.356494 s\n","1020 \t 15.689562 \t 8.418388 \t7.206136 \t 0.065037 \t 637.939260 s\n","1021 \t 15.457019 \t 7.960594 \t7.476239 \t 0.020186 \t 638.507963 s\n","1022 \t 15.856265 \t 8.221420 \t7.607391 \t 0.027454 \t 639.114287 s\n","1023 \t 16.064068 \t 8.300289 \t7.712250 \t 0.051528 \t 639.762154 s\n","1024 \t 16.228925 \t 7.905222 \t8.283621 \t 0.040082 \t 640.500884 s\n","1025 \t 15.487443 \t 8.036368 \t7.389799 \t 0.061277 \t 640.975181 s\n","1026 \t 15.333589 \t 8.162268 \t7.140375 \t 0.030946 \t 641.447245 s\n","1027 \t 15.032370 \t 7.740907 \t7.259374 \t 0.032090 \t 641.927511 s\n","1028 \t 15.404777 \t 7.986981 \t7.389349 \t 0.028447 \t 642.403348 s\n","1029 \t 15.521626 \t 8.027795 \t7.473326 \t 0.020505 \t 642.890015 s\n","1030 \t 16.083647 \t 8.146142 \t7.886981 \t 0.050524 \t 643.375844 s\n","1031 \t 15.688672 \t 8.212257 \t7.458767 \t 0.017648 \t 643.857124 s\n","1032 \t 15.745388 \t 8.261086 \t7.458857 \t 0.025445 \t 644.343266 s\n","1033 \t 15.906807 \t 8.199065 \t7.672762 \t 0.034980 \t 644.824288 s\n","1034 \t 15.207083 \t 7.934050 \t7.207148 \t 0.065885 \t 645.317435 s\n","1035 \t 14.909825 \t 7.813615 \t7.076757 \t 0.019453 \t 645.802795 s\n","1036 \t 15.473794 \t 7.817826 \t7.619179 \t 0.036788 \t 646.291850 s\n","1037 \t 16.283828 \t 8.388720 \t7.859121 \t 0.035986 \t 646.935371 s\n","1038 \t 16.113903 \t 8.442667 \t7.626161 \t 0.045075 \t 647.421710 s\n","1039 \t 15.149280 \t 7.409740 \t7.611431 \t 0.128109 \t 647.893336 s\n","1040 \t 15.931931 \t 8.037577 \t7.866333 \t 0.028020 \t 648.384020 s\n","1041 \t 15.208880 \t 8.173417 \t6.998455 \t 0.037009 \t 648.865941 s\n","1042 \t 15.659041 \t 8.026847 \t7.605860 \t 0.026334 \t 649.342335 s\n","1043 \t 15.173904 \t 7.940603 \t7.204721 \t 0.028579 \t 649.820754 s\n","1044 \t 15.979998 \t 8.190809 \t7.739690 \t 0.049499 \t 650.360491 s\n","1045 \t 15.167897 \t 8.052059 \t7.086521 \t 0.029316 \t 650.946475 s\n","1046 \t 15.301856 \t 7.874162 \t7.359856 \t 0.067838 \t 651.519492 s\n","1047 \t 15.638537 \t 7.874332 \t7.730672 \t 0.033533 \t 652.089018 s\n","1048 \t 15.882170 \t 8.157383 \t7.675103 \t 0.049683 \t 652.743279 s\n","1049 \t 15.154847 \t 7.673289 \t7.459898 \t 0.021660 \t 653.329327 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:15<00:00,  8.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4485\n","Hit@10: 0.5462\n","Hit@3: 0.4538\n","Hit@1: 0.3923\n","Link Prediction on Validation Set (All)\n","MRR: 0.4485\n","Hit@10: 0.5462\n","Hit@3: 0.4538\n","Hit@1: 0.3923\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2921\n","Hit@10: 0.4846\n","Hit@3: 0.3077\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6170\n","Hit@10: 0.7302\n","Hit@3: 0.6389\n","Hit@1: 0.5595\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1179\n"]}]},{"cell_type":"code","source":["KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTHNKG-NQ/checkpoint/Reproduce/VTHNKG-NQ/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    # sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    # nvp_tri_se += sq_error\n","                    # nvp_tri_se_num += 1\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","\n","                else: # qualifier\n","                    sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n"],"metadata":{"id":"ChVIC_5BHELi","executionInfo":{"status":"ok","timestamp":1746166618501,"user_tz":-540,"elapsed":19702,"user":{"displayName":"URP","userId":"16515248769931109428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1caa283b-1699-462b-d6f6-49b3a8509cdb"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 132/132 [00:15<00:00,  8.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4180\n","Hit@10: 0.5492\n","Hit@3: 0.4242\n","Hit@1: 0.3523\n","Link Prediction on Validation Set (All)\n","MRR: 0.4180\n","Hit@10: 0.5492\n","Hit@3: 0.4242\n","Hit@1: 0.3523\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4093\n","Hit@10: 0.6742\n","Hit@3: 0.4545\n","Hit@1: 0.2879\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6664\n","Hit@10: 0.8252\n","Hit@3: 0.7033\n","Hit@1: 0.5894\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1431\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}