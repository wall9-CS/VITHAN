{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyPXUm7IXpcSL72Y6aT4F4Jk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747361372522,"user_tz":-540,"elapsed":19082,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"f993564a-c40e-4f9b-ce93-c29d8a997b4e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1747361378653,"user_tz":-540,"elapsed":4395,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1747361378655,"user_tz":-540,"elapsed":39,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0: # numeric triplet 처리\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","        else:\n","            self.proj_ent_vis = nn.Identity()\n","            self.proj_rel_vis = nn.Identity()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1747361378656,"user_tz":-540,"elapsed":38,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1747361462707,"user_tz":-540,"elapsed":143,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTKG-I/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile(self.dir + 'visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(self.dir + 'visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTKG-I/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1747361385643,"user_tz":-540,"elapsed":7,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"cf1491d7-517d-436a-e843-29a106abfd6b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTKG-I\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTKG-I\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747362066966,"user_tz":-540,"elapsed":598080,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"918fa112-15ca-4a0d-a8d2-6a89b1838271"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-57f05cdbc00c>:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","<ipython-input-9-57f05cdbc00c>:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","<ipython-input-9-57f05cdbc00c>:258: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","<ipython-input-9-57f05cdbc00c>:259: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 25.636213 \t 13.004923 \t12.631289 \t 0.000000 \t 2.368701 s\n","1 \t 20.761380 \t 10.243934 \t10.517447 \t 0.000000 \t 3.058620 s\n","2 \t 20.199930 \t 9.637381 \t10.562549 \t 0.000000 \t 3.499641 s\n","3 \t 19.774443 \t 9.221674 \t10.552770 \t 0.000000 \t 3.950550 s\n","4 \t 19.931991 \t 9.578770 \t10.353220 \t 0.000000 \t 4.380559 s\n","5 \t 18.406928 \t 9.050463 \t9.356464 \t 0.000000 \t 4.976862 s\n","6 \t 19.088047 \t 8.662194 \t10.425853 \t 0.000000 \t 5.512410 s\n","7 \t 19.508545 \t 9.357252 \t10.151292 \t 0.000000 \t 6.126508 s\n","8 \t 18.990530 \t 9.329453 \t9.661078 \t 0.000000 \t 6.557155 s\n","9 \t 19.529662 \t 9.536521 \t9.993141 \t 0.000000 \t 7.010830 s\n","10 \t 18.962641 \t 9.498888 \t9.463753 \t 0.000000 \t 7.455930 s\n","11 \t 18.623192 \t 9.327845 \t9.295347 \t 0.000000 \t 7.916895 s\n","12 \t 19.183164 \t 8.979695 \t10.203468 \t 0.000000 \t 8.497603 s\n","13 \t 19.450764 \t 9.248050 \t10.202714 \t 0.000000 \t 8.936251 s\n","14 \t 19.163177 \t 9.245950 \t9.917228 \t 0.000000 \t 9.378047 s\n","15 \t 19.549744 \t 9.288419 \t10.261326 \t 0.000000 \t 9.813249 s\n","16 \t 18.618155 \t 9.098746 \t9.519409 \t 0.000000 \t 10.246269 s\n","17 \t 19.112531 \t 9.207328 \t9.905203 \t 0.000000 \t 10.675370 s\n","18 \t 19.279353 \t 9.093087 \t10.186266 \t 0.000000 \t 11.129675 s\n","19 \t 18.534403 \t 8.876755 \t9.657648 \t 0.000000 \t 11.564021 s\n","20 \t 18.902020 \t 8.919352 \t9.982668 \t 0.000000 \t 12.005225 s\n","21 \t 18.961226 \t 8.955041 \t10.006186 \t 0.000000 \t 12.429357 s\n","22 \t 19.307372 \t 9.387156 \t9.920216 \t 0.000000 \t 12.945533 s\n","23 \t 18.886623 \t 9.227654 \t9.658969 \t 0.000000 \t 13.475546 s\n","24 \t 18.763462 \t 9.042341 \t9.721121 \t 0.000000 \t 14.191403 s\n","25 \t 19.587893 \t 9.569988 \t10.017906 \t 0.000000 \t 14.735437 s\n","26 \t 19.517438 \t 9.180222 \t10.337216 \t 0.000000 \t 15.345486 s\n","27 \t 18.824751 \t 8.798127 \t10.026624 \t 0.000000 \t 15.906136 s\n","28 \t 18.992463 \t 9.130775 \t9.861688 \t 0.000000 \t 16.352576 s\n","29 \t 19.418976 \t 9.443954 \t9.975022 \t 0.000000 \t 16.781639 s\n","30 \t 19.439925 \t 9.054687 \t10.385238 \t 0.000000 \t 17.229544 s\n","31 \t 19.246090 \t 9.172861 \t10.073229 \t 0.000000 \t 17.683902 s\n","32 \t 18.560047 \t 8.912799 \t9.647247 \t 0.000000 \t 18.119285 s\n","33 \t 19.303682 \t 9.378264 \t9.925418 \t 0.000000 \t 18.568792 s\n","34 \t 18.941742 \t 9.035003 \t9.906738 \t 0.000000 \t 18.999488 s\n","35 \t 19.152585 \t 8.922825 \t10.229760 \t 0.000000 \t 19.437757 s\n","36 \t 19.422749 \t 9.188111 \t10.234639 \t 0.000000 \t 20.021764 s\n","37 \t 19.416197 \t 9.265045 \t10.151153 \t 0.000000 \t 20.449966 s\n","38 \t 18.929439 \t 9.278801 \t9.650638 \t 0.000000 \t 20.887104 s\n","39 \t 17.840779 \t 8.821051 \t9.019729 \t 0.000000 \t 21.335209 s\n","40 \t 18.449662 \t 8.767594 \t9.682068 \t 0.000000 \t 21.768411 s\n","41 \t 18.332289 \t 8.584052 \t9.748237 \t 0.000000 \t 22.206202 s\n","42 \t 17.694353 \t 8.629046 \t9.065308 \t 0.000000 \t 22.653494 s\n","43 \t 19.034843 \t 9.009397 \t10.025446 \t 0.000000 \t 23.094632 s\n","44 \t 19.083363 \t 9.330443 \t9.752920 \t 0.000000 \t 23.530884 s\n","45 \t 18.987516 \t 9.144450 \t9.843066 \t 0.000000 \t 23.970589 s\n","46 \t 18.555483 \t 8.798911 \t9.756572 \t 0.000000 \t 24.414833 s\n","47 \t 18.163631 \t 8.233707 \t9.929924 \t 0.000000 \t 24.852266 s\n","48 \t 19.019189 \t 8.706378 \t10.312811 \t 0.000000 \t 25.436475 s\n","49 \t 18.113715 \t 8.664581 \t9.449135 \t 0.000000 \t 25.917181 s\n","50 \t 18.560432 \t 8.588731 \t9.971703 \t 0.000000 \t 26.456965 s\n","51 \t 18.983906 \t 9.461111 \t9.522795 \t 0.000000 \t 27.008102 s\n","52 \t 18.699294 \t 9.109436 \t9.589858 \t 0.000000 \t 27.549758 s\n","53 \t 18.708256 \t 9.062267 \t9.645988 \t 0.000000 \t 28.131504 s\n","54 \t 18.345170 \t 8.823235 \t9.521936 \t 0.000000 \t 28.721091 s\n","55 \t 18.920964 \t 9.057697 \t9.863267 \t 0.000000 \t 29.250075 s\n","56 \t 17.771679 \t 8.426538 \t9.345140 \t 0.000000 \t 29.691978 s\n","57 \t 18.687992 \t 9.112848 \t9.575143 \t 0.000000 \t 30.132442 s\n","58 \t 18.695889 \t 8.863323 \t9.832565 \t 0.000000 \t 30.564551 s\n","59 \t 18.879867 \t 8.953589 \t9.926277 \t 0.000000 \t 31.139828 s\n","60 \t 18.070259 \t 8.217870 \t9.852389 \t 0.000000 \t 31.765842 s\n","61 \t 18.761335 \t 9.231852 \t9.529484 \t 0.000000 \t 32.204967 s\n","62 \t 18.394140 \t 8.780256 \t9.613883 \t 0.000000 \t 32.819587 s\n","63 \t 18.641931 \t 8.690633 \t9.951298 \t 0.000000 \t 33.410510 s\n","64 \t 18.326001 \t 8.572438 \t9.753563 \t 0.000000 \t 33.865061 s\n","65 \t 18.751038 \t 8.824847 \t9.926191 \t 0.000000 \t 34.304968 s\n","66 \t 18.358688 \t 8.777095 \t9.581594 \t 0.000000 \t 34.738512 s\n","67 \t 18.452692 \t 8.971705 \t9.480986 \t 0.000000 \t 35.186398 s\n","68 \t 18.750272 \t 8.705338 \t10.044934 \t 0.000000 \t 35.761835 s\n","69 \t 18.116654 \t 8.509195 \t9.607460 \t 0.000000 \t 36.210016 s\n","70 \t 18.289375 \t 8.202751 \t10.086624 \t 0.000000 \t 36.643418 s\n","71 \t 18.280337 \t 8.911778 \t9.368559 \t 0.000000 \t 37.087829 s\n","72 \t 18.273663 \t 8.572680 \t9.700983 \t 0.000000 \t 37.512298 s\n","73 \t 19.060703 \t 8.937205 \t10.123499 \t 0.000000 \t 37.955048 s\n","74 \t 18.424022 \t 8.498705 \t9.925316 \t 0.000000 \t 38.395808 s\n","75 \t 18.040925 \t 8.463443 \t9.577482 \t 0.000000 \t 38.834746 s\n","76 \t 18.298590 \t 8.437534 \t9.861056 \t 0.000000 \t 39.339057 s\n","77 \t 18.066264 \t 8.418530 \t9.647734 \t 0.000000 \t 39.865721 s\n","78 \t 18.520634 \t 8.670789 \t9.849844 \t 0.000000 \t 40.562297 s\n","79 \t 17.593209 \t 8.257023 \t9.336185 \t 0.000000 \t 41.105536 s\n","80 \t 18.636484 \t 8.756488 \t9.879997 \t 0.000000 \t 41.690104 s\n","81 \t 17.670305 \t 8.335204 \t9.335102 \t 0.000000 \t 42.256113 s\n","82 \t 17.794666 \t 8.190149 \t9.604517 \t 0.000000 \t 42.687544 s\n","83 \t 18.417315 \t 8.828629 \t9.588685 \t 0.000000 \t 43.119859 s\n","84 \t 17.762918 \t 8.686974 \t9.075944 \t 0.000000 \t 43.574462 s\n","85 \t 18.752476 \t 8.669244 \t10.083232 \t 0.000000 \t 44.009880 s\n","86 \t 17.781633 \t 8.248639 \t9.532995 \t 0.000000 \t 44.440316 s\n","87 \t 17.896185 \t 8.817125 \t9.079060 \t 0.000000 \t 44.876728 s\n","88 \t 18.543772 \t 8.788230 \t9.755542 \t 0.000000 \t 45.451325 s\n","89 \t 18.454357 \t 8.958313 \t9.496044 \t 0.000000 \t 45.893814 s\n","90 \t 17.936464 \t 8.746188 \t9.190276 \t 0.000000 \t 46.343868 s\n","91 \t 17.938973 \t 8.511934 \t9.427039 \t 0.000000 \t 46.773604 s\n","92 \t 17.996582 \t 8.570477 \t9.426105 \t 0.000000 \t 47.205701 s\n","93 \t 17.552914 \t 8.457534 \t9.095379 \t 0.000000 \t 47.651721 s\n","94 \t 18.767564 \t 8.814647 \t9.952917 \t 0.000000 \t 48.093571 s\n","95 \t 18.290714 \t 8.612685 \t9.678030 \t 0.000000 \t 48.543160 s\n","96 \t 17.505486 \t 8.449113 \t9.056372 \t 0.000000 \t 48.977335 s\n","97 \t 17.227097 \t 8.310698 \t8.916399 \t 0.000000 \t 49.420501 s\n","98 \t 17.975970 \t 8.611343 \t9.364627 \t 0.000000 \t 50.017067 s\n","99 \t 17.834750 \t 8.659257 \t9.175494 \t 0.000000 \t 50.448151 s\n","100 \t 16.966913 \t 8.106703 \t8.860209 \t 0.000000 \t 50.896819 s\n","101 \t 17.466936 \t 8.613655 \t8.853281 \t 0.000000 \t 51.343118 s\n","102 \t 17.774724 \t 8.428131 \t9.346593 \t 0.000000 \t 51.792846 s\n","103 \t 18.585951 \t 8.902605 \t9.683345 \t 0.000000 \t 52.270368 s\n","104 \t 17.834247 \t 8.526057 \t9.308189 \t 0.000000 \t 52.828027 s\n","105 \t 17.799735 \t 8.217003 \t9.582733 \t 0.000000 \t 53.358693 s\n","106 \t 18.204886 \t 8.660369 \t9.544518 \t 0.000000 \t 53.909729 s\n","107 \t 17.854238 \t 8.280750 \t9.573488 \t 0.000000 \t 54.465068 s\n","108 \t 17.846737 \t 8.469814 \t9.376922 \t 0.000000 \t 55.077143 s\n","109 \t 17.745692 \t 8.796029 \t8.949664 \t 0.000000 \t 55.736794 s\n","110 \t 16.767097 \t 8.340346 \t8.426751 \t 0.000000 \t 56.195065 s\n","111 \t 18.361699 \t 8.802612 \t9.559087 \t 0.000000 \t 56.626036 s\n","112 \t 17.831279 \t 8.644681 \t9.186598 \t 0.000000 \t 57.078342 s\n","113 \t 17.690769 \t 8.278002 \t9.412767 \t 0.000000 \t 57.508065 s\n","114 \t 17.587815 \t 8.513595 \t9.074221 \t 0.000000 \t 57.955029 s\n","115 \t 17.756635 \t 8.393985 \t9.362650 \t 0.000000 \t 58.396071 s\n","116 \t 18.667317 \t 8.950976 \t9.716341 \t 0.000000 \t 58.840185 s\n","117 \t 18.336569 \t 8.676108 \t9.660461 \t 0.000000 \t 59.299266 s\n","118 \t 17.483707 \t 8.172229 \t9.311479 \t 0.000000 \t 59.745784 s\n","119 \t 17.424857 \t 8.578058 \t8.846798 \t 0.000000 \t 60.186263 s\n","120 \t 17.587008 \t 8.562692 \t9.024316 \t 0.000000 \t 60.625468 s\n","121 \t 17.984211 \t 8.464739 \t9.519472 \t 0.000000 \t 61.204592 s\n","122 \t 18.125344 \t 8.244291 \t9.881054 \t 0.000000 \t 61.640204 s\n","123 \t 17.622917 \t 8.195065 \t9.427853 \t 0.000000 \t 62.097380 s\n","124 \t 18.100346 \t 8.701684 \t9.398662 \t 0.000000 \t 62.527961 s\n","125 \t 17.530537 \t 8.484456 \t9.046081 \t 0.000000 \t 62.963825 s\n","126 \t 17.626642 \t 8.380845 \t9.245798 \t 0.000000 \t 63.427022 s\n","127 \t 17.850988 \t 8.219896 \t9.631093 \t 0.000000 \t 63.879435 s\n","128 \t 18.178189 \t 8.660429 \t9.517760 \t 0.000000 \t 64.331954 s\n","129 \t 17.414474 \t 8.462624 \t8.951850 \t 0.000000 \t 64.776762 s\n","130 \t 17.884912 \t 8.688100 \t9.196813 \t 0.000000 \t 65.236816 s\n","131 \t 17.836719 \t 8.703218 \t9.133500 \t 0.000000 \t 65.785592 s\n","132 \t 17.695683 \t 8.388059 \t9.307624 \t 0.000000 \t 66.341707 s\n","133 \t 18.018703 \t 8.709938 \t9.308764 \t 0.000000 \t 67.041889 s\n","134 \t 16.990930 \t 8.371942 \t8.618988 \t 0.000000 \t 67.610828 s\n","135 \t 17.673177 \t 8.556172 \t9.117004 \t 0.000000 \t 68.219558 s\n","136 \t 17.836952 \t 8.422694 \t9.414258 \t 0.000000 \t 68.773979 s\n","137 \t 17.410280 \t 8.310395 \t9.099885 \t 0.000000 \t 69.214999 s\n","138 \t 17.399246 \t 8.144276 \t9.254971 \t 0.000000 \t 69.662171 s\n","139 \t 17.543108 \t 8.353947 \t9.189161 \t 0.000000 \t 70.105778 s\n","140 \t 18.258539 \t 8.724738 \t9.533801 \t 0.000000 \t 70.544006 s\n","141 \t 17.879888 \t 8.621177 \t9.258711 \t 0.000000 \t 71.129400 s\n","142 \t 17.823309 \t 8.169383 \t9.653926 \t 0.000000 \t 71.566798 s\n","143 \t 17.166460 \t 8.132386 \t9.034075 \t 0.000000 \t 72.005867 s\n","144 \t 16.808803 \t 8.238827 \t8.569975 \t 0.000000 \t 72.444675 s\n","145 \t 17.599911 \t 8.296091 \t9.303821 \t 0.000000 \t 72.902543 s\n","146 \t 17.583238 \t 8.363524 \t9.219714 \t 0.000000 \t 73.347606 s\n","147 \t 17.675147 \t 8.829759 \t8.845389 \t 0.000000 \t 73.814723 s\n","148 \t 17.405112 \t 8.509673 \t8.895440 \t 0.000000 \t 74.257770 s\n","149 \t 17.791548 \t 8.515177 \t9.276370 \t 0.000000 \t 74.851575 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/130 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 130/130 [00:10<00:00, 12.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3763\n","Hit@10: 0.5115\n","Hit@3: 0.3692\n","Hit@1: 0.3077\n","Link Prediction on Validation Set (All)\n","MRR: 0.3763\n","Hit@10: 0.5115\n","Hit@3: 0.3692\n","Hit@1: 0.3077\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3404\n","Hit@10: 0.5615\n","Hit@3: 0.3846\n","Hit@1: 0.2154\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3404\n","Hit@10: 0.5615\n","Hit@3: 0.3846\n","Hit@1: 0.2154\n","150 \t 17.473083 \t 8.374986 \t9.098098 \t 0.000000 \t 86.986400 s\n","151 \t 18.563162 \t 8.748568 \t9.814594 \t 0.000000 \t 87.437237 s\n","152 \t 18.614215 \t 8.684093 \t9.930122 \t 0.000000 \t 87.879763 s\n","153 \t 17.312187 \t 8.618975 \t8.693211 \t 0.000000 \t 88.331503 s\n","154 \t 18.882762 \t 8.821733 \t10.061029 \t 0.000000 \t 88.774761 s\n","155 \t 18.252318 \t 9.030895 \t9.221423 \t 0.000000 \t 89.207100 s\n","156 \t 17.881041 \t 8.567379 \t9.313661 \t 0.000000 \t 89.662073 s\n","157 \t 17.635552 \t 8.441000 \t9.194551 \t 0.000000 \t 90.103371 s\n","158 \t 18.001227 \t 8.730734 \t9.270494 \t 0.000000 \t 90.692794 s\n","159 \t 18.020631 \t 8.518376 \t9.502254 \t 0.000000 \t 91.136898 s\n","160 \t 18.551932 \t 8.915454 \t9.636478 \t 0.000000 \t 91.570254 s\n","161 \t 18.664392 \t 8.716981 \t9.947412 \t 0.000000 \t 92.022061 s\n","162 \t 17.787996 \t 8.348699 \t9.439296 \t 0.000000 \t 92.931906 s\n","163 \t 18.131701 \t 8.613060 \t9.518641 \t 0.000000 \t 93.929270 s\n","164 \t 17.643272 \t 8.244957 \t9.398315 \t 0.000000 \t 94.794180 s\n","165 \t 17.809233 \t 8.511789 \t9.297444 \t 0.000000 \t 95.715779 s\n","166 \t 17.795842 \t 8.556904 \t9.238938 \t 0.000000 \t 96.311633 s\n","167 \t 17.956301 \t 8.981551 \t8.974749 \t 0.000000 \t 96.896167 s\n","168 \t 17.947535 \t 8.701869 \t9.245665 \t 0.000000 \t 97.346165 s\n","169 \t 17.921040 \t 8.716356 \t9.204683 \t 0.000000 \t 97.798961 s\n","170 \t 17.386102 \t 8.132404 \t9.253697 \t 0.000000 \t 98.235174 s\n","171 \t 18.457241 \t 9.061779 \t9.395463 \t 0.000000 \t 98.674401 s\n","172 \t 17.457804 \t 8.433557 \t9.024247 \t 0.000000 \t 99.123853 s\n","173 \t 17.877398 \t 8.810440 \t9.066957 \t 0.000000 \t 99.558875 s\n","174 \t 17.875422 \t 8.840221 \t9.035200 \t 0.000000 \t 99.996135 s\n","175 \t 17.202021 \t 8.427153 \t8.774868 \t 0.000000 \t 100.425531 s\n","176 \t 17.664005 \t 8.856123 \t8.807882 \t 0.000000 \t 100.884623 s\n","177 \t 18.178876 \t 8.635884 \t9.542991 \t 0.000000 \t 101.325703 s\n","178 \t 17.576782 \t 8.686131 \t8.890651 \t 0.000000 \t 101.910725 s\n","179 \t 17.600717 \t 8.869872 \t8.730844 \t 0.000000 \t 102.352342 s\n","180 \t 17.975080 \t 8.821995 \t9.153086 \t 0.000000 \t 102.798707 s\n","181 \t 17.727360 \t 8.351776 \t9.375585 \t 0.000000 \t 103.250715 s\n","182 \t 17.024614 \t 8.179280 \t8.845335 \t 0.000000 \t 103.685745 s\n","183 \t 17.122573 \t 8.560693 \t8.561881 \t 0.000000 \t 104.132778 s\n","184 \t 17.533544 \t 8.363073 \t9.170470 \t 0.000000 \t 104.566951 s\n","185 \t 16.572731 \t 7.729751 \t8.842980 \t 0.000000 \t 105.028664 s\n","186 \t 17.543438 \t 8.174950 \t9.368488 \t 0.000000 \t 105.466181 s\n","187 \t 17.268778 \t 8.152654 \t9.116124 \t 0.000000 \t 105.912733 s\n","188 \t 17.152748 \t 8.698381 \t8.454367 \t 0.000000 \t 106.494026 s\n","189 \t 17.314160 \t 8.262755 \t9.051405 \t 0.000000 \t 107.032059 s\n","190 \t 17.214317 \t 8.731550 \t8.482767 \t 0.000000 \t 107.739838 s\n","191 \t 17.802033 \t 8.437294 \t9.364740 \t 0.000000 \t 108.283593 s\n","192 \t 17.198094 \t 8.219781 \t8.978313 \t 0.000000 \t 108.877214 s\n","193 \t 17.313568 \t 8.619333 \t8.694236 \t 0.000000 \t 109.426566 s\n","194 \t 17.330496 \t 8.468019 \t8.862476 \t 0.000000 \t 109.860630 s\n","195 \t 17.679068 \t 8.722640 \t8.956427 \t 0.000000 \t 110.307554 s\n","196 \t 17.219284 \t 8.282677 \t8.936608 \t 0.000000 \t 110.749040 s\n","197 \t 17.139603 \t 8.010106 \t9.129497 \t 0.000000 \t 111.185663 s\n","198 \t 17.070786 \t 8.301550 \t8.769236 \t 0.000000 \t 111.644627 s\n","199 \t 17.238602 \t 8.054185 \t9.184416 \t 0.000000 \t 112.081645 s\n","200 \t 17.404171 \t 8.354767 \t9.049404 \t 0.000000 \t 112.657956 s\n","201 \t 17.358396 \t 7.993917 \t9.364478 \t 0.000000 \t 113.099315 s\n","202 \t 17.091820 \t 8.306650 \t8.785171 \t 0.000000 \t 113.544833 s\n","203 \t 17.421659 \t 8.347356 \t9.074304 \t 0.000000 \t 113.994162 s\n","204 \t 17.178596 \t 8.278505 \t8.900090 \t 0.000000 \t 114.454375 s\n","205 \t 16.919542 \t 8.394994 \t8.524549 \t 0.000000 \t 114.896623 s\n","206 \t 17.572666 \t 8.510294 \t9.062372 \t 0.000000 \t 115.333982 s\n","207 \t 17.311295 \t 8.616735 \t8.694559 \t 0.000000 \t 115.782933 s\n","208 \t 17.557523 \t 8.285337 \t9.272186 \t 0.000000 \t 116.221382 s\n","209 \t 16.665751 \t 8.328525 \t8.337225 \t 0.000000 \t 116.796109 s\n","210 \t 16.751251 \t 8.129172 \t8.622079 \t 0.000000 \t 117.238501 s\n","211 \t 17.793185 \t 8.412262 \t9.380923 \t 0.000000 \t 117.683060 s\n","212 \t 17.493914 \t 8.344684 \t9.149229 \t 0.000000 \t 118.117550 s\n","213 \t 16.665420 \t 7.929521 \t8.735899 \t 0.000000 \t 118.554714 s\n","214 \t 16.839217 \t 8.116064 \t8.723154 \t 0.000000 \t 118.986737 s\n","215 \t 16.740177 \t 7.816191 \t8.923986 \t 0.000000 \t 119.476364 s\n","216 \t 16.940623 \t 8.070305 \t8.870318 \t 0.000000 \t 120.027892 s\n","217 \t 16.938625 \t 8.198633 \t8.739992 \t 0.000000 \t 120.544397 s\n","218 \t 16.291605 \t 7.882457 \t8.409148 \t 0.000000 \t 121.241010 s\n","219 \t 17.191730 \t 8.402860 \t8.788870 \t 0.000000 \t 121.837279 s\n","220 \t 16.873310 \t 8.354415 \t8.518895 \t 0.000000 \t 122.393691 s\n","221 \t 17.038353 \t 8.404150 \t8.634204 \t 0.000000 \t 122.831836 s\n","222 \t 17.268239 \t 8.641747 \t8.626491 \t 0.000000 \t 123.262616 s\n","223 \t 16.797504 \t 8.250355 \t8.547149 \t 0.000000 \t 123.696172 s\n","224 \t 16.471526 \t 8.327662 \t8.143864 \t 0.000000 \t 124.136588 s\n","225 \t 16.091934 \t 7.618535 \t8.473399 \t 0.000000 \t 124.567943 s\n","226 \t 16.963990 \t 8.304870 \t8.659121 \t 0.000000 \t 125.020902 s\n","227 \t 17.083378 \t 8.208267 \t8.875112 \t 0.000000 \t 125.595129 s\n","228 \t 17.086405 \t 8.479051 \t8.607354 \t 0.000000 \t 126.063589 s\n","229 \t 16.992448 \t 8.359466 \t8.632981 \t 0.000000 \t 126.495036 s\n","230 \t 16.527026 \t 7.784990 \t8.742036 \t 0.000000 \t 126.942163 s\n","231 \t 16.978758 \t 8.383394 \t8.595363 \t 0.000000 \t 127.389086 s\n","232 \t 16.430084 \t 8.165716 \t8.264369 \t 0.000000 \t 127.824993 s\n","233 \t 17.426349 \t 8.262979 \t9.163370 \t 0.000000 \t 128.278981 s\n","234 \t 16.885638 \t 8.023226 \t8.862412 \t 0.000000 \t 128.714155 s\n","235 \t 17.441851 \t 8.202286 \t9.239565 \t 0.000000 \t 129.153602 s\n","236 \t 16.544433 \t 8.191437 \t8.352995 \t 0.000000 \t 129.596710 s\n","237 \t 16.471530 \t 8.231824 \t8.239706 \t 0.000000 \t 130.174061 s\n","238 \t 16.307386 \t 8.165468 \t8.141919 \t 0.000000 \t 130.605677 s\n","239 \t 16.923739 \t 8.269525 \t8.654215 \t 0.000000 \t 131.052511 s\n","240 \t 16.139799 \t 8.007509 \t8.132290 \t 0.000000 \t 131.504067 s\n","241 \t 16.761537 \t 8.329067 \t8.432470 \t 0.000000 \t 131.937279 s\n","242 \t 16.402127 \t 7.693153 \t8.708974 \t 0.000000 \t 132.423044 s\n","243 \t 15.827110 \t 8.056515 \t7.770595 \t 0.000000 \t 132.965838 s\n","244 \t 16.079286 \t 7.963174 \t8.116112 \t 0.000000 \t 133.497010 s\n","245 \t 16.608802 \t 8.044960 \t8.563842 \t 0.000000 \t 134.048672 s\n","246 \t 16.397925 \t 8.117908 \t8.280016 \t 0.000000 \t 134.595695 s\n","247 \t 16.338989 \t 8.073454 \t8.265535 \t 0.000000 \t 135.196649 s\n","248 \t 16.476767 \t 8.072426 \t8.404341 \t 0.000000 \t 135.861222 s\n","249 \t 16.311037 \t 7.757047 \t8.553990 \t 0.000000 \t 136.300992 s\n","250 \t 16.621040 \t 8.356697 \t8.264344 \t 0.000000 \t 136.759140 s\n","251 \t 16.560703 \t 8.009666 \t8.551036 \t 0.000000 \t 137.198421 s\n","252 \t 15.953438 \t 8.018757 \t7.934681 \t 0.000000 \t 137.629928 s\n","253 \t 16.599135 \t 7.919683 \t8.679452 \t 0.000000 \t 138.068023 s\n","254 \t 16.854125 \t 8.122990 \t8.731135 \t 0.000000 \t 138.509315 s\n","255 \t 16.200106 \t 8.068107 \t8.131999 \t 0.000000 \t 138.945086 s\n","256 \t 16.531774 \t 8.153930 \t8.377843 \t 0.000000 \t 139.385942 s\n","257 \t 16.310684 \t 8.024653 \t8.286030 \t 0.000000 \t 139.842866 s\n","258 \t 16.088315 \t 7.973785 \t8.114530 \t 0.000000 \t 140.416070 s\n","259 \t 16.098696 \t 7.756399 \t8.342298 \t 0.000000 \t 140.866834 s\n","260 \t 16.411998 \t 8.132504 \t8.279494 \t 0.000000 \t 141.308682 s\n","261 \t 15.474277 \t 7.192569 \t8.281709 \t 0.000000 \t 141.750037 s\n","262 \t 16.257374 \t 7.973490 \t8.283884 \t 0.000000 \t 142.190815 s\n","263 \t 15.991166 \t 7.468426 \t8.522741 \t 0.000000 \t 142.630228 s\n","264 \t 15.577640 \t 7.857483 \t7.720157 \t 0.000000 \t 143.063730 s\n","265 \t 16.363183 \t 8.354061 \t8.009122 \t 0.000000 \t 143.489632 s\n","266 \t 16.165096 \t 8.308086 \t7.857010 \t 0.000000 \t 143.943972 s\n","267 \t 15.836379 \t 7.913082 \t7.923296 \t 0.000000 \t 144.385122 s\n","268 \t 15.912785 \t 7.853786 \t8.059000 \t 0.000000 \t 144.982884 s\n","269 \t 15.883102 \t 7.808029 \t8.075074 \t 0.000000 \t 145.416148 s\n","270 \t 16.165686 \t 7.885789 \t8.279897 \t 0.000000 \t 145.952751 s\n","271 \t 16.259365 \t 8.026560 \t8.232804 \t 0.000000 \t 146.474949 s\n","272 \t 16.335440 \t 8.140361 \t8.195078 \t 0.000000 \t 147.024954 s\n","273 \t 15.438539 \t 7.637587 \t7.800952 \t 0.000000 \t 147.555634 s\n","274 \t 15.990486 \t 7.912828 \t8.077659 \t 0.000000 \t 148.163358 s\n","275 \t 15.826827 \t 7.684375 \t8.142452 \t 0.000000 \t 148.725978 s\n","276 \t 15.375226 \t 7.691678 \t7.683548 \t 0.000000 \t 149.175976 s\n","277 \t 15.926112 \t 7.711236 \t8.214876 \t 0.000000 \t 149.609395 s\n","278 \t 16.465652 \t 8.243823 \t8.221829 \t 0.000000 \t 150.059074 s\n","279 \t 16.165289 \t 8.056837 \t8.108451 \t 0.000000 \t 150.632716 s\n","280 \t 15.923073 \t 7.578879 \t8.344193 \t 0.000000 \t 151.068233 s\n","281 \t 15.422548 \t 7.611377 \t7.811171 \t 0.000000 \t 151.501639 s\n","282 \t 16.031865 \t 7.837318 \t8.194546 \t 0.000000 \t 151.943047 s\n","283 \t 15.712174 \t 7.827240 \t7.884934 \t 0.000000 \t 152.380816 s\n","284 \t 16.347414 \t 7.854906 \t8.492509 \t 0.000000 \t 152.817822 s\n","285 \t 15.745904 \t 7.752710 \t7.993195 \t 0.000000 \t 153.266858 s\n","286 \t 15.527339 \t 7.981299 \t7.546040 \t 0.000000 \t 153.707873 s\n","287 \t 15.680949 \t 7.619345 \t8.061603 \t 0.000000 \t 154.158504 s\n","288 \t 15.708448 \t 7.891037 \t7.817411 \t 0.000000 \t 154.619525 s\n","289 \t 15.298596 \t 7.392936 \t7.905660 \t 0.000000 \t 155.068253 s\n","290 \t 15.700239 \t 7.881428 \t7.818811 \t 0.000000 \t 155.500552 s\n","291 \t 15.851649 \t 7.809445 \t8.042204 \t 0.000000 \t 156.104407 s\n","292 \t 15.963053 \t 7.903677 \t8.059376 \t 0.000000 \t 156.556731 s\n","293 \t 16.168847 \t 7.757501 \t8.411346 \t 0.000000 \t 157.005402 s\n","294 \t 15.909847 \t 7.627257 \t8.282591 \t 0.000000 \t 157.450051 s\n","295 \t 15.172080 \t 7.498899 \t7.673181 \t 0.000000 \t 157.899389 s\n","296 \t 15.528815 \t 7.605717 \t7.923099 \t 0.000000 \t 158.348094 s\n","297 \t 15.234188 \t 7.304900 \t7.929288 \t 0.000000 \t 158.845046 s\n","298 \t 15.635776 \t 7.601817 \t8.033959 \t 0.000000 \t 159.406912 s\n","299 \t 16.036685 \t 7.740377 \t8.296308 \t 0.000000 \t 159.922339 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:10<00:00, 12.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4217\n","Hit@10: 0.5538\n","Hit@3: 0.4423\n","Hit@1: 0.3462\n","Link Prediction on Validation Set (All)\n","MRR: 0.4217\n","Hit@10: 0.5538\n","Hit@3: 0.4423\n","Hit@1: 0.3462\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3570\n","Hit@10: 0.5846\n","Hit@3: 0.4154\n","Hit@1: 0.2308\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3570\n","Hit@10: 0.5846\n","Hit@3: 0.4154\n","Hit@1: 0.2308\n","300 \t 15.562824 \t 7.789575 \t7.773249 \t 0.000000 \t 171.791039 s\n","301 \t 15.343771 \t 7.609359 \t7.734411 \t 0.000000 \t 172.268229 s\n","302 \t 15.557363 \t 7.699746 \t7.857617 \t 0.000000 \t 172.803257 s\n","303 \t 14.930713 \t 7.462297 \t7.468417 \t 0.000000 \t 173.503564 s\n","304 \t 15.333899 \t 7.218429 \t8.115470 \t 0.000000 \t 174.057224 s\n","305 \t 15.496430 \t 7.345034 \t8.151396 \t 0.000000 \t 174.657650 s\n","306 \t 15.801717 \t 7.712039 \t8.089677 \t 0.000000 \t 175.241078 s\n","307 \t 16.242444 \t 7.719696 \t8.522749 \t 0.000000 \t 175.669163 s\n","308 \t 14.902731 \t 7.219163 \t7.683569 \t 0.000000 \t 176.112231 s\n","309 \t 15.387195 \t 7.479358 \t7.907837 \t 0.000000 \t 176.549165 s\n","310 \t 14.886191 \t 7.433599 \t7.452591 \t 0.000000 \t 177.072758 s\n","311 \t 15.212473 \t 7.201686 \t8.010787 \t 0.000000 \t 177.745465 s\n","312 \t 15.389462 \t 7.460025 \t7.929436 \t 0.000000 \t 178.386863 s\n","313 \t 15.442149 \t 7.558352 \t7.883797 \t 0.000000 \t 178.941912 s\n","314 \t 15.653570 \t 7.609202 \t8.044368 \t 0.000000 \t 179.539241 s\n","315 \t 15.644170 \t 7.591235 \t8.052935 \t 0.000000 \t 180.198015 s\n","316 \t 15.005636 \t 7.301038 \t7.704599 \t 0.000000 \t 180.626156 s\n","317 \t 15.477980 \t 7.728333 \t7.749647 \t 0.000000 \t 181.066901 s\n","318 \t 15.240839 \t 7.234551 \t8.006288 \t 0.000000 \t 181.500268 s\n","319 \t 15.182196 \t 7.682954 \t7.499242 \t 0.000000 \t 181.935263 s\n","320 \t 15.494302 \t 7.507822 \t7.986481 \t 0.000000 \t 182.379525 s\n","321 \t 15.254644 \t 7.420683 \t7.833962 \t 0.000000 \t 182.807694 s\n","322 \t 15.387637 \t 7.554873 \t7.832764 \t 0.000000 \t 183.232526 s\n","323 \t 15.232091 \t 7.228197 \t8.003894 \t 0.000000 \t 183.679492 s\n","324 \t 15.317141 \t 7.404778 \t7.912363 \t 0.000000 \t 184.117066 s\n","325 \t 14.945979 \t 7.279054 \t7.666925 \t 0.000000 \t 184.547098 s\n","326 \t 14.164552 \t 7.094767 \t7.069785 \t 0.000000 \t 184.993145 s\n","327 \t 15.443791 \t 7.556912 \t7.886879 \t 0.000000 \t 185.681205 s\n","328 \t 15.506570 \t 7.607291 \t7.899279 \t 0.000000 \t 186.203232 s\n","329 \t 14.891747 \t 7.326853 \t7.564894 \t 0.000000 \t 186.753850 s\n","330 \t 14.845387 \t 7.283026 \t7.562360 \t 0.000000 \t 187.260976 s\n","331 \t 15.069547 \t 7.672074 \t7.397473 \t 0.000000 \t 187.863378 s\n","332 \t 15.296122 \t 7.448264 \t7.847858 \t 0.000000 \t 188.462710 s\n","333 \t 14.673607 \t 6.982900 \t7.690707 \t 0.000000 \t 188.909860 s\n","334 \t 15.047042 \t 7.267333 \t7.779710 \t 0.000000 \t 189.349255 s\n","335 \t 15.867951 \t 7.615618 \t8.252333 \t 0.000000 \t 189.950379 s\n","336 \t 14.882622 \t 7.363922 \t7.518700 \t 0.000000 \t 190.393615 s\n","337 \t 15.608856 \t 7.599221 \t8.009635 \t 0.000000 \t 190.825858 s\n","338 \t 14.768548 \t 6.958387 \t7.810161 \t 0.000000 \t 191.254784 s\n","339 \t 14.731171 \t 7.053057 \t7.678114 \t 0.000000 \t 191.696385 s\n","340 \t 15.250406 \t 7.494998 \t7.755409 \t 0.000000 \t 192.127090 s\n","341 \t 15.011745 \t 7.321892 \t7.689853 \t 0.000000 \t 192.560099 s\n","342 \t 14.494416 \t 7.173959 \t7.320457 \t 0.000000 \t 193.012668 s\n","343 \t 15.293556 \t 7.307400 \t7.986156 \t 0.000000 \t 193.439889 s\n","344 \t 15.603086 \t 7.580469 \t8.022617 \t 0.000000 \t 194.016957 s\n","345 \t 15.204381 \t 7.327478 \t7.876903 \t 0.000000 \t 194.446669 s\n","346 \t 15.032205 \t 7.203320 \t7.828884 \t 0.000000 \t 194.896650 s\n","347 \t 14.394847 \t 7.016126 \t7.378721 \t 0.000000 \t 195.330534 s\n","348 \t 15.448213 \t 7.795187 \t7.653025 \t 0.000000 \t 195.774755 s\n","349 \t 14.898299 \t 7.166876 \t7.731423 \t 0.000000 \t 196.210943 s\n","350 \t 14.889155 \t 7.307575 \t7.581581 \t 0.000000 \t 196.638958 s\n","351 \t 15.339605 \t 7.298815 \t8.040790 \t 0.000000 \t 197.073580 s\n","352 \t 14.975238 \t 7.164203 \t7.811036 \t 0.000000 \t 197.499852 s\n","353 \t 14.812688 \t 7.189395 \t7.623293 \t 0.000000 \t 198.061898 s\n","354 \t 14.997861 \t 7.339345 \t7.658515 \t 0.000000 \t 198.524220 s\n","355 \t 14.955101 \t 7.148638 \t7.806463 \t 0.000000 \t 199.068989 s\n","356 \t 15.021467 \t 7.561693 \t7.459774 \t 0.000000 \t 199.591001 s\n","357 \t 14.972754 \t 7.242286 \t7.730468 \t 0.000000 \t 200.140825 s\n","358 \t 14.650720 \t 6.976127 \t7.674593 \t 0.000000 \t 200.668700 s\n","359 \t 14.777494 \t 7.212630 \t7.564864 \t 0.000000 \t 201.268903 s\n","360 \t 14.541511 \t 7.116228 \t7.425283 \t 0.000000 \t 201.804837 s\n","361 \t 14.684919 \t 7.137553 \t7.547367 \t 0.000000 \t 202.234993 s\n","362 \t 14.919635 \t 7.099255 \t7.820380 \t 0.000000 \t 202.807852 s\n","363 \t 14.992341 \t 7.235271 \t7.757070 \t 0.000000 \t 203.247634 s\n","364 \t 15.032606 \t 7.404375 \t7.628231 \t 0.000000 \t 203.673943 s\n","365 \t 14.973743 \t 7.420549 \t7.553194 \t 0.000000 \t 204.109683 s\n","366 \t 15.127116 \t 7.446539 \t7.680577 \t 0.000000 \t 204.555782 s\n","367 \t 15.245716 \t 7.355423 \t7.890292 \t 0.000000 \t 204.992053 s\n","368 \t 15.367979 \t 7.497507 \t7.870472 \t 0.000000 \t 205.430552 s\n","369 \t 14.592206 \t 6.980416 \t7.611790 \t 0.000000 \t 205.863617 s\n","370 \t 14.641040 \t 7.051197 \t7.589844 \t 0.000000 \t 206.308272 s\n","371 \t 14.734874 \t 6.999485 \t7.735389 \t 0.000000 \t 206.877419 s\n","372 \t 14.651619 \t 7.115786 \t7.535833 \t 0.000000 \t 207.331716 s\n","373 \t 14.652980 \t 6.982369 \t7.670611 \t 0.000000 \t 207.775433 s\n","374 \t 14.639716 \t 7.180752 \t7.458965 \t 0.000000 \t 208.213706 s\n","375 \t 14.490573 \t 7.064472 \t7.426101 \t 0.000000 \t 208.666815 s\n","376 \t 14.734442 \t 7.308350 \t7.426092 \t 0.000000 \t 209.095706 s\n","377 \t 14.676170 \t 7.155293 \t7.520877 \t 0.000000 \t 209.523119 s\n","378 \t 14.867848 \t 7.402506 \t7.465342 \t 0.000000 \t 209.955863 s\n","379 \t 14.671756 \t 7.158041 \t7.513715 \t 0.000000 \t 210.404955 s\n","380 \t 14.697744 \t 7.075688 \t7.622056 \t 0.000000 \t 210.965987 s\n","381 \t 14.883754 \t 7.012547 \t7.871207 \t 0.000000 \t 211.403528 s\n","382 \t 14.456370 \t 7.019638 \t7.436733 \t 0.000000 \t 211.877006 s\n","383 \t 14.840316 \t 7.101254 \t7.739062 \t 0.000000 \t 212.396008 s\n","384 \t 15.091444 \t 7.240891 \t7.850554 \t 0.000000 \t 212.939061 s\n","385 \t 14.902282 \t 6.959973 \t7.942309 \t 0.000000 \t 213.464604 s\n","386 \t 14.580554 \t 7.089715 \t7.490839 \t 0.000000 \t 214.056093 s\n","387 \t 14.209144 \t 6.790138 \t7.419006 \t 0.000000 \t 214.643117 s\n","388 \t 15.180967 \t 7.431126 \t7.749841 \t 0.000000 \t 215.072341 s\n","389 \t 14.741351 \t 6.819448 \t7.921904 \t 0.000000 \t 215.633542 s\n","390 \t 14.847021 \t 7.391405 \t7.455616 \t 0.000000 \t 216.070375 s\n","391 \t 14.856610 \t 7.237041 \t7.619569 \t 0.000000 \t 216.518653 s\n","392 \t 14.535374 \t 7.128214 \t7.407160 \t 0.000000 \t 216.951036 s\n","393 \t 14.857573 \t 7.080037 \t7.777536 \t 0.000000 \t 217.383289 s\n","394 \t 14.360254 \t 7.083591 \t7.276663 \t 0.000000 \t 217.828327 s\n","395 \t 14.222934 \t 6.859702 \t7.363233 \t 0.000000 \t 218.265226 s\n","396 \t 14.505278 \t 6.887007 \t7.618271 \t 0.000000 \t 218.697589 s\n","397 \t 14.601840 \t 7.440596 \t7.161244 \t 0.000000 \t 219.145867 s\n","398 \t 14.948987 \t 7.184946 \t7.764042 \t 0.000000 \t 219.713145 s\n","399 \t 14.478882 \t 7.110292 \t7.368589 \t 0.000000 \t 220.158823 s\n","400 \t 14.790975 \t 7.455812 \t7.335162 \t 0.000000 \t 220.582550 s\n","401 \t 14.641056 \t 7.026863 \t7.614192 \t 0.000000 \t 221.016532 s\n","402 \t 14.403188 \t 6.940706 \t7.462482 \t 0.000000 \t 221.440943 s\n","403 \t 14.961732 \t 7.112569 \t7.849163 \t 0.000000 \t 221.869650 s\n","404 \t 14.996707 \t 7.023780 \t7.972928 \t 0.000000 \t 222.301502 s\n","405 \t 14.388254 \t 7.102914 \t7.285339 \t 0.000000 \t 222.731512 s\n","406 \t 14.811551 \t 7.357761 \t7.453790 \t 0.000000 \t 223.167930 s\n","407 \t 14.564261 \t 7.224460 \t7.339802 \t 0.000000 \t 223.591369 s\n","408 \t 14.754839 \t 7.102781 \t7.652058 \t 0.000000 \t 224.151628 s\n","409 \t 14.403005 \t 7.028263 \t7.374743 \t 0.000000 \t 224.569101 s\n","410 \t 14.840290 \t 7.128826 \t7.711464 \t 0.000000 \t 225.126971 s\n","411 \t 14.593336 \t 7.251156 \t7.342180 \t 0.000000 \t 225.634737 s\n","412 \t 15.036984 \t 7.475457 \t7.561528 \t 0.000000 \t 226.155753 s\n","413 \t 15.005140 \t 7.536051 \t7.469089 \t 0.000000 \t 226.676282 s\n","414 \t 14.636534 \t 7.103445 \t7.533089 \t 0.000000 \t 227.292541 s\n","415 \t 14.672744 \t 7.288144 \t7.384600 \t 0.000000 \t 227.848363 s\n","416 \t 14.480205 \t 7.059066 \t7.421138 \t 0.000000 \t 228.286828 s\n","417 \t 14.315733 \t 6.929471 \t7.386262 \t 0.000000 \t 228.728146 s\n","418 \t 14.473876 \t 7.062311 \t7.411565 \t 0.000000 \t 229.298742 s\n","419 \t 14.581654 \t 6.925801 \t7.655854 \t 0.000000 \t 229.735652 s\n","420 \t 14.424352 \t 7.105618 \t7.318734 \t 0.000000 \t 230.169119 s\n","421 \t 14.588175 \t 7.184693 \t7.403482 \t 0.000000 \t 230.614960 s\n","422 \t 14.180589 \t 6.978004 \t7.202585 \t 0.000000 \t 231.046634 s\n","423 \t 14.844058 \t 7.277151 \t7.566907 \t 0.000000 \t 231.482553 s\n","424 \t 14.456033 \t 7.076061 \t7.379972 \t 0.000000 \t 231.917356 s\n","425 \t 14.458491 \t 7.205217 \t7.253274 \t 0.000000 \t 232.363014 s\n","426 \t 14.308231 \t 7.001655 \t7.306576 \t 0.000000 \t 232.799867 s\n","427 \t 14.707637 \t 7.234468 \t7.473169 \t 0.000000 \t 233.233053 s\n","428 \t 14.666667 \t 7.003863 \t7.662804 \t 0.000000 \t 233.802730 s\n","429 \t 14.274416 \t 6.889915 \t7.384501 \t 0.000000 \t 234.229617 s\n","430 \t 14.399903 \t 6.920022 \t7.479882 \t 0.000000 \t 234.678049 s\n","431 \t 14.635077 \t 7.082559 \t7.552519 \t 0.000000 \t 235.110833 s\n","432 \t 14.994519 \t 7.214155 \t7.780364 \t 0.000000 \t 235.542065 s\n","433 \t 14.324701 \t 6.989220 \t7.335480 \t 0.000000 \t 235.960768 s\n","434 \t 14.414197 \t 7.167767 \t7.246430 \t 0.000000 \t 236.391713 s\n","435 \t 14.310652 \t 7.102228 \t7.208424 \t 0.000000 \t 236.820010 s\n","436 \t 14.977133 \t 7.244342 \t7.732791 \t 0.000000 \t 237.249869 s\n","437 \t 14.673858 \t 7.373801 \t7.300057 \t 0.000000 \t 237.681711 s\n","438 \t 14.956459 \t 7.060749 \t7.895710 \t 0.000000 \t 238.205432 s\n","439 \t 14.700114 \t 7.072612 \t7.627502 \t 0.000000 \t 238.897026 s\n","440 \t 14.702276 \t 7.125556 \t7.576720 \t 0.000000 \t 239.425545 s\n","441 \t 15.265271 \t 7.379646 \t7.885625 \t 0.000000 \t 239.965178 s\n","442 \t 14.387785 \t 7.099268 \t7.288517 \t 0.000000 \t 240.548250 s\n","443 \t 14.841430 \t 7.153233 \t7.688197 \t 0.000000 \t 241.099679 s\n","444 \t 15.105032 \t 7.399441 \t7.705591 \t 0.000000 \t 241.526667 s\n","445 \t 14.710295 \t 7.087459 \t7.622835 \t 0.000000 \t 241.972633 s\n","446 \t 14.031957 \t 6.873166 \t7.158791 \t 0.000000 \t 242.400966 s\n","447 \t 14.898449 \t 7.106897 \t7.791553 \t 0.000000 \t 242.831608 s\n","448 \t 14.888339 \t 7.292577 \t7.595762 \t 0.000000 \t 243.262177 s\n","449 \t 14.483956 \t 7.150039 \t7.333917 \t 0.000000 \t 243.693257 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:09<00:00, 13.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4162\n","Hit@10: 0.5846\n","Hit@3: 0.4269\n","Hit@1: 0.3423\n","Link Prediction on Validation Set (All)\n","MRR: 0.4162\n","Hit@10: 0.5846\n","Hit@3: 0.4269\n","Hit@1: 0.3423\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3544\n","Hit@10: 0.5846\n","Hit@3: 0.4000\n","Hit@1: 0.2462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3544\n","Hit@10: 0.5846\n","Hit@3: 0.4000\n","Hit@1: 0.2462\n","450 \t 14.572945 \t 7.081843 \t7.491101 \t 0.000000 \t 255.408399 s\n","451 \t 15.061537 \t 7.330910 \t7.730627 \t 0.000000 \t 255.838233 s\n","452 \t 14.754431 \t 7.098207 \t7.656224 \t 0.000000 \t 256.278868 s\n","453 \t 15.333663 \t 7.236632 \t8.097031 \t 0.000000 \t 256.707601 s\n","454 \t 15.188526 \t 7.137762 \t8.050764 \t 0.000000 \t 257.143121 s\n","455 \t 15.059367 \t 7.275721 \t7.783646 \t 0.000000 \t 257.575373 s\n","456 \t 14.758905 \t 7.044743 \t7.714162 \t 0.000000 \t 258.006072 s\n","457 \t 14.753765 \t 7.013705 \t7.740059 \t 0.000000 \t 258.438969 s\n","458 \t 14.627267 \t 7.219795 \t7.407472 \t 0.000000 \t 258.862432 s\n","459 \t 15.173818 \t 7.253116 \t7.920703 \t 0.000000 \t 259.293635 s\n","460 \t 15.063079 \t 7.376878 \t7.686201 \t 0.000000 \t 259.739031 s\n","461 \t 15.003968 \t 7.336943 \t7.667025 \t 0.000000 \t 260.359849 s\n","462 \t 14.778223 \t 6.931683 \t7.846541 \t 0.000000 \t 261.085567 s\n","463 \t 15.093635 \t 7.161105 \t7.932530 \t 0.000000 \t 261.676747 s\n","464 \t 14.954597 \t 7.327383 \t7.627214 \t 0.000000 \t 262.282565 s\n","465 \t 14.991983 \t 7.110469 \t7.881514 \t 0.000000 \t 262.940116 s\n","466 \t 15.308840 \t 7.437401 \t7.871440 \t 0.000000 \t 263.499697 s\n","467 \t 15.094896 \t 7.378766 \t7.716130 \t 0.000000 \t 263.947176 s\n","468 \t 15.153613 \t 7.198816 \t7.954798 \t 0.000000 \t 264.430113 s\n","469 \t 14.907654 \t 7.068468 \t7.839186 \t 0.000000 \t 264.972995 s\n","470 \t 15.167707 \t 7.184675 \t7.983032 \t 0.000000 \t 265.517169 s\n","471 \t 15.053437 \t 7.328355 \t7.725082 \t 0.000000 \t 266.212176 s\n","472 \t 14.814797 \t 6.909838 \t7.904959 \t 0.000000 \t 266.798175 s\n","473 \t 14.962294 \t 7.543128 \t7.419166 \t 0.000000 \t 267.378827 s\n","474 \t 15.305376 \t 7.467256 \t7.838120 \t 0.000000 \t 267.817975 s\n","475 \t 14.968299 \t 7.079850 \t7.888449 \t 0.000000 \t 268.250212 s\n","476 \t 14.775171 \t 7.187257 \t7.587915 \t 0.000000 \t 268.680469 s\n","477 \t 14.950533 \t 7.228100 \t7.722434 \t 0.000000 \t 269.113189 s\n","478 \t 15.067829 \t 7.314927 \t7.752902 \t 0.000000 \t 269.537291 s\n","479 \t 14.841727 \t 7.169690 \t7.672037 \t 0.000000 \t 269.974937 s\n","480 \t 14.625465 \t 7.199590 \t7.425875 \t 0.000000 \t 270.409868 s\n","481 \t 15.754442 \t 7.335855 \t8.418588 \t 0.000000 \t 271.005692 s\n","482 \t 14.880380 \t 7.167593 \t7.712787 \t 0.000000 \t 271.447995 s\n","483 \t 15.054873 \t 7.150007 \t7.904866 \t 0.000000 \t 271.875125 s\n","484 \t 15.167113 \t 7.189304 \t7.977809 \t 0.000000 \t 272.321835 s\n","485 \t 14.820459 \t 7.201329 \t7.619130 \t 0.000000 \t 272.745059 s\n","486 \t 15.098693 \t 7.194479 \t7.904214 \t 0.000000 \t 273.179896 s\n","487 \t 15.003143 \t 7.253713 \t7.749430 \t 0.000000 \t 273.604859 s\n","488 \t 14.931770 \t 7.284198 \t7.647572 \t 0.000000 \t 274.052460 s\n","489 \t 15.249352 \t 7.445928 \t7.803423 \t 0.000000 \t 274.491974 s\n","490 \t 14.945534 \t 7.124118 \t7.821415 \t 0.000000 \t 274.921875 s\n","491 \t 15.007074 \t 7.400611 \t7.606463 \t 0.000000 \t 275.380260 s\n","492 \t 14.890973 \t 7.368769 \t7.522204 \t 0.000000 \t 275.938082 s\n","493 \t 15.100006 \t 7.222138 \t7.877868 \t 0.000000 \t 276.384189 s\n","494 \t 15.273098 \t 7.135465 \t8.137633 \t 0.000000 \t 276.819342 s\n","495 \t 14.896153 \t 7.135384 \t7.760769 \t 0.000000 \t 277.255067 s\n","496 \t 15.108859 \t 7.018704 \t8.090155 \t 0.000000 \t 277.803988 s\n","497 \t 14.650045 \t 7.124525 \t7.525521 \t 0.000000 \t 278.343378 s\n","498 \t 14.843638 \t 6.951850 \t7.891788 \t 0.000000 \t 278.856576 s\n","499 \t 15.124079 \t 7.117354 \t8.006725 \t 0.000000 \t 279.389965 s\n","500 \t 15.389644 \t 7.451864 \t7.937780 \t 0.000000 \t 279.966841 s\n","501 \t 14.823826 \t 7.218630 \t7.605196 \t 0.000000 \t 280.516405 s\n","502 \t 14.495915 \t 7.157282 \t7.338633 \t 0.000000 \t 280.949734 s\n","503 \t 14.655766 \t 6.898140 \t7.757625 \t 0.000000 \t 281.399321 s\n","504 \t 14.396826 \t 6.904182 \t7.492643 \t 0.000000 \t 281.963860 s\n","505 \t 14.787331 \t 7.195100 \t7.592231 \t 0.000000 \t 282.405910 s\n","506 \t 15.278739 \t 7.220872 \t8.057867 \t 0.000000 \t 282.832038 s\n","507 \t 14.810399 \t 7.320174 \t7.490225 \t 0.000000 \t 283.260068 s\n","508 \t 14.048594 \t 6.847720 \t7.200874 \t 0.000000 \t 283.695399 s\n","509 \t 14.703036 \t 6.809547 \t7.893489 \t 0.000000 \t 284.123054 s\n","510 \t 14.794146 \t 7.273501 \t7.520644 \t 0.000000 \t 284.562785 s\n","511 \t 14.544488 \t 7.109534 \t7.434954 \t 0.000000 \t 284.988298 s\n","512 \t 15.067699 \t 7.071921 \t7.995779 \t 0.000000 \t 285.562273 s\n","513 \t 14.487316 \t 6.875589 \t7.611726 \t 0.000000 \t 285.997906 s\n","514 \t 14.456678 \t 6.869744 \t7.586935 \t 0.000000 \t 286.425397 s\n","515 \t 14.773687 \t 7.029408 \t7.744279 \t 0.000000 \t 286.877692 s\n","516 \t 14.572791 \t 6.962057 \t7.610734 \t 0.000000 \t 287.308829 s\n","517 \t 14.510315 \t 6.691924 \t7.818392 \t 0.000000 \t 287.742074 s\n","518 \t 14.252576 \t 6.751553 \t7.501023 \t 0.000000 \t 288.167975 s\n","519 \t 14.411757 \t 7.092349 \t7.319408 \t 0.000000 \t 288.606161 s\n","520 \t 14.207661 \t 6.938462 \t7.269199 \t 0.000000 \t 289.037753 s\n","521 \t 14.771832 \t 6.828159 \t7.943673 \t 0.000000 \t 289.602596 s\n","522 \t 14.665617 \t 7.195678 \t7.469939 \t 0.000000 \t 290.033250 s\n","523 \t 14.439848 \t 6.740511 \t7.699337 \t 0.000000 \t 290.509919 s\n","524 \t 14.593719 \t 6.832886 \t7.760834 \t 0.000000 \t 291.058493 s\n","525 \t 14.628770 \t 6.758502 \t7.870268 \t 0.000000 \t 291.574896 s\n","526 \t 14.443535 \t 6.851789 \t7.591745 \t 0.000000 \t 292.128510 s\n","527 \t 14.314867 \t 6.734342 \t7.580524 \t 0.000000 \t 292.663308 s\n","528 \t 14.014307 \t 6.854563 \t7.159744 \t 0.000000 \t 293.234165 s\n","529 \t 14.249711 \t 7.194778 \t7.054932 \t 0.000000 \t 293.740836 s\n","530 \t 14.718256 \t 6.947160 \t7.771096 \t 0.000000 \t 294.311700 s\n","531 \t 14.722048 \t 7.178453 \t7.543595 \t 0.000000 \t 294.734926 s\n","532 \t 14.431336 \t 6.911727 \t7.519609 \t 0.000000 \t 295.177706 s\n","533 \t 14.412057 \t 7.048584 \t7.363473 \t 0.000000 \t 295.609353 s\n","534 \t 14.541577 \t 7.132227 \t7.409350 \t 0.000000 \t 296.044753 s\n","535 \t 14.700969 \t 7.292195 \t7.408774 \t 0.000000 \t 296.481719 s\n","536 \t 14.664923 \t 7.082105 \t7.582818 \t 0.000000 \t 296.926581 s\n","537 \t 14.799065 \t 7.176471 \t7.622595 \t 0.000000 \t 297.365200 s\n","538 \t 15.467777 \t 7.142604 \t8.325172 \t 0.000000 \t 297.802807 s\n","539 \t 14.439384 \t 7.063948 \t7.375435 \t 0.000000 \t 298.387648 s\n","540 \t 14.773261 \t 7.239965 \t7.533296 \t 0.000000 \t 298.812601 s\n","541 \t 14.929389 \t 7.025156 \t7.904233 \t 0.000000 \t 299.254036 s\n","542 \t 14.605489 \t 6.916400 \t7.689089 \t 0.000000 \t 299.688907 s\n","543 \t 14.913162 \t 7.204361 \t7.708801 \t 0.000000 \t 300.131333 s\n","544 \t 14.635712 \t 6.970711 \t7.665000 \t 0.000000 \t 300.574596 s\n","545 \t 14.555367 \t 7.286114 \t7.269254 \t 0.000000 \t 301.015377 s\n","546 \t 14.781953 \t 7.142605 \t7.639348 \t 0.000000 \t 301.460753 s\n","547 \t 14.514718 \t 6.983662 \t7.531056 \t 0.000000 \t 301.900459 s\n","548 \t 14.230840 \t 7.012788 \t7.218052 \t 0.000000 \t 302.486844 s\n","549 \t 14.579711 \t 6.889344 \t7.690367 \t 0.000000 \t 302.917461 s\n","550 \t 14.558927 \t 6.899383 \t7.659544 \t 0.000000 \t 303.358534 s\n","551 \t 14.821202 \t 7.286534 \t7.534668 \t 0.000000 \t 303.885569 s\n","552 \t 14.354681 \t 6.772709 \t7.581972 \t 0.000000 \t 304.422987 s\n","553 \t 14.476643 \t 6.971059 \t7.505584 \t 0.000000 \t 304.930345 s\n","554 \t 14.501830 \t 7.269968 \t7.231862 \t 0.000000 \t 305.472073 s\n","555 \t 14.540880 \t 6.816407 \t7.724473 \t 0.000000 \t 306.066188 s\n","556 \t 14.499751 \t 6.926465 \t7.573286 \t 0.000000 \t 306.638180 s\n","557 \t 14.507301 \t 6.975290 \t7.532012 \t 0.000000 \t 307.219601 s\n","558 \t 14.450374 \t 6.862399 \t7.587975 \t 0.000000 \t 307.666340 s\n","559 \t 14.263888 \t 6.864282 \t7.399606 \t 0.000000 \t 308.104901 s\n","560 \t 14.398427 \t 6.958138 \t7.440288 \t 0.000000 \t 308.538244 s\n","561 \t 14.607039 \t 6.985379 \t7.621661 \t 0.000000 \t 308.968270 s\n","562 \t 14.256661 \t 7.029744 \t7.226918 \t 0.000000 \t 309.416680 s\n","563 \t 13.930086 \t 6.611801 \t7.318285 \t 0.000000 \t 309.844255 s\n","564 \t 14.559525 \t 6.982940 \t7.576586 \t 0.000000 \t 310.276611 s\n","565 \t 15.173677 \t 7.077431 \t8.096247 \t 0.000000 \t 310.713393 s\n","566 \t 14.166759 \t 6.797670 \t7.369089 \t 0.000000 \t 311.143845 s\n","567 \t 14.415380 \t 7.018760 \t7.396621 \t 0.000000 \t 311.717258 s\n","568 \t 14.057711 \t 6.980701 \t7.077010 \t 0.000000 \t 312.150838 s\n","569 \t 14.743379 \t 7.032884 \t7.710495 \t 0.000000 \t 312.592315 s\n","570 \t 14.116575 \t 6.852815 \t7.263760 \t 0.000000 \t 313.028466 s\n","571 \t 14.601693 \t 7.263637 \t7.338057 \t 0.000000 \t 313.458244 s\n","572 \t 13.772396 \t 6.842505 \t6.929891 \t 0.000000 \t 313.907648 s\n","573 \t 15.145774 \t 7.284226 \t7.861547 \t 0.000000 \t 314.346918 s\n","574 \t 14.777740 \t 7.081273 \t7.696467 \t 0.000000 \t 314.783699 s\n","575 \t 14.852272 \t 6.990772 \t7.861500 \t 0.000000 \t 315.213570 s\n","576 \t 14.195262 \t 6.816736 \t7.378526 \t 0.000000 \t 315.652207 s\n","577 \t 14.441858 \t 6.993618 \t7.448241 \t 0.000000 \t 316.084291 s\n","578 \t 14.309054 \t 6.814346 \t7.494707 \t 0.000000 \t 316.699019 s\n","579 \t 14.424918 \t 6.982506 \t7.442412 \t 0.000000 \t 317.231839 s\n","580 \t 14.220338 \t 6.871268 \t7.349071 \t 0.000000 \t 317.775370 s\n","581 \t 14.045012 \t 6.900238 \t7.144774 \t 0.000000 \t 318.306922 s\n","582 \t 14.305555 \t 7.297441 \t7.008114 \t 0.000000 \t 318.866888 s\n","583 \t 14.736605 \t 7.064985 \t7.671620 \t 0.000000 \t 319.450998 s\n","584 \t 14.594894 \t 6.891524 \t7.703370 \t 0.000000 \t 319.958866 s\n","585 \t 14.443352 \t 6.803452 \t7.639900 \t 0.000000 \t 320.398374 s\n","586 \t 14.375676 \t 7.072817 \t7.302859 \t 0.000000 \t 320.839993 s\n","587 \t 14.456900 \t 6.927726 \t7.529174 \t 0.000000 \t 321.290807 s\n","588 \t 14.236109 \t 6.742283 \t7.493826 \t 0.000000 \t 321.719988 s\n","589 \t 14.243697 \t 6.913940 \t7.329757 \t 0.000000 \t 322.302691 s\n","590 \t 14.281193 \t 7.067595 \t7.213598 \t 0.000000 \t 322.729624 s\n","591 \t 14.274570 \t 7.099176 \t7.175394 \t 0.000000 \t 323.164201 s\n","592 \t 14.698825 \t 7.065951 \t7.632874 \t 0.000000 \t 323.595239 s\n","593 \t 13.863569 \t 6.841252 \t7.022317 \t 0.000000 \t 324.041121 s\n","594 \t 14.469658 \t 7.185984 \t7.283674 \t 0.000000 \t 324.468925 s\n","595 \t 13.789709 \t 6.795545 \t6.994164 \t 0.000000 \t 324.902402 s\n","596 \t 14.232810 \t 6.896593 \t7.336216 \t 0.000000 \t 325.353593 s\n","597 \t 14.460696 \t 6.991421 \t7.469275 \t 0.000000 \t 325.784730 s\n","598 \t 14.103819 \t 6.651058 \t7.452761 \t 0.000000 \t 326.212308 s\n","599 \t 14.116509 \t 6.804931 \t7.311578 \t 0.000000 \t 326.642282 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:10<00:00, 12.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4021\n","Hit@10: 0.5308\n","Hit@3: 0.4038\n","Hit@1: 0.3385\n","Link Prediction on Validation Set (All)\n","MRR: 0.4021\n","Hit@10: 0.5308\n","Hit@3: 0.4038\n","Hit@1: 0.3385\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3380\n","Hit@10: 0.5308\n","Hit@3: 0.3769\n","Hit@1: 0.2385\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3380\n","Hit@10: 0.5308\n","Hit@3: 0.3769\n","Hit@1: 0.2385\n","600 \t 14.792862 \t 6.826635 \t7.966227 \t 0.000000 \t 338.299463 s\n","601 \t 14.528919 \t 7.206757 \t7.322162 \t 0.000000 \t 338.881881 s\n","602 \t 14.324985 \t 7.000397 \t7.324588 \t 0.000000 \t 339.314983 s\n","603 \t 14.707405 \t 7.111455 \t7.595949 \t 0.000000 \t 339.753593 s\n","604 \t 14.440514 \t 6.924541 \t7.515973 \t 0.000000 \t 340.181317 s\n","605 \t 14.370374 \t 6.925956 \t7.444417 \t 0.000000 \t 340.612546 s\n","606 \t 14.469767 \t 7.178004 \t7.291763 \t 0.000000 \t 341.074674 s\n","607 \t 14.336092 \t 6.711643 \t7.624450 \t 0.000000 \t 341.501100 s\n","608 \t 14.520956 \t 6.715466 \t7.805490 \t 0.000000 \t 341.941928 s\n","609 \t 14.304850 \t 6.979667 \t7.325183 \t 0.000000 \t 342.380129 s\n","610 \t 14.631766 \t 6.968808 \t7.662958 \t 0.000000 \t 342.824590 s\n","611 \t 13.830245 \t 6.639236 \t7.191009 \t 0.000000 \t 343.319766 s\n","612 \t 14.715055 \t 7.343130 \t7.371925 \t 0.000000 \t 343.998105 s\n","613 \t 14.653846 \t 7.274632 \t7.379213 \t 0.000000 \t 344.745413 s\n","614 \t 13.998066 \t 6.806013 \t7.192053 \t 0.000000 \t 345.845368 s\n","615 \t 14.272294 \t 6.811367 \t7.460927 \t 0.000000 \t 347.039762 s\n","616 \t 14.376705 \t 6.712731 \t7.663974 \t 0.000000 \t 347.531775 s\n","617 \t 14.190451 \t 7.063913 \t7.126537 \t 0.000000 \t 347.973000 s\n","618 \t 14.351412 \t 6.881011 \t7.470401 \t 0.000000 \t 348.421278 s\n","619 \t 13.951886 \t 6.878139 \t7.073748 \t 0.000000 \t 348.857172 s\n","620 \t 14.326427 \t 7.071699 \t7.254728 \t 0.000000 \t 349.432539 s\n","621 \t 14.870489 \t 7.055201 \t7.815288 \t 0.000000 \t 349.863980 s\n","622 \t 14.482551 \t 6.962489 \t7.520062 \t 0.000000 \t 350.304802 s\n","623 \t 14.374394 \t 6.951244 \t7.423151 \t 0.000000 \t 350.747984 s\n","624 \t 14.114143 \t 6.830578 \t7.283565 \t 0.000000 \t 351.201195 s\n","625 \t 14.342787 \t 6.868777 \t7.474010 \t 0.000000 \t 351.645688 s\n","626 \t 14.137111 \t 6.972422 \t7.164689 \t 0.000000 \t 352.083934 s\n","627 \t 14.350450 \t 7.011375 \t7.339075 \t 0.000000 \t 352.527836 s\n","628 \t 14.274497 \t 6.791185 \t7.483312 \t 0.000000 \t 353.092554 s\n","629 \t 14.025220 \t 6.716618 \t7.308602 \t 0.000000 \t 353.536704 s\n","630 \t 14.609341 \t 6.935398 \t7.673944 \t 0.000000 \t 353.969749 s\n","631 \t 14.326531 \t 6.787067 \t7.539464 \t 0.000000 \t 354.411023 s\n","632 \t 14.369900 \t 6.765608 \t7.604293 \t 0.000000 \t 354.848730 s\n","633 \t 14.224524 \t 6.842083 \t7.382441 \t 0.000000 \t 355.298198 s\n","634 \t 14.544011 \t 7.112792 \t7.431219 \t 0.000000 \t 355.735064 s\n","635 \t 14.592948 \t 6.964659 \t7.628289 \t 0.000000 \t 356.165674 s\n","636 \t 14.039910 \t 6.699481 \t7.340429 \t 0.000000 \t 356.605320 s\n","637 \t 14.037017 \t 6.837285 \t7.199732 \t 0.000000 \t 357.234033 s\n","638 \t 14.062961 \t 6.714508 \t7.348453 \t 0.000000 \t 357.797882 s\n","639 \t 14.210760 \t 6.692244 \t7.518516 \t 0.000000 \t 358.321967 s\n","640 \t 14.300008 \t 7.124582 \t7.175426 \t 0.000000 \t 358.861260 s\n","641 \t 14.297866 \t 6.958513 \t7.339354 \t 0.000000 \t 359.431795 s\n","642 \t 14.122420 \t 6.580358 \t7.542063 \t 0.000000 \t 360.048845 s\n","643 \t 14.421071 \t 6.866340 \t7.554731 \t 0.000000 \t 360.493966 s\n","644 \t 14.501149 \t 6.710036 \t7.791113 \t 0.000000 \t 360.957903 s\n","645 \t 14.093688 \t 6.457870 \t7.635818 \t 0.000000 \t 361.400333 s\n","646 \t 13.950365 \t 6.769206 \t7.181159 \t 0.000000 \t 361.994967 s\n","647 \t 13.781197 \t 6.738911 \t7.042285 \t 0.000000 \t 362.444018 s\n","648 \t 14.420592 \t 6.836890 \t7.583702 \t 0.000000 \t 362.905557 s\n","649 \t 13.952447 \t 6.839189 \t7.113259 \t 0.000000 \t 363.351246 s\n","650 \t 14.184043 \t 6.734165 \t7.449878 \t 0.000000 \t 363.794521 s\n","651 \t 14.030225 \t 6.938175 \t7.092049 \t 0.000000 \t 364.238963 s\n","652 \t 14.407185 \t 6.990128 \t7.417057 \t 0.000000 \t 364.681804 s\n","653 \t 14.003155 \t 6.649828 \t7.353327 \t 0.000000 \t 365.144203 s\n","654 \t 13.931053 \t 6.703684 \t7.227368 \t 0.000000 \t 365.581961 s\n","655 \t 14.417964 \t 6.790508 \t7.627456 \t 0.000000 \t 366.040331 s\n","656 \t 14.123665 \t 6.849524 \t7.274141 \t 0.000000 \t 366.474865 s\n","657 \t 14.148622 \t 6.810127 \t7.338495 \t 0.000000 \t 367.064717 s\n","658 \t 14.101445 \t 6.803632 \t7.297812 \t 0.000000 \t 367.495095 s\n","659 \t 14.040959 \t 6.962078 \t7.078881 \t 0.000000 \t 367.945165 s\n","660 \t 14.081056 \t 6.796398 \t7.284658 \t 0.000000 \t 368.389638 s\n","661 \t 14.676343 \t 6.834977 \t7.841367 \t 0.000000 \t 368.821557 s\n","662 \t 14.514999 \t 6.931126 \t7.583873 \t 0.000000 \t 369.272959 s\n","663 \t 14.391156 \t 7.155240 \t7.235916 \t 0.000000 \t 369.702320 s\n","664 \t 13.753345 \t 6.685738 \t7.067606 \t 0.000000 \t 370.169790 s\n","665 \t 14.139516 \t 6.871222 \t7.268293 \t 0.000000 \t 370.707154 s\n","666 \t 13.807092 \t 6.685333 \t7.121759 \t 0.000000 \t 371.229500 s\n","667 \t 14.433868 \t 6.779223 \t7.654645 \t 0.000000 \t 371.759392 s\n","668 \t 14.201976 \t 6.693992 \t7.507984 \t 0.000000 \t 372.507724 s\n","669 \t 13.333162 \t 6.591657 \t6.741505 \t 0.000000 \t 373.106094 s\n","670 \t 14.060422 \t 6.962598 \t7.097824 \t 0.000000 \t 373.535274 s\n","671 \t 14.441696 \t 6.962867 \t7.478828 \t 0.000000 \t 373.970555 s\n","672 \t 14.059616 \t 6.750461 \t7.309155 \t 0.000000 \t 374.410982 s\n","673 \t 14.231765 \t 7.049504 \t7.182261 \t 0.000000 \t 374.843592 s\n","674 \t 14.240919 \t 6.913018 \t7.327902 \t 0.000000 \t 375.282202 s\n","675 \t 14.058964 \t 6.719380 \t7.339583 \t 0.000000 \t 375.713418 s\n","676 \t 13.982487 \t 6.613666 \t7.368820 \t 0.000000 \t 376.147700 s\n","677 \t 14.294947 \t 7.123941 \t7.171005 \t 0.000000 \t 376.592671 s\n","678 \t 14.199146 \t 6.763223 \t7.435923 \t 0.000000 \t 377.029810 s\n","679 \t 14.236376 \t 7.055376 \t7.181000 \t 0.000000 \t 377.464580 s\n","680 \t 14.046623 \t 6.720938 \t7.325685 \t 0.000000 \t 378.047939 s\n","681 \t 14.573924 \t 6.996620 \t7.577304 \t 0.000000 \t 378.475325 s\n","682 \t 13.927118 \t 6.878360 \t7.048759 \t 0.000000 \t 378.908051 s\n","683 \t 13.697821 \t 6.497375 \t7.200446 \t 0.000000 \t 379.362959 s\n","684 \t 14.424645 \t 6.823868 \t7.600777 \t 0.000000 \t 379.805349 s\n","685 \t 13.965301 \t 6.776635 \t7.188665 \t 0.000000 \t 380.227704 s\n","686 \t 14.031487 \t 6.580260 \t7.451227 \t 0.000000 \t 380.672740 s\n","687 \t 14.159032 \t 6.676541 \t7.482491 \t 0.000000 \t 381.111218 s\n","688 \t 13.834947 \t 6.527343 \t7.307604 \t 0.000000 \t 381.540677 s\n","689 \t 14.322178 \t 6.910172 \t7.412006 \t 0.000000 \t 381.978641 s\n","690 \t 14.294457 \t 6.815746 \t7.478711 \t 0.000000 \t 382.417275 s\n","691 \t 14.211318 \t 7.037425 \t7.173892 \t 0.000000 \t 382.851795 s\n","692 \t 13.981731 \t 6.816967 \t7.164763 \t 0.000000 \t 383.481470 s\n","693 \t 13.749439 \t 6.560482 \t7.188958 \t 0.000000 \t 384.015544 s\n","694 \t 13.722135 \t 6.569450 \t7.152685 \t 0.000000 \t 384.526713 s\n","695 \t 13.928474 \t 6.606737 \t7.321738 \t 0.000000 \t 385.059475 s\n","696 \t 13.956442 \t 6.601367 \t7.355076 \t 0.000000 \t 385.628409 s\n","697 \t 14.061575 \t 6.646568 \t7.415007 \t 0.000000 \t 386.211710 s\n","698 \t 14.084959 \t 6.863328 \t7.221631 \t 0.000000 \t 386.643531 s\n","699 \t 14.138580 \t 7.006328 \t7.132252 \t 0.000000 \t 387.086797 s\n","700 \t 14.022872 \t 6.816978 \t7.205894 \t 0.000000 \t 387.513693 s\n","701 \t 14.338447 \t 6.783738 \t7.554709 \t 0.000000 \t 387.958147 s\n","702 \t 13.831518 \t 6.741799 \t7.089719 \t 0.000000 \t 388.398673 s\n","703 \t 14.071895 \t 6.734896 \t7.336999 \t 0.000000 \t 388.837384 s\n","704 \t 13.650514 \t 6.531928 \t7.118586 \t 0.000000 \t 389.402754 s\n","705 \t 13.735121 \t 6.578409 \t7.156712 \t 0.000000 \t 389.839903 s\n","706 \t 13.041418 \t 6.516161 \t6.525257 \t 0.000000 \t 390.266471 s\n","707 \t 13.638059 \t 6.402486 \t7.235573 \t 0.000000 \t 390.703243 s\n","708 \t 14.104788 \t 6.996718 \t7.108070 \t 0.000000 \t 391.149003 s\n","709 \t 14.440513 \t 6.958194 \t7.482318 \t 0.000000 \t 391.576000 s\n","710 \t 13.741283 \t 6.730967 \t7.010316 \t 0.000000 \t 392.012208 s\n","711 \t 13.676267 \t 6.584001 \t7.092267 \t 0.000000 \t 392.437174 s\n","712 \t 13.867282 \t 6.678910 \t7.188373 \t 0.000000 \t 392.878075 s\n","713 \t 14.046376 \t 6.617301 \t7.429075 \t 0.000000 \t 393.426881 s\n","714 \t 13.934485 \t 6.954025 \t6.980460 \t 0.000000 \t 393.864259 s\n","715 \t 13.696930 \t 6.717633 \t6.979297 \t 0.000000 \t 394.301615 s\n","716 \t 13.833280 \t 6.664567 \t7.168713 \t 0.000000 \t 394.742660 s\n","717 \t 13.574655 \t 6.799071 \t6.775584 \t 0.000000 \t 395.176693 s\n","718 \t 13.874668 \t 6.932738 \t6.941930 \t 0.000000 \t 395.608238 s\n","719 \t 13.929393 \t 6.940099 \t6.989294 \t 0.000000 \t 396.049471 s\n","720 \t 13.998769 \t 6.543568 \t7.455200 \t 0.000000 \t 396.530386 s\n","721 \t 13.908067 \t 6.821133 \t7.086934 \t 0.000000 \t 397.083192 s\n","722 \t 13.963086 \t 6.720339 \t7.242747 \t 0.000000 \t 397.772544 s\n","723 \t 13.934858 \t 6.567777 \t7.367081 \t 0.000000 \t 398.299668 s\n","724 \t 14.154303 \t 6.459701 \t7.694602 \t 0.000000 \t 398.882035 s\n","725 \t 13.650844 \t 6.792687 \t6.858158 \t 0.000000 \t 399.443962 s\n","726 \t 13.169735 \t 6.500532 \t6.669203 \t 0.000000 \t 399.875266 s\n","727 \t 13.909090 \t 6.840282 \t7.068808 \t 0.000000 \t 400.310683 s\n","728 \t 13.906380 \t 6.687317 \t7.219062 \t 0.000000 \t 400.736052 s\n","729 \t 13.836624 \t 6.661562 \t7.175062 \t 0.000000 \t 401.169779 s\n","730 \t 13.997447 \t 6.776078 \t7.221370 \t 0.000000 \t 401.615380 s\n","731 \t 13.924773 \t 6.592938 \t7.331835 \t 0.000000 \t 402.050225 s\n","732 \t 13.354875 \t 6.630071 \t6.724803 \t 0.000000 \t 402.613581 s\n","733 \t 14.005543 \t 6.586030 \t7.419513 \t 0.000000 \t 403.044964 s\n","734 \t 13.951292 \t 6.897428 \t7.053863 \t 0.000000 \t 403.476696 s\n","735 \t 13.966603 \t 6.742073 \t7.224529 \t 0.000000 \t 403.904830 s\n","736 \t 14.027257 \t 6.789982 \t7.237275 \t 0.000000 \t 404.347511 s\n","737 \t 13.866014 \t 6.770998 \t7.095016 \t 0.000000 \t 404.795072 s\n","738 \t 14.302794 \t 6.859195 \t7.443599 \t 0.000000 \t 405.227242 s\n","739 \t 13.956162 \t 6.627384 \t7.328779 \t 0.000000 \t 405.673563 s\n","740 \t 13.953830 \t 6.810596 \t7.143235 \t 0.000000 \t 406.101657 s\n","741 \t 13.511803 \t 6.628099 \t6.883704 \t 0.000000 \t 406.530655 s\n","742 \t 14.046098 \t 6.791627 \t7.254470 \t 0.000000 \t 406.961372 s\n","743 \t 14.726624 \t 6.995452 \t7.731172 \t 0.000000 \t 407.528155 s\n","744 \t 14.026307 \t 6.724572 \t7.301736 \t 0.000000 \t 407.957009 s\n","745 \t 13.719760 \t 6.575956 \t7.143804 \t 0.000000 \t 408.385880 s\n","746 \t 14.186707 \t 6.729411 \t7.457296 \t 0.000000 \t 408.831734 s\n","747 \t 14.286773 \t 7.008116 \t7.278657 \t 0.000000 \t 409.263343 s\n","748 \t 13.829633 \t 6.863196 \t6.966438 \t 0.000000 \t 409.787766 s\n","749 \t 13.639847 \t 6.571352 \t7.068495 \t 0.000000 \t 410.304686 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:09<00:00, 13.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4083\n","Hit@10: 0.5538\n","Hit@3: 0.4231\n","Hit@1: 0.3385\n","Link Prediction on Validation Set (All)\n","MRR: 0.4083\n","Hit@10: 0.5538\n","Hit@3: 0.4231\n","Hit@1: 0.3385\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3029\n","Hit@10: 0.5615\n","Hit@3: 0.3308\n","Hit@1: 0.1923\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3029\n","Hit@10: 0.5615\n","Hit@3: 0.3308\n","Hit@1: 0.1923\n","750 \t 14.497453 \t 6.644810 \t7.852643 \t 0.000000 \t 421.734488 s\n","751 \t 13.547054 \t 6.559711 \t6.987343 \t 0.000000 \t 422.176118 s\n","752 \t 13.841603 \t 6.829258 \t7.012345 \t 0.000000 \t 422.620872 s\n","753 \t 14.250336 \t 6.820607 \t7.429729 \t 0.000000 \t 423.206982 s\n","754 \t 14.138066 \t 6.611177 \t7.526890 \t 0.000000 \t 423.873097 s\n","755 \t 13.607272 \t 6.605781 \t7.001491 \t 0.000000 \t 424.408513 s\n","756 \t 14.192923 \t 6.784643 \t7.408279 \t 0.000000 \t 424.952989 s\n","757 \t 13.905303 \t 6.595308 \t7.309995 \t 0.000000 \t 425.538630 s\n","758 \t 14.090187 \t 6.626978 \t7.463209 \t 0.000000 \t 426.039110 s\n","759 \t 13.644310 \t 6.649161 \t6.995149 \t 0.000000 \t 426.467530 s\n","760 \t 13.772040 \t 6.716844 \t7.055197 \t 0.000000 \t 426.900198 s\n","761 \t 13.639249 \t 6.655344 \t6.983905 \t 0.000000 \t 427.377983 s\n","762 \t 14.040855 \t 6.669067 \t7.371788 \t 0.000000 \t 428.016060 s\n","763 \t 14.087867 \t 6.647348 \t7.440519 \t 0.000000 \t 428.599618 s\n","764 \t 13.799500 \t 6.640851 \t7.158648 \t 0.000000 \t 429.238472 s\n","765 \t 13.981134 \t 6.661265 \t7.319869 \t 0.000000 \t 429.745200 s\n","766 \t 13.484131 \t 6.683957 \t6.800174 \t 0.000000 \t 430.271999 s\n","767 \t 14.221301 \t 6.844047 \t7.377254 \t 0.000000 \t 430.816150 s\n","768 \t 13.647131 \t 6.576103 \t7.071028 \t 0.000000 \t 431.370974 s\n","769 \t 14.475074 \t 6.656254 \t7.818820 \t 0.000000 \t 431.986567 s\n","770 \t 14.001865 \t 6.564781 \t7.437083 \t 0.000000 \t 432.467566 s\n","771 \t 14.040093 \t 6.523455 \t7.516637 \t 0.000000 \t 432.933224 s\n","772 \t 14.139408 \t 6.635219 \t7.504189 \t 0.000000 \t 433.379121 s\n","773 \t 13.619681 \t 6.411392 \t7.208289 \t 0.000000 \t 433.817391 s\n","774 \t 13.747614 \t 6.609633 \t7.137981 \t 0.000000 \t 434.394994 s\n","775 \t 13.654420 \t 6.671895 \t6.982525 \t 0.000000 \t 434.838203 s\n","776 \t 13.490235 \t 6.547290 \t6.942945 \t 0.000000 \t 435.264771 s\n","777 \t 13.843342 \t 6.786992 \t7.056350 \t 0.000000 \t 435.713058 s\n","778 \t 13.743402 \t 6.685685 \t7.057717 \t 0.000000 \t 436.264744 s\n","779 \t 13.778865 \t 6.569394 \t7.209471 \t 0.000000 \t 436.786392 s\n","780 \t 13.615985 \t 6.621734 \t6.994250 \t 0.000000 \t 437.302919 s\n","781 \t 14.105370 \t 6.779880 \t7.325490 \t 0.000000 \t 437.843909 s\n","782 \t 13.617994 \t 6.573757 \t7.044238 \t 0.000000 \t 438.431399 s\n","783 \t 13.762056 \t 6.650276 \t7.111780 \t 0.000000 \t 438.973863 s\n","784 \t 13.600388 \t 6.560945 \t7.039442 \t 0.000000 \t 439.536123 s\n","785 \t 14.361395 \t 6.674434 \t7.686961 \t 0.000000 \t 439.969187 s\n","786 \t 13.699252 \t 6.430830 \t7.268422 \t 0.000000 \t 440.406991 s\n","787 \t 13.289192 \t 6.671633 \t6.617558 \t 0.000000 \t 440.850732 s\n","788 \t 13.535188 \t 6.703040 \t6.832148 \t 0.000000 \t 441.281780 s\n","789 \t 13.824122 \t 6.569937 \t7.254186 \t 0.000000 \t 441.708602 s\n","790 \t 14.140144 \t 6.854560 \t7.285585 \t 0.000000 \t 442.148716 s\n","791 \t 14.129776 \t 6.676161 \t7.453615 \t 0.000000 \t 442.581713 s\n","792 \t 13.827327 \t 6.691021 \t7.136306 \t 0.000000 \t 443.021882 s\n","793 \t 13.638650 \t 6.679768 \t6.958882 \t 0.000000 \t 443.451918 s\n","794 \t 13.563626 \t 6.526520 \t7.037106 \t 0.000000 \t 443.878142 s\n","795 \t 13.744615 \t 6.549169 \t7.195446 \t 0.000000 \t 444.468919 s\n","796 \t 13.792115 \t 6.664742 \t7.127374 \t 0.000000 \t 444.903337 s\n","797 \t 13.879086 \t 6.655273 \t7.223813 \t 0.000000 \t 445.356789 s\n","798 \t 13.664940 \t 6.614453 \t7.050487 \t 0.000000 \t 445.788489 s\n","799 \t 14.341856 \t 7.176443 \t7.165413 \t 0.000000 \t 446.217716 s\n","800 \t 13.886589 \t 6.906290 \t6.980299 \t 0.000000 \t 446.643381 s\n","801 \t 13.878646 \t 6.553043 \t7.325603 \t 0.000000 \t 447.092743 s\n","802 \t 13.855034 \t 6.687315 \t7.167719 \t 0.000000 \t 447.525532 s\n","803 \t 13.982219 \t 6.501726 \t7.480493 \t 0.000000 \t 447.962564 s\n","804 \t 13.854211 \t 6.605412 \t7.248800 \t 0.000000 \t 448.425615 s\n","805 \t 14.241157 \t 6.695591 \t7.545566 \t 0.000000 \t 448.908724 s\n","806 \t 13.637648 \t 6.559956 \t7.077692 \t 0.000000 \t 449.460138 s\n","807 \t 14.330764 \t 6.831535 \t7.499229 \t 0.000000 \t 450.160346 s\n","808 \t 13.968747 \t 6.663671 \t7.305076 \t 0.000000 \t 450.673417 s\n","809 \t 13.545612 \t 6.658349 \t6.887263 \t 0.000000 \t 451.257125 s\n","810 \t 13.655675 \t 6.709074 \t6.946600 \t 0.000000 \t 451.831365 s\n","811 \t 13.855414 \t 6.733261 \t7.122153 \t 0.000000 \t 452.266015 s\n","812 \t 13.920039 \t 6.601296 \t7.318743 \t 0.000000 \t 452.714566 s\n","813 \t 13.679946 \t 6.648747 \t7.031200 \t 0.000000 \t 453.155459 s\n","814 \t 13.963830 \t 6.634786 \t7.329043 \t 0.000000 \t 453.587775 s\n","815 \t 14.003871 \t 6.734587 \t7.269284 \t 0.000000 \t 454.021727 s\n","816 \t 13.362205 \t 6.535870 \t6.826334 \t 0.000000 \t 454.462681 s\n","817 \t 14.096401 \t 6.799223 \t7.297178 \t 0.000000 \t 454.900553 s\n","818 \t 13.740825 \t 6.674303 \t7.066523 \t 0.000000 \t 455.333895 s\n","819 \t 13.806560 \t 6.551779 \t7.254781 \t 0.000000 \t 455.919700 s\n","820 \t 14.171026 \t 6.713264 \t7.457762 \t 0.000000 \t 456.363822 s\n","821 \t 13.496803 \t 6.604833 \t6.891970 \t 0.000000 \t 456.822841 s\n","822 \t 13.753775 \t 6.704879 \t7.048896 \t 0.000000 \t 457.252105 s\n","823 \t 13.312638 \t 6.598872 \t6.713766 \t 0.000000 \t 457.702785 s\n","824 \t 14.016941 \t 6.818678 \t7.198263 \t 0.000000 \t 458.143650 s\n","825 \t 14.279716 \t 6.822499 \t7.457218 \t 0.000000 \t 458.586720 s\n","826 \t 13.877068 \t 6.676588 \t7.200480 \t 0.000000 \t 459.020602 s\n","827 \t 14.107545 \t 6.866943 \t7.240602 \t 0.000000 \t 459.607971 s\n","828 \t 13.521190 \t 6.610886 \t6.910304 \t 0.000000 \t 460.044560 s\n","829 \t 13.823847 \t 6.531616 \t7.292231 \t 0.000000 \t 460.473926 s\n","830 \t 13.562363 \t 6.518072 \t7.044290 \t 0.000000 \t 460.922677 s\n","831 \t 13.791166 \t 6.753703 \t7.037464 \t 0.000000 \t 461.363168 s\n","832 \t 13.452547 \t 6.550019 \t6.902528 \t 0.000000 \t 461.812514 s\n","833 \t 13.136838 \t 6.375269 \t6.761569 \t 0.000000 \t 462.359185 s\n","834 \t 13.871518 \t 6.680397 \t7.191121 \t 0.000000 \t 462.903292 s\n","835 \t 13.564333 \t 6.510339 \t7.053993 \t 0.000000 \t 463.428491 s\n","836 \t 13.594562 \t 6.670599 \t6.923963 \t 0.000000 \t 464.162991 s\n","837 \t 14.197845 \t 6.704197 \t7.493648 \t 0.000000 \t 464.745146 s\n","838 \t 13.441446 \t 6.483177 \t6.958269 \t 0.000000 \t 465.206968 s\n","839 \t 14.036105 \t 6.704820 \t7.331284 \t 0.000000 \t 465.641494 s\n","840 \t 13.589691 \t 6.311915 \t7.277775 \t 0.000000 \t 466.080198 s\n","841 \t 13.865212 \t 6.737965 \t7.127247 \t 0.000000 \t 466.513828 s\n","842 \t 13.818995 \t 6.651843 \t7.167152 \t 0.000000 \t 466.964308 s\n","843 \t 14.012808 \t 6.619088 \t7.393720 \t 0.000000 \t 467.397593 s\n","844 \t 13.875085 \t 6.594477 \t7.280608 \t 0.000000 \t 467.978446 s\n","845 \t 14.026517 \t 6.513180 \t7.513337 \t 0.000000 \t 468.411764 s\n","846 \t 13.838605 \t 6.690390 \t7.148215 \t 0.000000 \t 468.848547 s\n","847 \t 13.772272 \t 6.394340 \t7.377932 \t 0.000000 \t 469.309996 s\n","848 \t 13.763889 \t 6.457484 \t7.306406 \t 0.000000 \t 469.739078 s\n","849 \t 13.896996 \t 6.754976 \t7.142020 \t 0.000000 \t 470.175089 s\n","850 \t 13.725379 \t 6.686586 \t7.038793 \t 0.000000 \t 470.611172 s\n","851 \t 13.333877 \t 6.610602 \t6.723274 \t 0.000000 \t 471.063620 s\n","852 \t 13.460100 \t 6.537457 \t6.922642 \t 0.000000 \t 471.501504 s\n","853 \t 13.797742 \t 6.693712 \t7.104031 \t 0.000000 \t 472.075712 s\n","854 \t 14.030484 \t 6.907129 \t7.123354 \t 0.000000 \t 472.504765 s\n","855 \t 13.397120 \t 6.598779 \t6.798341 \t 0.000000 \t 472.937967 s\n","856 \t 13.883047 \t 6.582321 \t7.300726 \t 0.000000 \t 473.394350 s\n","857 \t 13.773428 \t 6.516794 \t7.256634 \t 0.000000 \t 473.825314 s\n","858 \t 13.565575 \t 6.492495 \t7.073080 \t 0.000000 \t 474.262001 s\n","859 \t 13.798770 \t 6.635124 \t7.163646 \t 0.000000 \t 474.694339 s\n","860 \t 14.112648 \t 6.728440 \t7.384207 \t 0.000000 \t 475.228434 s\n","861 \t 14.163787 \t 6.592191 \t7.571596 \t 0.000000 \t 475.744225 s\n","862 \t 14.024507 \t 6.869682 \t7.154825 \t 0.000000 \t 476.425773 s\n","863 \t 13.864342 \t 6.699984 \t7.164358 \t 0.000000 \t 476.971990 s\n","864 \t 13.963246 \t 6.727358 \t7.235888 \t 0.000000 \t 477.588064 s\n","865 \t 13.567560 \t 6.531564 \t7.035996 \t 0.000000 \t 478.121274 s\n","866 \t 13.694457 \t 6.542483 \t7.151973 \t 0.000000 \t 478.554470 s\n","867 \t 13.520509 \t 6.455099 \t7.065410 \t 0.000000 \t 478.988357 s\n","868 \t 13.648479 \t 6.486597 \t7.161881 \t 0.000000 \t 479.459687 s\n","869 \t 13.193552 \t 6.581439 \t6.612112 \t 0.000000 \t 479.889012 s\n","870 \t 13.740770 \t 6.659999 \t7.080771 \t 0.000000 \t 480.331249 s\n","871 \t 14.241196 \t 6.749134 \t7.492062 \t 0.000000 \t 480.785405 s\n","872 \t 14.320530 \t 7.049620 \t7.270910 \t 0.000000 \t 481.351182 s\n","873 \t 13.513862 \t 6.386470 \t7.127392 \t 0.000000 \t 481.797385 s\n","874 \t 13.884715 \t 6.698948 \t7.185766 \t 0.000000 \t 482.226719 s\n","875 \t 13.742653 \t 6.478392 \t7.264261 \t 0.000000 \t 482.655505 s\n","876 \t 14.156216 \t 6.825099 \t7.331117 \t 0.000000 \t 483.094420 s\n","877 \t 14.002582 \t 6.808356 \t7.194226 \t 0.000000 \t 483.530618 s\n","878 \t 13.990296 \t 6.603834 \t7.386462 \t 0.000000 \t 483.966372 s\n","879 \t 14.055458 \t 6.816665 \t7.238793 \t 0.000000 \t 484.403687 s\n","880 \t 13.667217 \t 6.636523 \t7.030694 \t 0.000000 \t 484.850892 s\n","881 \t 13.434218 \t 6.532968 \t6.901250 \t 0.000000 \t 485.289733 s\n","882 \t 13.551823 \t 6.412567 \t7.139256 \t 0.000000 \t 485.851343 s\n","883 \t 13.960305 \t 6.753514 \t7.206791 \t 0.000000 \t 486.293280 s\n","884 \t 13.709223 \t 6.746714 \t6.962508 \t 0.000000 \t 486.726687 s\n","885 \t 13.602156 \t 6.693661 \t6.908494 \t 0.000000 \t 487.155346 s\n","886 \t 13.858674 \t 6.486608 \t7.372066 \t 0.000000 \t 487.611888 s\n","887 \t 14.018942 \t 6.594637 \t7.424305 \t 0.000000 \t 488.134472 s\n","888 \t 13.709037 \t 6.677543 \t7.031494 \t 0.000000 \t 488.690647 s\n","889 \t 13.696460 \t 6.549830 \t7.146630 \t 0.000000 \t 489.225671 s\n","890 \t 14.199594 \t 6.654153 \t7.545440 \t 0.000000 \t 489.777266 s\n","891 \t 14.056751 \t 6.632435 \t7.424316 \t 0.000000 \t 490.332212 s\n","892 \t 13.516497 \t 6.419432 \t7.097065 \t 0.000000 \t 490.928249 s\n","893 \t 13.598106 \t 6.452854 \t7.145252 \t 0.000000 \t 491.561992 s\n","894 \t 13.587722 \t 6.404571 \t7.183151 \t 0.000000 \t 491.997092 s\n","895 \t 14.099501 \t 6.816409 \t7.283092 \t 0.000000 \t 492.428412 s\n","896 \t 13.666062 \t 6.610200 \t7.055862 \t 0.000000 \t 492.859678 s\n","897 \t 13.021248 \t 6.499488 \t6.521761 \t 0.000000 \t 493.310532 s\n","898 \t 13.770310 \t 6.484088 \t7.286222 \t 0.000000 \t 493.736960 s\n","899 \t 13.528469 \t 6.411197 \t7.117272 \t 0.000000 \t 494.174093 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:10<00:00, 12.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4046\n","Hit@10: 0.5269\n","Hit@3: 0.4192\n","Hit@1: 0.3385\n","Link Prediction on Validation Set (All)\n","MRR: 0.4046\n","Hit@10: 0.5269\n","Hit@3: 0.4192\n","Hit@1: 0.3385\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3349\n","Hit@10: 0.6000\n","Hit@3: 0.3769\n","Hit@1: 0.2154\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3349\n","Hit@10: 0.6000\n","Hit@3: 0.3769\n","Hit@1: 0.2154\n","900 \t 14.217438 \t 6.751946 \t7.465491 \t 0.000000 \t 505.925468 s\n","901 \t 13.858150 \t 6.565416 \t7.292733 \t 0.000000 \t 506.363496 s\n","902 \t 13.786541 \t 6.567328 \t7.219212 \t 0.000000 \t 506.795871 s\n","903 \t 13.790933 \t 6.488369 \t7.302565 \t 0.000000 \t 507.231852 s\n","904 \t 13.750501 \t 6.782715 \t6.967786 \t 0.000000 \t 507.803053 s\n","905 \t 13.835845 \t 6.759481 \t7.076364 \t 0.000000 \t 508.231608 s\n","906 \t 13.539177 \t 6.642965 \t6.896212 \t 0.000000 \t 508.668874 s\n","907 \t 13.286455 \t 6.434941 \t6.851514 \t 0.000000 \t 509.093558 s\n","908 \t 14.163680 \t 6.703821 \t7.459858 \t 0.000000 \t 509.526705 s\n","909 \t 13.425027 \t 6.785239 \t6.639788 \t 0.000000 \t 509.978758 s\n","910 \t 13.625699 \t 6.587940 \t7.037758 \t 0.000000 \t 510.408983 s\n","911 \t 13.223741 \t 6.479983 \t6.743757 \t 0.000000 \t 510.849871 s\n","912 \t 13.668736 \t 6.728196 \t6.940540 \t 0.000000 \t 511.276209 s\n","913 \t 13.962350 \t 6.616779 \t7.345571 \t 0.000000 \t 511.709106 s\n","914 \t 13.678633 \t 6.493979 \t7.184654 \t 0.000000 \t 512.137911 s\n","915 \t 13.923012 \t 6.651352 \t7.271660 \t 0.000000 \t 512.696732 s\n","916 \t 14.052388 \t 6.687285 \t7.365102 \t 0.000000 \t 513.160521 s\n","917 \t 13.815027 \t 6.652665 \t7.162362 \t 0.000000 \t 513.615788 s\n","918 \t 13.529433 \t 6.760250 \t6.769184 \t 0.000000 \t 514.178819 s\n","919 \t 13.737125 \t 6.663041 \t7.074084 \t 0.000000 \t 514.878861 s\n","920 \t 13.566502 \t 6.633871 \t6.932631 \t 0.000000 \t 515.670131 s\n","921 \t 13.795840 \t 6.806484 \t6.989356 \t 0.000000 \t 516.507492 s\n","922 \t 13.257998 \t 6.284267 \t6.973731 \t 0.000000 \t 517.365150 s\n","923 \t 13.849702 \t 6.928430 \t6.921272 \t 0.000000 \t 518.105625 s\n","924 \t 14.138588 \t 6.872568 \t7.266020 \t 0.000000 \t 518.601560 s\n","925 \t 13.513064 \t 6.695067 \t6.817997 \t 0.000000 \t 519.040792 s\n","926 \t 13.722347 \t 6.562649 \t7.159698 \t 0.000000 \t 519.626709 s\n","927 \t 13.531091 \t 6.653727 \t6.877364 \t 0.000000 \t 520.059852 s\n","928 \t 13.996137 \t 6.978761 \t7.017375 \t 0.000000 \t 520.505760 s\n","929 \t 13.533485 \t 6.337152 \t7.196333 \t 0.000000 \t 520.932433 s\n","930 \t 14.131872 \t 6.737237 \t7.394634 \t 0.000000 \t 521.371413 s\n","931 \t 13.762661 \t 6.492469 \t7.270191 \t 0.000000 \t 521.798677 s\n","932 \t 13.853703 \t 6.590628 \t7.263075 \t 0.000000 \t 522.230197 s\n","933 \t 13.652926 \t 6.709554 \t6.943372 \t 0.000000 \t 522.659526 s\n","934 \t 13.819742 \t 6.693189 \t7.126554 \t 0.000000 \t 523.084308 s\n","935 \t 13.756479 \t 6.617821 \t7.138658 \t 0.000000 \t 523.532161 s\n","936 \t 13.546598 \t 6.465118 \t7.081480 \t 0.000000 \t 523.962166 s\n","937 \t 13.774096 \t 6.595628 \t7.178468 \t 0.000000 \t 524.401036 s\n","938 \t 13.489633 \t 6.618205 \t6.871427 \t 0.000000 \t 524.972148 s\n","939 \t 13.708797 \t 6.469884 \t7.238913 \t 0.000000 \t 525.411713 s\n","940 \t 13.795430 \t 6.863948 \t6.931482 \t 0.000000 \t 525.850744 s\n","941 \t 13.848613 \t 6.710233 \t7.138380 \t 0.000000 \t 526.277879 s\n","942 \t 14.399422 \t 7.198289 \t7.201133 \t 0.000000 \t 526.717553 s\n","943 \t 13.858463 \t 6.729359 \t7.129105 \t 0.000000 \t 527.149857 s\n","944 \t 13.805847 \t 6.679213 \t7.126633 \t 0.000000 \t 527.585569 s\n","945 \t 13.514019 \t 6.494210 \t7.019808 \t 0.000000 \t 528.020094 s\n","946 \t 13.433792 \t 6.429858 \t7.003934 \t 0.000000 \t 528.516901 s\n","947 \t 14.189467 \t 6.997310 \t7.192156 \t 0.000000 \t 529.052633 s\n","948 \t 14.155636 \t 6.642708 \t7.512928 \t 0.000000 \t 529.572916 s\n","949 \t 14.037134 \t 6.764654 \t7.272481 \t 0.000000 \t 530.097229 s\n","950 \t 14.032472 \t 6.797376 \t7.235096 \t 0.000000 \t 530.813434 s\n","951 \t 13.440948 \t 6.449552 \t6.991397 \t 0.000000 \t 531.383004 s\n","952 \t 13.897090 \t 6.707602 \t7.189488 \t 0.000000 \t 531.824275 s\n","953 \t 14.032753 \t 6.867756 \t7.164997 \t 0.000000 \t 532.252553 s\n","954 \t 14.045646 \t 6.627971 \t7.417675 \t 0.000000 \t 532.694112 s\n","955 \t 14.202059 \t 6.838389 \t7.363670 \t 0.000000 \t 533.125040 s\n","956 \t 13.498408 \t 6.623336 \t6.875073 \t 0.000000 \t 533.560640 s\n","957 \t 14.202939 \t 7.028348 \t7.174591 \t 0.000000 \t 534.023669 s\n","958 \t 13.475995 \t 6.520342 \t6.955653 \t 0.000000 \t 534.581284 s\n","959 \t 13.707460 \t 6.367206 \t7.340254 \t 0.000000 \t 535.039076 s\n","960 \t 13.830742 \t 6.523609 \t7.307134 \t 0.000000 \t 535.464630 s\n","961 \t 13.897094 \t 6.585210 \t7.311883 \t 0.000000 \t 535.907954 s\n","962 \t 13.851948 \t 6.638232 \t7.213717 \t 0.000000 \t 536.343826 s\n","963 \t 13.433943 \t 6.843371 \t6.590572 \t 0.000000 \t 536.778969 s\n","964 \t 13.473234 \t 6.528823 \t6.944411 \t 0.000000 \t 537.209485 s\n","965 \t 13.755777 \t 6.365779 \t7.389999 \t 0.000000 \t 537.635921 s\n","966 \t 13.527273 \t 6.452425 \t7.074848 \t 0.000000 \t 538.073689 s\n","967 \t 13.817211 \t 6.869810 \t6.947401 \t 0.000000 \t 538.635516 s\n","968 \t 13.898719 \t 6.735096 \t7.163623 \t 0.000000 \t 539.073756 s\n","969 \t 13.715185 \t 6.711637 \t7.003548 \t 0.000000 \t 539.501317 s\n","970 \t 13.506870 \t 6.486944 \t7.019926 \t 0.000000 \t 539.960601 s\n","971 \t 13.684410 \t 6.514234 \t7.170176 \t 0.000000 \t 540.404990 s\n","972 \t 13.912492 \t 6.605185 \t7.307307 \t 0.000000 \t 540.835738 s\n","973 \t 14.338375 \t 6.697383 \t7.640992 \t 0.000000 \t 541.284547 s\n","974 \t 13.590534 \t 6.513669 \t7.076865 \t 0.000000 \t 541.823530 s\n","975 \t 13.812300 \t 6.560086 \t7.252213 \t 0.000000 \t 542.358160 s\n","976 \t 13.992244 \t 6.685942 \t7.306302 \t 0.000000 \t 542.869463 s\n","977 \t 13.673297 \t 6.681549 \t6.991749 \t 0.000000 \t 543.600246 s\n","978 \t 13.930956 \t 6.657760 \t7.273197 \t 0.000000 \t 544.188140 s\n","979 \t 13.420059 \t 6.576103 \t6.843955 \t 0.000000 \t 544.684630 s\n","980 \t 13.680077 \t 6.457551 \t7.222526 \t 0.000000 \t 545.125478 s\n","981 \t 14.061459 \t 6.632201 \t7.429257 \t 0.000000 \t 545.559654 s\n","982 \t 13.360670 \t 6.529221 \t6.831449 \t 0.000000 \t 545.987388 s\n","983 \t 13.678844 \t 6.566561 \t7.112283 \t 0.000000 \t 546.423115 s\n","984 \t 13.831507 \t 6.593810 \t7.237698 \t 0.000000 \t 546.866680 s\n","985 \t 13.542267 \t 6.564459 \t6.977808 \t 0.000000 \t 547.305372 s\n","986 \t 13.571739 \t 6.624685 \t6.947054 \t 0.000000 \t 547.736511 s\n","987 \t 13.530755 \t 6.566580 \t6.964175 \t 0.000000 \t 548.352048 s\n","988 \t 14.016441 \t 6.806239 \t7.210202 \t 0.000000 \t 548.776500 s\n","989 \t 13.920454 \t 6.741903 \t7.178551 \t 0.000000 \t 549.205594 s\n","990 \t 13.544512 \t 6.717300 \t6.827212 \t 0.000000 \t 549.655806 s\n","991 \t 13.646585 \t 6.515482 \t7.131104 \t 0.000000 \t 550.085027 s\n","992 \t 13.978248 \t 6.759363 \t7.218884 \t 0.000000 \t 550.511800 s\n","993 \t 13.828034 \t 6.565787 \t7.262247 \t 0.000000 \t 550.936582 s\n","994 \t 13.898537 \t 6.570379 \t7.328157 \t 0.000000 \t 551.386609 s\n","995 \t 13.708891 \t 6.572349 \t7.136542 \t 0.000000 \t 551.813643 s\n","996 \t 13.411988 \t 6.491505 \t6.920483 \t 0.000000 \t 552.240413 s\n","997 \t 13.819790 \t 6.837031 \t6.982759 \t 0.000000 \t 552.809574 s\n","998 \t 13.764407 \t 6.597636 \t7.166772 \t 0.000000 \t 553.234592 s\n","999 \t 13.496352 \t 6.380646 \t7.115706 \t 0.000000 \t 553.668168 s\n","1000 \t 13.585096 \t 6.634247 \t6.950850 \t 0.000000 \t 554.104615 s\n","1001 \t 13.747355 \t 6.669156 \t7.078198 \t 0.000000 \t 554.588292 s\n","1002 \t 14.004508 \t 6.531106 \t7.473402 \t 0.000000 \t 555.123758 s\n","1003 \t 13.786712 \t 6.591798 \t7.194914 \t 0.000000 \t 555.637795 s\n","1004 \t 14.052041 \t 6.579587 \t7.472454 \t 0.000000 \t 556.164423 s\n","1005 \t 13.399485 \t 6.553166 \t6.846318 \t 0.000000 \t 556.695232 s\n","1006 \t 13.902051 \t 6.644143 \t7.257909 \t 0.000000 \t 557.270210 s\n","1007 \t 13.577651 \t 6.737622 \t6.840029 \t 0.000000 \t 557.916643 s\n","1008 \t 13.752843 \t 6.541229 \t7.211614 \t 0.000000 \t 558.347971 s\n","1009 \t 13.833452 \t 6.717675 \t7.115777 \t 0.000000 \t 558.796011 s\n","1010 \t 14.201196 \t 6.809004 \t7.392192 \t 0.000000 \t 559.224189 s\n","1011 \t 13.791389 \t 6.648523 \t7.142866 \t 0.000000 \t 559.654671 s\n","1012 \t 13.931990 \t 6.601692 \t7.330298 \t 0.000000 \t 560.100401 s\n","1013 \t 13.596313 \t 6.453282 \t7.143032 \t 0.000000 \t 560.523669 s\n","1014 \t 13.738433 \t 6.698510 \t7.039923 \t 0.000000 \t 560.959249 s\n","1015 \t 13.958793 \t 6.735882 \t7.222911 \t 0.000000 \t 561.396023 s\n","1016 \t 13.771723 \t 6.510416 \t7.261307 \t 0.000000 \t 561.834330 s\n","1017 \t 14.091100 \t 6.696486 \t7.394614 \t 0.000000 \t 562.399975 s\n","1018 \t 13.896007 \t 6.542965 \t7.353041 \t 0.000000 \t 562.839309 s\n","1019 \t 13.890970 \t 6.683579 \t7.207392 \t 0.000000 \t 563.266857 s\n","1020 \t 13.904332 \t 6.866075 \t7.038257 \t 0.000000 \t 563.693058 s\n","1021 \t 13.629853 \t 6.398885 \t7.230968 \t 0.000000 \t 564.141089 s\n","1022 \t 13.690165 \t 6.562738 \t7.127427 \t 0.000000 \t 564.570106 s\n","1023 \t 13.993568 \t 6.674264 \t7.319304 \t 0.000000 \t 565.006859 s\n","1024 \t 13.113116 \t 6.454438 \t6.658677 \t 0.000000 \t 565.431790 s\n","1025 \t 13.780305 \t 6.426307 \t7.353998 \t 0.000000 \t 565.871616 s\n","1026 \t 13.670236 \t 6.598281 \t7.071955 \t 0.000000 \t 566.305101 s\n","1027 \t 14.067942 \t 6.656754 \t7.411188 \t 0.000000 \t 566.878207 s\n","1028 \t 14.053252 \t 6.788296 \t7.264957 \t 0.000000 \t 567.330937 s\n","1029 \t 14.144168 \t 6.470353 \t7.673815 \t 0.000000 \t 567.845100 s\n","1030 \t 14.250834 \t 6.911929 \t7.338906 \t 0.000000 \t 568.382765 s\n","1031 \t 13.789009 \t 6.654399 \t7.134610 \t 0.000000 \t 568.889961 s\n","1032 \t 13.562695 \t 6.669230 \t6.893464 \t 0.000000 \t 569.443341 s\n","1033 \t 13.340064 \t 6.448807 \t6.891257 \t 0.000000 \t 570.029334 s\n","1034 \t 13.749046 \t 6.552306 \t7.196741 \t 0.000000 \t 570.612846 s\n","1035 \t 14.179878 \t 6.772258 \t7.407619 \t 0.000000 \t 571.050736 s\n","1036 \t 13.834233 \t 6.831399 \t7.002835 \t 0.000000 \t 571.502591 s\n","1037 \t 13.699042 \t 6.557631 \t7.141412 \t 0.000000 \t 571.934264 s\n","1038 \t 13.568393 \t 6.757061 \t6.811332 \t 0.000000 \t 572.535731 s\n","1039 \t 13.784246 \t 6.829488 \t6.954759 \t 0.000000 \t 572.973136 s\n","1040 \t 13.692519 \t 6.543500 \t7.149019 \t 0.000000 \t 573.420541 s\n","1041 \t 13.821771 \t 6.665297 \t7.156474 \t 0.000000 \t 573.848816 s\n","1042 \t 14.202334 \t 6.611775 \t7.590559 \t 0.000000 \t 574.289998 s\n","1043 \t 13.742023 \t 6.633828 \t7.108194 \t 0.000000 \t 574.719090 s\n","1044 \t 14.028539 \t 6.758995 \t7.269544 \t 0.000000 \t 575.150532 s\n","1045 \t 13.917294 \t 6.786837 \t7.130457 \t 0.000000 \t 575.587897 s\n","1046 \t 13.478209 \t 6.471085 \t7.007124 \t 0.000000 \t 576.019393 s\n","1047 \t 13.629717 \t 6.428484 \t7.201232 \t 0.000000 \t 576.455783 s\n","1048 \t 13.912830 \t 6.801579 \t7.111251 \t 0.000000 \t 577.023520 s\n","1049 \t 13.840583 \t 6.665548 \t7.175035 \t 0.000000 \t 577.457301 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:10<00:00, 12.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4113\n","Hit@10: 0.5462\n","Hit@3: 0.4231\n","Hit@1: 0.3462\n","Link Prediction on Validation Set (All)\n","MRR: 0.4113\n","Hit@10: 0.5462\n","Hit@3: 0.4231\n","Hit@1: 0.3462\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3621\n","Hit@10: 0.6154\n","Hit@3: 0.4385\n","Hit@1: 0.2462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.3621\n","Hit@10: 0.6154\n","Hit@3: 0.4385\n","Hit@1: 0.2462\n"]}]},{"cell_type":"code","source":["KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTKG-I/checkpoint/Reproduce/VTKG-I/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                else: # qualifier\n","                    sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChVIC_5BHELi","executionInfo":{"status":"ok","timestamp":1747362127882,"user_tz":-540,"elapsed":14168,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"21e7e343-adae-44b0-d297-65c0372b4bdc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-57f05cdbc00c>:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","<ipython-input-9-57f05cdbc00c>:208: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","<ipython-input-9-57f05cdbc00c>:258: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","<ipython-input-9-57f05cdbc00c>:259: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","<ipython-input-11-7c7791560f63>:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTKG-I/checkpoint/Reproduce/VTKG-I/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","100%|██████████| 130/130 [00:10<00:00, 12.52it/s]"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3807\n","Hit@10: 0.5538\n","Hit@3: 0.4231\n","Hit@1: 0.2885\n","Link Prediction on Validation Set (All)\n","MRR: 0.3807\n","Hit@10: 0.5538\n","Hit@3: 0.4231\n","Hit@1: 0.2885\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2783\n","Hit@10: 0.4846\n","Hit@3: 0.3154\n","Hit@1: 0.1846\n","Relation Prediction on Validation Set (All)\n","MRR: 0.2783\n","Hit@10: 0.4846\n","Hit@3: 0.3154\n","Hit@1: 0.1846\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}