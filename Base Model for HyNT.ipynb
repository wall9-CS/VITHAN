{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyN5XVFymfOoJt0mAazaOrv4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749984999723,"user_tz":-540,"elapsed":1274,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"19a8f996-8e8b-44d6-bcae-33c0ddcaec65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n","\n","import pandas as pd"],"metadata":{"id":"xWGfSBgsm1r2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive_dir = \"/content/drive/MyDrive/code/\"\n","dataset_dir = \"VTHNKG-NQ/\"\n","dataset_name = \"VTHNKG-NQ\"\n","exp_name = \"hynt\"\n","exp_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n","test_epoch = \"1050\""],"metadata":{"id":"uUIY_MEKNGsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2h6HjbDCO-tg","executionInfo":{"status":"ok","timestamp":1749985005502,"user_tz":-540,"elapsed":7,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"4b3d68a0-8c1f-4bfb-cc9f-27b02f47fdda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code\n"]}]},{"cell_type":"code","source":["# 파서 정의 (--'옵션 이름', default = 기본값, 자료형)\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--data', default = dataset_name + \"_\" + exp_name + \"_\" + exp_date, type = str) # 데이터셋 이름\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int) # embedding 차원\n","parser.add_argument('--num_epoch', default=1050, type=int)\n","parser.add_argument('--valid_epoch', default=150, type=int) # 150 epoch 마다 validation 수행\n","parser.add_argument('--exp', default=dataset_name) # 실험 이름\n","parser.add_argument('--no_write', action='store_true') # 결과 저장을 비활성화\n","parser.add_argument('--num_enc_layer', default=4, type=int)\n","parser.add_argument('--num_dec_layer', default=4, type=int)\n","parser.add_argument('--num_head', default=8, type=int)\n","parser.add_argument('--hidden_dim', default = 2048, type = int) # transformer 내부 FFN의 차원\n","parser.add_argument('--dropout', default = 0.15, type = float)\n","parser.add_argument('--smoothing', default = 0.4, type = float) # 라벨 스무딩 정도\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int) # 학습 스케줄링 주기\n","parser.add_argument('--emb_as_proj', action = 'store_true') # True면 numeric value 고려\n","parser.add_argument('--epoch', default=1050, type=int)\n","args, unknown = parser.parse_known_args()"],"metadata":{"id":"n3IoE1gDLXVi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import time\n","\n","class HyNT(nn.Module):\n","    def __init__(self, num_ent, num_rel, dim_model, num_head, dim_hid, num_enc_layer, num_dec_layer, dropout = 0.1, emb_as_proj = False):\n","        super(HyNT, self).__init__()\n","        # dimension, multihead attention head 개수, hidden layer의 dimension, encoder layer 개수, decoder layer 개수, dropout rate 정의\n","        self.dim_model = dim_model\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_enc_layer = num_enc_layer\n","        self.num_dec_layer = num_dec_layer\n","        self.dropout = dropout\n","\n","        # positional encoding 정의\n","        self.pri_pos = nn.Parameter(torch.Tensor(1, 1, dim_model))\n","        self.qv_pos = nn.Parameter(torch.Tensor(1, 1, dim_model))\n","        self.h_pos = nn.Parameter(torch.Tensor(1, 1, dim_model))\n","        self.r_pos = nn.Parameter(torch.Tensor(1, 1, dim_model))\n","        self.t_pos = nn.Parameter(torch.Tensor(1, 1, dim_model))\n","        self.q_pos = nn.Parameter(torch.Tensor(1, 1, dim_model))\n","        self.v_pos = nn.Parameter(torch.Tensor(1, 1, dim_model))\n","\n","        # entity, relation embedding\n","        self.ent_embeddings = nn.Embedding(num_ent+1+num_rel, dim_model) # num_ent+1+num_rel의 의미?\n","        self.rel_embeddings = nn.Embedding(num_rel+1, dim_model) # num_rel+1의 의미?\n","        self.pri_enc = nn.Linear(dim_model*3, dim_model) # triplet encoding layer (head, relation, tail)\n","        self.qv_enc = nn.Linear(dim_model*2, dim_model) # qualifier encoding layer (relation, entity)\n","\n","        self.ent_dec = nn.Linear(dim_model, num_ent) # decoder에서 (discrete) entity prediction 결과\n","        self.rel_dec = nn.Linear(dim_model, num_rel) # relation prediction 결과\n","        self.num_dec = nn.Linear(dim_model, num_rel) # numeric literal prediction 결과\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5)) # numeric literal masking parameter\n","\n","        # Transformer encoder, decoder 정의\n","        encoder_layer = nn.TransformerEncoderLayer(dim_model, num_head, dim_hid, dropout, batch_first = True)\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_enc_layer)\n","        decoder_layer = nn.TransformerEncoderLayer(dim_model, num_head, dim_hid, dropout, batch_first = True)\n","        self.decoder = nn.TransformerEncoder(decoder_layer, num_dec_layer)\n","\n","        self.emb_as_proj = emb_as_proj # embedding projection\n","        self.num_ent = num_ent # entity 개수\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        # parameter value initialization\n","        nn.init.xavier_uniform_(self.pri_pos)\n","        nn.init.xavier_uniform_(self.qv_pos)\n","        nn.init.xavier_uniform_(self.h_pos)\n","        nn.init.xavier_uniform_(self.r_pos)\n","        nn.init.xavier_uniform_(self.t_pos)\n","        nn.init.xavier_uniform_(self.q_pos)\n","        nn.init.xavier_uniform_(self.v_pos)\n","        nn.init.xavier_uniform_(self.ent_embeddings.weight)\n","        nn.init.xavier_uniform_(self.rel_embeddings.weight)\n","        nn.init.xavier_uniform_(self.pri_enc.weight)\n","        nn.init.xavier_uniform_(self.qv_enc.weight)\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        # set bias to zero\n","        self.pri_enc.bias.data.zero_()\n","        self.qv_enc.bias.data.zero_()\n","        self.ent_dec.bias.data.zero_()\n","        self.rel_dec.bias.data.zero_()\n","        self.num_dec.bias.data.zero_()\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask) # num_values가 있으면 num_values, 없으면 num_mask를 사용\n","        # torch.where: https://deepdata.tistory.com/1167\n","\n","        # h, r, t, q, v sequence embedding의 차원 조정\n","        # 구체적으로 어떻게 하는 건지?\n","        h_seq = self.ent_embeddings(src[...,0]).view(batch_size, 1, self.dim_model)\n","        r_seq = self.rel_embeddings(src[...,1]).view(batch_size, 1, self.dim_model)\n","        t_seq = (self.ent_embeddings(src[...,2])*num_val[...,0:1]).view(batch_size, 1, self.dim_model)\n","        q_seq = self.rel_embeddings(src[...,3::2].flatten()).view(batch_size, -1, self.dim_model)\n","        v_seq = (self.ent_embeddings(src[...,4::2].flatten())*num_val[...,1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_model)\n","\n","        # triplet, qualifier encoding\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim = -1)) + self.pri_pos\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim= -1)) + self.qv_pos\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim = 1) # encoder의 입력\n","        enc_out_seq = self.encoder(enc_in_seq, src_key_padding_mask = src_key_padding_mask) # encoder 실행 결과\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_model)\n","        triplet = torch.stack([h_seq + self.h_pos, r_seq + self.r_pos, t_seq + self.t_pos], dim = 2) # triplet tensor 생성\n","        qv = torch.stack([q_seq + self.q_pos, v_seq + self.v_pos, torch.zeros_like(v_seq)], dim = 2) # qualifier tensor 생성\n","        dec_in_part = torch.cat([triplet,qv], dim = 1)[mask_locs] # mask_locs = 1인 부분만 decoder의 입력으로 사용\n","        # stack, cat 차이 공부\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim = 1) # decoder 입력\n","        dec_in_mask = torch.full((batch_size,4),False).cuda() # decoder mask\n","        dec_in_mask[torch.nonzero(mask_locs==1)[:,1]!=0,3] = True\n","        dec_out_seq = self.decoder(dec_in_seq, src_key_padding_mask = dec_in_mask) # decoder 결과\n","\n","        # prediction (entity, relation, numeric literal)\n","        if self.emb_as_proj:\n","            ent_out = torch.matmul(dec_out_seq, self.ent_embeddings.weight[:self.num_ent].T) + self.ent_dec.bias\n","        else:\n","            ent_out = self.ent_dec(dec_out_seq)\n","\n","        return ent_out, self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)\n"],"metadata":{"id":"2CgXgeAXmg-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTMmNF8Cl5it"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","\n","class HNKG(Dataset):\n","    def __init__(self, data, test = False):\n","        self.data = data\n","        self.dir = drive_dir + dataset_dir\n","\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","\n","        self.num_test = len(self.test)\n","\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n","\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","import argparse\n","import torch\n","import torch.nn as nn\n","import datetime\n","import time\n","import os\n","import copy\n","import math\n","import random\n","\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True # cudnn: nvidia GPU에서 CNN연산을 최적화하는 라이브러리, 학습 속도 최적화\n","torch.set_num_threads(8) # CPU thread 개수를 8개로 설정\n","torch.cuda.empty_cache() # 메모리 정리, 메모리 효율성 확보\n","\n","# 재현성을 위한 코드 블럭\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","\n","# 데이터셋 로드\n","KG = HNKG(args.data, test = False)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size, shuffle=True)\n","\n","# 모델 초기화, GPU 연동\n","model = HyNT(\n","\tnum_ent = KG.num_ent,\n","\tnum_rel = KG.num_rel,\n","    dim_model = args.dim,\n","    num_head = args.num_head,\n","    dim_hid = args.hidden_dim,\n","    num_enc_layer = args.num_enc_layer,\n","    num_dec_layer = args.num_dec_layer,\n","    dropout = args.dropout,\n","    emb_as_proj = args.emb_as_proj\n",").cuda()\n","\n","# 손실함수 정의: CELoss와 MSELoss\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) # 옵티마이저\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2) # 스케줄러\n","\n","# 결과 파일 이름\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\" + \\\n","              f\"elayer_{args.num_enc_layer}_dlayer_{args.num_dec_layer}_head_{args.num_head}_hid_{args.hidden_dim}_\" + \\\n","              f\"drop_{args.dropout}_smoothing_{args.smoothing}_batch_{args.batch_size}_\" + \\\n","              f\"steplr_{args.step_size}\"\n","\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\n","# 결과 저장 디렉토리 생성\n","if not args.no_write:\n","    os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","    os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","    with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","        f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","# 학습 시작\n","start = time.time()\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","    total_loss = 0.0 # 전체 loss (전체 epoch 실행 후 최종 loss 저장)\n","    total_ent_loss = 0.0 # entity loss\n","    total_rel_loss = 0.0 # relation loss\n","    total_num_loss = 0.0 # numeric loss\n","    for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","        batch_len = max(batch_real_len)\n","        batch = batch[:,:batch_len]\n","        batch_pad = batch_pad[:,:batch_len//2]\n","        batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","        batch_num = batch_num[:,:batch_len//2] # num value\n","\n","        # 모델 예측 수행\n","        ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","\n","        # masking 거르기? mask!=0인 값을 가지는 위치를 필터링(True로 변환)\n","        real_ent_mask = (ent_mask.cuda()!=0).squeeze() # squeeze()는 불필요한 차원 제거\n","        real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","        real_num_mask = (num_mask.cuda()!=0).squeeze()\n","        answer = answers.cuda() # 실제 정답값\n","        mask_idx = mask_idx.cuda() # 모델이 예측해야할 위치(마스킹된 위치) 인덱스\n","\n","        loss = 0\n","        if torch.any(ent_mask): # entity(discrete) loss, ent_mask != 0인 부분이 존재하면 entity loss 계산\n","            real_ent_mask = real_ent_mask.cuda()\n","            ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long()) # cross entropy loss 사용\n","            # 모델이 예측한 점수: ent_score[...][...], 정답: answer[real_ent_mask].long()\n","            loss += ent_loss\n","            total_ent_loss += ent_loss.item()\n","\n","        if torch.any(rel_mask): # relation loss, 위와 유사\n","            real_rel_mask = real_rel_mask.cuda()\n","            rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long()) # cross entropy\n","            loss += rel_loss\n","            total_rel_loss += rel_loss.item()\n","\n","        if torch.any(num_mask): # numeric value loss, 위와 유사\n","            real_num_mask = real_num_mask.cuda()\n","            num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask]) # MSE\n","            loss += num_loss\n","            total_num_loss += num_loss.item()\n","\n","        ## loss = entity loss + relation loss + numeric value loss\n","\n","        # 최적화\n","        optimizer.zero_grad() # gradient 초기화\n","        loss.backward() # backprop\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # gradient clipping (gradient explosion 현상 해결)\n","        optimizer.step() # 최적화 수행\n","        total_loss += loss.item() # 최종 학습 손실\n","\n","    # 스케줄러 업데이트\n","    scheduler.step()\n","\n","    print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","          f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","\t### VALIDATION ### 전체적으로 작동 원리를 모르겠다 ...... 살려주세요 ...\n","    if (epoch + 1) % args.valid_epoch == 0: # valid_epoch 주기마다 validation 수행\n","        model.eval() # evaluation mode로 변경 -> 모델 업데이트 X\n","\n","        lp_tri_list_rank = [] # triplet 엔티티 링크 예측 결과\n","        lp_all_list_rank = [] # all(hyper-relational facts) 엔티티 링크 예측 결과\n","\n","        rp_tri_list_rank = [] # triplet relation 링크 예측 결과\n","        rp_all_list_rank = [] # all(hyper-relational facts) relation 링크 예측 결과\n","\n","        nvp_tri_se = 0 # triplet의 numeric value prediction의 squared error\n","        nvp_tri_se_num = 0 # squared error의 데이터(샘플) 수, 초기화\n","\n","        nvp_all_se = 0 # hyper-relational fact의 numeric value prediction의 squared error\n","        nvp_all_se_num = 0 # squared error(all)의 데이터 수\n","\n","        with torch.no_grad(): # 그래디언트 X(업데이트 X)\n","            for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)): # test 데이터에 대해 수행\n","\n","                tri_len = len(tri) # 현재 triplet length\n","                pad_idx = 0 # 패딩 위치 인덱스 초기화\n","                for ent_idx in range((tri_len+1)//2): # 엔티티 index에 대해 반복문 / 왜 tri_len+1//2 인가?\n","                    if tri_pad[pad_idx]:\n","                        break # 패딩된 위치라면 중단\n","                    if ent_idx != 0:\n","                        pad_idx += 1 # 첫번째 인덱스가 아닌 경우 패딩 인덱스 업데이트\n","                    test_triplet = torch.tensor([tri]) # 현재 triplet을 tensor로 변환\n","\n","                    mask_locs = torch.full((1,(KG.max_len-3)//2+1), False) # masking location tensor 생성\n","                    if ent_idx < 2:\n","                        mask_locs[0,0] = True\n","                    else:\n","                        mask_locs[0,ent_idx-1] = True\n","\n","                    if tri[ent_idx*2] >= KG.num_ent: # numeric prediction이 필요한 경우\n","                        assert ent_idx != 0\n","                        test_num = torch.tensor([tri_num])\n","                        test_num[0,ent_idx-1] = -1 # 숫자 값 예측할 위치 마스킹\n","\n","                        # 모델에 입력하여 숫자 값 예측\n","                        _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                        score_num = score_num.detach().cpu().numpy() # score_num을 cpu로 이동\n","\n","                        # squared error 계산\n","                        if ent_idx == 1: # triplet se계산\n","                            sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                            nvp_tri_se += sq_error\n","                            nvp_tri_se_num += 1\n","                        else: # qualifier se계산\n","                            sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_all_se += sq_error\n","                        nvp_all_se_num += 1\n","\n","                    else: # numeric prediction 필요 없는 경우 -> entity prediction\n","                        test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel\n","                        filt_tri = copy.deepcopy(tri)\n","                        filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                        # re_pair: relation pair..?\n","                        if ent_idx != 1 and filt_tri[2] >= KG.num_ent: # qualifier 붙어있고 numeric value prediction인 상황\n","                            re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])]\n","                        else:\n","                            re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                        for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                            if tri_pad[qual_idx+1]:\n","                                break\n","                            if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                                re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                            else:\n","                                re_pair.append((q,v))\n","                        re_pair.sort()\n","                        filt = KG.filter_dict[tuple(re_pair)] # 필터링 데이터 설정..?\n","                        # entity prediction 수행\n","                        score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                        score_ent = score_ent.detach().cpu().numpy()\n","\n","                        # entity prediction의 rank 계산\n","                        if ent_idx < 2:\n","                            rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                            lp_tri_list_rank.append(rank)\n","                        else:\n","                            rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                        lp_all_list_rank.append(rank)\n","\n","                for rel_idx in range(tri_len//2): # relation prediction\n","                    if tri_pad[rel_idx]:\n","                        break # 패딩된 경우 중단\n","\n","                    # masking location tensor 생성\n","                    mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                    mask_locs[0,rel_idx] = True\n","\n","                    test_triplet = torch.tensor([tri]) # 현재 triplet -> tensor\n","                    orig_rels = tri[1::2] # original relation triplet..?의 relation 추출\n","                    test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                    if test_triplet[0, rel_idx*2+2] >= KG.num_ent:\n","                        test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                    if filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if v >= KG.num_ent:\n","                            re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","\n","                    # 모델로 relation prediction 수행\n","                    _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_rel = score_rel.detach().cpu().numpy()\n","\n","                    # relation prediction의 rank 계산\n","                    if rel_idx == 0:\n","                        rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                        rp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                    rp_all_list_rank.append(rank)\n","\n","        lp_tri_list_rank = np.array(lp_tri_list_rank)\n","        lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","        print(\"Link Prediction on Validation Set (Tri)\")\n","        print(f\"MRR: {lp_tri_mrr:.4f}\")\n","        print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","        print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","        print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","        lp_all_list_rank = np.array(lp_all_list_rank)\n","        lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","        print(\"Link Prediction on Validation Set (All)\")\n","        print(f\"MRR: {lp_all_mrr:.4f}\")\n","        print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","        print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","        print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","        rp_tri_list_rank = np.array(rp_tri_list_rank)\n","        rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","        print(\"Relation Prediction on Validation Set (Tri)\")\n","        print(f\"MRR: {rp_tri_mrr:.4f}\")\n","        print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","        print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","        print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","        rp_all_list_rank = np.array(rp_all_list_rank)\n","        rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","        print(\"Relation Prediction on Validation Set (All)\")\n","        print(f\"MRR: {rp_all_mrr:.4f}\")\n","        print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","        print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","        print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","        if nvp_tri_se_num > 0:\n","            nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","            print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","            print(f\"RMSE: {nvp_tri_rmse:.4f}\") # numeric value는 RMSE 출력\n","\n","        if nvp_all_se_num > 0:\n","            nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","            print(\"Numeric Value Prediction on Validation Set (All)\")\n","            print(f\"RMSE: {nvp_all_rmse:.4f}\") # numeric value는 RMSE 출력\n","\n","        # 결과를 저장하고 체크포인트 생성\n","        if not args.no_write:\n","            with open(f\"./result/{file_format}.txt\", 'a') as f:\n","                f.write(f\"Epoch: {epoch+1}\\n\")\n","                f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","                f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","                f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","                f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","                if nvp_tri_se_num > 0:\n","                    f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","                if nvp_all_se_num > 0:\n","                    f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","            # 모델 상태와 옵티마이저 상태 저장\n","            torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                        f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","        # 모델을 학습 모들 전환\n","        model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bfa5bd91-7f96-471e-93bf-ffc0ab86bfa8","collapsed":true,"executionInfo":{"status":"ok","timestamp":1749985461683,"user_tz":-540,"elapsed":456013,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 98.054724 \t 11.422297 \t11.181762 \t 75.450668 \t 1.218189 s\n","1 \t 36.389626 \t 10.484588 \t10.020288 \t 15.884751 \t 1.639241 s\n","2 \t 23.204644 \t 10.632239 \t10.595163 \t 1.977242 \t 1.971555 s\n","3 \t 20.892530 \t 10.230041 \t9.987883 \t 0.674607 \t 2.314446 s\n","4 \t 19.787910 \t 9.588834 \t9.615107 \t 0.583969 \t 2.792181 s\n","5 \t 17.897653 \t 8.677933 \t9.105692 \t 0.114027 \t 3.121045 s\n","6 \t 19.486317 \t 9.460188 \t9.418166 \t 0.607963 \t 3.467452 s\n","7 \t 19.877160 \t 9.797842 \t8.748521 \t 1.330796 \t 3.831574 s\n","8 \t 19.557459 \t 8.952703 \t9.780379 \t 0.824377 \t 4.273701 s\n","9 \t 18.588915 \t 9.672749 \t8.529939 \t 0.386227 \t 4.706764 s\n","10 \t 18.056291 \t 9.099777 \t8.792569 \t 0.163945 \t 5.128744 s\n","11 \t 19.556061 \t 10.214568 \t8.902164 \t 0.439328 \t 5.561720 s\n","12 \t 19.286710 \t 9.892500 \t9.206644 \t 0.187568 \t 5.992773 s\n","13 \t 17.803753 \t 9.391156 \t8.203830 \t 0.208767 \t 6.468241 s\n","14 \t 18.150623 \t 9.272659 \t8.382494 \t 0.495470 \t 6.917452 s\n","15 \t 17.680601 \t 8.905916 \t8.532754 \t 0.241930 \t 7.400059 s\n","16 \t 18.583647 \t 9.405317 \t8.805717 \t 0.372612 \t 7.746498 s\n","17 \t 18.745697 \t 9.461401 \t9.073905 \t 0.210392 \t 8.085211 s\n","18 \t 17.420444 \t 8.786105 \t8.294563 \t 0.339775 \t 8.419530 s\n","19 \t 18.259097 \t 9.077705 \t8.938776 \t 0.242616 \t 8.778545 s\n","20 \t 17.500074 \t 8.720775 \t8.567425 \t 0.211875 \t 9.110932 s\n","21 \t 17.764298 \t 9.055794 \t8.554410 \t 0.154095 \t 9.449358 s\n","22 \t 18.071285 \t 9.360800 \t8.549796 \t 0.160689 \t 9.801728 s\n","23 \t 18.325794 \t 9.287554 \t8.387916 \t 0.650325 \t 10.136761 s\n","24 \t 18.556056 \t 9.350397 \t8.822838 \t 0.382821 \t 10.475379 s\n","25 \t 18.204316 \t 8.712775 \t9.072392 \t 0.419149 \t 10.830353 s\n","26 \t 17.515213 \t 8.837594 \t8.374529 \t 0.303090 \t 11.163512 s\n","27 \t 17.760528 \t 9.141561 \t8.151234 \t 0.467732 \t 11.645217 s\n","28 \t 17.204629 \t 9.081758 \t7.996306 \t 0.126566 \t 11.977736 s\n","29 \t 17.696068 \t 8.812522 \t8.509961 \t 0.373585 \t 12.311227 s\n","30 \t 17.102462 \t 8.332204 \t8.662652 \t 0.107606 \t 12.647944 s\n","31 \t 18.827148 \t 9.079370 \t8.543709 \t 1.204069 \t 12.979381 s\n","32 \t 18.494761 \t 9.427140 \t8.589180 \t 0.478442 \t 13.314873 s\n","33 \t 17.776670 \t 9.454799 \t8.025321 \t 0.296552 \t 13.659973 s\n","34 \t 17.761374 \t 9.030671 \t8.493956 \t 0.236749 \t 13.991907 s\n","35 \t 18.002689 \t 9.243912 \t8.225415 \t 0.533363 \t 14.330242 s\n","36 \t 18.065757 \t 9.084311 \t8.825122 \t 0.156324 \t 14.681821 s\n","37 \t 18.515424 \t 9.242519 \t9.127873 \t 0.145031 \t 15.015790 s\n","38 \t 16.979026 \t 8.479576 \t8.271789 \t 0.227662 \t 15.493287 s\n","39 \t 17.277503 \t 8.548982 \t8.542396 \t 0.186126 \t 15.832091 s\n","40 \t 17.458493 \t 8.619670 \t8.523409 \t 0.315415 \t 16.203050 s\n","41 \t 18.135828 \t 9.205216 \t8.780033 \t 0.150580 \t 16.653695 s\n","42 \t 17.502128 \t 9.179702 \t8.139755 \t 0.182670 \t 17.052483 s\n","43 \t 17.175054 \t 8.488966 \t8.567228 \t 0.118859 \t 17.469414 s\n","44 \t 16.878820 \t 8.695993 \t8.047009 \t 0.135817 \t 17.891386 s\n","45 \t 17.809459 \t 9.182007 \t8.385599 \t 0.241853 \t 18.332360 s\n","46 \t 17.903803 \t 9.468349 \t8.253814 \t 0.181639 \t 18.758616 s\n","47 \t 17.885054 \t 9.267419 \t8.340695 \t 0.276939 \t 19.747412 s\n","48 \t 17.513271 \t 9.141656 \t8.290665 \t 0.080949 \t 20.970714 s\n","49 \t 17.469202 \t 8.614138 \t8.523079 \t 0.331986 \t 21.303392 s\n","50 \t 17.700855 \t 8.804809 \t8.798569 \t 0.097478 \t 21.640844 s\n","51 \t 17.031102 \t 8.313984 \t8.597321 \t 0.119797 \t 21.988415 s\n","52 \t 17.849607 \t 8.905948 \t8.854454 \t 0.089206 \t 22.331262 s\n","53 \t 17.318357 \t 8.500808 \t8.711441 \t 0.106108 \t 22.674832 s\n","54 \t 17.559481 \t 8.919515 \t8.417157 \t 0.222808 \t 23.018853 s\n","55 \t 17.255980 \t 8.817852 \t8.309542 \t 0.128586 \t 23.351117 s\n","56 \t 16.716762 \t 8.681406 \t7.950774 \t 0.084582 \t 23.688157 s\n","57 \t 18.321695 \t 8.810562 \t9.411020 \t 0.100113 \t 24.024317 s\n","58 \t 15.450603 \t 7.793605 \t7.543736 \t 0.113262 \t 24.511305 s\n","59 \t 17.999037 \t 8.890234 \t8.984486 \t 0.124317 \t 24.842820 s\n","60 \t 17.016683 \t 8.588885 \t8.350058 \t 0.077740 \t 25.193362 s\n","61 \t 17.158010 \t 9.277712 \t7.813028 \t 0.067268 \t 25.530335 s\n","62 \t 16.717318 \t 8.581665 \t8.060064 \t 0.075589 \t 25.863962 s\n","63 \t 17.571024 \t 8.698473 \t8.625198 \t 0.247352 \t 26.202725 s\n","64 \t 16.880649 \t 8.887001 \t7.944745 \t 0.048904 \t 26.537385 s\n","65 \t 17.603395 \t 9.028506 \t8.500130 \t 0.074759 \t 26.869407 s\n","66 \t 17.771608 \t 9.144225 \t8.486697 \t 0.140687 \t 27.215037 s\n","67 \t 17.332520 \t 8.868169 \t8.380011 \t 0.084340 \t 27.566821 s\n","68 \t 16.843158 \t 8.307300 \t8.448996 \t 0.086862 \t 28.046149 s\n","69 \t 17.706313 \t 8.697271 \t8.852906 \t 0.156136 \t 28.397348 s\n","70 \t 17.456395 \t 8.501805 \t8.842770 \t 0.111820 \t 28.735141 s\n","71 \t 18.112113 \t 8.928278 \t9.060755 \t 0.123080 \t 29.074757 s\n","72 \t 16.946796 \t 8.908218 \t7.903361 \t 0.135218 \t 29.422027 s\n","73 \t 17.022778 \t 8.547731 \t8.344405 \t 0.130641 \t 29.757877 s\n","74 \t 17.203229 \t 8.357069 \t8.605511 \t 0.240650 \t 30.091693 s\n","75 \t 17.293900 \t 8.751908 \t8.305812 \t 0.236179 \t 30.470434 s\n","76 \t 16.645053 \t 8.779731 \t7.737339 \t 0.127982 \t 30.896159 s\n","77 \t 17.218629 \t 8.854446 \t8.274254 \t 0.089928 \t 31.323031 s\n","78 \t 18.099534 \t 9.116128 \t8.815094 \t 0.168311 \t 31.935248 s\n","79 \t 16.485092 \t 8.270945 \t7.560051 \t 0.654097 \t 32.356070 s\n","80 \t 16.875413 \t 8.746180 \t8.043195 \t 0.086038 \t 32.801381 s\n","81 \t 17.735563 \t 8.388572 \t9.278974 \t 0.068016 \t 33.258710 s\n","82 \t 16.591335 \t 8.797542 \t7.726426 \t 0.067368 \t 33.673810 s\n","83 \t 17.352872 \t 8.878555 \t8.387374 \t 0.086943 \t 34.009216 s\n","84 \t 17.273338 \t 8.977963 \t8.216752 \t 0.078623 \t 34.345257 s\n","85 \t 16.631063 \t 8.366847 \t8.177412 \t 0.086804 \t 34.700353 s\n","86 \t 17.723879 \t 9.180834 \t8.220788 \t 0.322257 \t 35.040320 s\n","87 \t 16.597713 \t 8.979254 \t7.376211 \t 0.242249 \t 35.369881 s\n","88 \t 17.554809 \t 8.743622 \t8.695479 \t 0.115707 \t 35.719183 s\n","89 \t 16.682341 \t 8.729678 \t7.856525 \t 0.096138 \t 36.195915 s\n","90 \t 17.377866 \t 9.113143 \t7.850636 \t 0.414087 \t 36.545846 s\n","91 \t 17.041643 \t 8.092756 \t8.851868 \t 0.097019 \t 36.879329 s\n","92 \t 17.030670 \t 8.875151 \t8.019475 \t 0.136045 \t 37.215705 s\n","93 \t 16.289122 \t 8.263044 \t7.908222 \t 0.117854 \t 37.561125 s\n","94 \t 16.851332 \t 8.686694 \t8.064060 \t 0.100579 \t 37.898248 s\n","95 \t 17.946441 \t 9.042093 \t8.754758 \t 0.149590 \t 38.233310 s\n","96 \t 16.390710 \t 8.593174 \t7.654901 \t 0.142636 \t 38.567399 s\n","97 \t 16.852707 \t 9.159397 \t7.593402 \t 0.099907 \t 38.925252 s\n","98 \t 16.615282 \t 8.428136 \t8.047998 \t 0.139148 \t 39.259671 s\n","99 \t 17.589022 \t 8.862715 \t8.548101 \t 0.178206 \t 39.746264 s\n","100 \t 16.630513 \t 8.184456 \t8.217453 \t 0.228603 \t 40.082038 s\n","101 \t 16.707843 \t 8.513422 \t8.028246 \t 0.166175 \t 40.421862 s\n","102 \t 17.113329 \t 8.974147 \t8.010072 \t 0.129112 \t 40.760884 s\n","103 \t 17.202974 \t 8.258289 \t8.880025 \t 0.064662 \t 41.097714 s\n","104 \t 16.891431 \t 8.848202 \t7.960966 \t 0.082263 \t 41.432722 s\n","105 \t 16.214986 \t 8.290438 \t7.877927 \t 0.046622 \t 41.774840 s\n","106 \t 17.129788 \t 8.918230 \t8.155343 \t 0.056215 \t 42.113876 s\n","107 \t 16.463285 \t 8.057713 \t8.319633 \t 0.085939 \t 42.449436 s\n","108 \t 16.598581 \t 8.906929 \t7.629874 \t 0.061778 \t 42.790946 s\n","109 \t 17.140293 \t 9.183843 \t7.865718 \t 0.090733 \t 43.134529 s\n","110 \t 16.573318 \t 8.349052 \t8.119824 \t 0.104442 \t 43.660356 s\n","111 \t 17.378951 \t 8.985879 \t8.350753 \t 0.042318 \t 44.110028 s\n","112 \t 16.757196 \t 8.956929 \t7.713403 \t 0.086866 \t 44.525689 s\n","113 \t 17.288857 \t 8.906039 \t8.272099 \t 0.110718 \t 44.970410 s\n","114 \t 16.769451 \t 9.078457 \t7.618108 \t 0.072887 \t 45.481646 s\n","115 \t 16.926406 \t 8.699149 \t8.166683 \t 0.060573 \t 45.974885 s\n","116 \t 17.015660 \t 8.839587 \t8.076671 \t 0.099403 \t 46.431047 s\n","117 \t 16.781424 \t 8.698055 \t8.013866 \t 0.069502 \t 46.873521 s\n","118 \t 16.659330 \t 8.497540 \t8.095513 \t 0.066277 \t 47.227814 s\n","119 \t 17.419600 \t 8.939792 \t8.380166 \t 0.099642 \t 47.563354 s\n","120 \t 16.467246 \t 8.383364 \t8.016589 \t 0.067293 \t 48.048383 s\n","121 \t 16.243826 \t 8.225641 \t7.959904 \t 0.058281 \t 48.383406 s\n","122 \t 15.651425 \t 8.057279 \t7.533179 \t 0.060968 \t 48.719977 s\n","123 \t 16.221306 \t 8.094647 \t8.029221 \t 0.097438 \t 49.070569 s\n","124 \t 16.076734 \t 8.629674 \t7.408099 \t 0.038960 \t 49.409353 s\n","125 \t 16.741074 \t 8.690058 \t7.953854 \t 0.097162 \t 49.744521 s\n","126 \t 17.276003 \t 8.975207 \t8.120917 \t 0.179879 \t 50.092608 s\n","127 \t 15.582973 \t 7.959182 \t7.424489 \t 0.199302 \t 50.431255 s\n","128 \t 17.086493 \t 8.808379 \t8.221247 \t 0.056868 \t 50.768323 s\n","129 \t 16.253241 \t 8.401287 \t7.801408 \t 0.050546 \t 51.121034 s\n","130 \t 16.338648 \t 8.529367 \t7.735111 \t 0.074171 \t 51.602883 s\n","131 \t 15.975116 \t 8.178396 \t7.742597 \t 0.054123 \t 51.946436 s\n","132 \t 15.706474 \t 8.424015 \t7.220423 \t 0.062036 \t 52.286602 s\n","133 \t 17.226446 \t 9.341051 \t7.853183 \t 0.032211 \t 52.622576 s\n","134 \t 16.432767 \t 8.551485 \t7.815106 \t 0.066176 \t 52.954375 s\n","135 \t 17.285479 \t 9.000166 \t8.246965 \t 0.038347 \t 53.290297 s\n","136 \t 17.505520 \t 9.292290 \t8.172256 \t 0.040973 \t 53.628658 s\n","137 \t 16.723415 \t 8.380941 \t8.303626 \t 0.038849 \t 53.965162 s\n","138 \t 16.541975 \t 8.545532 \t7.941971 \t 0.054472 \t 54.302449 s\n","139 \t 16.389161 \t 8.631308 \t7.698747 \t 0.059107 \t 54.637472 s\n","140 \t 16.991356 \t 8.850281 \t8.108690 \t 0.032385 \t 55.104204 s\n","141 \t 16.620095 \t 8.071096 \t8.508579 \t 0.040420 \t 55.450771 s\n","142 \t 17.164731 \t 9.090909 \t8.016459 \t 0.057364 \t 55.787014 s\n","143 \t 17.025270 \t 8.952758 \t7.983079 \t 0.089433 \t 56.126009 s\n","144 \t 16.202202 \t 8.757539 \t7.338743 \t 0.105920 \t 56.478469 s\n","145 \t 16.015711 \t 8.379046 \t7.594667 \t 0.041997 \t 56.850225 s\n","146 \t 16.418539 \t 8.502497 \t7.853056 \t 0.062984 \t 57.260959 s\n","147 \t 16.983100 \t 8.508029 \t8.365955 \t 0.109116 \t 58.122539 s\n","148 \t 16.413913 \t 8.516375 \t7.851756 \t 0.045781 \t 58.896661 s\n","149 \t 16.754547 \t 9.106131 \t7.581450 \t 0.066967 \t 59.553785 s\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/130 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 130/130 [00:06<00:00, 19.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4436\n","Hit@10: 0.5500\n","Hit@3: 0.4423\n","Hit@1: 0.3923\n","Link Prediction on Validation Set (All)\n","MRR: 0.4436\n","Hit@10: 0.5500\n","Hit@3: 0.4423\n","Hit@1: 0.3923\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3341\n","Hit@10: 0.5615\n","Hit@3: 0.3538\n","Hit@1: 0.2385\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6060\n","Hit@10: 0.7654\n","Hit@3: 0.6543\n","Hit@1: 0.5185\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1817\n","150 \t 16.672423 \t 8.241529 \t8.355275 \t 0.075620 \t 67.219217 s\n","151 \t 17.403188 \t 8.446293 \t8.716946 \t 0.239948 \t 67.559978 s\n","152 \t 17.838463 \t 8.907747 \t8.298499 \t 0.632217 \t 67.903912 s\n","153 \t 17.521722 \t 9.347879 \t8.068068 \t 0.105775 \t 68.239023 s\n","154 \t 17.125260 \t 9.083606 \t7.931289 \t 0.110365 \t 68.579194 s\n","155 \t 17.569936 \t 9.270632 \t8.174565 \t 0.124739 \t 68.920987 s\n","156 \t 17.293886 \t 8.306046 \t8.774125 \t 0.213716 \t 69.258306 s\n","157 \t 17.526419 \t 8.844810 \t8.502436 \t 0.179172 \t 69.608782 s\n","158 \t 17.414177 \t 9.006206 \t8.191541 \t 0.216431 \t 69.947054 s\n","159 \t 17.372478 \t 9.313010 \t7.845023 \t 0.214444 \t 70.311659 s\n","160 \t 16.187442 \t 8.471299 \t7.622208 \t 0.093935 \t 70.746950 s\n","161 \t 17.091510 \t 8.362970 \t8.617881 \t 0.110659 \t 71.198374 s\n","162 \t 17.137873 \t 8.814170 \t8.207759 \t 0.115944 \t 71.803619 s\n","163 \t 16.642659 \t 8.105237 \t8.434736 \t 0.102686 \t 72.236824 s\n","164 \t 16.484028 \t 8.542424 \t7.708724 \t 0.232880 \t 72.671776 s\n","165 \t 16.425835 \t 8.622576 \t7.731397 \t 0.071862 \t 73.228272 s\n","166 \t 17.243269 \t 8.864320 \t8.201885 \t 0.177063 \t 73.718156 s\n","167 \t 17.333219 \t 9.256669 \t8.024076 \t 0.052474 \t 74.123348 s\n","168 \t 16.942102 \t 8.787681 \t8.041706 \t 0.112716 \t 74.533477 s\n","169 \t 17.315333 \t 9.005482 \t8.205240 \t 0.104611 \t 74.899761 s\n","170 \t 16.865590 \t 8.563231 \t8.224267 \t 0.078092 \t 75.242072 s\n","171 \t 16.656624 \t 8.378123 \t8.136834 \t 0.141666 \t 75.575188 s\n","172 \t 17.687226 \t 8.936470 \t8.689781 \t 0.060975 \t 75.910661 s\n","173 \t 16.750966 \t 8.641612 \t8.033564 \t 0.075789 \t 76.415367 s\n","174 \t 16.591547 \t 8.279558 \t8.254035 \t 0.057953 \t 76.747989 s\n","175 \t 17.102342 \t 9.124844 \t7.900381 \t 0.077117 \t 77.111811 s\n","176 \t 15.995528 \t 8.179984 \t7.718329 \t 0.097216 \t 77.462237 s\n","177 \t 17.820354 \t 9.374047 \t8.272696 \t 0.173611 \t 77.795406 s\n","178 \t 16.508704 \t 8.329870 \t8.074013 \t 0.104820 \t 78.127190 s\n","179 \t 16.261784 \t 8.184604 \t7.806799 \t 0.270381 \t 78.483584 s\n","180 \t 16.766209 \t 8.868966 \t7.825408 \t 0.071833 \t 78.820356 s\n","181 \t 16.910392 \t 8.778697 \t7.954287 \t 0.177409 \t 79.167248 s\n","182 \t 16.541857 \t 8.979068 \t7.428945 \t 0.133844 \t 79.518779 s\n","183 \t 16.659065 \t 8.141012 \t8.422274 \t 0.095780 \t 79.856324 s\n","184 \t 16.546351 \t 8.846112 \t7.597638 \t 0.102601 \t 80.350436 s\n","185 \t 16.293359 \t 8.225813 \t7.837287 \t 0.230260 \t 80.692738 s\n","186 \t 16.518106 \t 8.684519 \t7.753103 \t 0.080484 \t 81.028770 s\n","187 \t 16.733118 \t 8.709055 \t7.892042 \t 0.132021 \t 81.368739 s\n","188 \t 16.589543 \t 8.387130 \t8.047160 \t 0.155254 \t 81.707743 s\n","189 \t 16.619692 \t 8.989289 \t7.554684 \t 0.075719 \t 82.045516 s\n","190 \t 17.369537 \t 8.663259 \t8.631380 \t 0.074899 \t 82.395120 s\n","191 \t 17.805991 \t 8.910369 \t8.612670 \t 0.282952 \t 82.729899 s\n","192 \t 16.480529 \t 8.665755 \t7.733783 \t 0.080991 \t 83.060725 s\n","193 \t 16.390121 \t 8.093680 \t8.224837 \t 0.071604 \t 83.391826 s\n","194 \t 16.635375 \t 8.744796 \t7.766351 \t 0.124228 \t 83.990312 s\n","195 \t 16.215792 \t 8.712234 \t7.411393 \t 0.092164 \t 84.412870 s\n","196 \t 16.837383 \t 8.789912 \t7.932708 \t 0.114764 \t 84.866540 s\n","197 \t 16.150751 \t 8.232764 \t7.697250 \t 0.220736 \t 85.291036 s\n","198 \t 16.166614 \t 8.012288 \t7.952771 \t 0.201556 \t 85.712691 s\n","199 \t 16.437087 \t 8.393198 \t7.902598 \t 0.141292 \t 86.193996 s\n","200 \t 16.238948 \t 8.048509 \t8.031781 \t 0.158658 \t 86.660033 s\n","201 \t 16.090055 \t 7.533195 \t8.417819 \t 0.139042 \t 86.993579 s\n","202 \t 16.046784 \t 8.519653 \t7.472233 \t 0.054898 \t 87.334059 s\n","203 \t 16.178846 \t 7.754122 \t8.272098 \t 0.152627 \t 87.680837 s\n","204 \t 16.363638 \t 8.435167 \t7.745846 \t 0.182625 \t 88.021810 s\n","205 \t 16.520927 \t 8.730078 \t7.738764 \t 0.052088 \t 88.488211 s\n","206 \t 16.329540 \t 8.377923 \t7.866497 \t 0.085121 \t 88.827261 s\n","207 \t 17.053082 \t 9.088843 \t7.861359 \t 0.102881 \t 89.162802 s\n","208 \t 17.520550 \t 8.979987 \t8.470602 \t 0.069960 \t 89.495167 s\n","209 \t 16.486798 \t 8.387741 \t7.965363 \t 0.133694 \t 89.845506 s\n","210 \t 16.382417 \t 8.928101 \t7.361618 \t 0.092698 \t 90.180452 s\n","211 \t 16.116801 \t 8.272674 \t7.734203 \t 0.109925 \t 90.517519 s\n","212 \t 17.549374 \t 8.853590 \t8.485830 \t 0.209953 \t 90.859571 s\n","213 \t 16.209201 \t 8.043917 \t7.921565 \t 0.243719 \t 91.195205 s\n","214 \t 16.752824 \t 8.351465 \t8.220910 \t 0.180450 \t 91.537183 s\n","215 \t 16.070873 \t 8.449812 \t7.531697 \t 0.089363 \t 91.877865 s\n","216 \t 17.039196 \t 8.816979 \t8.052752 \t 0.169466 \t 92.350965 s\n","217 \t 15.845028 \t 8.195722 \t7.604453 \t 0.044854 \t 92.700125 s\n","218 \t 16.433588 \t 8.527089 \t7.830270 \t 0.076229 \t 93.057957 s\n","219 \t 16.289751 \t 8.415962 \t7.805802 \t 0.067987 \t 93.394239 s\n","220 \t 16.654571 \t 8.083369 \t8.458432 \t 0.112770 \t 93.725438 s\n","221 \t 16.910296 \t 9.144121 \t7.707602 \t 0.058574 \t 94.083174 s\n","222 \t 16.387005 \t 9.047670 \t7.288364 \t 0.050970 \t 94.417480 s\n","223 \t 16.885115 \t 8.653959 \t8.150449 \t 0.080705 \t 94.752001 s\n","224 \t 16.698018 \t 8.809607 \t7.823446 \t 0.064967 \t 95.101055 s\n","225 \t 16.861326 \t 9.275482 \t7.526587 \t 0.059258 \t 95.434446 s\n","226 \t 17.576289 \t 9.078902 \t8.199314 \t 0.298073 \t 95.923709 s\n","227 \t 16.235923 \t 8.136336 \t8.039837 \t 0.059749 \t 96.263249 s\n","228 \t 17.089402 \t 8.770653 \t8.237030 \t 0.081719 \t 96.603969 s\n","229 \t 16.632479 \t 8.127004 \t8.454266 \t 0.051211 \t 97.070694 s\n","230 \t 16.259736 \t 8.411118 \t7.812572 \t 0.036045 \t 97.482446 s\n","231 \t 15.707774 \t 8.183829 \t7.456811 \t 0.067134 \t 97.933643 s\n","232 \t 15.844339 \t 8.412847 \t7.381631 \t 0.049862 \t 98.365781 s\n","233 \t 16.631823 \t 8.059853 \t8.493927 \t 0.078043 \t 98.773118 s\n","234 \t 16.136497 \t 8.303678 \t7.791215 \t 0.041604 \t 99.218204 s\n","235 \t 16.155145 \t 8.119091 \t7.962683 \t 0.073372 \t 99.700858 s\n","236 \t 16.507283 \t 8.645261 \t7.823299 \t 0.038724 \t 100.195324 s\n","237 \t 15.806703 \t 8.430679 \t7.297980 \t 0.078043 \t 100.531472 s\n","238 \t 16.952868 \t 8.821915 \t8.070817 \t 0.060137 \t 100.864220 s\n","239 \t 15.481132 \t 8.145126 \t7.274421 \t 0.061584 \t 101.208417 s\n","240 \t 16.538760 \t 8.585117 \t7.925533 \t 0.028110 \t 101.540394 s\n","241 \t 16.341796 \t 8.679157 \t7.589647 \t 0.072991 \t 101.871583 s\n","242 \t 15.859434 \t 8.606111 \t7.171973 \t 0.081351 \t 102.225523 s\n","243 \t 16.990885 \t 9.000774 \t7.928153 \t 0.061958 \t 102.569979 s\n","244 \t 15.624138 \t 8.274880 \t7.223568 \t 0.125690 \t 102.909339 s\n","245 \t 15.763366 \t 8.388354 \t7.303792 \t 0.071221 \t 103.278967 s\n","246 \t 16.305583 \t 8.241051 \t7.996738 \t 0.067794 \t 103.759450 s\n","247 \t 16.816383 \t 8.573165 \t8.160664 \t 0.082555 \t 104.097606 s\n","248 \t 16.443649 \t 9.140693 \t7.245587 \t 0.057369 \t 104.441011 s\n","249 \t 16.649671 \t 8.426954 \t8.165864 \t 0.056853 \t 104.783199 s\n","250 \t 15.926968 \t 8.183357 \t7.676126 \t 0.067486 \t 105.124279 s\n","251 \t 15.992845 \t 8.035394 \t7.889223 \t 0.068228 \t 105.470949 s\n","252 \t 16.733900 \t 8.303699 \t8.346464 \t 0.083737 \t 105.803795 s\n","253 \t 16.498866 \t 8.545412 \t7.920402 \t 0.033052 \t 106.145033 s\n","254 \t 16.552003 \t 8.384213 \t8.146965 \t 0.020825 \t 106.488873 s\n","255 \t 16.084115 \t 8.384421 \t7.648272 \t 0.051422 \t 106.828602 s\n","256 \t 16.264641 \t 8.495964 \t7.675404 \t 0.093273 \t 107.331315 s\n","257 \t 15.359764 \t 8.140598 \t7.160740 \t 0.058424 \t 107.696270 s\n","258 \t 15.778966 \t 7.978224 \t7.687645 \t 0.113097 \t 108.033328 s\n","259 \t 15.856455 \t 8.388787 \t7.429757 \t 0.037911 \t 108.367263 s\n","260 \t 16.233988 \t 8.812205 \t7.387052 \t 0.034731 \t 108.725640 s\n","261 \t 16.093009 \t 8.499240 \t7.541761 \t 0.052008 \t 109.062593 s\n","262 \t 15.605320 \t 8.227741 \t7.347750 \t 0.029828 \t 109.406641 s\n","263 \t 15.872250 \t 8.036428 \t7.740249 \t 0.095573 \t 109.757772 s\n","264 \t 16.572974 \t 8.770919 \t7.771926 \t 0.030128 \t 110.213239 s\n","265 \t 16.626095 \t 8.218014 \t8.351758 \t 0.056323 \t 110.640826 s\n","266 \t 16.662469 \t 8.619161 \t8.005516 \t 0.037792 \t 111.265362 s\n","267 \t 15.889633 \t 7.656011 \t8.088448 \t 0.145174 \t 111.676091 s\n","268 \t 16.079787 \t 8.221150 \t7.807468 \t 0.051168 \t 112.094687 s\n","269 \t 15.671360 \t 8.346820 \t7.166206 \t 0.158334 \t 112.576149 s\n","270 \t 15.598549 \t 8.031266 \t7.528502 \t 0.038781 \t 113.036902 s\n","271 \t 15.223567 \t 7.667514 \t7.508163 \t 0.047889 \t 113.376343 s\n","272 \t 16.090470 \t 8.313541 \t7.726136 \t 0.050793 \t 113.714796 s\n","273 \t 16.713769 \t 8.313696 \t8.343585 \t 0.056487 \t 114.050451 s\n","274 \t 16.491267 \t 8.527301 \t7.917511 \t 0.046455 \t 114.382849 s\n","275 \t 15.901333 \t 8.103740 \t7.740091 \t 0.057502 \t 114.727445 s\n","276 \t 16.440573 \t 8.443875 \t7.939551 \t 0.057146 \t 115.066114 s\n","277 \t 15.754633 \t 8.141171 \t7.555441 \t 0.058022 \t 115.401921 s\n","278 \t 15.836489 \t 8.157054 \t7.648365 \t 0.031069 \t 115.885050 s\n","279 \t 16.936488 \t 8.313684 \t8.556481 \t 0.066323 \t 116.221939 s\n","280 \t 16.845518 \t 8.486282 \t8.319743 \t 0.039494 \t 116.563392 s\n","281 \t 15.924284 \t 8.331998 \t7.499041 \t 0.093245 \t 116.909206 s\n","282 \t 15.173850 \t 8.006230 \t7.113704 \t 0.053914 \t 117.253972 s\n","283 \t 15.574590 \t 7.989078 \t7.538638 \t 0.046874 \t 117.592891 s\n","284 \t 15.760947 \t 7.904425 \t7.812994 \t 0.043528 \t 117.934204 s\n","285 \t 15.426709 \t 7.695736 \t7.693492 \t 0.037480 \t 118.274805 s\n","286 \t 16.990763 \t 8.528754 \t8.399263 \t 0.062746 \t 118.614814 s\n","287 \t 15.427670 \t 7.680745 \t7.669563 \t 0.077361 \t 118.960388 s\n","288 \t 16.578727 \t 8.356126 \t8.156422 \t 0.066179 \t 119.294685 s\n","289 \t 15.510041 \t 7.767941 \t7.667583 \t 0.074517 \t 119.773377 s\n","290 \t 16.025035 \t 7.723116 \t8.237060 \t 0.064860 \t 120.129137 s\n","291 \t 15.572018 \t 7.752317 \t7.778723 \t 0.040979 \t 120.467817 s\n","292 \t 15.482891 \t 8.234030 \t7.213979 \t 0.034882 \t 120.808229 s\n","293 \t 15.373548 \t 7.263317 \t8.070869 \t 0.039362 \t 121.159853 s\n","294 \t 16.138037 \t 7.844898 \t8.243843 \t 0.049297 \t 121.489979 s\n","295 \t 15.407909 \t 8.019620 \t7.348093 \t 0.040195 \t 121.824115 s\n","296 \t 15.921205 \t 7.745079 \t8.110113 \t 0.066013 \t 122.181613 s\n","297 \t 15.358825 \t 8.095048 \t7.238683 \t 0.025094 \t 122.516512 s\n","298 \t 15.891401 \t 7.944455 \t7.916726 \t 0.030221 \t 122.850256 s\n","299 \t 15.230590 \t 7.774908 \t7.408196 \t 0.047486 \t 123.296708 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:07<00:00, 18.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4743\n","Hit@10: 0.5808\n","Hit@3: 0.4923\n","Hit@1: 0.4077\n","Link Prediction on Validation Set (All)\n","MRR: 0.4743\n","Hit@10: 0.5808\n","Hit@3: 0.4923\n","Hit@1: 0.4077\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2910\n","Hit@10: 0.5385\n","Hit@3: 0.3462\n","Hit@1: 0.1692\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6125\n","Hit@10: 0.7490\n","Hit@3: 0.6461\n","Hit@1: 0.5432\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1574\n","300 \t 15.254488 \t 7.701904 \t7.448051 \t 0.104532 \t 131.385876 s\n","301 \t 15.835174 \t 7.461951 \t8.302504 \t 0.070720 \t 131.736968 s\n","302 \t 16.364195 \t 8.184923 \t8.150976 \t 0.028297 \t 132.076891 s\n","303 \t 15.731339 \t 8.011388 \t7.601124 \t 0.118827 \t 132.424522 s\n","304 \t 15.039761 \t 7.421851 \t7.585965 \t 0.031946 \t 132.763306 s\n","305 \t 15.284420 \t 7.779413 \t7.467144 \t 0.037864 \t 133.095714 s\n","306 \t 14.907942 \t 7.384462 \t7.453186 \t 0.070294 \t 133.428999 s\n","307 \t 15.032877 \t 7.434101 \t7.570627 \t 0.028149 \t 133.782704 s\n","308 \t 15.932565 \t 8.012867 \t7.877084 \t 0.042613 \t 134.121697 s\n","309 \t 15.764393 \t 7.704799 \t8.034528 \t 0.025067 \t 134.454931 s\n","310 \t 15.397937 \t 7.755242 \t7.612956 \t 0.029739 \t 134.812662 s\n","311 \t 16.160568 \t 8.257575 \t7.878426 \t 0.024567 \t 135.298221 s\n","312 \t 15.093252 \t 7.158439 \t7.902369 \t 0.032443 \t 135.648016 s\n","313 \t 15.625212 \t 7.602303 \t7.991323 \t 0.031586 \t 135.986279 s\n","314 \t 15.544782 \t 7.929550 \t7.595065 \t 0.020167 \t 136.344237 s\n","315 \t 15.446681 \t 7.773271 \t7.636916 \t 0.036494 \t 136.774024 s\n","316 \t 14.738017 \t 7.358318 \t7.358659 \t 0.021040 \t 137.225146 s\n","317 \t 16.412589 \t 7.866181 \t8.525309 \t 0.021098 \t 137.861145 s\n","318 \t 15.375773 \t 7.955899 \t7.404298 \t 0.015576 \t 138.412537 s\n","319 \t 15.411260 \t 8.060757 \t7.312161 \t 0.038342 \t 138.945926 s\n","320 \t 15.664465 \t 7.728771 \t7.898421 \t 0.037274 \t 139.422299 s\n","321 \t 15.983958 \t 7.756622 \t8.160442 \t 0.066894 \t 139.876790 s\n","322 \t 15.431016 \t 7.885520 \t7.516390 \t 0.029106 \t 140.218449 s\n","323 \t 15.041368 \t 7.305467 \t7.674054 \t 0.061848 \t 140.561835 s\n","324 \t 14.998327 \t 7.327771 \t7.645901 \t 0.024655 \t 141.055172 s\n","325 \t 14.905422 \t 7.617275 \t7.223144 \t 0.065003 \t 141.391574 s\n","326 \t 14.416803 \t 7.655726 \t6.709452 \t 0.051625 \t 141.734439 s\n","327 \t 14.916510 \t 7.609891 \t7.260831 \t 0.045788 \t 142.084379 s\n","328 \t 15.101808 \t 7.521916 \t7.557881 \t 0.022011 \t 142.424335 s\n","329 \t 15.467193 \t 7.470378 \t7.964345 \t 0.032469 \t 142.765470 s\n","330 \t 15.852175 \t 8.152105 \t7.653547 \t 0.046523 \t 143.118392 s\n","331 \t 15.375946 \t 7.634787 \t7.707122 \t 0.034036 \t 143.451776 s\n","332 \t 15.750500 \t 7.727543 \t8.001777 \t 0.021180 \t 143.783545 s\n","333 \t 14.452446 \t 7.152587 \t7.263347 \t 0.036512 \t 144.124253 s\n","334 \t 15.100352 \t 7.359277 \t7.713549 \t 0.027526 \t 144.462441 s\n","335 \t 15.896975 \t 8.311625 \t7.574034 \t 0.011318 \t 144.795450 s\n","336 \t 14.986433 \t 7.705123 \t7.260094 \t 0.021215 \t 145.278733 s\n","337 \t 14.672861 \t 7.219408 \t7.439204 \t 0.014249 \t 145.617899 s\n","338 \t 14.919156 \t 7.483020 \t7.406234 \t 0.029902 \t 145.955030 s\n","339 \t 14.877743 \t 7.511690 \t7.323914 \t 0.042138 \t 146.309082 s\n","340 \t 15.139158 \t 7.483916 \t7.619682 \t 0.035560 \t 146.641843 s\n","341 \t 14.880371 \t 7.217420 \t7.640838 \t 0.022113 \t 146.974661 s\n","342 \t 15.217052 \t 7.325202 \t7.832589 \t 0.059260 \t 147.328415 s\n","343 \t 14.211837 \t 7.298363 \t6.887253 \t 0.026220 \t 147.673688 s\n","344 \t 15.320743 \t 7.398032 \t7.901913 \t 0.020799 \t 148.006885 s\n","345 \t 14.932617 \t 7.333649 \t7.547503 \t 0.051465 \t 148.360224 s\n","346 \t 15.065024 \t 7.592961 \t7.441382 \t 0.030681 \t 148.692769 s\n","347 \t 14.944746 \t 7.382375 \t7.545427 \t 0.016944 \t 149.029169 s\n","348 \t 15.199306 \t 7.551984 \t7.628047 \t 0.019275 \t 149.500357 s\n","349 \t 14.927859 \t 7.545264 \t7.360031 \t 0.022564 \t 149.864349 s\n","350 \t 15.218818 \t 7.473607 \t7.721625 \t 0.023586 \t 150.306176 s\n","351 \t 15.703094 \t 7.933823 \t7.737612 \t 0.031660 \t 150.739359 s\n","352 \t 14.736452 \t 7.633911 \t7.077520 \t 0.025022 \t 151.176087 s\n","353 \t 14.274496 \t 7.027588 \t7.226857 \t 0.020051 \t 151.614962 s\n","354 \t 14.886712 \t 7.585360 \t7.283571 \t 0.017780 \t 152.045574 s\n","355 \t 14.670380 \t 6.984360 \t7.648455 \t 0.037565 \t 152.515411 s\n","356 \t 14.062477 \t 7.158868 \t6.891915 \t 0.011694 \t 152.980401 s\n","357 \t 14.974408 \t 7.373694 \t7.567685 \t 0.033028 \t 153.332020 s\n","358 \t 15.489954 \t 7.519897 \t7.954025 \t 0.016032 \t 153.676784 s\n","359 \t 15.388197 \t 7.537896 \t7.834123 \t 0.016177 \t 154.016291 s\n","360 \t 14.603579 \t 7.388432 \t7.165377 \t 0.049770 \t 154.507452 s\n","361 \t 15.224106 \t 7.510632 \t7.640301 \t 0.073174 \t 154.849982 s\n","362 \t 14.440259 \t 7.247446 \t7.146389 \t 0.046424 \t 155.187806 s\n","363 \t 14.636618 \t 7.413260 \t7.210059 \t 0.013298 \t 155.531458 s\n","364 \t 14.613533 \t 7.200320 \t7.378324 \t 0.034891 \t 155.869603 s\n","365 \t 14.658373 \t 7.256311 \t7.387579 \t 0.014483 \t 156.207060 s\n","366 \t 14.602696 \t 7.030210 \t7.560204 \t 0.012283 \t 156.547703 s\n","367 \t 14.955998 \t 7.506816 \t7.422756 \t 0.026426 \t 156.885345 s\n","368 \t 13.959501 \t 7.230231 \t6.707842 \t 0.021427 \t 157.226493 s\n","369 \t 14.221905 \t 7.148284 \t7.044353 \t 0.029267 \t 157.568512 s\n","370 \t 14.356776 \t 7.218110 \t7.120199 \t 0.018467 \t 157.902306 s\n","371 \t 14.456786 \t 7.109469 \t7.333640 \t 0.013677 \t 158.238554 s\n","372 \t 14.606240 \t 7.269408 \t7.323378 \t 0.013454 \t 158.740815 s\n","373 \t 13.861247 \t 6.770263 \t7.058726 \t 0.032258 \t 159.078734 s\n","374 \t 13.995201 \t 7.143673 \t6.799779 \t 0.051748 \t 159.414951 s\n","375 \t 14.856222 \t 7.419660 \t7.427303 \t 0.009259 \t 159.763348 s\n","376 \t 14.970143 \t 7.082283 \t7.875127 \t 0.012734 \t 160.103416 s\n","377 \t 15.137764 \t 7.370286 \t7.722948 \t 0.044530 \t 160.438225 s\n","378 \t 15.114828 \t 7.378261 \t7.719206 \t 0.017361 \t 160.788739 s\n","379 \t 14.500630 \t 7.214399 \t7.257260 \t 0.028971 \t 161.127877 s\n","380 \t 14.921641 \t 7.295473 \t7.613303 \t 0.012864 \t 161.472615 s\n","381 \t 14.513682 \t 7.093840 \t7.406909 \t 0.012934 \t 161.822428 s\n","382 \t 15.212631 \t 7.380960 \t7.807300 \t 0.024371 \t 162.162809 s\n","383 \t 15.291123 \t 7.282238 \t7.985609 \t 0.023276 \t 162.503494 s\n","384 \t 14.110895 \t 7.021962 \t7.073357 \t 0.015576 \t 163.043681 s\n","385 \t 14.090603 \t 6.766520 \t7.307241 \t 0.016842 \t 163.467220 s\n","386 \t 14.109120 \t 7.120163 \t6.954966 \t 0.033991 \t 163.898684 s\n","387 \t 14.183189 \t 6.813792 \t7.355589 \t 0.013808 \t 164.347262 s\n","388 \t 14.547633 \t 7.650978 \t6.838012 \t 0.058643 \t 164.780141 s\n","389 \t 14.571209 \t 7.368335 \t7.188470 \t 0.014404 \t 165.205304 s\n","390 \t 14.248930 \t 6.989535 \t7.236684 \t 0.022710 \t 165.686055 s\n","391 \t 14.201380 \t 7.026304 \t7.158455 \t 0.016621 \t 166.170081 s\n","392 \t 14.588735 \t 7.230273 \t7.340633 \t 0.017829 \t 166.507528 s\n","393 \t 13.968358 \t 6.927720 \t7.026510 \t 0.014128 \t 166.844239 s\n","394 \t 14.572266 \t 7.227015 \t7.317996 \t 0.027255 \t 167.226602 s\n","395 \t 14.014334 \t 7.229322 \t6.774072 \t 0.010940 \t 167.589006 s\n","396 \t 13.859838 \t 7.054418 \t6.783912 \t 0.021508 \t 168.069040 s\n","397 \t 13.578468 \t 7.030346 \t6.531831 \t 0.016291 \t 168.416503 s\n","398 \t 15.692071 \t 7.681894 \t7.996431 \t 0.013747 \t 168.747240 s\n","399 \t 13.733252 \t 6.906040 \t6.816167 \t 0.011045 \t 169.087732 s\n","400 \t 15.314319 \t 7.439053 \t7.858177 \t 0.017090 \t 169.423428 s\n","401 \t 14.251535 \t 6.936317 \t7.304191 \t 0.011027 \t 169.759199 s\n","402 \t 14.291349 \t 7.235734 \t7.040204 \t 0.015411 \t 170.098520 s\n","403 \t 14.279665 \t 7.052867 \t7.187500 \t 0.039299 \t 170.435093 s\n","404 \t 14.618030 \t 6.905744 \t7.696889 \t 0.015398 \t 170.772842 s\n","405 \t 14.692659 \t 7.327321 \t7.349439 \t 0.015899 \t 171.111363 s\n","406 \t 14.395223 \t 7.177188 \t7.202172 \t 0.015863 \t 171.452470 s\n","407 \t 14.222775 \t 7.079464 \t7.124382 \t 0.018928 \t 171.784424 s\n","408 \t 14.852859 \t 7.053394 \t7.786300 \t 0.013165 \t 172.277742 s\n","409 \t 14.472675 \t 7.115936 \t7.342073 \t 0.014666 \t 172.623348 s\n","410 \t 13.701374 \t 6.946105 \t6.739323 \t 0.015946 \t 172.972403 s\n","411 \t 14.809602 \t 7.063382 \t7.729177 \t 0.017042 \t 173.319636 s\n","412 \t 14.745691 \t 7.251349 \t7.481193 \t 0.013150 \t 173.660705 s\n","413 \t 14.041175 \t 6.959959 \t7.072168 \t 0.009048 \t 173.992857 s\n","414 \t 14.396408 \t 7.022678 \t7.355267 \t 0.018463 \t 174.339371 s\n","415 \t 14.454550 \t 6.939171 \t7.501032 \t 0.014347 \t 174.677152 s\n","416 \t 13.593476 \t 7.070271 \t6.508720 \t 0.014486 \t 175.010467 s\n","417 \t 14.392622 \t 7.658734 \t6.714283 \t 0.019604 \t 175.349828 s\n","418 \t 14.620946 \t 6.928442 \t7.667241 \t 0.025263 \t 175.684052 s\n","419 \t 13.901321 \t 7.098181 \t6.776649 \t 0.026491 \t 176.027151 s\n","420 \t 13.697567 \t 6.757642 \t6.925961 \t 0.013965 \t 176.635567 s\n","421 \t 14.490672 \t 6.950453 \t7.517587 \t 0.022632 \t 177.062463 s\n","422 \t 14.993932 \t 7.379534 \t7.592896 \t 0.021501 \t 177.519315 s\n","423 \t 14.319069 \t 7.086751 \t7.209584 \t 0.022735 \t 177.936377 s\n","424 \t 14.376374 \t 7.260895 \t7.104039 \t 0.011441 \t 178.352045 s\n","425 \t 14.417112 \t 7.117365 \t7.281456 \t 0.018291 \t 178.840580 s\n","426 \t 14.257646 \t 6.857291 \t7.382200 \t 0.018155 \t 179.301373 s\n","427 \t 14.241400 \t 7.172580 \t7.057602 \t 0.011218 \t 179.651203 s\n","428 \t 14.075645 \t 6.984546 \t7.078091 \t 0.013008 \t 179.986464 s\n","429 \t 14.608634 \t 7.130309 \t7.457574 \t 0.020750 \t 180.320595 s\n","430 \t 13.682756 \t 6.881789 \t6.786498 \t 0.014469 \t 180.660600 s\n","431 \t 13.818377 \t 6.777769 \t7.029509 \t 0.011099 \t 180.995326 s\n","432 \t 14.257604 \t 7.112313 \t7.134199 \t 0.011091 \t 181.470201 s\n","433 \t 14.489390 \t 7.222978 \t7.247357 \t 0.019056 \t 181.823219 s\n","434 \t 14.508406 \t 7.112118 \t7.376021 \t 0.020267 \t 182.160123 s\n","435 \t 14.703125 \t 7.261967 \t7.423152 \t 0.018007 \t 182.497913 s\n","436 \t 14.896146 \t 7.121670 \t7.755490 \t 0.018986 \t 182.853116 s\n","437 \t 14.931588 \t 6.864243 \t8.045393 \t 0.021952 \t 183.190460 s\n","438 \t 14.731515 \t 7.152732 \t7.566983 \t 0.011799 \t 183.528386 s\n","439 \t 14.267567 \t 7.008040 \t7.251151 \t 0.008376 \t 183.881392 s\n","440 \t 15.195342 \t 7.111655 \t8.071655 \t 0.012032 \t 184.223856 s\n","441 \t 14.227911 \t 6.811979 \t7.370008 \t 0.045923 \t 184.559445 s\n","442 \t 15.750459 \t 7.355788 \t8.373909 \t 0.020763 \t 184.912649 s\n","443 \t 15.316394 \t 7.635042 \t7.669791 \t 0.011561 \t 185.256109 s\n","444 \t 14.414736 \t 7.205088 \t7.192226 \t 0.017421 \t 185.729628 s\n","445 \t 14.409672 \t 6.973469 \t7.427314 \t 0.008890 \t 186.064951 s\n","446 \t 13.499069 \t 6.792621 \t6.695274 \t 0.011174 \t 186.403087 s\n","447 \t 14.611984 \t 7.170609 \t7.433842 \t 0.007533 \t 186.742417 s\n","448 \t 14.325533 \t 6.976702 \t7.332904 \t 0.015927 \t 187.076559 s\n","449 \t 13.901693 \t 6.798322 \t7.084233 \t 0.019138 \t 187.411738 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:07<00:00, 18.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4818\n","Hit@10: 0.6385\n","Hit@3: 0.4962\n","Hit@1: 0.4115\n","Link Prediction on Validation Set (All)\n","MRR: 0.4818\n","Hit@10: 0.6385\n","Hit@3: 0.4962\n","Hit@1: 0.4115\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3375\n","Hit@10: 0.5462\n","Hit@3: 0.3462\n","Hit@1: 0.2462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6396\n","Hit@10: 0.7531\n","Hit@3: 0.6461\n","Hit@1: 0.5885\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0636\n","450 \t 14.286220 \t 7.098154 \t7.177127 \t 0.010940 \t 195.900449 s\n","451 \t 14.210243 \t 7.322400 \t6.865620 \t 0.022223 \t 196.248633 s\n","452 \t 14.644467 \t 7.127602 \t7.500740 \t 0.016125 \t 196.584334 s\n","453 \t 14.365321 \t 7.222533 \t7.108233 \t 0.034555 \t 196.919337 s\n","454 \t 13.874311 \t 6.877182 \t6.982923 \t 0.014206 \t 197.265106 s\n","455 \t 14.651139 \t 7.448348 \t7.125917 \t 0.076874 \t 197.632727 s\n","456 \t 14.485416 \t 6.795914 \t7.665286 \t 0.024216 \t 198.102616 s\n","457 \t 14.768317 \t 7.430689 \t7.303803 \t 0.033825 \t 198.459378 s\n","458 \t 14.921777 \t 7.081387 \t7.809121 \t 0.031269 \t 198.795497 s\n","459 \t 14.704326 \t 7.093317 \t7.576613 \t 0.034396 \t 199.129082 s\n","460 \t 14.711361 \t 7.303699 \t7.386057 \t 0.021604 \t 199.475216 s\n","461 \t 14.521316 \t 7.513181 \t6.974304 \t 0.033831 \t 199.816162 s\n","462 \t 15.067483 \t 7.366323 \t7.646311 \t 0.054850 \t 200.154670 s\n","463 \t 14.314047 \t 7.074550 \t7.171741 \t 0.067755 \t 200.499997 s\n","464 \t 14.303961 \t 7.238436 \t7.031269 \t 0.034256 \t 200.835972 s\n","465 \t 15.349697 \t 7.896830 \t7.427261 \t 0.025607 \t 201.173666 s\n","466 \t 14.772592 \t 7.316294 \t7.416848 \t 0.039450 \t 201.538748 s\n","467 \t 15.079326 \t 7.266475 \t7.748206 \t 0.064645 \t 202.003724 s\n","468 \t 15.356155 \t 7.749572 \t7.575238 \t 0.031345 \t 202.497870 s\n","469 \t 14.391576 \t 7.500033 \t6.813982 \t 0.077561 \t 202.966791 s\n","470 \t 15.101631 \t 7.673878 \t7.400740 \t 0.027013 \t 203.565772 s\n","471 \t 14.594287 \t 7.110146 \t7.460498 \t 0.023643 \t 204.016736 s\n","472 \t 15.157420 \t 7.458295 \t7.650128 \t 0.048996 \t 204.441499 s\n","473 \t 14.743565 \t 7.168407 \t7.537035 \t 0.038122 \t 204.875933 s\n","474 \t 14.504689 \t 7.179201 \t7.291226 \t 0.034261 \t 205.353324 s\n","475 \t 14.451928 \t 6.964178 \t7.440186 \t 0.047564 \t 205.824992 s\n","476 \t 14.227474 \t 6.924203 \t7.279715 \t 0.023556 \t 206.157123 s\n","477 \t 14.202047 \t 7.438074 \t6.694536 \t 0.069436 \t 206.486438 s\n","478 \t 14.302336 \t 7.196248 \t7.070417 \t 0.035671 \t 206.838245 s\n","479 \t 14.673911 \t 6.992441 \t7.640672 \t 0.040798 \t 207.175461 s\n","480 \t 15.039913 \t 7.435016 \t7.564726 \t 0.040170 \t 207.526931 s\n","481 \t 14.891498 \t 7.087847 \t7.779738 \t 0.023913 \t 207.898345 s\n","482 \t 14.584479 \t 6.987860 \t7.446687 \t 0.149932 \t 208.231386 s\n","483 \t 15.429391 \t 7.398864 \t7.995524 \t 0.035003 \t 208.564309 s\n","484 \t 14.794093 \t 7.325256 \t7.434282 \t 0.034555 \t 209.044276 s\n","485 \t 14.607894 \t 7.509243 \t7.070609 \t 0.028042 \t 209.378728 s\n","486 \t 14.444260 \t 7.241364 \t7.170152 \t 0.032745 \t 209.743565 s\n","487 \t 14.407201 \t 7.584017 \t6.790604 \t 0.032581 \t 210.078777 s\n","488 \t 14.095057 \t 7.085911 \t6.986542 \t 0.022603 \t 210.421984 s\n","489 \t 14.022011 \t 7.105678 \t6.864523 \t 0.051810 \t 210.774694 s\n","490 \t 14.197204 \t 7.144618 \t7.031747 \t 0.020839 \t 211.117769 s\n","491 \t 13.646753 \t 6.783491 \t6.829745 \t 0.033518 \t 211.452214 s\n","492 \t 14.711319 \t 7.259511 \t7.398038 \t 0.053771 \t 211.805173 s\n","493 \t 14.958006 \t 7.257098 \t7.652948 \t 0.047960 \t 212.141171 s\n","494 \t 14.293582 \t 6.929641 \t7.329046 \t 0.034896 \t 212.474605 s\n","495 \t 14.033902 \t 7.069119 \t6.916980 \t 0.047802 \t 212.821324 s\n","496 \t 14.217345 \t 7.133613 \t7.062225 \t 0.021508 \t 213.156897 s\n","497 \t 14.889144 \t 7.460224 \t7.408323 \t 0.020598 \t 213.490759 s\n","498 \t 13.674829 \t 7.019394 \t6.604269 \t 0.051166 \t 213.970159 s\n","499 \t 14.689665 \t 6.863799 \t7.793438 \t 0.032428 \t 214.311038 s\n","500 \t 14.059263 \t 7.281211 \t6.755273 \t 0.022779 \t 214.654776 s\n","501 \t 14.823152 \t 7.551475 \t7.249688 \t 0.021989 \t 214.994947 s\n","502 \t 14.633097 \t 7.174292 \t7.441436 \t 0.017369 \t 215.330647 s\n","503 \t 14.354545 \t 6.948222 \t7.386825 \t 0.019499 \t 215.664938 s\n","504 \t 15.093387 \t 7.299146 \t7.778430 \t 0.015811 \t 216.069549 s\n","505 \t 14.278465 \t 6.884459 \t7.371171 \t 0.022835 \t 216.482871 s\n","506 \t 14.500247 \t 7.535666 \t6.942822 \t 0.021758 \t 216.905984 s\n","507 \t 14.439708 \t 7.157267 \t7.234531 \t 0.047910 \t 217.344234 s\n","508 \t 14.507837 \t 6.875869 \t7.612177 \t 0.019791 \t 217.760286 s\n","509 \t 14.738193 \t 7.337961 \t7.389260 \t 0.010973 \t 218.184432 s\n","510 \t 14.641181 \t 7.312338 \t7.309750 \t 0.019094 \t 218.655177 s\n","511 \t 14.046755 \t 7.120228 \t6.895190 \t 0.031338 \t 219.117087 s\n","512 \t 14.628189 \t 7.303574 \t7.277707 \t 0.046907 \t 219.598876 s\n","513 \t 13.993884 \t 6.862341 \t7.100161 \t 0.031382 \t 219.942907 s\n","514 \t 13.985115 \t 6.749061 \t7.214568 \t 0.021486 \t 220.292146 s\n","515 \t 14.504416 \t 7.309144 \t7.123236 \t 0.072036 \t 220.631179 s\n","516 \t 14.309142 \t 7.304980 \t6.995090 \t 0.009071 \t 220.966130 s\n","517 \t 13.638274 \t 6.832469 \t6.770398 \t 0.035407 \t 221.309984 s\n","518 \t 14.616185 \t 7.237473 \t7.358771 \t 0.019940 \t 221.650130 s\n","519 \t 14.754384 \t 6.852642 \t7.873616 \t 0.028126 \t 221.987404 s\n","520 \t 14.837921 \t 7.141701 \t7.677580 \t 0.018640 \t 222.330431 s\n","521 \t 14.554338 \t 6.936553 \t7.600252 \t 0.017533 \t 222.670648 s\n","522 \t 13.970085 \t 6.956502 \t7.001439 \t 0.012144 \t 223.012450 s\n","523 \t 14.271254 \t 7.109016 \t7.115526 \t 0.046712 \t 223.373956 s\n","524 \t 13.868117 \t 6.627134 \t7.205246 \t 0.035737 \t 223.709517 s\n","525 \t 13.908223 \t 6.871369 \t7.008549 \t 0.028304 \t 224.047533 s\n","526 \t 14.421801 \t 7.189015 \t7.209242 \t 0.023544 \t 224.538185 s\n","527 \t 14.021432 \t 6.908822 \t7.074811 \t 0.037800 \t 224.878465 s\n","528 \t 14.705792 \t 7.152666 \t7.525241 \t 0.027886 \t 225.218290 s\n","529 \t 15.016741 \t 6.831895 \t8.164858 \t 0.019988 \t 225.586994 s\n","530 \t 14.224500 \t 7.286734 \t6.909804 \t 0.027962 \t 225.925719 s\n","531 \t 15.196434 \t 7.191484 \t7.964414 \t 0.040536 \t 226.271966 s\n","532 \t 14.666635 \t 7.065586 \t7.554067 \t 0.046981 \t 226.631416 s\n","533 \t 14.676227 \t 7.121552 \t7.520111 \t 0.034564 \t 226.972005 s\n","534 \t 14.758574 \t 7.620084 \t7.081741 \t 0.056749 \t 227.314367 s\n","535 \t 14.822237 \t 7.078377 \t7.711680 \t 0.032180 \t 227.703250 s\n","536 \t 14.478092 \t 6.859021 \t7.589037 \t 0.030033 \t 228.038279 s\n","537 \t 14.180750 \t 6.874782 \t7.257452 \t 0.048517 \t 228.378246 s\n","538 \t 14.104574 \t 7.139210 \t6.947644 \t 0.017719 \t 228.739093 s\n","539 \t 14.054502 \t 6.957533 \t7.062592 \t 0.034377 \t 229.107333 s\n","540 \t 15.617120 \t 7.255827 \t8.332105 \t 0.029187 \t 229.701399 s\n","541 \t 13.691381 \t 6.922721 \t6.702887 \t 0.065773 \t 230.132740 s\n","542 \t 14.175406 \t 7.402842 \t6.751057 \t 0.021508 \t 230.558466 s\n","543 \t 13.707278 \t 6.430534 \t7.234443 \t 0.042301 \t 230.982441 s\n","544 \t 14.337751 \t 6.892501 \t7.308858 \t 0.136392 \t 231.421318 s\n","545 \t 13.708078 \t 6.698672 \t6.949458 \t 0.059949 \t 231.879796 s\n","546 \t 14.229444 \t 7.044591 \t7.161832 \t 0.023021 \t 232.295819 s\n","547 \t 14.168293 \t 6.846480 \t7.271039 \t 0.050774 \t 232.639383 s\n","548 \t 14.492737 \t 7.167047 \t7.263909 \t 0.061782 \t 232.988637 s\n","549 \t 14.386872 \t 6.921596 \t7.434718 \t 0.030558 \t 233.333251 s\n","550 \t 14.401881 \t 7.034653 \t7.344233 \t 0.022994 \t 233.679567 s\n","551 \t 13.382562 \t 6.541139 \t6.803481 \t 0.037942 \t 234.028051 s\n","552 \t 13.984457 \t 6.743300 \t7.219693 \t 0.021465 \t 234.367127 s\n","553 \t 14.152421 \t 6.988416 \t7.145659 \t 0.018345 \t 234.712220 s\n","554 \t 14.090021 \t 7.081659 \t6.972843 \t 0.035519 \t 235.194891 s\n","555 \t 13.675392 \t 6.814610 \t6.832800 \t 0.027981 \t 235.528398 s\n","556 \t 14.302695 \t 6.976964 \t7.293136 \t 0.032594 \t 235.863743 s\n","557 \t 14.441908 \t 6.837036 \t7.577306 \t 0.027567 \t 236.205704 s\n","558 \t 14.106019 \t 7.133899 \t6.950343 \t 0.021776 \t 236.540793 s\n","559 \t 13.782365 \t 7.076009 \t6.689418 \t 0.016937 \t 236.892001 s\n","560 \t 14.212888 \t 7.073592 \t7.117391 \t 0.021905 \t 237.230948 s\n","561 \t 13.473105 \t 7.086748 \t6.360310 \t 0.026047 \t 237.569842 s\n","562 \t 14.163654 \t 6.956121 \t7.155168 \t 0.052365 \t 237.908751 s\n","563 \t 14.124444 \t 7.379398 \t6.711303 \t 0.033743 \t 238.249736 s\n","564 \t 14.047461 \t 7.065556 \t6.955360 \t 0.026544 \t 238.599616 s\n","565 \t 13.955067 \t 6.508624 \t7.408818 \t 0.037626 \t 238.943428 s\n","566 \t 13.315567 \t 6.744416 \t6.553890 \t 0.017260 \t 239.287229 s\n","567 \t 13.983733 \t 6.819396 \t7.115314 \t 0.049023 \t 239.626538 s\n","568 \t 13.916424 \t 7.031927 \t6.866276 \t 0.018221 \t 240.132148 s\n","569 \t 13.387425 \t 6.773667 \t6.577348 \t 0.036410 \t 240.466388 s\n","570 \t 13.671321 \t 6.803488 \t6.850598 \t 0.017236 \t 240.803209 s\n","571 \t 14.589216 \t 7.173154 \t7.389763 \t 0.026300 \t 241.151743 s\n","572 \t 13.811584 \t 6.686418 \t7.104129 \t 0.021037 \t 241.489129 s\n","573 \t 14.018274 \t 6.965283 \t7.023869 \t 0.029122 \t 241.822130 s\n","574 \t 14.623481 \t 6.866016 \t7.738929 \t 0.018536 \t 242.194192 s\n","575 \t 13.499369 \t 6.762779 \t6.715690 \t 0.020900 \t 242.620605 s\n","576 \t 13.859495 \t 7.087161 \t6.759269 \t 0.013064 \t 243.050609 s\n","577 \t 13.998809 \t 7.087669 \t6.867076 \t 0.044063 \t 243.483017 s\n","578 \t 13.923641 \t 6.969440 \t6.933195 \t 0.021006 \t 243.893541 s\n","579 \t 13.727764 \t 6.766002 \t6.936382 \t 0.025380 \t 244.298651 s\n","580 \t 13.995546 \t 7.080852 \t6.889763 \t 0.024931 \t 244.773887 s\n","581 \t 14.162391 \t 7.092397 \t7.023598 \t 0.046397 \t 245.243187 s\n","582 \t 14.131513 \t 7.544818 \t6.566009 \t 0.020686 \t 245.730995 s\n","583 \t 14.482070 \t 7.332467 \t7.113576 \t 0.036028 \t 246.069413 s\n","584 \t 13.652832 \t 6.627991 \t6.997537 \t 0.027304 \t 246.413908 s\n","585 \t 13.767863 \t 7.070201 \t6.678176 \t 0.019485 \t 246.743936 s\n","586 \t 13.452030 \t 6.895596 \t6.495014 \t 0.061419 \t 247.081503 s\n","587 \t 14.201962 \t 7.289949 \t6.870449 \t 0.041563 \t 247.430312 s\n","588 \t 13.802470 \t 6.930629 \t6.850518 \t 0.021323 \t 247.771421 s\n","589 \t 13.549134 \t 6.885777 \t6.646497 \t 0.016860 \t 248.105429 s\n","590 \t 14.301547 \t 6.848895 \t7.432543 \t 0.020109 \t 248.449488 s\n","591 \t 13.310619 \t 6.923892 \t6.359968 \t 0.026760 \t 248.785330 s\n","592 \t 13.254168 \t 6.579923 \t6.661285 \t 0.012960 \t 249.119024 s\n","593 \t 14.247360 \t 7.296147 \t6.940080 \t 0.011133 \t 249.454138 s\n","594 \t 13.628342 \t 6.737281 \t6.877440 \t 0.013620 \t 249.789512 s\n","595 \t 13.008109 \t 6.656505 \t6.328072 \t 0.023533 \t 250.125718 s\n","596 \t 13.624476 \t 6.736694 \t6.869014 \t 0.018768 \t 250.613476 s\n","597 \t 13.518420 \t 6.727067 \t6.752753 \t 0.038599 \t 250.954851 s\n","598 \t 13.652389 \t 6.682960 \t6.954507 \t 0.014922 \t 251.300730 s\n","599 \t 13.821201 \t 6.940218 \t6.868551 \t 0.012431 \t 251.657597 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:07<00:00, 18.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5180\n","Hit@10: 0.6385\n","Hit@3: 0.5500\n","Hit@1: 0.4500\n","Link Prediction on Validation Set (All)\n","MRR: 0.5180\n","Hit@10: 0.6385\n","Hit@3: 0.5500\n","Hit@1: 0.4500\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3372\n","Hit@10: 0.5769\n","Hit@3: 0.3462\n","Hit@1: 0.2462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6372\n","Hit@10: 0.7737\n","Hit@3: 0.6502\n","Hit@1: 0.5802\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1206\n","600 \t 13.796997 \t 6.918752 \t6.867188 \t 0.011057 \t 259.697810 s\n","601 \t 13.623806 \t 6.639401 \t6.973873 \t 0.010532 \t 260.071434 s\n","602 \t 13.568660 \t 6.842940 \t6.716434 \t 0.009285 \t 260.410926 s\n","603 \t 13.387574 \t 6.817664 \t6.556163 \t 0.013746 \t 260.747269 s\n","604 \t 13.536368 \t 7.095405 \t6.420410 \t 0.020553 \t 261.100088 s\n","605 \t 13.777014 \t 6.909326 \t6.851923 \t 0.015765 \t 261.439332 s\n","606 \t 13.235550 \t 6.528723 \t6.649075 \t 0.057752 \t 261.922484 s\n","607 \t 13.939550 \t 7.118988 \t6.775162 \t 0.045400 \t 262.261966 s\n","608 \t 13.539342 \t 6.761140 \t6.762505 \t 0.015697 \t 262.601982 s\n","609 \t 13.257964 \t 6.824178 \t6.405129 \t 0.028657 \t 262.943188 s\n","610 \t 13.506042 \t 7.281150 \t6.200808 \t 0.024085 \t 263.285379 s\n","611 \t 13.867427 \t 6.932497 \t6.921254 \t 0.013677 \t 263.623759 s\n","612 \t 14.056328 \t 6.977579 \t7.051101 \t 0.027648 \t 263.968194 s\n","613 \t 12.965897 \t 6.513623 \t6.433475 \t 0.018799 \t 264.304911 s\n","614 \t 13.642298 \t 6.733710 \t6.883450 \t 0.025139 \t 264.650381 s\n","615 \t 13.644236 \t 6.724525 \t6.903407 \t 0.016304 \t 264.997841 s\n","616 \t 13.880676 \t 7.095146 \t6.769130 \t 0.016400 \t 265.472944 s\n","617 \t 13.512169 \t 6.806144 \t6.686788 \t 0.019236 \t 265.805232 s\n","618 \t 13.274148 \t 6.643262 \t6.596387 \t 0.034498 \t 266.149337 s\n","619 \t 13.701024 \t 6.959192 \t6.723271 \t 0.018560 \t 266.486935 s\n","620 \t 13.855510 \t 6.771755 \t7.054380 \t 0.029375 \t 266.820268 s\n","621 \t 13.795803 \t 7.136035 \t6.642491 \t 0.017276 \t 267.158968 s\n","622 \t 13.542604 \t 6.529131 \t7.000346 \t 0.013127 \t 267.508008 s\n","623 \t 13.234294 \t 6.698223 \t6.499357 \t 0.036714 \t 267.961221 s\n","624 \t 14.140977 \t 6.894601 \t7.231265 \t 0.015110 \t 268.404353 s\n","625 \t 13.913692 \t 7.112230 \t6.788229 \t 0.013233 \t 269.010241 s\n","626 \t 12.927307 \t 6.582103 \t6.321588 \t 0.023616 \t 269.575705 s\n","627 \t 13.923071 \t 6.767426 \t7.141343 \t 0.014301 \t 270.069231 s\n","628 \t 12.974298 \t 6.435483 \t6.525886 \t 0.012929 \t 270.514479 s\n","629 \t 13.121349 \t 6.529692 \t6.581926 \t 0.009731 \t 270.957958 s\n","630 \t 13.597182 \t 7.001861 \t6.550512 \t 0.044809 \t 271.413472 s\n","631 \t 13.573848 \t 6.771677 \t6.774107 \t 0.028064 \t 271.878443 s\n","632 \t 13.544634 \t 6.851100 \t6.657515 \t 0.036020 \t 272.308091 s\n","633 \t 13.994581 \t 6.956626 \t7.025729 \t 0.012225 \t 272.651413 s\n","634 \t 13.964998 \t 6.677188 \t7.273709 \t 0.014102 \t 272.986966 s\n","635 \t 13.268926 \t 6.452771 \t6.804843 \t 0.011313 \t 273.461590 s\n","636 \t 13.938709 \t 6.648160 \t7.276461 \t 0.014089 \t 273.796439 s\n","637 \t 13.593278 \t 6.672575 \t6.908801 \t 0.011902 \t 274.137828 s\n","638 \t 13.834217 \t 6.740591 \t7.077952 \t 0.015674 \t 274.481709 s\n","639 \t 13.703240 \t 7.053487 \t6.639722 \t 0.010031 \t 274.823882 s\n","640 \t 13.528912 \t 6.914059 \t6.597116 \t 0.017737 \t 275.162333 s\n","641 \t 13.367890 \t 6.791274 \t6.563396 \t 0.013221 \t 275.503300 s\n","642 \t 14.002171 \t 7.129764 \t6.861344 \t 0.011063 \t 275.844047 s\n","643 \t 14.012923 \t 7.167845 \t6.820827 \t 0.024251 \t 276.198207 s\n","644 \t 13.242785 \t 6.409842 \t6.815131 \t 0.017813 \t 276.540373 s\n","645 \t 13.033472 \t 6.533390 \t6.485251 \t 0.014832 \t 277.018357 s\n","646 \t 13.398744 \t 6.811589 \t6.572740 \t 0.014415 \t 277.362007 s\n","647 \t 13.172113 \t 6.538836 \t6.620098 \t 0.013179 \t 277.728173 s\n","648 \t 13.407928 \t 6.666121 \t6.729748 \t 0.012059 \t 278.065501 s\n","649 \t 13.370820 \t 6.785380 \t6.576933 \t 0.008507 \t 278.398164 s\n","650 \t 13.090663 \t 6.447981 \t6.628541 \t 0.014142 \t 278.748018 s\n","651 \t 13.440984 \t 6.474172 \t6.926735 \t 0.040077 \t 279.080015 s\n","652 \t 12.917625 \t 6.500616 \t6.397773 \t 0.019236 \t 279.422911 s\n","653 \t 13.888732 \t 6.984189 \t6.892783 \t 0.011761 \t 279.774932 s\n","654 \t 12.878749 \t 6.308156 \t6.530257 \t 0.040336 \t 280.115732 s\n","655 \t 14.082088 \t 6.505343 \t7.558558 \t 0.018186 \t 280.610876 s\n","656 \t 13.472414 \t 6.721245 \t6.737571 \t 0.013598 \t 280.953931 s\n","657 \t 13.712646 \t 6.840063 \t6.859448 \t 0.013135 \t 281.299401 s\n","658 \t 13.341002 \t 6.580945 \t6.748940 \t 0.011118 \t 281.654423 s\n","659 \t 12.976627 \t 6.366894 \t6.597821 \t 0.011912 \t 281.992734 s\n","660 \t 12.955399 \t 6.332231 \t6.607721 \t 0.015447 \t 282.393060 s\n","661 \t 13.725087 \t 7.049181 \t6.663490 \t 0.012416 \t 282.829443 s\n","662 \t 13.039258 \t 6.471504 \t6.539372 \t 0.028383 \t 283.276753 s\n","663 \t 13.205034 \t 6.674729 \t6.519522 \t 0.010782 \t 283.718694 s\n","664 \t 13.030858 \t 6.475945 \t6.545301 \t 0.009612 \t 284.150889 s\n","665 \t 14.128376 \t 7.082598 \t7.035415 \t 0.010363 \t 284.778509 s\n","666 \t 13.480155 \t 6.846088 \t6.618947 \t 0.015119 \t 285.256944 s\n","667 \t 13.277819 \t 6.654971 \t6.602688 \t 0.020160 \t 285.604932 s\n","668 \t 13.183507 \t 6.645067 \t6.521803 \t 0.016637 \t 285.946414 s\n","669 \t 13.137863 \t 6.554993 \t6.562064 \t 0.020806 \t 286.287052 s\n","670 \t 13.379185 \t 6.599889 \t6.759565 \t 0.019731 \t 286.624975 s\n","671 \t 13.662771 \t 6.859972 \t6.793534 \t 0.009265 \t 286.964889 s\n","672 \t 13.403480 \t 6.598534 \t6.792979 \t 0.011967 \t 287.300091 s\n","673 \t 13.555139 \t 6.967926 \t6.576451 \t 0.010762 \t 287.643963 s\n","674 \t 13.361533 \t 6.720850 \t6.629673 \t 0.011009 \t 288.018620 s\n","675 \t 12.703250 \t 6.551479 \t6.142976 \t 0.008795 \t 288.497093 s\n","676 \t 13.178037 \t 6.529899 \t6.635918 \t 0.012220 \t 288.832751 s\n","677 \t 13.599575 \t 6.758123 \t6.833951 \t 0.007501 \t 289.185992 s\n","678 \t 13.068610 \t 6.839867 \t6.216929 \t 0.011814 \t 289.530127 s\n","679 \t 12.989849 \t 6.455025 \t6.523789 \t 0.011035 \t 289.869949 s\n","680 \t 13.067839 \t 6.394188 \t6.656550 \t 0.017101 \t 290.236229 s\n","681 \t 13.772023 \t 6.996572 \t6.752543 \t 0.022908 \t 290.581949 s\n","682 \t 12.695963 \t 6.311452 \t6.368276 \t 0.016235 \t 290.918968 s\n","683 \t 13.121226 \t 6.610864 \t6.487083 \t 0.023279 \t 291.276930 s\n","684 \t 13.349552 \t 6.808679 \t6.520212 \t 0.020661 \t 291.625200 s\n","685 \t 13.559069 \t 6.656420 \t6.895726 \t 0.006922 \t 292.113581 s\n","686 \t 13.559914 \t 6.773024 \t6.757528 \t 0.029363 \t 292.455230 s\n","687 \t 13.290617 \t 6.685148 \t6.597141 \t 0.008328 \t 292.792041 s\n","688 \t 13.665064 \t 6.913526 \t6.741942 \t 0.009596 \t 293.135545 s\n","689 \t 12.852580 \t 6.487072 \t6.351574 \t 0.013935 \t 293.476038 s\n","690 \t 13.839133 \t 6.819328 \t7.009586 \t 0.010218 \t 293.809700 s\n","691 \t 12.802604 \t 6.481428 \t6.308937 \t 0.012240 \t 294.155205 s\n","692 \t 13.167217 \t 6.904516 \t6.249470 \t 0.013231 \t 294.492735 s\n","693 \t 12.829257 \t 6.442394 \t6.377510 \t 0.009354 \t 294.824110 s\n","694 \t 12.863585 \t 6.599350 \t6.228782 \t 0.035452 \t 295.170776 s\n","695 \t 13.033506 \t 6.394441 \t6.631356 \t 0.007709 \t 295.743162 s\n","696 \t 13.023156 \t 6.351016 \t6.657357 \t 0.014783 \t 296.150971 s\n","697 \t 12.952228 \t 6.459298 \t6.483863 \t 0.009067 \t 296.615112 s\n","698 \t 13.229603 \t 6.656406 \t6.563613 \t 0.009584 \t 297.030337 s\n","699 \t 13.116946 \t 6.519801 \t6.584846 \t 0.012299 \t 297.459071 s\n","700 \t 13.233345 \t 6.752856 \t6.472202 \t 0.008287 \t 297.919222 s\n","701 \t 13.212931 \t 6.686960 \t6.516976 \t 0.008995 \t 298.415583 s\n","702 \t 12.950160 \t 6.563809 \t6.374397 \t 0.011954 \t 298.750022 s\n","703 \t 12.924786 \t 6.480585 \t6.432350 \t 0.011851 \t 299.084915 s\n","704 \t 12.872792 \t 6.596815 \t6.260226 \t 0.015750 \t 299.423015 s\n","705 \t 12.842487 \t 6.345641 \t6.479877 \t 0.016970 \t 299.910805 s\n","706 \t 13.056423 \t 6.625664 \t6.422125 \t 0.008634 \t 300.252813 s\n","707 \t 13.422887 \t 6.675255 \t6.737823 \t 0.009808 \t 300.593668 s\n","708 \t 13.531289 \t 6.613944 \t6.908970 \t 0.008375 \t 300.930150 s\n","709 \t 13.149068 \t 6.554829 \t6.579707 \t 0.014533 \t 301.268180 s\n","710 \t 13.010555 \t 6.643269 \t6.350060 \t 0.017226 \t 301.606425 s\n","711 \t 13.021996 \t 6.464268 \t6.542636 \t 0.015093 \t 301.940197 s\n","712 \t 13.433618 \t 6.659254 \t6.749177 \t 0.025188 \t 302.288637 s\n","713 \t 12.732030 \t 6.457002 \t6.231007 \t 0.044022 \t 302.630219 s\n","714 \t 12.888763 \t 6.443087 \t6.409891 \t 0.035785 \t 302.964430 s\n","715 \t 13.507746 \t 6.556843 \t6.940030 \t 0.010874 \t 303.448125 s\n","716 \t 14.109802 \t 7.136971 \t6.963379 \t 0.009452 \t 303.799438 s\n","717 \t 12.923261 \t 6.492156 \t6.423214 \t 0.007891 \t 304.131343 s\n","718 \t 13.450497 \t 6.810126 \t6.621456 \t 0.018915 \t 304.470800 s\n","719 \t 13.003473 \t 6.498631 \t6.495659 \t 0.009183 \t 304.814521 s\n","720 \t 13.180727 \t 6.738612 \t6.434499 \t 0.007616 \t 305.147180 s\n","721 \t 13.822042 \t 6.901637 \t6.905704 \t 0.014701 \t 305.483304 s\n","722 \t 13.112148 \t 6.695247 \t6.405537 \t 0.011365 \t 305.821939 s\n","723 \t 12.576533 \t 6.382518 \t6.184961 \t 0.009053 \t 306.155295 s\n","724 \t 12.854662 \t 6.219405 \t6.619311 \t 0.015946 \t 306.492910 s\n","725 \t 13.143773 \t 6.498122 \t6.639677 \t 0.005975 \t 306.973725 s\n","726 \t 13.148998 \t 6.646755 \t6.493026 \t 0.009217 \t 307.307372 s\n","727 \t 12.333946 \t 6.152816 \t6.167521 \t 0.013609 \t 307.644456 s\n","728 \t 12.589391 \t 6.161473 \t6.418876 \t 0.009043 \t 307.998389 s\n","729 \t 13.271639 \t 6.692561 \t6.573727 \t 0.005351 \t 308.350229 s\n","730 \t 13.081270 \t 6.248851 \t6.818447 \t 0.013973 \t 308.785029 s\n","731 \t 13.164946 \t 6.354526 \t6.796767 \t 0.013653 \t 309.190887 s\n","732 \t 13.786602 \t 6.929821 \t6.832105 \t 0.024676 \t 309.626667 s\n","733 \t 13.078831 \t 6.626791 \t6.434392 \t 0.017649 \t 310.051294 s\n","734 \t 12.885740 \t 6.393390 \t6.481108 \t 0.011242 \t 310.467854 s\n","735 \t 13.102405 \t 6.412013 \t6.671501 \t 0.018891 \t 311.087888 s\n","736 \t 13.110441 \t 6.551561 \t6.550881 \t 0.008000 \t 311.545624 s\n","737 \t 13.019742 \t 6.446382 \t6.561605 \t 0.011755 \t 311.884379 s\n","738 \t 13.351150 \t 6.579964 \t6.762646 \t 0.008540 \t 312.249742 s\n","739 \t 13.164965 \t 6.360703 \t6.796255 \t 0.008008 \t 312.605475 s\n","740 \t 13.789239 \t 7.213377 \t6.557852 \t 0.018010 \t 312.942518 s\n","741 \t 13.202624 \t 6.662150 \t6.526320 \t 0.014154 \t 313.295933 s\n","742 \t 13.560125 \t 6.861055 \t6.692814 \t 0.006255 \t 313.640553 s\n","743 \t 13.621719 \t 6.675179 \t6.933825 \t 0.012714 \t 313.977716 s\n","744 \t 12.580649 \t 6.236540 \t6.336920 \t 0.007189 \t 314.333007 s\n","745 \t 12.936886 \t 6.267413 \t6.659337 \t 0.010136 \t 314.813233 s\n","746 \t 13.722811 \t 6.833570 \t6.880891 \t 0.008350 \t 315.154771 s\n","747 \t 12.309666 \t 6.207301 \t6.095172 \t 0.007191 \t 315.486745 s\n","748 \t 12.981365 \t 6.644296 \t6.326849 \t 0.010220 \t 315.821310 s\n","749 \t 13.324477 \t 6.752730 \t6.566391 \t 0.005355 \t 316.164316 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:06<00:00, 19.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5122\n","Hit@10: 0.6385\n","Hit@3: 0.5269\n","Hit@1: 0.4500\n","Link Prediction on Validation Set (All)\n","MRR: 0.5122\n","Hit@10: 0.6385\n","Hit@3: 0.5269\n","Hit@1: 0.4500\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3038\n","Hit@10: 0.4846\n","Hit@3: 0.3462\n","Hit@1: 0.2000\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6255\n","Hit@10: 0.7243\n","Hit@3: 0.6502\n","Hit@1: 0.5679\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0682\n","750 \t 13.333360 \t 6.679820 \t6.645905 \t 0.007635 \t 323.788731 s\n","751 \t 13.671810 \t 6.926121 \t6.735352 \t 0.010337 \t 324.228437 s\n","752 \t 13.444224 \t 6.795337 \t6.634205 \t 0.014682 \t 324.711903 s\n","753 \t 12.790702 \t 6.367588 \t6.416049 \t 0.007066 \t 325.114719 s\n","754 \t 13.234694 \t 6.645764 \t6.577698 \t 0.011233 \t 325.457995 s\n","755 \t 13.177082 \t 6.623482 \t6.545870 \t 0.007730 \t 325.966797 s\n","756 \t 13.054680 \t 6.416193 \t6.632344 \t 0.006143 \t 326.305933 s\n","757 \t 13.080168 \t 6.243442 \t6.825048 \t 0.011678 \t 326.647859 s\n","758 \t 13.354214 \t 6.726930 \t6.620628 \t 0.006655 \t 326.978745 s\n","759 \t 13.235177 \t 6.828136 \t6.399118 \t 0.007923 \t 327.311610 s\n","760 \t 13.004913 \t 6.492054 \t6.503089 \t 0.009771 \t 327.656711 s\n","761 \t 13.206797 \t 6.910474 \t6.283459 \t 0.012864 \t 328.014314 s\n","762 \t 13.770458 \t 6.821160 \t6.940493 \t 0.008805 \t 328.359718 s\n","763 \t 13.099721 \t 6.594609 \t6.495336 \t 0.009776 \t 328.701896 s\n","764 \t 13.116629 \t 6.425807 \t6.676832 \t 0.013989 \t 329.049392 s\n","765 \t 13.723176 \t 6.899904 \t6.807644 \t 0.015628 \t 329.524185 s\n","766 \t 13.414674 \t 6.717731 \t6.684768 \t 0.012176 \t 329.900998 s\n","767 \t 13.112207 \t 6.448946 \t6.653674 \t 0.009587 \t 330.360471 s\n","768 \t 12.888973 \t 6.725358 \t6.152907 \t 0.010708 \t 330.786420 s\n","769 \t 12.931725 \t 6.537390 \t6.388453 \t 0.005882 \t 331.228640 s\n","770 \t 13.027269 \t 6.350995 \t6.666433 \t 0.009842 \t 331.567635 s\n","771 \t 13.073076 \t 6.666153 \t6.402319 \t 0.004604 \t 331.906252 s\n","772 \t 13.148765 \t 6.604887 \t6.536591 \t 0.007287 \t 332.237708 s\n","773 \t 13.381103 \t 6.588944 \t6.785235 \t 0.006924 \t 332.583732 s\n","774 \t 13.134814 \t 6.822071 \t6.299484 \t 0.013259 \t 333.069243 s\n","775 \t 12.968400 \t 6.446170 \t6.506636 \t 0.015594 \t 333.409803 s\n","776 \t 13.643201 \t 6.719575 \t6.907100 \t 0.016526 \t 333.744489 s\n","777 \t 13.435938 \t 6.803763 \t6.625698 \t 0.006477 \t 334.096977 s\n","778 \t 13.209860 \t 6.474958 \t6.716119 \t 0.018784 \t 334.431278 s\n","779 \t 13.477937 \t 6.539596 \t6.932206 \t 0.006135 \t 334.769015 s\n","780 \t 13.407576 \t 6.894192 \t6.504900 \t 0.008484 \t 335.186922 s\n","781 \t 13.314110 \t 6.948223 \t6.360118 \t 0.005769 \t 335.599556 s\n","782 \t 13.131711 \t 6.599864 \t6.524003 \t 0.007844 \t 336.036483 s\n","783 \t 13.251060 \t 6.672796 \t6.572022 \t 0.006241 \t 336.629623 s\n","784 \t 12.975233 \t 6.524422 \t6.442289 \t 0.008522 \t 337.055279 s\n","785 \t 13.081038 \t 6.432387 \t6.637081 \t 0.011571 \t 337.522935 s\n","786 \t 13.368087 \t 6.747958 \t6.614556 \t 0.005573 \t 337.984254 s\n","787 \t 13.023166 \t 6.434041 \t6.582038 \t 0.007087 \t 338.336284 s\n","788 \t 13.444090 \t 6.744283 \t6.687879 \t 0.011928 \t 338.674548 s\n","789 \t 13.813087 \t 6.743711 \t7.063983 \t 0.005393 \t 339.018663 s\n","790 \t 12.847063 \t 6.249510 \t6.581088 \t 0.016466 \t 339.368906 s\n","791 \t 13.433406 \t 6.880083 \t6.546629 \t 0.006694 \t 339.708782 s\n","792 \t 12.838698 \t 6.487263 \t6.346014 \t 0.005422 \t 340.048094 s\n","793 \t 13.063419 \t 6.411937 \t6.643781 \t 0.007702 \t 340.532618 s\n","794 \t 13.300636 \t 6.589702 \t6.697491 \t 0.013444 \t 340.869174 s\n","795 \t 13.284612 \t 6.729753 \t6.547604 \t 0.007254 \t 341.207490 s\n","796 \t 12.985854 \t 6.536061 \t6.436603 \t 0.013191 \t 341.543504 s\n","797 \t 13.362905 \t 7.001907 \t6.355403 \t 0.005595 \t 341.877427 s\n","798 \t 13.042004 \t 6.499354 \t6.530412 \t 0.012239 \t 342.217891 s\n","799 \t 12.796999 \t 6.242736 \t6.550155 \t 0.004109 \t 342.552882 s\n","800 \t 13.017953 \t 6.680304 \t6.305851 \t 0.031798 \t 342.884796 s\n","801 \t 12.995877 \t 6.391227 \t6.597982 \t 0.006668 \t 343.238981 s\n","802 \t 12.871225 \t 6.478957 \t6.384154 \t 0.008114 \t 343.710219 s\n","803 \t 13.031145 \t 6.591086 \t6.429123 \t 0.010936 \t 344.040915 s\n","804 \t 12.736111 \t 6.210991 \t6.515280 \t 0.009839 \t 344.390241 s\n","805 \t 13.206776 \t 6.872255 \t6.326523 \t 0.007998 \t 344.727334 s\n","806 \t 13.025454 \t 6.523630 \t6.494567 \t 0.007258 \t 345.062291 s\n","807 \t 13.098754 \t 6.662150 \t6.425914 \t 0.010690 \t 345.404977 s\n","808 \t 12.990132 \t 6.522164 \t6.461795 \t 0.006173 \t 345.736625 s\n","809 \t 13.541061 \t 6.693290 \t6.827747 \t 0.020024 \t 346.072294 s\n","810 \t 12.702034 \t 6.398275 \t6.298028 \t 0.005731 \t 346.415371 s\n","811 \t 12.595525 \t 6.442002 \t6.147830 \t 0.005693 \t 346.748380 s\n","812 \t 13.566229 \t 6.782671 \t6.779841 \t 0.003716 \t 347.226393 s\n","813 \t 13.141021 \t 6.498058 \t6.637216 \t 0.005746 \t 347.583081 s\n","814 \t 13.591458 \t 6.973808 \t6.609695 \t 0.007956 \t 347.920729 s\n","815 \t 12.807596 \t 6.491322 \t6.311763 \t 0.004511 \t 348.410804 s\n","816 \t 12.732852 \t 6.439439 \t6.273936 \t 0.019477 \t 348.826582 s\n","817 \t 13.032952 \t 6.467109 \t6.561996 \t 0.003846 \t 349.279769 s\n","818 \t 12.766117 \t 6.402158 \t6.352756 \t 0.011203 \t 349.712521 s\n","819 \t 13.148770 \t 6.270251 \t6.873581 \t 0.004938 \t 350.131658 s\n","820 \t 13.056651 \t 6.639463 \t6.408574 \t 0.008614 \t 350.570484 s\n","821 \t 12.717256 \t 6.310785 \t6.391092 \t 0.015379 \t 351.047073 s\n","822 \t 12.799168 \t 6.329654 \t6.459189 \t 0.010326 \t 351.569356 s\n","823 \t 12.560141 \t 6.212008 \t6.341599 \t 0.006534 \t 351.911180 s\n","824 \t 12.994054 \t 6.673998 \t6.307942 \t 0.012114 \t 352.246671 s\n","825 \t 12.753592 \t 6.442215 \t6.304271 \t 0.007106 \t 352.596083 s\n","826 \t 13.164477 \t 6.573701 \t6.580633 \t 0.010143 \t 352.934281 s\n","827 \t 13.458132 \t 6.618912 \t6.826236 \t 0.012984 \t 353.270110 s\n","828 \t 13.749723 \t 6.688799 \t7.051885 \t 0.009039 \t 353.622189 s\n","829 \t 12.992408 \t 6.687900 \t6.300697 \t 0.003810 \t 353.955576 s\n","830 \t 13.335472 \t 6.667202 \t6.661442 \t 0.006828 \t 354.295624 s\n","831 \t 13.080143 \t 6.588381 \t6.485092 \t 0.006670 \t 354.649464 s\n","832 \t 13.193016 \t 6.619622 \t6.568512 \t 0.004882 \t 355.120194 s\n","833 \t 13.308289 \t 6.605561 \t6.698708 \t 0.004020 \t 355.451583 s\n","834 \t 12.798635 \t 6.417658 \t6.374028 \t 0.006949 \t 355.792665 s\n","835 \t 13.345399 \t 6.617933 \t6.721591 \t 0.005876 \t 356.126843 s\n","836 \t 13.162953 \t 6.516987 \t6.635506 \t 0.010460 \t 356.465605 s\n","837 \t 12.831935 \t 6.371388 \t6.449830 \t 0.010717 \t 356.806401 s\n","838 \t 13.219589 \t 6.554451 \t6.658402 \t 0.006736 \t 357.138689 s\n","839 \t 12.775994 \t 6.261233 \t6.498426 \t 0.016335 \t 357.482321 s\n","840 \t 12.622762 \t 6.065406 \t6.550969 \t 0.006387 \t 357.821713 s\n","841 \t 13.354697 \t 6.619246 \t6.728489 \t 0.006963 \t 358.157451 s\n","842 \t 13.133870 \t 6.640911 \t6.488901 \t 0.004058 \t 358.632487 s\n","843 \t 13.151662 \t 6.596627 \t6.547406 \t 0.007629 \t 358.989389 s\n","844 \t 12.606613 \t 6.279148 \t6.315887 \t 0.011578 \t 359.327660 s\n","845 \t 12.755445 \t 6.644294 \t6.104173 \t 0.006979 \t 359.665379 s\n","846 \t 13.208787 \t 6.427403 \t6.775110 \t 0.006275 \t 360.016155 s\n","847 \t 13.126363 \t 6.696263 \t6.426327 \t 0.003774 \t 360.352235 s\n","848 \t 12.715623 \t 6.517789 \t6.191517 \t 0.006317 \t 360.699571 s\n","849 \t 12.980150 \t 6.626862 \t6.346825 \t 0.006463 \t 361.050916 s\n","850 \t 12.747814 \t 6.256167 \t6.486070 \t 0.005576 \t 361.451511 s\n","851 \t 12.689566 \t 6.244141 \t6.435321 \t 0.010104 \t 361.882278 s\n","852 \t 12.910001 \t 6.463670 \t6.439045 \t 0.007286 \t 362.491705 s\n","853 \t 13.523037 \t 6.638725 \t6.878763 \t 0.005549 \t 362.923408 s\n","854 \t 13.052927 \t 6.513364 \t6.530575 \t 0.008988 \t 363.336746 s\n","855 \t 13.084916 \t 6.357533 \t6.723196 \t 0.004188 \t 363.812262 s\n","856 \t 12.978446 \t 6.698070 \t6.270430 \t 0.009945 \t 364.278437 s\n","857 \t 13.310884 \t 6.618327 \t6.682114 \t 0.010443 \t 364.617282 s\n","858 \t 13.046969 \t 6.678387 \t6.363023 \t 0.005558 \t 364.950141 s\n","859 \t 12.885251 \t 6.601599 \t6.277735 \t 0.005918 \t 365.303835 s\n","860 \t 13.014968 \t 6.506834 \t6.503195 \t 0.004940 \t 365.639455 s\n","861 \t 13.010782 \t 6.647318 \t6.357100 \t 0.006365 \t 365.974525 s\n","862 \t 12.625124 \t 6.359902 \t6.259248 \t 0.005974 \t 366.471425 s\n","863 \t 12.755225 \t 6.391383 \t6.348151 \t 0.015691 \t 366.804170 s\n","864 \t 12.580464 \t 6.194834 \t6.369104 \t 0.016526 \t 367.148725 s\n","865 \t 13.009050 \t 6.492303 \t6.511133 \t 0.005614 \t 367.483731 s\n","866 \t 13.096731 \t 6.507583 \t6.582571 \t 0.006577 \t 367.817424 s\n","867 \t 12.905743 \t 6.366397 \t6.536223 \t 0.003122 \t 368.163961 s\n","868 \t 13.295842 \t 6.669933 \t6.618754 \t 0.007155 \t 368.497691 s\n","869 \t 13.176634 \t 6.595444 \t6.564914 \t 0.016276 \t 368.831165 s\n","870 \t 12.766942 \t 6.360506 \t6.401304 \t 0.005132 \t 369.171043 s\n","871 \t 12.756841 \t 6.339745 \t6.413259 \t 0.003837 \t 369.511977 s\n","872 \t 12.501248 \t 6.310580 \t6.181000 \t 0.009669 \t 369.982034 s\n","873 \t 12.528862 \t 6.339412 \t6.180109 \t 0.009340 \t 370.324780 s\n","874 \t 13.133533 \t 6.626184 \t6.502742 \t 0.004607 \t 370.662478 s\n","875 \t 13.289804 \t 6.507034 \t6.778507 \t 0.004262 \t 371.000182 s\n","876 \t 13.426096 \t 6.610849 \t6.810265 \t 0.004982 \t 371.347856 s\n","877 \t 13.090804 \t 6.393212 \t6.690632 \t 0.006960 \t 371.686343 s\n","878 \t 13.229505 \t 6.686551 \t6.519193 \t 0.023760 \t 372.017898 s\n","879 \t 12.788471 \t 6.494470 \t6.287476 \t 0.006526 \t 372.357427 s\n","880 \t 12.940048 \t 6.317919 \t6.603933 \t 0.018196 \t 372.699250 s\n","881 \t 12.970522 \t 6.647263 \t6.316734 \t 0.006526 \t 373.037342 s\n","882 \t 13.234365 \t 6.402587 \t6.825146 \t 0.006631 \t 373.534803 s\n","883 \t 13.094744 \t 6.511184 \t6.567223 \t 0.016337 \t 373.871778 s\n","884 \t 13.451104 \t 6.943657 \t6.501862 \t 0.005584 \t 374.211726 s\n","885 \t 12.939123 \t 6.250367 \t6.681723 \t 0.007034 \t 374.657655 s\n","886 \t 12.579775 \t 6.413666 \t6.160969 \t 0.005139 \t 375.072129 s\n","887 \t 12.645844 \t 6.265626 \t6.374362 \t 0.005856 \t 375.517304 s\n","888 \t 12.915198 \t 6.568901 \t6.342165 \t 0.004132 \t 375.931615 s\n","889 \t 12.698788 \t 6.155881 \t6.538523 \t 0.004385 \t 376.358724 s\n","890 \t 13.422309 \t 6.964566 \t6.452904 \t 0.004839 \t 376.797410 s\n","891 \t 13.220388 \t 6.606832 \t6.608803 \t 0.004753 \t 377.255878 s\n","892 \t 12.817818 \t 6.447789 \t6.365078 \t 0.004950 \t 377.774384 s\n","893 \t 12.935073 \t 6.542712 \t6.389094 \t 0.003267 \t 378.125777 s\n","894 \t 12.477202 \t 6.142492 \t6.329280 \t 0.005429 \t 378.470143 s\n","895 \t 12.579105 \t 6.471757 \t6.087228 \t 0.020121 \t 378.822400 s\n","896 \t 13.480318 \t 6.671228 \t6.800579 \t 0.008511 \t 379.155361 s\n","897 \t 13.218294 \t 6.837119 \t6.375347 \t 0.005828 \t 379.490337 s\n","898 \t 13.080553 \t 6.599503 \t6.472015 \t 0.009036 \t 379.837518 s\n","899 \t 13.388149 \t 6.875274 \t6.506342 \t 0.006533 \t 380.177542 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:06<00:00, 21.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4934\n","Hit@10: 0.6115\n","Hit@3: 0.5115\n","Hit@1: 0.4308\n","Link Prediction on Validation Set (All)\n","MRR: 0.4934\n","Hit@10: 0.6115\n","Hit@3: 0.5115\n","Hit@1: 0.4308\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2999\n","Hit@10: 0.4923\n","Hit@3: 0.3308\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6234\n","Hit@10: 0.7284\n","Hit@3: 0.6420\n","Hit@1: 0.5720\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0464\n","900 \t 13.196531 \t 6.666519 \t6.507659 \t 0.022353 \t 387.214460 s\n","901 \t 12.844793 \t 6.624139 \t6.207880 \t 0.012774 \t 387.613680 s\n","902 \t 13.299679 \t 6.601347 \t6.693972 \t 0.004360 \t 388.237553 s\n","903 \t 12.823082 \t 6.298141 \t6.519402 \t 0.005541 \t 388.685903 s\n","904 \t 12.884723 \t 6.325267 \t6.551762 \t 0.007695 \t 389.126031 s\n","905 \t 13.277358 \t 6.401571 \t6.872014 \t 0.003773 \t 389.535958 s\n","906 \t 12.752525 \t 6.369138 \t6.374653 \t 0.008734 \t 390.003977 s\n","907 \t 13.131019 \t 6.297302 \t6.830144 \t 0.003572 \t 390.466658 s\n","908 \t 12.606534 \t 6.314861 \t6.265119 \t 0.026554 \t 390.813396 s\n","909 \t 12.809040 \t 6.266605 \t6.537815 \t 0.004620 \t 391.172504 s\n","910 \t 12.932687 \t 6.349011 \t6.565952 \t 0.017725 \t 391.519755 s\n","911 \t 13.259606 \t 6.510718 \t6.740369 \t 0.008519 \t 391.856164 s\n","912 \t 13.305686 \t 6.541228 \t6.759030 \t 0.005429 \t 392.328579 s\n","913 \t 13.171254 \t 6.804850 \t6.360431 \t 0.005973 \t 392.674663 s\n","914 \t 13.106576 \t 6.202731 \t6.892606 \t 0.011240 \t 393.011534 s\n","915 \t 13.140646 \t 6.710341 \t6.423805 \t 0.006499 \t 393.480426 s\n","916 \t 13.016765 \t 6.505455 \t6.506885 \t 0.004424 \t 393.902914 s\n","917 \t 12.970748 \t 6.648082 \t6.314575 \t 0.008091 \t 394.325156 s\n","918 \t 13.023123 \t 6.457401 \t6.561988 \t 0.003733 \t 394.728581 s\n","919 \t 13.003992 \t 6.318011 \t6.679674 \t 0.006306 \t 395.059047 s\n","920 \t 13.310858 \t 6.752347 \t6.553194 \t 0.005317 \t 395.396784 s\n","921 \t 12.653429 \t 6.387301 \t6.258946 \t 0.007182 \t 395.729324 s\n","922 \t 13.074429 \t 6.471015 \t6.592780 \t 0.010634 \t 396.206662 s\n","923 \t 12.914743 \t 6.561731 \t6.348406 \t 0.004607 \t 396.559943 s\n","924 \t 12.661081 \t 6.204467 \t6.452683 \t 0.003931 \t 396.902953 s\n","925 \t 12.361398 \t 6.286185 \t6.065466 \t 0.009748 \t 397.235117 s\n","926 \t 12.831021 \t 6.281623 \t6.541604 \t 0.007795 \t 397.591924 s\n","927 \t 12.889537 \t 6.515638 \t6.351856 \t 0.022044 \t 397.923912 s\n","928 \t 12.677223 \t 6.400347 \t6.270670 \t 0.006205 \t 398.263187 s\n","929 \t 13.141454 \t 6.504678 \t6.629133 \t 0.007643 \t 398.615362 s\n","930 \t 12.831175 \t 6.500025 \t6.323881 \t 0.007270 \t 398.950971 s\n","931 \t 12.919238 \t 6.603959 \t6.310990 \t 0.004288 \t 399.308397 s\n","932 \t 12.976021 \t 6.473318 \t6.496582 \t 0.006121 \t 399.666483 s\n","933 \t 12.878469 \t 6.344851 \t6.526906 \t 0.006711 \t 400.004577 s\n","934 \t 12.430682 \t 6.188341 \t6.238034 \t 0.004306 \t 400.528334 s\n","935 \t 13.443995 \t 6.588949 \t6.841651 \t 0.013394 \t 400.975052 s\n","936 \t 12.987489 \t 6.347474 \t6.635392 \t 0.004623 \t 401.391189 s\n","937 \t 12.791227 \t 6.186340 \t6.598876 \t 0.006011 \t 401.852511 s\n","938 \t 12.348142 \t 6.251637 \t6.091324 \t 0.005180 \t 402.267842 s\n","939 \t 13.152095 \t 6.507941 \t6.640896 \t 0.003258 \t 402.689669 s\n","940 \t 13.077916 \t 6.420373 \t6.653830 \t 0.003712 \t 403.158500 s\n","941 \t 12.742475 \t 6.312120 \t6.426897 \t 0.003458 \t 403.619178 s\n","942 \t 12.932609 \t 6.457145 \t6.471338 \t 0.004126 \t 403.980529 s\n","943 \t 13.080729 \t 6.475356 \t6.601723 \t 0.003650 \t 404.315169 s\n","944 \t 13.399584 \t 6.816556 \t6.573727 \t 0.009301 \t 404.652382 s\n","945 \t 12.846382 \t 6.561587 \t6.273300 \t 0.011495 \t 405.147690 s\n","946 \t 13.042987 \t 6.807534 \t6.231635 \t 0.003818 \t 405.484027 s\n","947 \t 13.473853 \t 6.729545 \t6.739228 \t 0.005080 \t 405.825732 s\n","948 \t 13.002877 \t 6.433352 \t6.565949 \t 0.003576 \t 406.166003 s\n","949 \t 12.980510 \t 6.516990 \t6.458655 \t 0.004865 \t 406.498439 s\n","950 \t 12.754912 \t 6.311229 \t6.440049 \t 0.003635 \t 406.838039 s\n","951 \t 13.207057 \t 6.628973 \t6.572076 \t 0.006008 \t 407.169311 s\n","952 \t 12.737722 \t 6.366491 \t6.358764 \t 0.012466 \t 407.506038 s\n","953 \t 12.936553 \t 6.539709 \t6.394297 \t 0.002547 \t 407.856367 s\n","954 \t 13.159671 \t 6.556652 \t6.587407 \t 0.015612 \t 408.203071 s\n","955 \t 12.681092 \t 6.316260 \t6.347397 \t 0.017436 \t 408.563178 s\n","956 \t 13.056189 \t 6.511570 \t6.537425 \t 0.007194 \t 409.037245 s\n","957 \t 12.757743 \t 6.501820 \t6.251615 \t 0.004309 \t 409.378349 s\n","958 \t 12.751570 \t 6.439094 \t6.303295 \t 0.009181 \t 409.724625 s\n","959 \t 12.718524 \t 6.299369 \t6.414927 \t 0.004228 \t 410.065968 s\n","960 \t 12.386028 \t 6.198726 \t6.177128 \t 0.010175 \t 410.405404 s\n","961 \t 13.146693 \t 6.627070 \t6.514264 \t 0.005358 \t 410.744240 s\n","962 \t 12.713470 \t 6.340857 \t6.366740 \t 0.005873 \t 411.081877 s\n","963 \t 13.396297 \t 6.577244 \t6.816149 \t 0.002904 \t 411.418496 s\n","964 \t 13.074010 \t 6.572041 \t6.497174 \t 0.004796 \t 411.761281 s\n","965 \t 12.904600 \t 6.615574 \t6.285054 \t 0.003972 \t 412.102486 s\n","966 \t 12.803393 \t 6.739798 \t6.060124 \t 0.003471 \t 412.577792 s\n","967 \t 12.462204 \t 6.286373 \t6.169493 \t 0.006338 \t 412.918384 s\n","968 \t 13.468923 \t 6.701174 \t6.761556 \t 0.006193 \t 413.269857 s\n","969 \t 13.331787 \t 6.697806 \t6.629488 \t 0.004493 \t 413.618846 s\n","970 \t 12.729902 \t 6.244224 \t6.478714 \t 0.006964 \t 414.072659 s\n","971 \t 12.935854 \t 6.707345 \t6.223068 \t 0.005441 \t 414.482613 s\n","972 \t 13.246387 \t 6.697208 \t6.544037 \t 0.005143 \t 414.925597 s\n","973 \t 12.766984 \t 6.452861 \t6.309396 \t 0.004727 \t 415.358466 s\n","974 \t 13.258952 \t 6.756018 \t6.495708 \t 0.007225 \t 415.771479 s\n","975 \t 12.772330 \t 6.417139 \t6.348938 \t 0.006253 \t 416.220340 s\n","976 \t 13.092545 \t 6.426486 \t6.662307 \t 0.003752 \t 416.865722 s\n","977 \t 12.933028 \t 6.200787 \t6.727237 \t 0.005004 \t 417.196314 s\n","978 \t 13.267896 \t 6.450660 \t6.812289 \t 0.004946 \t 417.546407 s\n","979 \t 12.520122 \t 6.219298 \t6.293060 \t 0.007763 \t 417.882081 s\n","980 \t 12.766054 \t 6.231978 \t6.530145 \t 0.003931 \t 418.219504 s\n","981 \t 12.849375 \t 6.376359 \t6.468488 \t 0.004528 \t 418.571652 s\n","982 \t 12.884955 \t 6.629166 \t6.249855 \t 0.005934 \t 418.911793 s\n","983 \t 12.799007 \t 6.396655 \t6.396022 \t 0.006330 \t 419.245721 s\n","984 \t 12.889039 \t 6.313063 \t6.570569 \t 0.005407 \t 419.602479 s\n","985 \t 12.766168 \t 6.528722 \t6.232380 \t 0.005066 \t 419.935287 s\n","986 \t 12.517100 \t 5.993268 \t6.515759 \t 0.008072 \t 420.407183 s\n","987 \t 13.128064 \t 6.626302 \t6.491154 \t 0.010607 \t 420.742829 s\n","988 \t 12.974043 \t 6.351892 \t6.618733 \t 0.003419 \t 421.076365 s\n","989 \t 12.835743 \t 6.389239 \t6.431883 \t 0.014621 \t 421.417211 s\n","990 \t 12.489992 \t 6.262798 \t6.222229 \t 0.004965 \t 421.752388 s\n","991 \t 12.799273 \t 6.493694 \t6.301652 \t 0.003927 \t 422.090015 s\n","992 \t 12.776259 \t 6.483433 \t6.288855 \t 0.003971 \t 422.438525 s\n","993 \t 12.650574 \t 6.229449 \t6.415803 \t 0.005323 \t 422.775913 s\n","994 \t 12.977835 \t 6.437533 \t6.537479 \t 0.002823 \t 423.106304 s\n","995 \t 12.763956 \t 6.379885 \t6.380433 \t 0.003638 \t 423.453358 s\n","996 \t 13.052835 \t 6.512455 \t6.530410 \t 0.009971 \t 423.945589 s\n","997 \t 13.159170 \t 6.564172 \t6.588598 \t 0.006400 \t 424.284838 s\n","998 \t 12.659758 \t 6.501933 \t6.154303 \t 0.003522 \t 424.633691 s\n","999 \t 12.684223 \t 6.357269 \t6.322723 \t 0.004231 \t 424.968092 s\n","1000 \t 13.101882 \t 6.609140 \t6.482820 \t 0.009923 \t 425.300220 s\n","1001 \t 12.866971 \t 6.428400 \t6.434177 \t 0.004394 \t 425.644232 s\n","1002 \t 12.833309 \t 6.486446 \t6.340114 \t 0.006748 \t 425.978744 s\n","1003 \t 12.730354 \t 6.294714 \t6.430501 \t 0.005139 \t 426.312130 s\n","1004 \t 13.090244 \t 6.471073 \t6.605402 \t 0.013769 \t 426.661157 s\n","1005 \t 13.466913 \t 6.645452 \t6.817852 \t 0.003610 \t 427.071446 s\n","1006 \t 12.835567 \t 6.510373 \t6.321631 \t 0.003564 \t 427.481675 s\n","1007 \t 12.631608 \t 6.206660 \t6.419639 \t 0.005309 \t 428.089045 s\n","1008 \t 12.958554 \t 6.389883 \t6.564227 \t 0.004444 \t 428.503009 s\n","1009 \t 13.245968 \t 6.651460 \t6.586532 \t 0.007976 \t 428.943939 s\n","1010 \t 12.803476 \t 6.564553 \t6.232083 \t 0.006840 \t 429.406472 s\n","1011 \t 12.723537 \t 6.546666 \t6.172388 \t 0.004484 \t 429.866838 s\n","1012 \t 12.796548 \t 6.329623 \t6.459785 \t 0.007141 \t 430.200436 s\n","1013 \t 13.226777 \t 6.667050 \t6.556397 \t 0.003330 \t 430.535769 s\n","1014 \t 12.795583 \t 6.362237 \t6.427939 \t 0.005407 \t 430.875988 s\n","1015 \t 12.752875 \t 6.221712 \t6.524468 \t 0.006695 \t 431.208654 s\n","1016 \t 12.849806 \t 6.365129 \t6.479083 \t 0.005594 \t 431.539434 s\n","1017 \t 13.511409 \t 6.755558 \t6.750482 \t 0.005369 \t 431.888173 s\n","1018 \t 12.530370 \t 6.355974 \t6.160524 \t 0.013872 \t 432.363160 s\n","1019 \t 12.488503 \t 6.215835 \t6.261703 \t 0.010966 \t 432.703911 s\n","1020 \t 12.721262 \t 6.268897 \t6.445583 \t 0.006782 \t 433.039108 s\n","1021 \t 12.888798 \t 6.434763 \t6.448175 \t 0.005860 \t 433.387640 s\n","1022 \t 12.878905 \t 6.416210 \t6.456537 \t 0.006158 \t 433.719832 s\n","1023 \t 12.849922 \t 6.354461 \t6.491824 \t 0.003637 \t 434.066462 s\n","1024 \t 12.926023 \t 6.502228 \t6.419450 \t 0.004345 \t 434.410229 s\n","1025 \t 12.882672 \t 6.684258 \t6.194849 \t 0.003565 \t 434.748397 s\n","1026 \t 12.611903 \t 6.303950 \t6.301616 \t 0.006337 \t 435.085491 s\n","1027 \t 12.964114 \t 6.406001 \t6.552942 \t 0.005171 \t 435.421639 s\n","1028 \t 12.805061 \t 6.712761 \t6.086175 \t 0.006124 \t 435.901318 s\n","1029 \t 13.265504 \t 6.499788 \t6.759621 \t 0.006095 \t 436.266731 s\n","1030 \t 13.037757 \t 6.555223 \t6.477537 \t 0.004997 \t 436.610483 s\n","1031 \t 12.679089 \t 6.292604 \t6.380970 \t 0.005516 \t 436.946010 s\n","1032 \t 12.823942 \t 6.364698 \t6.456146 \t 0.003098 \t 437.298459 s\n","1033 \t 12.756635 \t 6.169049 \t6.581548 \t 0.006039 \t 437.631999 s\n","1034 \t 13.076831 \t 6.521340 \t6.548715 \t 0.006777 \t 437.970262 s\n","1035 \t 13.409992 \t 6.805251 \t6.599758 \t 0.004983 \t 438.339278 s\n","1036 \t 12.665691 \t 6.392820 \t6.269441 \t 0.003431 \t 438.690023 s\n","1037 \t 12.378879 \t 6.040891 \t6.328216 \t 0.009772 \t 439.026435 s\n","1038 \t 12.812405 \t 6.231028 \t6.577072 \t 0.004305 \t 439.376682 s\n","1039 \t 13.038276 \t 6.923932 \t6.110548 \t 0.003795 \t 439.851319 s\n","1040 \t 13.508538 \t 6.601701 \t6.902482 \t 0.004354 \t 440.307649 s\n","1041 \t 12.672554 \t 6.138509 \t6.530214 \t 0.003830 \t 440.723481 s\n","1042 \t 12.643048 \t 6.258818 \t6.380177 \t 0.004054 \t 441.174364 s\n","1043 \t 12.530286 \t 6.240200 \t6.285445 \t 0.004641 \t 441.601394 s\n","1044 \t 13.118743 \t 6.520721 \t6.591033 \t 0.006990 \t 442.023050 s\n","1045 \t 12.861610 \t 6.474950 \t6.380109 \t 0.006550 \t 442.455701 s\n","1046 \t 13.024918 \t 6.731229 \t6.290074 \t 0.003615 \t 442.927164 s\n","1047 \t 12.888748 \t 6.346664 \t6.537288 \t 0.004796 \t 443.310784 s\n","1048 \t 13.059178 \t 6.590140 \t6.463469 \t 0.005569 \t 443.672193 s\n","1049 \t 13.008548 \t 6.516343 \t6.489108 \t 0.003097 \t 444.008672 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:06<00:00, 21.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5049\n","Hit@10: 0.6231\n","Hit@3: 0.5231\n","Hit@1: 0.4462\n","Link Prediction on Validation Set (All)\n","MRR: 0.5049\n","Hit@10: 0.6231\n","Hit@3: 0.5231\n","Hit@1: 0.4462\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3207\n","Hit@10: 0.4923\n","Hit@3: 0.3692\n","Hit@1: 0.2231\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6345\n","Hit@10: 0.7284\n","Hit@3: 0.6626\n","Hit@1: 0.5802\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0344\n"]}]},{"cell_type":"markdown","source":["# Test.py"],"metadata":{"id":"p0M6asfiPDtD"}},{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","import argparse\n","import torch\n","import torch.nn as nn\n","import datetime\n","import time\n","import os\n","import copy\n","import math\n","import random\n","\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","KG = HNKG(args.data, test= True)\n","\n","batch_size = args.batch_size\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = batch_size, shuffle=True)\n","model = HyNT(\n","\tnum_ent = KG.num_ent,\n","\tnum_rel = KG.num_rel,\n","    dim_model = args.dim,\n","    num_head = args.num_head,\n","    dim_hid = args.hidden_dim,\n","    num_enc_layer = args.num_enc_layer,\n","    num_dec_layer = args.num_dec_layer,\n","    dropout = args.dropout,\n","    emb_as_proj = args.emb_as_proj\n",").cuda()\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\" + \\\n","              f\"elayer_{args.num_enc_layer}_dlayer_{args.num_dec_layer}_head_{args.num_head}_hid_{args.hidden_dim}_\" + \\\n","              f\"drop_{args.dropout}_smoothing_{args.smoothing}_batch_{args.batch_size}_\" + \\\n","              f\"steplr_{args.step_size}\"\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\n","if not args.no_write:\n","\tos.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","\twith open(f\"./result/{file_format}_test.txt\", \"w\") as f:\n","\t\tf.write(f\"{datetime.datetime.now()}\\n\")\n","\n","model_path = f\"./checkpoint/{file_format}_{test_epoch}.ckpt\"\n","\n","def load_id_mapping(file_path):\n","    id2name = {}\n","    with open(drive_dir + dataset_dir + file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip() == \"\" or line.startswith(\"#\"):  # 주석 또는 공백 무시\n","                continue\n","            parts = line.strip().split('\\t')\n","            if len(parts) != 2:\n","                continue\n","            name, idx = parts\n","            id2name[int(idx)] = name\n","    return id2name\n","\n","id2ent = load_id_mapping(\"entity2id.txt\")\n","id2rel = load_id_mapping(\"relation2id.txt\")\n","\n","def convert_triplet_ids_to_names(triplet, id2ent, id2rel, num_ent, num_rel):\n","    triplet_named = []\n","    for idx, val in enumerate(triplet):\n","        if idx % 2 == 0:  # entity or numeric value\n","            if val < num_ent:\n","                triplet_named.append(id2ent.get(val, f\"[ENT:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[NUM:{val - num_ent}]\")\n","        else:  # relation\n","            if val < num_rel:\n","                triplet_named.append(id2rel.get(val, f\"[REL:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[MASK_REL]\")\n","    return triplet_named\n","\n","model.load_state_dict(torch.load(f\"./checkpoint/{file_format}_{args.epoch}.ckpt\")[\"model_state_dict\"])\n","\n","### EVALUATION ###\n","model.eval()\n","lp_all_list_rank = []\n","lp_tri_list_rank = []\n","\n","rp_all_list_rank = []\n","rp_tri_list_rank = []\n","\n","nvp_tri_se = 0\n","nvp_tri_se_num = 0\n","nvp_all_se = 0\n","nvp_all_se_num = 0\n","\n","with torch.no_grad():\n","    entity_pred_log = []\n","    relation_pred_log = []\n","    numeric_pred_log = []\n","\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2):\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","            test_triplet = torch.tensor([tri])\n","\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent:\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1:\n","                    # sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    numeric_pred_log.append({\n","                      \"triplet_id\": str(tri),\n","                      \"triplet_named\": \":\".join(named_triplet),\n","                      \"position\": ent_idx,\n","                      \"type\": \"triplet\",\n","                      \"gt\": float(gt),\n","                      \"pred\": float(pred),\n","                      \"se\": float(sq_error)\n","                    })\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                else:\n","                    pred = score_num[0, 2, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    numeric_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"qualifier\",\n","                        \"gt\": float(gt),\n","                        \"pred\": float(pred),\n","                        \"se\": float(sq_error)\n","                    })\n","                    # sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            elif tri[ent_idx*2] < KG.num_ent:\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                    topk = np.argsort(-score_ent[0,1+2*ent_idx])[:5]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    try:\n","                      topk = np.argsort(-score_ent[0,2])[:5]\n","                    except:\n","                      topk = np.argsort(-score_ent[0,2])[:]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                lp_all_list_rank.append(rank)\n","\n","        for rel_idx in range(tri_len//2):\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent:\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                topk = np.argsort(-score_rel[0,2])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"relation\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                topk = np.argsort(-score_rel[0,1])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"qualifier\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            rp_all_list_rank.append(rank)\n","\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","if not args.no_write:\n","    with open(f\"./result/{file_format}_test.txt\", 'a') as f:\n","        f.write(f\"Epoch: {args.epoch}\\n\")\n","        f.write(f\"Link Prediction (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","\n","        f.write(f\"Relation Prediction (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction (All): {nvp_all_rmse:.4f}\\n\")\n","\n","os.makedirs(f\"./visualization/{file_format}_{test_epoch}\", exist_ok=True)\n","pd.DataFrame(entity_pred_log).to_csv(f\"./visualization/{file_format}_{test_epoch}/entity_predictions.csv\", index=False)\n","pd.DataFrame(relation_pred_log).to_csv(f\"./visualization/{file_format}_{test_epoch}/relation_predictions.csv\", index=False)\n","pd.DataFrame(numeric_pred_log).to_csv(f\"./visualization/{file_format}_{test_epoch}/numeric_predictions.csv\", index=False)"],"metadata":{"id":"z1vrsHJ4J-DL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749985470257,"user_tz":-540,"elapsed":8569,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"eebbc010-b187-463b-bb52-dee20173fa2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 132/132 [00:07<00:00, 16.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction (Tri)\n","MRR: 0.4914\n","Hit@10: 0.6061\n","Hit@3: 0.5000\n","Hit@1: 0.4356\n","Link Prediction (All)\n","MRR: 0.4914\n","Hit@10: 0.6061\n","Hit@3: 0.5000\n","Hit@1: 0.4356\n","Relation Prediction (Tri)\n","MRR: 0.2104\n","Hit@10: 0.3939\n","Hit@3: 0.2348\n","Hit@1: 0.1212\n","Relation Prediction (All)\n","MRR: 0.5756\n","Hit@10: 0.6800\n","Hit@3: 0.5920\n","Hit@1: 0.5240\n","Numeric Value Prediction (All)\n","RMSE: 0.0361\n"]}]}]}