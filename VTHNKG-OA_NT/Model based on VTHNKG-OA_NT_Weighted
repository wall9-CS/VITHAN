{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748302842607,"user_tz":-540,"elapsed":24989,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"1d99fd95-c363-40c5-b320-e73c37909c7c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","from collections import Counter\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1748302849808,"user_tz":-540,"elapsed":7203,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1748302849810,"user_tz":-540,"elapsed":11,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0: # numeric triplet 처리\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","        else:\n","            self.proj_ent_vis = nn.Identity()\n","            self.proj_rel_vis = nn.Identity()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1748302849871,"user_tz":-540,"elapsed":64,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1748302850135,"user_tz":-540,"elapsed":262,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-OA_NT/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile(self.dir + 'visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(self.dir + 'visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-OA_NT/\""],"metadata":{"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1748302850331,"user_tz":-540,"elapsed":195,"user":{"displayName":"URP","userId":"16515248769931109428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8915722-e5f7-477b-a41f-124866a2cb24"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-OA_NT\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNOA_NT_maximg==3_Weighted\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()"],"metadata":{"id":"HmgS2m1upzp1","executionInfo":{"status":"ok","timestamp":1748302850608,"user_tz":-540,"elapsed":247,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","rel_counts = Counter()\n","for tri in KG.train:\n","    for idx in range(1, len(tri), 2):\n","        rel_id = tri[idx]\n","        if rel_id < KG.num_rel:\n","            rel_counts[rel_id] += 1\n","\n","nSamples = np.zeros(KG.num_rel)\n","for rel_id, cnt in rel_counts.items():\n","    nSamples[rel_id] = cnt\n","\n","# 2. Weight 계산: 1 - (freq / 전체 freq 합)\n","sum_samples = np.sum(nSamples)\n","# 0 division 방지, 등장 안한 relation은 sum_samples로 처리(즉, 0 weight)\n","normedWeights = [1 - (x / sum_samples) if sum_samples > 0 else 0.0 for x in nSamples]\n","\n","# 3. tensor로 변환\n","normedWeights = torch.FloatTensor(normedWeights).cuda()  # device=GPU\n","\n","# 4. CrossEntropyLoss 정의\n","rel_criterion = nn.CrossEntropyLoss(weight=normedWeights, label_smoothing=args.smoothing)\n","\n","# entity는 기존처럼 smoothing만 사용\n","ent_criterion = nn.CrossEntropyLoss(label_smoothing=args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = ent_criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = rel_criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()"],"metadata":{"id":"1bX-xxnbmPYo","executionInfo":{"status":"ok","timestamp":1748303962571,"user_tz":-540,"elapsed":885689,"user":{"displayName":"URP","userId":"16515248769931109428"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"be570d09-d24b-4988-f30f-7ae52368af2a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 65.750591 \t 11.357917 \t11.863542 \t 42.529132 \t 0.709304 s\n","1 \t 34.135513 \t 10.968610 \t11.275203 \t 11.891700 \t 1.273698 s\n","2 \t 25.982759 \t 10.688506 \t10.909377 \t 4.384877 \t 1.876211 s\n","3 \t 21.725267 \t 10.428289 \t10.817648 \t 0.479331 \t 2.441886 s\n","4 \t 20.991350 \t 10.154064 \t10.592983 \t 0.244303 \t 3.069025 s\n","5 \t 20.952352 \t 10.305267 \t10.372894 \t 0.274190 \t 3.750382 s\n","6 \t 20.788983 \t 10.092206 \t10.318804 \t 0.377973 \t 4.425100 s\n","7 \t 20.740975 \t 10.114161 \t10.060773 \t 0.566041 \t 5.148934 s\n","8 \t 20.370638 \t 9.943271 \t10.114704 \t 0.312664 \t 6.043384 s\n","9 \t 20.410864 \t 9.942083 \t10.004083 \t 0.464697 \t 6.606409 s\n","10 \t 19.953690 \t 9.794649 \t9.979964 \t 0.179076 \t 7.199887 s\n","11 \t 20.018551 \t 9.789739 \t10.019686 \t 0.209126 \t 7.816305 s\n","12 \t 19.796346 \t 9.718571 \t9.812740 \t 0.265035 \t 8.381312 s\n","13 \t 20.129633 \t 9.846369 \t9.893784 \t 0.389481 \t 8.950091 s\n","14 \t 20.032908 \t 9.683895 \t9.965631 \t 0.383383 \t 9.518845 s\n","15 \t 19.426267 \t 9.650414 \t9.655707 \t 0.120146 \t 10.094928 s\n","16 \t 19.575629 \t 9.675542 \t9.697026 \t 0.203061 \t 10.650223 s\n","17 \t 19.750332 \t 9.630115 \t9.827766 \t 0.292450 \t 11.227626 s\n","18 \t 19.777252 \t 9.728761 \t9.846749 \t 0.201742 \t 11.952708 s\n","19 \t 19.510141 \t 9.417711 \t9.758270 \t 0.334159 \t 12.524522 s\n","20 \t 19.588467 \t 9.529886 \t9.817814 \t 0.240767 \t 13.091625 s\n","21 \t 19.324066 \t 9.479430 \t9.630599 \t 0.214037 \t 13.653443 s\n","22 \t 19.379582 \t 9.434051 \t9.664821 \t 0.280711 \t 14.219680 s\n","23 \t 19.109029 \t 9.343122 \t9.580632 \t 0.185275 \t 14.783027 s\n","24 \t 19.592127 \t 9.574673 \t9.798808 \t 0.218646 \t 15.359911 s\n","25 \t 19.604403 \t 9.528618 \t9.657997 \t 0.417788 \t 15.952066 s\n","26 \t 19.650855 \t 9.518055 \t9.769969 \t 0.362832 \t 16.837017 s\n","27 \t 19.342951 \t 9.414858 \t9.634511 \t 0.293582 \t 17.521108 s\n","28 \t 19.396437 \t 9.477292 \t9.690428 \t 0.228716 \t 18.228241 s\n","29 \t 19.434058 \t 9.462310 \t9.679096 \t 0.292652 \t 18.989205 s\n","30 \t 19.333126 \t 9.442846 \t9.682995 \t 0.207285 \t 19.568780 s\n","31 \t 19.052854 \t 9.177183 \t9.620957 \t 0.254714 \t 20.148992 s\n","32 \t 19.054945 \t 9.314512 \t9.444646 \t 0.295786 \t 20.738694 s\n","33 \t 19.227805 \t 9.346664 \t9.647554 \t 0.233587 \t 21.312848 s\n","34 \t 18.894474 \t 9.273582 \t9.337224 \t 0.283667 \t 21.881117 s\n","35 \t 19.109586 \t 9.303236 \t9.570979 \t 0.235371 \t 22.601197 s\n","36 \t 18.584047 \t 9.091776 \t9.333270 \t 0.159002 \t 23.160188 s\n","37 \t 18.859977 \t 9.185622 \t9.401188 \t 0.273167 \t 23.732853 s\n","38 \t 18.646877 \t 9.166405 \t9.302128 \t 0.178345 \t 24.299775 s\n","39 \t 18.782947 \t 9.217349 \t9.245605 \t 0.319994 \t 24.891437 s\n","40 \t 18.549960 \t 9.004325 \t9.300391 \t 0.245243 \t 25.463048 s\n","41 \t 18.455715 \t 9.192564 \t9.076868 \t 0.186283 \t 26.035276 s\n","42 \t 18.410280 \t 9.043347 \t9.203214 \t 0.163720 \t 26.609753 s\n","43 \t 18.302437 \t 8.995955 \t9.166443 \t 0.140039 \t 27.181754 s\n","44 \t 18.334983 \t 9.087295 \t9.044979 \t 0.202709 \t 27.756279 s\n","45 \t 18.429038 \t 9.119707 \t9.186728 \t 0.122604 \t 28.477707 s\n","46 \t 18.313592 \t 9.164996 \t9.012799 \t 0.135797 \t 29.085736 s\n","47 \t 18.472228 \t 9.212654 \t9.004368 \t 0.255207 \t 29.795782 s\n","48 \t 18.678909 \t 9.190883 \t9.244680 \t 0.243346 \t 30.501478 s\n","49 \t 18.080674 \t 8.920400 \t8.970631 \t 0.189643 \t 31.225987 s\n","50 \t 18.235151 \t 9.033577 \t9.022903 \t 0.178672 \t 31.992446 s\n","51 \t 18.207061 \t 8.935963 \t9.133763 \t 0.137334 \t 32.604566 s\n","52 \t 18.268232 \t 9.034912 \t9.081331 \t 0.151989 \t 33.185538 s\n","53 \t 18.028337 \t 8.942322 \t8.939266 \t 0.146749 \t 33.754540 s\n","54 \t 17.878619 \t 8.903120 \t8.841567 \t 0.133932 \t 34.337840 s\n","55 \t 17.771577 \t 8.945640 \t8.693298 \t 0.132639 \t 35.064097 s\n","56 \t 18.008969 \t 9.024856 \t8.807567 \t 0.176548 \t 35.631178 s\n","57 \t 18.077432 \t 9.024474 \t8.859287 \t 0.193671 \t 36.214604 s\n","58 \t 17.875879 \t 8.964612 \t8.775568 \t 0.135700 \t 36.804749 s\n","59 \t 17.952362 \t 8.841985 \t8.866914 \t 0.243463 \t 37.385358 s\n","60 \t 17.816246 \t 8.859334 \t8.764438 \t 0.192474 \t 37.982284 s\n","61 \t 17.919742 \t 8.949851 \t8.829786 \t 0.140105 \t 38.557974 s\n","62 \t 17.941548 \t 8.897560 \t8.799649 \t 0.244341 \t 39.139683 s\n","63 \t 17.748279 \t 8.976645 \t8.629553 \t 0.142081 \t 39.707881 s\n","64 \t 17.665376 \t 8.833303 \t8.749570 \t 0.082504 \t 40.451265 s\n","65 \t 17.592587 \t 8.905165 \t8.609029 \t 0.078393 \t 41.052876 s\n","66 \t 17.622479 \t 8.794900 \t8.693483 \t 0.134096 \t 41.624417 s\n","67 \t 17.626453 \t 8.878885 \t8.635947 \t 0.111622 \t 42.248735 s\n","68 \t 17.673824 \t 8.774396 \t8.763343 \t 0.136085 \t 42.959286 s\n","69 \t 17.651310 \t 8.806128 \t8.765897 \t 0.079285 \t 43.657103 s\n","70 \t 17.773741 \t 8.978302 \t8.685991 \t 0.109450 \t 44.399552 s\n","71 \t 17.824200 \t 8.978331 \t8.785295 \t 0.060574 \t 45.163648 s\n","72 \t 17.647182 \t 8.726372 \t8.814560 \t 0.106249 \t 45.910208 s\n","73 \t 17.503139 \t 8.784014 \t8.640308 \t 0.078817 \t 46.481895 s\n","74 \t 17.316327 \t 8.800359 \t8.431291 \t 0.084678 \t 47.050630 s\n","75 \t 17.680305 \t 8.833802 \t8.776030 \t 0.070473 \t 47.619517 s\n","76 \t 17.448391 \t 8.656594 \t8.671880 \t 0.119917 \t 48.183864 s\n","77 \t 17.890798 \t 8.896642 \t8.869187 \t 0.124968 \t 48.757635 s\n","78 \t 17.272567 \t 8.637277 \t8.525013 \t 0.110277 \t 49.329835 s\n","79 \t 17.522024 \t 8.731056 \t8.715765 \t 0.075205 \t 49.921212 s\n","80 \t 17.377769 \t 8.776986 \t8.531813 \t 0.068968 \t 50.499948 s\n","81 \t 17.358303 \t 8.778120 \t8.485969 \t 0.094216 \t 51.085156 s\n","82 \t 17.265432 \t 8.722832 \t8.451036 \t 0.091564 \t 51.818666 s\n","83 \t 17.421198 \t 8.718154 \t8.586053 \t 0.116991 \t 52.386239 s\n","84 \t 17.218523 \t 8.618299 \t8.500539 \t 0.099685 \t 52.973356 s\n","85 \t 17.263512 \t 8.646770 \t8.530250 \t 0.086492 \t 53.554313 s\n","86 \t 17.550115 \t 8.778175 \t8.690750 \t 0.081191 \t 54.125571 s\n","87 \t 17.114350 \t 8.603539 \t8.448670 \t 0.062142 \t 54.723379 s\n","88 \t 17.172914 \t 8.637555 \t8.418522 \t 0.116837 \t 55.332910 s\n","89 \t 17.401283 \t 8.674168 \t8.641345 \t 0.085769 \t 56.044631 s\n","90 \t 17.326989 \t 8.647958 \t8.589824 \t 0.089207 \t 56.737921 s\n","91 \t 16.943842 \t 8.455954 \t8.424010 \t 0.063878 \t 57.427187 s\n","92 \t 17.236942 \t 8.625910 \t8.542926 \t 0.068106 \t 58.397725 s\n","93 \t 17.207105 \t 8.613815 \t8.513899 \t 0.079391 \t 58.974535 s\n","94 \t 16.939419 \t 8.530891 \t8.363387 \t 0.045141 \t 59.545225 s\n","95 \t 17.024568 \t 8.531036 \t8.434196 \t 0.059335 \t 60.119722 s\n","96 \t 17.125176 \t 8.636695 \t8.436513 \t 0.051969 \t 60.701566 s\n","97 \t 17.199115 \t 8.667899 \t8.458653 \t 0.072564 \t 61.295214 s\n","98 \t 17.151440 \t 8.584691 \t8.468066 \t 0.098682 \t 61.885619 s\n","99 \t 17.037375 \t 8.547665 \t8.425565 \t 0.064145 \t 62.452891 s\n","100 \t 17.186260 \t 8.569915 \t8.546904 \t 0.069441 \t 63.035492 s\n","101 \t 17.077609 \t 8.610487 \t8.395078 \t 0.072045 \t 63.608226 s\n","102 \t 17.007359 \t 8.560630 \t8.386606 \t 0.060123 \t 64.333272 s\n","103 \t 16.687901 \t 8.428925 \t8.204563 \t 0.054413 \t 64.907891 s\n","104 \t 16.885417 \t 8.466132 \t8.345190 \t 0.074095 \t 65.490541 s\n","105 \t 16.989752 \t 8.517023 \t8.384056 \t 0.088672 \t 66.072283 s\n","106 \t 16.973728 \t 8.504737 \t8.422399 \t 0.046592 \t 66.652157 s\n","107 \t 16.902734 \t 8.497676 \t8.368197 \t 0.036861 \t 67.237572 s\n","108 \t 16.824559 \t 8.411017 \t8.358132 \t 0.055410 \t 67.820366 s\n","109 \t 16.682726 \t 8.361878 \t8.271197 \t 0.049652 \t 68.445345 s\n","110 \t 16.876543 \t 8.534750 \t8.284141 \t 0.057652 \t 69.342843 s\n","111 \t 17.172774 \t 8.653233 \t8.478002 \t 0.041541 \t 70.016062 s\n","112 \t 16.814627 \t 8.499752 \t8.272187 \t 0.042687 \t 70.739281 s\n","113 \t 16.729043 \t 8.388076 \t8.300361 \t 0.040606 \t 71.479366 s\n","114 \t 16.642047 \t 8.440278 \t8.150666 \t 0.051103 \t 72.055179 s\n","115 \t 16.838967 \t 8.458104 \t8.317441 \t 0.063422 \t 72.633297 s\n","116 \t 16.930934 \t 8.480389 \t8.413361 \t 0.037184 \t 73.204911 s\n","117 \t 16.787300 \t 8.503487 \t8.246757 \t 0.037057 \t 73.788272 s\n","118 \t 16.881895 \t 8.433114 \t8.414625 \t 0.034156 \t 74.357935 s\n","119 \t 16.525279 \t 8.370905 \t8.099515 \t 0.054859 \t 75.110408 s\n","120 \t 16.851156 \t 8.394948 \t8.412193 \t 0.044015 \t 75.682738 s\n","121 \t 16.662576 \t 8.344594 \t8.264102 \t 0.053880 \t 76.243185 s\n","122 \t 16.889084 \t 8.394057 \t8.433541 \t 0.061486 \t 76.819450 s\n","123 \t 16.810646 \t 8.283090 \t8.489138 \t 0.038418 \t 77.387746 s\n","124 \t 16.548994 \t 8.273564 \t8.235894 \t 0.039536 \t 77.979994 s\n","125 \t 16.675905 \t 8.351416 \t8.287195 \t 0.037294 \t 78.565306 s\n","126 \t 16.795577 \t 8.468806 \t8.276886 \t 0.049885 \t 79.147482 s\n","127 \t 16.839550 \t 8.440927 \t8.358158 \t 0.040466 \t 79.750686 s\n","128 \t 16.769766 \t 8.436784 \t8.306491 \t 0.026491 \t 80.320922 s\n","129 \t 16.736772 \t 8.460001 \t8.238924 \t 0.037846 \t 81.062236 s\n","130 \t 16.677426 \t 8.338571 \t8.295763 \t 0.043092 \t 81.714419 s\n","131 \t 16.631704 \t 8.328140 \t8.260507 \t 0.043056 \t 82.411530 s\n","132 \t 16.484390 \t 8.228174 \t8.218654 \t 0.037563 \t 83.100548 s\n","133 \t 16.892711 \t 8.543466 \t8.299536 \t 0.049708 \t 83.815335 s\n","134 \t 16.486195 \t 8.340930 \t8.111032 \t 0.034231 \t 84.582397 s\n","135 \t 16.442834 \t 8.250957 \t8.155091 \t 0.036786 \t 85.163312 s\n","136 \t 16.712710 \t 8.392340 \t8.275097 \t 0.045272 \t 85.738600 s\n","137 \t 16.742132 \t 8.454473 \t8.246369 \t 0.041290 \t 86.312142 s\n","138 \t 16.614555 \t 8.420900 \t8.164467 \t 0.029187 \t 86.883764 s\n","139 \t 16.534638 \t 8.219182 \t8.267766 \t 0.047691 \t 87.626835 s\n","140 \t 16.527908 \t 8.313736 \t8.174957 \t 0.039216 \t 88.201316 s\n","141 \t 16.547487 \t 8.285230 \t8.233647 \t 0.028610 \t 88.771790 s\n","142 \t 16.682666 \t 8.358228 \t8.279027 \t 0.045411 \t 89.336661 s\n","143 \t 16.652795 \t 8.281484 \t8.330169 \t 0.041143 \t 89.906020 s\n","144 \t 16.577971 \t 8.344410 \t8.184398 \t 0.049162 \t 90.496084 s\n","145 \t 16.520382 \t 8.245161 \t8.220652 \t 0.054569 \t 91.077561 s\n","146 \t 16.799859 \t 8.452702 \t8.307248 \t 0.039909 \t 91.661084 s\n","147 \t 16.518349 \t 8.293959 \t8.185630 \t 0.038762 \t 92.249325 s\n","148 \t 16.624350 \t 8.329490 \t8.259814 \t 0.035045 \t 92.995514 s\n","149 \t 16.594349 \t 8.303771 \t8.249457 \t 0.041121 \t 93.838743 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/162 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 162/162 [00:30<00:00,  5.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3828\n","Hit@10: 0.5753\n","Hit@3: 0.3973\n","Hit@1: 0.3014\n","Link Prediction on Validation Set (All)\n","MRR: 0.4001\n","Hit@10: 0.6535\n","Hit@3: 0.4554\n","Hit@1: 0.2805\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4571\n","Hit@10: 0.7346\n","Hit@3: 0.5617\n","Hit@1: 0.2963\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5026\n","Hit@10: 0.7584\n","Hit@3: 0.5882\n","Hit@1: 0.3592\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2203\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2203\n","150 \t 16.697599 \t 8.419348 \t8.206304 \t 0.071948 \t 128.005161 s\n","151 \t 17.316154 \t 8.570320 \t8.650285 \t 0.095548 \t 128.609373 s\n","152 \t 17.106160 \t 8.525355 \t8.461007 \t 0.119798 \t 129.194538 s\n","153 \t 17.093898 \t 8.531421 \t8.471729 \t 0.090748 \t 129.776201 s\n","154 \t 17.347303 \t 8.594673 \t8.642378 \t 0.110252 \t 130.343088 s\n","155 \t 17.034790 \t 8.523982 \t8.448722 \t 0.062086 \t 130.921106 s\n","156 \t 17.118568 \t 8.550856 \t8.481854 \t 0.085857 \t 131.640743 s\n","157 \t 16.856491 \t 8.380204 \t8.407050 \t 0.069237 \t 132.222470 s\n","158 \t 16.998946 \t 8.496323 \t8.437715 \t 0.064908 \t 132.791066 s\n","159 \t 17.095778 \t 8.458718 \t8.506432 \t 0.130628 \t 133.377439 s\n","160 \t 17.010355 \t 8.488467 \t8.463690 \t 0.058199 \t 133.951998 s\n","161 \t 16.832666 \t 8.363122 \t8.391145 \t 0.078399 \t 134.608082 s\n","162 \t 16.942753 \t 8.448541 \t8.400730 \t 0.093481 \t 135.631281 s\n","163 \t 17.013138 \t 8.531042 \t8.358685 \t 0.123411 \t 137.240244 s\n","164 \t 16.948386 \t 8.411132 \t8.444004 \t 0.093251 \t 137.968864 s\n","165 \t 16.776284 \t 8.430853 \t8.285493 \t 0.059938 \t 138.554240 s\n","166 \t 16.738357 \t 8.285121 \t8.385475 \t 0.067760 \t 139.290081 s\n","167 \t 16.687398 \t 8.413104 \t8.193260 \t 0.081033 \t 139.867571 s\n","168 \t 16.812154 \t 8.434707 \t8.322519 \t 0.054928 \t 140.453887 s\n","169 \t 16.721508 \t 8.270037 \t8.393230 \t 0.058241 \t 141.032485 s\n","170 \t 16.624490 \t 8.326136 \t8.246709 \t 0.051644 \t 141.600914 s\n","171 \t 16.634640 \t 8.280394 \t8.277622 \t 0.076624 \t 142.190824 s\n","172 \t 16.556673 \t 8.215748 \t8.259289 \t 0.081636 \t 142.780742 s\n","173 \t 16.634660 \t 8.351302 \t8.234212 \t 0.049147 \t 143.358826 s\n","174 \t 16.487926 \t 8.113201 \t8.326893 \t 0.047831 \t 143.932162 s\n","175 \t 16.646232 \t 8.281765 \t8.292682 \t 0.071785 \t 144.680800 s\n","176 \t 16.467204 \t 8.148499 \t8.265465 \t 0.053239 \t 145.268891 s\n","177 \t 16.351735 \t 8.124698 \t8.194843 \t 0.032193 \t 145.860773 s\n","178 \t 16.408201 \t 8.288492 \t8.065939 \t 0.053770 \t 146.436937 s\n","179 \t 16.300507 \t 8.123204 \t8.142023 \t 0.035280 \t 147.022694 s\n","180 \t 16.401398 \t 8.224344 \t8.111385 \t 0.065668 \t 147.599816 s\n","181 \t 16.403173 \t 8.109323 \t8.240406 \t 0.053444 \t 148.234018 s\n","182 \t 16.056174 \t 7.978962 \t8.008323 \t 0.068889 \t 148.937570 s\n","183 \t 16.519556 \t 8.327290 \t8.088386 \t 0.103881 \t 149.634057 s\n","184 \t 16.379074 \t 8.115734 \t8.168275 \t 0.095065 \t 150.543261 s\n","185 \t 16.365419 \t 8.187622 \t8.111263 \t 0.066534 \t 151.247674 s\n","186 \t 16.282517 \t 8.086673 \t8.104630 \t 0.091216 \t 151.835520 s\n","187 \t 16.241419 \t 8.122494 \t8.084440 \t 0.034486 \t 152.416706 s\n","188 \t 16.116847 \t 7.985159 \t8.086246 \t 0.045442 \t 153.054867 s\n","189 \t 16.398143 \t 8.133567 \t8.201218 \t 0.063357 \t 153.638563 s\n","190 \t 16.183118 \t 7.996508 \t8.110536 \t 0.076075 \t 154.222895 s\n","191 \t 16.295574 \t 8.173559 \t8.074663 \t 0.047353 \t 154.796200 s\n","192 \t 16.197062 \t 7.986851 \t8.145321 \t 0.064890 \t 155.375377 s\n","193 \t 16.336187 \t 8.103551 \t8.153617 \t 0.079018 \t 156.112134 s\n","194 \t 16.103499 \t 8.034192 \t7.979702 \t 0.089606 \t 156.702313 s\n","195 \t 16.041477 \t 7.893530 \t8.075690 \t 0.072258 \t 157.286585 s\n","196 \t 16.176172 \t 8.058351 \t8.049040 \t 0.068782 \t 157.863717 s\n","197 \t 16.017805 \t 8.003439 \t7.936573 \t 0.077794 \t 158.465752 s\n","198 \t 15.955409 \t 7.866403 \t8.016353 \t 0.072652 \t 159.046208 s\n","199 \t 16.095202 \t 8.103926 \t7.943737 \t 0.047538 \t 159.619020 s\n","200 \t 15.987206 \t 7.897799 \t8.002088 \t 0.087319 \t 160.212857 s\n","201 \t 16.278343 \t 8.035427 \t8.087505 \t 0.155411 \t 160.786781 s\n","202 \t 15.948736 \t 7.866298 \t7.984423 \t 0.098015 \t 161.440218 s\n","203 \t 15.819988 \t 7.911732 \t7.855518 \t 0.052738 \t 162.348528 s\n","204 \t 16.022227 \t 7.963296 \t7.932938 \t 0.125993 \t 163.044027 s\n","205 \t 15.888185 \t 7.867277 \t7.955097 \t 0.065811 \t 163.781567 s\n","206 \t 15.859450 \t 7.965442 \t7.830925 \t 0.063084 \t 164.478077 s\n","207 \t 15.818779 \t 7.839707 \t7.897232 \t 0.081839 \t 165.054794 s\n","208 \t 15.649687 \t 7.719829 \t7.843200 \t 0.086658 \t 165.618970 s\n","209 \t 15.798605 \t 7.805514 \t7.936469 \t 0.056622 \t 166.198046 s\n","210 \t 15.756094 \t 7.834342 \t7.881796 \t 0.039956 \t 166.775503 s\n","211 \t 15.808287 \t 7.933090 \t7.810568 \t 0.064629 \t 167.354575 s\n","212 \t 15.818784 \t 7.880914 \t7.870726 \t 0.067143 \t 167.940491 s\n","213 \t 15.779904 \t 7.871845 \t7.849658 \t 0.058401 \t 168.670458 s\n","214 \t 15.777740 \t 7.871010 \t7.847114 \t 0.059615 \t 169.247346 s\n","215 \t 15.549106 \t 7.821670 \t7.690092 \t 0.037345 \t 169.825080 s\n","216 \t 15.769736 \t 7.845095 \t7.879868 \t 0.044773 \t 170.399241 s\n","217 \t 15.578678 \t 7.808328 \t7.730441 \t 0.039909 \t 170.983968 s\n","218 \t 15.648537 \t 7.826838 \t7.770274 \t 0.051426 \t 171.552074 s\n","219 \t 15.594372 \t 7.752932 \t7.802223 \t 0.039218 \t 172.133841 s\n","220 \t 15.494278 \t 7.745581 \t7.714929 \t 0.033767 \t 172.710088 s\n","221 \t 15.629779 \t 7.718332 \t7.847364 \t 0.064083 \t 173.441029 s\n","222 \t 15.605422 \t 7.733088 \t7.836946 \t 0.035389 \t 174.030765 s\n","223 \t 15.634081 \t 7.702394 \t7.895502 \t 0.036185 \t 174.702197 s\n","224 \t 15.726315 \t 7.770628 \t7.909852 \t 0.045835 \t 175.394565 s\n","225 \t 15.666426 \t 7.763609 \t7.854003 \t 0.048814 \t 176.077779 s\n","226 \t 15.384449 \t 7.612937 \t7.712141 \t 0.059370 \t 176.786999 s\n","227 \t 15.446938 \t 7.704548 \t7.694567 \t 0.047823 \t 177.531104 s\n","228 \t 15.436965 \t 7.656696 \t7.747408 \t 0.032862 \t 178.103539 s\n","229 \t 15.614847 \t 7.832389 \t7.731596 \t 0.050863 \t 178.672030 s\n","230 \t 15.164214 \t 7.594788 \t7.538634 \t 0.030791 \t 179.399706 s\n","231 \t 15.361739 \t 7.616923 \t7.696338 \t 0.048477 \t 179.968288 s\n","232 \t 15.336909 \t 7.607568 \t7.678748 \t 0.050594 \t 180.546022 s\n","233 \t 15.367320 \t 7.626372 \t7.716946 \t 0.024002 \t 181.127121 s\n","234 \t 15.341124 \t 7.667716 \t7.635611 \t 0.037797 \t 181.706301 s\n","235 \t 15.372531 \t 7.579470 \t7.744284 \t 0.048777 \t 182.284092 s\n","236 \t 15.252321 \t 7.487620 \t7.688647 \t 0.076054 \t 182.856081 s\n","237 \t 15.384576 \t 7.653603 \t7.681713 \t 0.049260 \t 183.431776 s\n","238 \t 15.176698 \t 7.592451 \t7.554971 \t 0.029276 \t 184.008552 s\n","239 \t 15.408857 \t 7.666960 \t7.688201 \t 0.053696 \t 184.594886 s\n","240 \t 15.159930 \t 7.515655 \t7.605435 \t 0.038840 \t 185.343775 s\n","241 \t 15.237999 \t 7.518218 \t7.668253 \t 0.051528 \t 185.914744 s\n","242 \t 15.103574 \t 7.432778 \t7.633724 \t 0.037071 \t 186.485381 s\n","243 \t 15.106959 \t 7.372888 \t7.671154 \t 0.062917 \t 187.052902 s\n","244 \t 15.207964 \t 7.587387 \t7.576867 \t 0.043711 \t 187.711366 s\n","245 \t 15.112031 \t 7.509838 \t7.544458 \t 0.057735 \t 188.447486 s\n","246 \t 15.057116 \t 7.460510 \t7.543668 \t 0.052937 \t 189.133013 s\n","247 \t 15.092626 \t 7.460157 \t7.595869 \t 0.036601 \t 189.845943 s\n","248 \t 15.067452 \t 7.534316 \t7.495164 \t 0.037971 \t 190.623601 s\n","249 \t 14.967209 \t 7.450440 \t7.480611 \t 0.036158 \t 191.212720 s\n","250 \t 14.934617 \t 7.431908 \t7.457889 \t 0.044820 \t 191.949987 s\n","251 \t 15.130695 \t 7.473954 \t7.615159 \t 0.041583 \t 192.522770 s\n","252 \t 15.156057 \t 7.504634 \t7.586303 \t 0.065121 \t 193.110643 s\n","253 \t 14.918965 \t 7.462279 \t7.404167 \t 0.052519 \t 193.694290 s\n","254 \t 14.889295 \t 7.350482 \t7.509587 \t 0.029226 \t 194.268017 s\n","255 \t 14.988022 \t 7.468846 \t7.492353 \t 0.026822 \t 194.846628 s\n","256 \t 14.824794 \t 7.280218 \t7.512210 \t 0.032366 \t 195.421922 s\n","257 \t 14.843260 \t 7.482459 \t7.331929 \t 0.028873 \t 195.998323 s\n","258 \t 14.890553 \t 7.426491 \t7.410949 \t 0.053113 \t 196.567228 s\n","259 \t 14.980045 \t 7.479414 \t7.459875 \t 0.040756 \t 197.150885 s\n","260 \t 14.816393 \t 7.334581 \t7.452808 \t 0.029005 \t 197.887525 s\n","261 \t 14.770315 \t 7.436794 \t7.297980 \t 0.035542 \t 198.457415 s\n","262 \t 14.723975 \t 7.362742 \t7.333679 \t 0.027554 \t 199.041318 s\n","263 \t 14.803862 \t 7.330457 \t7.396777 \t 0.076628 \t 199.611253 s\n","264 \t 14.893099 \t 7.414673 \t7.415136 \t 0.063290 \t 200.204552 s\n","265 \t 14.903953 \t 7.414048 \t7.420945 \t 0.068959 \t 200.807998 s\n","266 \t 14.711415 \t 7.295234 \t7.360914 \t 0.055266 \t 201.523866 s\n","267 \t 14.794542 \t 7.276598 \t7.441015 \t 0.076930 \t 202.215203 s\n","268 \t 14.863927 \t 7.297715 \t7.523415 \t 0.042797 \t 203.108306 s\n","269 \t 14.828356 \t 7.434186 \t7.321185 \t 0.072985 \t 203.882395 s\n","270 \t 14.639146 \t 7.245613 \t7.335271 \t 0.058262 \t 204.462078 s\n","271 \t 14.775395 \t 7.285892 \t7.438690 \t 0.050813 \t 205.043804 s\n","272 \t 14.592463 \t 7.185490 \t7.356213 \t 0.050760 \t 205.626112 s\n","273 \t 14.694054 \t 7.257274 \t7.410191 \t 0.026589 \t 206.202955 s\n","274 \t 14.609028 \t 7.320527 \t7.261077 \t 0.027424 \t 206.783593 s\n","275 \t 14.578106 \t 7.278824 \t7.267931 \t 0.031351 \t 207.350842 s\n","276 \t 14.687108 \t 7.304308 \t7.352218 \t 0.030582 \t 207.942021 s\n","277 \t 14.791594 \t 7.265401 \t7.484545 \t 0.041648 \t 208.697199 s\n","278 \t 14.628193 \t 7.223909 \t7.373167 \t 0.031118 \t 209.275171 s\n","279 \t 14.605783 \t 7.223770 \t7.349753 \t 0.032260 \t 209.856096 s\n","280 \t 14.559966 \t 7.268045 \t7.247518 \t 0.044402 \t 210.429219 s\n","281 \t 14.708256 \t 7.285191 \t7.385273 \t 0.037792 \t 211.017731 s\n","282 \t 14.644199 \t 7.264470 \t7.359170 \t 0.020559 \t 211.592707 s\n","283 \t 14.331367 \t 7.066632 \t7.233028 \t 0.031706 \t 212.172479 s\n","284 \t 14.526510 \t 7.229406 \t7.265877 \t 0.031226 \t 212.757785 s\n","285 \t 14.400753 \t 7.150920 \t7.233043 \t 0.016790 \t 213.329442 s\n","286 \t 14.427450 \t 7.144237 \t7.244387 \t 0.038825 \t 213.929029 s\n","287 \t 14.403854 \t 7.174361 \t7.211509 \t 0.017984 \t 214.839640 s\n","288 \t 14.409852 \t 7.168613 \t7.217818 \t 0.023420 \t 215.526704 s\n","289 \t 14.350198 \t 7.223371 \t7.101020 \t 0.025808 \t 216.259177 s\n","290 \t 14.242916 \t 7.131001 \t7.090680 \t 0.021235 \t 217.049603 s\n","291 \t 14.197344 \t 7.038176 \t7.135735 \t 0.023433 \t 217.615884 s\n","292 \t 14.186413 \t 7.019940 \t7.143762 \t 0.022712 \t 218.198225 s\n","293 \t 14.308820 \t 7.145427 \t7.130826 \t 0.032567 \t 218.791460 s\n","294 \t 14.330723 \t 7.195397 \t7.116922 \t 0.018403 \t 219.372857 s\n","295 \t 14.276886 \t 7.172016 \t7.087909 \t 0.016961 \t 219.944125 s\n","296 \t 14.201284 \t 7.095324 \t7.089158 \t 0.016802 \t 220.531053 s\n","297 \t 14.214598 \t 7.121922 \t7.058447 \t 0.034229 \t 221.256651 s\n","298 \t 14.264457 \t 7.112563 \t7.120804 \t 0.031090 \t 221.836939 s\n","299 \t 13.979100 \t 6.990324 \t6.964996 \t 0.023780 \t 222.426702 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:29<00:00,  5.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4485\n","Hit@10: 0.6062\n","Hit@3: 0.4863\n","Hit@1: 0.3664\n","Link Prediction on Validation Set (All)\n","MRR: 0.4549\n","Hit@10: 0.6832\n","Hit@3: 0.5281\n","Hit@1: 0.3350\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4066\n","Hit@10: 0.6605\n","Hit@3: 0.5247\n","Hit@1: 0.2469\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4919\n","Hit@10: 0.7479\n","Hit@3: 0.5714\n","Hit@1: 0.3550\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2097\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2097\n","300 \t 14.334025 \t 7.134833 \t7.169652 \t 0.029540 \t 253.906292 s\n","301 \t 14.361076 \t 7.179653 \t7.154238 \t 0.027185 \t 254.604249 s\n","302 \t 14.156359 \t 7.061134 \t7.055932 \t 0.039293 \t 255.298662 s\n","303 \t 14.148417 \t 7.039371 \t7.091470 \t 0.017576 \t 256.027292 s\n","304 \t 14.269172 \t 7.045602 \t7.168197 \t 0.055373 \t 256.759286 s\n","305 \t 14.211874 \t 7.067149 \t7.112953 \t 0.031772 \t 257.507396 s\n","306 \t 14.085379 \t 7.001261 \t7.051543 \t 0.032575 \t 258.087600 s\n","307 \t 14.139649 \t 7.049794 \t7.051497 \t 0.038357 \t 258.674361 s\n","308 \t 14.133116 \t 6.994308 \t7.108959 \t 0.029848 \t 259.660687 s\n","309 \t 14.080404 \t 7.008798 \t7.034933 \t 0.036673 \t 260.723284 s\n","310 \t 14.058245 \t 6.970677 \t7.043752 \t 0.043816 \t 261.480946 s\n","311 \t 14.027656 \t 7.055346 \t6.936034 \t 0.036275 \t 262.065244 s\n","312 \t 13.928187 \t 6.899712 \t6.991512 \t 0.036964 \t 262.640128 s\n","313 \t 14.122371 \t 6.996725 \t7.095861 \t 0.029786 \t 263.380018 s\n","314 \t 13.922603 \t 6.933472 \t6.940222 \t 0.048909 \t 263.947991 s\n","315 \t 13.938709 \t 7.001911 \t6.905709 \t 0.031088 \t 264.528513 s\n","316 \t 13.998121 \t 6.984243 \t6.987606 \t 0.026272 \t 265.104402 s\n","317 \t 13.882451 \t 6.889314 \t6.964839 \t 0.028299 \t 265.702356 s\n","318 \t 13.843565 \t 6.934162 \t6.883632 \t 0.025772 \t 266.287916 s\n","319 \t 13.995366 \t 7.011492 \t6.964406 \t 0.019468 \t 266.941349 s\n","320 \t 13.908443 \t 6.876299 \t7.001750 \t 0.030393 \t 267.654779 s\n","321 \t 13.833631 \t 6.820502 \t6.991230 \t 0.021898 \t 268.358607 s\n","322 \t 13.963737 \t 6.971946 \t6.956551 \t 0.035241 \t 269.075827 s\n","323 \t 13.792336 \t 6.944778 \t6.825718 \t 0.021839 \t 270.026071 s\n","324 \t 13.995297 \t 6.980209 \t6.981926 \t 0.033162 \t 270.604742 s\n","325 \t 13.879183 \t 6.953009 \t6.903010 \t 0.023164 \t 271.176812 s\n","326 \t 13.785989 \t 6.751634 \t7.013215 \t 0.021140 \t 271.752937 s\n","327 \t 13.795248 \t 6.845200 \t6.920980 \t 0.029067 \t 272.321210 s\n","328 \t 13.759243 \t 6.934958 \t6.802063 \t 0.022223 \t 272.900881 s\n","329 \t 13.887324 \t 6.889515 \t6.979119 \t 0.018689 \t 273.474197 s\n","330 \t 13.810354 \t 6.904477 \t6.884028 \t 0.021849 \t 274.069033 s\n","331 \t 13.766901 \t 6.837548 \t6.885780 \t 0.043575 \t 274.651329 s\n","332 \t 13.872833 \t 6.894248 \t6.961029 \t 0.017555 \t 275.235717 s\n","333 \t 13.706320 \t 6.790166 \t6.883195 \t 0.032958 \t 275.966752 s\n","334 \t 13.756355 \t 6.889498 \t6.845956 \t 0.020901 \t 276.546680 s\n","335 \t 13.733669 \t 6.789336 \t6.927099 \t 0.017233 \t 277.146287 s\n","336 \t 13.708300 \t 6.828130 \t6.862600 \t 0.017569 \t 277.735665 s\n","337 \t 13.938559 \t 6.891901 \t7.010893 \t 0.035765 \t 278.319180 s\n","338 \t 13.555800 \t 6.714515 \t6.826560 \t 0.014725 \t 278.935473 s\n","339 \t 13.713842 \t 6.818559 \t6.872183 \t 0.023101 \t 279.522102 s\n","340 \t 13.868899 \t 6.909948 \t6.939039 \t 0.019911 \t 280.183250 s\n","341 \t 13.778335 \t 6.817172 \t6.928180 \t 0.032983 \t 281.087060 s\n","342 \t 13.674478 \t 6.742843 \t6.903747 \t 0.027888 \t 281.789213 s\n","343 \t 13.718380 \t 6.884092 \t6.815277 \t 0.019011 \t 282.544446 s\n","344 \t 13.781686 \t 6.895277 \t6.867602 \t 0.018807 \t 283.253107 s\n","345 \t 13.809286 \t 6.874470 \t6.911138 \t 0.023677 \t 283.828667 s\n","346 \t 13.756368 \t 6.893814 \t6.845616 \t 0.016939 \t 284.397802 s\n","347 \t 13.652053 \t 6.844435 \t6.779458 \t 0.028160 \t 284.977103 s\n","348 \t 13.603016 \t 6.679000 \t6.908879 \t 0.015137 \t 285.573312 s\n","349 \t 13.649318 \t 6.833959 \t6.795413 \t 0.019947 \t 286.157390 s\n","350 \t 13.621365 \t 6.818545 \t6.775611 \t 0.027209 \t 286.892164 s\n","351 \t 13.607518 \t 6.785386 \t6.801926 \t 0.020206 \t 287.469879 s\n","352 \t 13.744697 \t 6.861544 \t6.862933 \t 0.020220 \t 288.041355 s\n","353 \t 13.656603 \t 6.804794 \t6.834683 \t 0.017125 \t 288.621073 s\n","354 \t 13.492945 \t 6.700188 \t6.773714 \t 0.019042 \t 289.211859 s\n","355 \t 13.478930 \t 6.672518 \t6.784372 \t 0.022041 \t 289.804734 s\n","356 \t 13.625423 \t 6.838834 \t6.771115 \t 0.015474 \t 290.396759 s\n","357 \t 13.514467 \t 6.741184 \t6.754735 \t 0.018548 \t 290.977906 s\n","358 \t 13.539741 \t 6.730065 \t6.782861 \t 0.026814 \t 291.565789 s\n","359 \t 13.519872 \t 6.733092 \t6.767116 \t 0.019663 \t 292.146183 s\n","360 \t 13.588919 \t 6.790179 \t6.782584 \t 0.016155 \t 292.891690 s\n","361 \t 13.661654 \t 6.859914 \t6.776230 \t 0.025509 \t 293.599565 s\n","362 \t 13.470044 \t 6.713757 \t6.737027 \t 0.019260 \t 294.278553 s\n","363 \t 13.439576 \t 6.682076 \t6.741682 \t 0.015819 \t 294.986348 s\n","364 \t 13.386016 \t 6.705441 \t6.658220 \t 0.022355 \t 295.731347 s\n","365 \t 13.553324 \t 6.793979 \t6.735446 \t 0.023898 \t 296.413755 s\n","366 \t 13.486411 \t 6.742762 \t6.719006 \t 0.024644 \t 296.992511 s\n","367 \t 13.439568 \t 6.713722 \t6.708179 \t 0.017667 \t 297.567547 s\n","368 \t 13.519003 \t 6.716843 \t6.792968 \t 0.009192 \t 298.156256 s\n","369 \t 13.509701 \t 6.756360 \t6.740187 \t 0.013154 \t 298.732642 s\n","370 \t 13.520380 \t 6.759001 \t6.744973 \t 0.016406 \t 299.451688 s\n","371 \t 13.507902 \t 6.824476 \t6.662869 \t 0.020557 \t 300.027154 s\n","372 \t 13.455317 \t 6.718811 \t6.717905 \t 0.018601 \t 300.596738 s\n","373 \t 13.430713 \t 6.699700 \t6.713912 \t 0.017100 \t 301.178936 s\n","374 \t 13.339859 \t 6.651407 \t6.675972 \t 0.012480 \t 301.762756 s\n","375 \t 13.340434 \t 6.637755 \t6.686187 \t 0.016492 \t 302.340928 s\n","376 \t 13.421041 \t 6.730485 \t6.674106 \t 0.016450 \t 302.921934 s\n","377 \t 13.433525 \t 6.664314 \t6.756088 \t 0.013123 \t 303.509914 s\n","378 \t 13.421706 \t 6.703888 \t6.700891 \t 0.016927 \t 304.093309 s\n","379 \t 13.477159 \t 6.777919 \t6.682510 \t 0.016730 \t 304.667841 s\n","380 \t 13.379612 \t 6.678629 \t6.680726 \t 0.020257 \t 305.410468 s\n","381 \t 13.491927 \t 6.784680 \t6.692053 \t 0.015194 \t 305.985012 s\n","382 \t 13.428691 \t 6.712317 \t6.704122 \t 0.012252 \t 306.624413 s\n","383 \t 13.392132 \t 6.639746 \t6.729496 \t 0.022890 \t 307.301424 s\n","384 \t 13.370338 \t 6.640771 \t6.712010 \t 0.017557 \t 307.983337 s\n","385 \t 13.371623 \t 6.636875 \t6.714077 \t 0.020672 \t 308.733030 s\n","386 \t 13.338465 \t 6.659876 \t6.658213 \t 0.020377 \t 309.523458 s\n","387 \t 13.354741 \t 6.654053 \t6.662629 \t 0.038059 \t 310.095304 s\n","388 \t 13.301301 \t 6.635661 \t6.637754 \t 0.027887 \t 310.685686 s\n","389 \t 13.441059 \t 6.738074 \t6.683496 \t 0.019489 \t 311.413304 s\n","390 \t 13.312178 \t 6.655436 \t6.646398 \t 0.010344 \t 311.977181 s\n","391 \t 13.364899 \t 6.685796 \t6.654234 \t 0.024870 \t 312.538844 s\n","392 \t 13.312864 \t 6.629707 \t6.668555 \t 0.014602 \t 313.102847 s\n","393 \t 13.332726 \t 6.651055 \t6.663745 \t 0.017926 \t 313.686998 s\n","394 \t 13.405665 \t 6.665359 \t6.723304 \t 0.017002 \t 314.263955 s\n","395 \t 13.343289 \t 6.651882 \t6.673011 \t 0.018396 \t 314.841544 s\n","396 \t 13.257116 \t 6.669425 \t6.570670 \t 0.017022 \t 315.427191 s\n","397 \t 13.268167 \t 6.690217 \t6.560379 \t 0.017570 \t 316.160828 s\n","398 \t 13.307149 \t 6.628809 \t6.656205 \t 0.022135 \t 316.735164 s\n","399 \t 13.206951 \t 6.564950 \t6.628869 \t 0.013132 \t 317.304476 s\n","400 \t 13.373394 \t 6.688138 \t6.668962 \t 0.016294 \t 317.893024 s\n","401 \t 13.308247 \t 6.683114 \t6.608045 \t 0.017088 \t 318.466031 s\n","402 \t 13.220030 \t 6.557719 \t6.646876 \t 0.015435 \t 319.036606 s\n","403 \t 13.385058 \t 6.695071 \t6.674092 \t 0.015895 \t 319.708968 s\n","404 \t 13.290501 \t 6.656008 \t6.607589 \t 0.026904 \t 320.393320 s\n","405 \t 13.315284 \t 6.690574 \t6.608083 \t 0.016627 \t 321.121053 s\n","406 \t 13.314844 \t 6.638063 \t6.659982 \t 0.016798 \t 321.845589 s\n","407 \t 13.404255 \t 6.748371 \t6.641308 \t 0.014576 \t 322.739295 s\n","408 \t 13.451099 \t 6.746607 \t6.692008 \t 0.012484 \t 323.316065 s\n","409 \t 13.317856 \t 6.640118 \t6.658975 \t 0.018763 \t 323.896135 s\n","410 \t 13.342916 \t 6.686282 \t6.643958 \t 0.012676 \t 324.462975 s\n","411 \t 13.432509 \t 6.732130 \t6.687579 \t 0.012802 \t 325.031460 s\n","412 \t 13.252727 \t 6.610965 \t6.630822 \t 0.010939 \t 325.594556 s\n","413 \t 13.217270 \t 6.559642 \t6.642305 \t 0.015323 \t 326.174650 s\n","414 \t 13.314211 \t 6.616264 \t6.677721 \t 0.020227 \t 326.739938 s\n","415 \t 13.396867 \t 6.741133 \t6.640422 \t 0.015312 \t 327.334414 s\n","416 \t 13.337907 \t 6.648965 \t6.666617 \t 0.022325 \t 327.912700 s\n","417 \t 13.317265 \t 6.652379 \t6.642064 \t 0.022821 \t 328.641009 s\n","418 \t 13.092473 \t 6.499145 \t6.577002 \t 0.016326 \t 329.215206 s\n","419 \t 13.315083 \t 6.696481 \t6.606952 \t 0.011650 \t 329.775951 s\n","420 \t 13.269020 \t 6.609004 \t6.643368 \t 0.016647 \t 330.348644 s\n","421 \t 13.235344 \t 6.594565 \t6.625778 \t 0.015001 \t 330.913003 s\n","422 \t 13.436288 \t 6.686796 \t6.730905 \t 0.018587 \t 331.499192 s\n","423 \t 13.303427 \t 6.699874 \t6.588911 \t 0.014642 \t 332.068287 s\n","424 \t 13.300741 \t 6.660788 \t6.631379 \t 0.008573 \t 332.665320 s\n","425 \t 13.249803 \t 6.628890 \t6.602622 \t 0.018291 \t 333.559871 s\n","426 \t 13.290773 \t 6.654340 \t6.623129 \t 0.013303 \t 334.259422 s\n","427 \t 13.283022 \t 6.604460 \t6.666234 \t 0.012328 \t 334.983506 s\n","428 \t 13.360053 \t 6.738243 \t6.599229 \t 0.022581 \t 335.732872 s\n","429 \t 13.248464 \t 6.606872 \t6.629178 \t 0.012414 \t 336.305983 s\n","430 \t 13.190532 \t 6.575854 \t6.601152 \t 0.013527 \t 336.883602 s\n","431 \t 13.370297 \t 6.704679 \t6.651920 \t 0.013698 \t 337.478803 s\n","432 \t 13.229648 \t 6.547152 \t6.672967 \t 0.009529 \t 338.055418 s\n","433 \t 13.298141 \t 6.630223 \t6.649025 \t 0.018893 \t 338.630104 s\n","434 \t 13.233469 \t 6.577996 \t6.634410 \t 0.021062 \t 339.398541 s\n","435 \t 13.183715 \t 6.558827 \t6.608159 \t 0.016730 \t 339.984221 s\n","436 \t 13.289430 \t 6.642102 \t6.626845 \t 0.020484 \t 340.569853 s\n","437 \t 13.270690 \t 6.624896 \t6.628414 \t 0.017380 \t 341.139902 s\n","438 \t 13.232273 \t 6.627201 \t6.591576 \t 0.013496 \t 341.723820 s\n","439 \t 13.256922 \t 6.618937 \t6.623531 \t 0.014453 \t 342.305440 s\n","440 \t 13.251515 \t 6.596383 \t6.636993 \t 0.018139 \t 342.883393 s\n","441 \t 13.190278 \t 6.601554 \t6.575208 \t 0.013516 \t 343.457920 s\n","442 \t 13.299370 \t 6.663549 \t6.616852 \t 0.018970 \t 344.055338 s\n","443 \t 13.216346 \t 6.611972 \t6.589847 \t 0.014527 \t 344.638055 s\n","444 \t 13.200952 \t 6.605273 \t6.578291 \t 0.017388 \t 345.371428 s\n","445 \t 13.278720 \t 6.613228 \t6.652537 \t 0.012955 \t 346.016364 s\n","446 \t 13.215447 \t 6.560749 \t6.635401 \t 0.019297 \t 346.705472 s\n","447 \t 13.255601 \t 6.585455 \t6.656579 \t 0.013567 \t 347.402714 s\n","448 \t 13.351714 \t 6.633172 \t6.700110 \t 0.018432 \t 348.118435 s\n","449 \t 13.409484 \t 6.696475 \t6.697925 \t 0.015083 \t 348.888488 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:30<00:00,  5.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4620\n","Hit@10: 0.6370\n","Hit@3: 0.5171\n","Hit@1: 0.3699\n","Link Prediction on Validation Set (All)\n","MRR: 0.4542\n","Hit@10: 0.6832\n","Hit@3: 0.5264\n","Hit@1: 0.3333\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4076\n","Hit@10: 0.6358\n","Hit@3: 0.5247\n","Hit@1: 0.2531\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5032\n","Hit@10: 0.7101\n","Hit@3: 0.5693\n","Hit@1: 0.3845\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2143\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2143\n","450 \t 13.490343 \t 6.732742 \t6.737774 \t 0.019827 \t 380.480757 s\n","451 \t 13.968701 \t 6.965676 \t6.915112 \t 0.087913 \t 381.044443 s\n","452 \t 13.932595 \t 7.047216 \t6.857772 \t 0.027607 \t 381.627658 s\n","453 \t 13.862644 \t 6.914433 \t6.873512 \t 0.074700 \t 382.373631 s\n","454 \t 13.854261 \t 6.940948 \t6.873658 \t 0.039655 \t 382.948976 s\n","455 \t 13.777357 \t 6.889966 \t6.853156 \t 0.034234 \t 383.519225 s\n","456 \t 13.661188 \t 6.757209 \t6.852715 \t 0.051263 \t 384.104883 s\n","457 \t 13.794750 \t 6.953998 \t6.797539 \t 0.043214 \t 384.692549 s\n","458 \t 13.654697 \t 6.795315 \t6.795456 \t 0.063927 \t 385.268828 s\n","459 \t 13.729334 \t 6.803638 \t6.898246 \t 0.027451 \t 386.291924 s\n","460 \t 13.691236 \t 6.889182 \t6.750350 \t 0.051704 \t 387.328489 s\n","461 \t 13.664913 \t 6.744690 \t6.904140 \t 0.016082 \t 388.378807 s\n","462 \t 13.838438 \t 6.947760 \t6.857923 \t 0.032755 \t 389.169362 s\n","463 \t 13.766858 \t 6.850232 \t6.881549 \t 0.035077 \t 390.000069 s\n","464 \t 13.776399 \t 6.922269 \t6.817737 \t 0.036393 \t 390.580286 s\n","465 \t 13.788242 \t 6.879718 \t6.857108 \t 0.051416 \t 391.160248 s\n","466 \t 13.539917 \t 6.723815 \t6.791673 \t 0.024427 \t 391.739759 s\n","467 \t 13.644380 \t 6.817483 \t6.795318 \t 0.031579 \t 392.310132 s\n","468 \t 13.646322 \t 6.800322 \t6.811599 \t 0.034402 \t 392.902469 s\n","469 \t 13.546031 \t 6.692610 \t6.818758 \t 0.034663 \t 393.486023 s\n","470 \t 13.590074 \t 6.782712 \t6.776048 \t 0.031314 \t 394.066138 s\n","471 \t 13.687403 \t 6.874010 \t6.786465 \t 0.026927 \t 394.640478 s\n","472 \t 13.683713 \t 6.761955 \t6.877316 \t 0.044443 \t 395.377872 s\n","473 \t 13.599933 \t 6.796182 \t6.777828 \t 0.025922 \t 395.979197 s\n","474 \t 13.698958 \t 6.807863 \t6.830591 \t 0.060504 \t 396.570356 s\n","475 \t 13.454196 \t 6.705691 \t6.713484 \t 0.035022 \t 397.167601 s\n","476 \t 13.822641 \t 6.905568 \t6.872641 \t 0.044432 \t 397.737353 s\n","477 \t 13.689168 \t 6.764734 \t6.880962 \t 0.043472 \t 398.327095 s\n","478 \t 13.574144 \t 6.775316 \t6.763293 \t 0.035535 \t 398.920203 s\n","479 \t 13.676459 \t 6.899219 \t6.747629 \t 0.029611 \t 399.599031 s\n","480 \t 13.489751 \t 6.688631 \t6.729434 \t 0.071687 \t 400.341144 s\n","481 \t 13.603900 \t 6.804279 \t6.767810 \t 0.031812 \t 401.235265 s\n","482 \t 13.634799 \t 6.841078 \t6.764889 \t 0.028831 \t 401.954475 s\n","483 \t 13.456121 \t 6.648758 \t6.772063 \t 0.035300 \t 402.727921 s\n","484 \t 13.376912 \t 6.640188 \t6.710218 \t 0.026506 \t 403.320158 s\n","485 \t 13.577411 \t 6.760166 \t6.791275 \t 0.025970 \t 403.894622 s\n","486 \t 13.575886 \t 6.761724 \t6.766902 \t 0.047259 \t 404.472747 s\n","487 \t 13.574131 \t 6.726812 \t6.813611 \t 0.033708 \t 405.048706 s\n","488 \t 13.501044 \t 6.745853 \t6.730989 \t 0.024202 \t 405.632698 s\n","489 \t 13.648456 \t 6.808563 \t6.809888 \t 0.030005 \t 406.207609 s\n","490 \t 13.490814 \t 6.724019 \t6.733687 \t 0.033108 \t 406.817531 s\n","491 \t 13.613911 \t 6.776833 \t6.767800 \t 0.069278 \t 407.553698 s\n","492 \t 13.374760 \t 6.615505 \t6.730526 \t 0.028729 \t 408.134857 s\n","493 \t 13.487502 \t 6.734441 \t6.713502 \t 0.039559 \t 408.720775 s\n","494 \t 13.666371 \t 6.856857 \t6.750144 \t 0.059371 \t 409.290359 s\n","495 \t 13.538939 \t 6.777323 \t6.739821 \t 0.021794 \t 409.880270 s\n","496 \t 13.551571 \t 6.746658 \t6.783365 \t 0.021549 \t 410.462096 s\n","497 \t 13.457312 \t 6.663104 \t6.770400 \t 0.023808 \t 411.042977 s\n","498 \t 13.634365 \t 6.738676 \t6.818453 \t 0.077236 \t 411.627071 s\n","499 \t 13.673086 \t 6.997683 \t6.658325 \t 0.017078 \t 412.202375 s\n","500 \t 13.355928 \t 6.617609 \t6.706101 \t 0.032219 \t 412.804265 s\n","501 \t 13.497891 \t 6.761348 \t6.704691 \t 0.031851 \t 413.722023 s\n","502 \t 13.401820 \t 6.663769 \t6.708071 \t 0.029980 \t 414.411797 s\n","503 \t 13.437074 \t 6.742117 \t6.669838 \t 0.025118 \t 415.139186 s\n","504 \t 13.452353 \t 6.663517 \t6.764567 \t 0.024268 \t 415.907892 s\n","505 \t 13.465213 \t 6.734845 \t6.691997 \t 0.038371 \t 416.471258 s\n","506 \t 13.499140 \t 6.819036 \t6.647081 \t 0.033023 \t 417.043468 s\n","507 \t 13.459109 \t 6.740304 \t6.695128 \t 0.023677 \t 417.613633 s\n","508 \t 13.531623 \t 6.714351 \t6.787517 \t 0.029756 \t 418.193944 s\n","509 \t 13.354613 \t 6.671157 \t6.649519 \t 0.033937 \t 418.933325 s\n","510 \t 13.474604 \t 6.738778 \t6.700149 \t 0.035678 \t 419.502313 s\n","511 \t 13.384728 \t 6.640088 \t6.709811 \t 0.034829 \t 420.083194 s\n","512 \t 13.288493 \t 6.574533 \t6.662599 \t 0.051361 \t 420.664607 s\n","513 \t 13.423253 \t 6.735264 \t6.647949 \t 0.040039 \t 421.237723 s\n","514 \t 13.383252 \t 6.660095 \t6.694140 \t 0.029017 \t 421.808295 s\n","515 \t 13.383631 \t 6.640315 \t6.711382 \t 0.031934 \t 422.402535 s\n","516 \t 13.369951 \t 6.680271 \t6.650276 \t 0.039404 \t 422.999748 s\n","517 \t 13.338067 \t 6.665834 \t6.643155 \t 0.029077 \t 423.581942 s\n","518 \t 13.469638 \t 6.704466 \t6.722340 \t 0.042833 \t 424.324217 s\n","519 \t 13.417409 \t 6.712539 \t6.675463 \t 0.029407 \t 424.895386 s\n","520 \t 13.391879 \t 6.677789 \t6.680574 \t 0.033517 \t 425.468366 s\n","521 \t 13.282854 \t 6.609964 \t6.655597 \t 0.017292 \t 426.113093 s\n","522 \t 13.343966 \t 6.648113 \t6.667469 \t 0.028384 \t 426.825217 s\n","523 \t 13.330495 \t 6.666135 \t6.636837 \t 0.027523 \t 427.549859 s\n","524 \t 13.253818 \t 6.604564 \t6.624368 \t 0.024886 \t 428.273063 s\n","525 \t 13.312860 \t 6.659936 \t6.630405 \t 0.022518 \t 429.044348 s\n","526 \t 13.292061 \t 6.568572 \t6.683991 \t 0.039498 \t 429.622492 s\n","527 \t 13.305820 \t 6.626503 \t6.648862 \t 0.030455 \t 430.209818 s\n","528 \t 13.280868 \t 6.553845 \t6.670793 \t 0.056230 \t 430.949635 s\n","529 \t 13.256776 \t 6.554296 \t6.680006 \t 0.022474 \t 431.545051 s\n","530 \t 13.332590 \t 6.660654 \t6.649988 \t 0.021949 \t 432.115042 s\n","531 \t 13.218087 \t 6.561581 \t6.633764 \t 0.022742 \t 432.699778 s\n","532 \t 13.328434 \t 6.639998 \t6.646365 \t 0.042072 \t 433.268713 s\n","533 \t 13.387200 \t 6.750929 \t6.603459 \t 0.032812 \t 433.848526 s\n","534 \t 13.266811 \t 6.636174 \t6.596288 \t 0.034348 \t 434.420985 s\n","535 \t 13.261436 \t 6.583594 \t6.643353 \t 0.034489 \t 435.020879 s\n","536 \t 13.359457 \t 6.684187 \t6.637064 \t 0.038207 \t 435.612561 s\n","537 \t 13.268790 \t 6.633482 \t6.595150 \t 0.040158 \t 436.194118 s\n","538 \t 13.158041 \t 6.609168 \t6.522729 \t 0.026143 \t 436.931633 s\n","539 \t 13.147636 \t 6.575285 \t6.540112 \t 0.032239 \t 437.505013 s\n","540 \t 13.262112 \t 6.614794 \t6.618708 \t 0.028610 \t 438.095262 s\n","541 \t 13.237034 \t 6.636454 \t6.551368 \t 0.049212 \t 438.677105 s\n","542 \t 13.253071 \t 6.663634 \t6.561577 \t 0.027859 \t 439.307349 s\n","543 \t 13.271547 \t 6.695570 \t6.531114 \t 0.044863 \t 440.011681 s\n","544 \t 13.126586 \t 6.525399 \t6.554262 \t 0.046925 \t 440.694952 s\n","545 \t 13.187763 \t 6.556807 \t6.602424 \t 0.028532 \t 441.444815 s\n","546 \t 13.265391 \t 6.663393 \t6.554403 \t 0.047596 \t 442.218518 s\n","547 \t 13.156086 \t 6.603235 \t6.530950 \t 0.021901 \t 442.796131 s\n","548 \t 13.172265 \t 6.593100 \t6.542490 \t 0.036675 \t 443.540115 s\n","549 \t 13.278231 \t 6.619760 \t6.590309 \t 0.068161 \t 444.126777 s\n","550 \t 13.174121 \t 6.604741 \t6.511857 \t 0.057523 \t 444.700572 s\n","551 \t 13.113668 \t 6.543604 \t6.515570 \t 0.054494 \t 445.265657 s\n","552 \t 13.190762 \t 6.639953 \t6.519192 \t 0.031617 \t 445.837458 s\n","553 \t 13.286730 \t 6.668019 \t6.578166 \t 0.040546 \t 446.424352 s\n","554 \t 13.103761 \t 6.558992 \t6.517120 \t 0.027648 \t 447.007548 s\n","555 \t 13.065983 \t 6.510391 \t6.505943 \t 0.049649 \t 447.593731 s\n","556 \t 13.080711 \t 6.492063 \t6.570559 \t 0.018090 \t 448.181252 s\n","557 \t 13.045221 \t 6.461157 \t6.547543 \t 0.036522 \t 448.910406 s\n","558 \t 13.061664 \t 6.528936 \t6.509565 \t 0.023164 \t 449.475747 s\n","559 \t 13.079195 \t 6.497074 \t6.560598 \t 0.021523 \t 450.059463 s\n","560 \t 13.120559 \t 6.588790 \t6.503950 \t 0.027819 \t 450.640734 s\n","561 \t 13.043381 \t 6.524405 \t6.495538 \t 0.023438 \t 451.217308 s\n","562 \t 13.200459 \t 6.641090 \t6.526731 \t 0.032638 \t 451.794404 s\n","563 \t 13.128466 \t 6.603801 \t6.504328 \t 0.020337 \t 452.427886 s\n","564 \t 13.023079 \t 6.505262 \t6.501448 \t 0.016369 \t 453.131425 s\n","565 \t 13.111294 \t 6.575036 \t6.514961 \t 0.021297 \t 454.022341 s\n","566 \t 13.129380 \t 6.625411 \t6.490010 \t 0.013959 \t 454.772740 s\n","567 \t 13.046773 \t 6.556537 \t6.465795 \t 0.024441 \t 455.492460 s\n","568 \t 13.269218 \t 6.730737 \t6.510106 \t 0.028375 \t 456.064328 s\n","569 \t 13.079201 \t 6.579162 \t6.479964 \t 0.020074 \t 456.646810 s\n","570 \t 13.140280 \t 6.613392 \t6.505964 \t 0.020923 \t 457.228194 s\n","571 \t 13.001850 \t 6.426071 \t6.552076 \t 0.023703 \t 457.803101 s\n","572 \t 13.133202 \t 6.538207 \t6.559978 \t 0.035017 \t 458.367676 s\n","573 \t 13.019161 \t 6.495340 \t6.487856 \t 0.035965 \t 458.953050 s\n","574 \t 12.949940 \t 6.479380 \t6.444777 \t 0.025784 \t 459.532119 s\n","575 \t 13.129137 \t 6.574528 \t6.518518 \t 0.036091 \t 460.284427 s\n","576 \t 12.991801 \t 6.455386 \t6.493005 \t 0.043410 \t 460.858825 s\n","577 \t 13.105270 \t 6.546268 \t6.518813 \t 0.040190 \t 461.420768 s\n","578 \t 12.999000 \t 6.465517 \t6.490354 \t 0.043128 \t 461.994196 s\n","579 \t 13.020613 \t 6.473444 \t6.522032 \t 0.025136 \t 462.562399 s\n","580 \t 12.982538 \t 6.487885 \t6.455500 \t 0.039154 \t 463.149165 s\n","581 \t 12.907739 \t 6.403953 \t6.480568 \t 0.023218 \t 463.722803 s\n","582 \t 12.908600 \t 6.402277 \t6.471619 \t 0.034705 \t 464.284296 s\n","583 \t 13.017196 \t 6.491489 \t6.499352 \t 0.026356 \t 464.877625 s\n","584 \t 12.949870 \t 6.497315 \t6.427353 \t 0.025202 \t 465.462914 s\n","585 \t 13.059083 \t 6.564423 \t6.466224 \t 0.028436 \t 466.370410 s\n","586 \t 13.100107 \t 6.592242 \t6.487780 \t 0.020084 \t 467.062790 s\n","587 \t 13.129350 \t 6.551977 \t6.526553 \t 0.050820 \t 467.784669 s\n","588 \t 13.001858 \t 6.540212 \t6.439201 \t 0.022444 \t 468.543768 s\n","589 \t 13.035815 \t 6.505157 \t6.503474 \t 0.027184 \t 469.124134 s\n","590 \t 12.956962 \t 6.490597 \t6.447222 \t 0.019142 \t 469.690364 s\n","591 \t 12.954769 \t 6.433843 \t6.485965 \t 0.034962 \t 470.263633 s\n","592 \t 12.986909 \t 6.479179 \t6.489455 \t 0.018275 \t 470.842460 s\n","593 \t 12.921745 \t 6.424342 \t6.480733 \t 0.016670 \t 471.416676 s\n","594 \t 12.945016 \t 6.421178 \t6.492588 \t 0.031251 \t 471.988698 s\n","595 \t 12.962938 \t 6.453732 \t6.483453 \t 0.025753 \t 472.711708 s\n","596 \t 13.032800 \t 6.494664 \t6.513440 \t 0.024696 \t 473.291456 s\n","597 \t 12.920037 \t 6.474107 \t6.415196 \t 0.030734 \t 473.862361 s\n","598 \t 12.962775 \t 6.465286 \t6.469191 \t 0.028298 \t 474.435704 s\n","599 \t 13.039744 \t 6.481425 \t6.531692 \t 0.026627 \t 475.008543 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:29<00:00,  5.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4479\n","Hit@10: 0.6233\n","Hit@3: 0.4863\n","Hit@1: 0.3562\n","Link Prediction on Validation Set (All)\n","MRR: 0.4448\n","Hit@10: 0.6617\n","Hit@3: 0.4818\n","Hit@1: 0.3416\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3837\n","Hit@10: 0.5926\n","Hit@3: 0.4938\n","Hit@1: 0.2346\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4805\n","Hit@10: 0.6807\n","Hit@3: 0.5546\n","Hit@1: 0.3676\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2612\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2612\n","600 \t 12.942864 \t 6.485153 \t6.434302 \t 0.023409 \t 506.109361 s\n","601 \t 13.035720 \t 6.563492 \t6.414582 \t 0.057646 \t 506.821659 s\n","602 \t 13.053360 \t 6.538491 \t6.498664 \t 0.016206 \t 507.543395 s\n","603 \t 12.888120 \t 6.473872 \t6.397502 \t 0.016747 \t 508.418191 s\n","604 \t 12.930791 \t 6.427696 \t6.467824 \t 0.035272 \t 508.992098 s\n","605 \t 12.855695 \t 6.374430 \t6.457233 \t 0.024032 \t 509.563500 s\n","606 \t 13.014838 \t 6.529783 \t6.435901 \t 0.049153 \t 510.141448 s\n","607 \t 13.020193 \t 6.502506 \t6.475205 \t 0.042481 \t 510.713364 s\n","608 \t 12.940955 \t 6.483827 \t6.429952 \t 0.027177 \t 511.299763 s\n","609 \t 12.932932 \t 6.442154 \t6.459492 \t 0.031286 \t 511.879930 s\n","610 \t 12.884806 \t 6.352194 \t6.508828 \t 0.023784 \t 512.802999 s\n","611 \t 12.915391 \t 6.427982 \t6.450994 \t 0.036416 \t 513.906537 s\n","612 \t 12.820723 \t 6.419523 \t6.377060 \t 0.024139 \t 514.641171 s\n","613 \t 12.830483 \t 6.391862 \t6.417003 \t 0.021619 \t 515.208145 s\n","614 \t 12.917364 \t 6.442899 \t6.462984 \t 0.011480 \t 515.786901 s\n","615 \t 12.818911 \t 6.395591 \t6.393043 \t 0.030277 \t 516.352348 s\n","616 \t 12.825634 \t 6.402038 \t6.403405 \t 0.020192 \t 516.928595 s\n","617 \t 12.748593 \t 6.332245 \t6.396346 \t 0.020002 \t 517.521084 s\n","618 \t 12.869900 \t 6.406194 \t6.425663 \t 0.038043 \t 518.095959 s\n","619 \t 13.077400 \t 6.592839 \t6.415563 \t 0.068998 \t 518.817982 s\n","620 \t 12.911538 \t 6.472446 \t6.397819 \t 0.041273 \t 519.504744 s\n","621 \t 12.842211 \t 6.392671 \t6.434045 \t 0.015495 \t 520.418030 s\n","622 \t 12.889146 \t 6.426935 \t6.411902 \t 0.050308 \t 521.195696 s\n","623 \t 12.783839 \t 6.375122 \t6.378275 \t 0.030443 \t 521.853377 s\n","624 \t 12.931882 \t 6.472961 \t6.431703 \t 0.027218 \t 522.436568 s\n","625 \t 12.836261 \t 6.398646 \t6.398222 \t 0.039392 \t 523.021154 s\n","626 \t 12.731945 \t 6.293979 \t6.413609 \t 0.024357 \t 523.597419 s\n","627 \t 12.840530 \t 6.437646 \t6.382434 \t 0.020449 \t 524.161424 s\n","628 \t 12.870952 \t 6.474415 \t6.376662 \t 0.019875 \t 524.731853 s\n","629 \t 12.922544 \t 6.490219 \t6.412289 \t 0.020035 \t 525.298214 s\n","630 \t 12.835828 \t 6.454967 \t6.363550 \t 0.017310 \t 525.882444 s\n","631 \t 12.881694 \t 6.444538 \t6.408876 \t 0.028280 \t 526.616819 s\n","632 \t 12.781822 \t 6.352342 \t6.404611 \t 0.024870 \t 527.191655 s\n","633 \t 12.821244 \t 6.395977 \t6.395996 \t 0.029271 \t 527.781362 s\n","634 \t 12.829708 \t 6.358713 \t6.456598 \t 0.014397 \t 528.348775 s\n","635 \t 12.737396 \t 6.312945 \t6.406556 \t 0.017895 \t 528.925617 s\n","636 \t 12.836647 \t 6.400817 \t6.397466 \t 0.038363 \t 529.496972 s\n","637 \t 12.921260 \t 6.515315 \t6.391859 \t 0.014086 \t 530.087315 s\n","638 \t 12.937506 \t 6.481572 \t6.413463 \t 0.042472 \t 530.666273 s\n","639 \t 12.801022 \t 6.378247 \t6.404819 \t 0.017955 \t 531.418431 s\n","640 \t 12.832143 \t 6.403762 \t6.390965 \t 0.037416 \t 532.152778 s\n","641 \t 12.786607 \t 6.389179 \t6.374377 \t 0.023051 \t 532.899426 s\n","642 \t 12.742944 \t 6.339046 \t6.374304 \t 0.029593 \t 533.640714 s\n","643 \t 12.797667 \t 6.350652 \t6.416396 \t 0.030619 \t 534.439129 s\n","644 \t 12.713537 \t 6.334416 \t6.343680 \t 0.035440 \t 535.078910 s\n","645 \t 12.818854 \t 6.441240 \t6.359125 \t 0.018490 \t 535.644769 s\n","646 \t 12.717339 \t 6.291159 \t6.401121 \t 0.025059 \t 536.237675 s\n","647 \t 12.685758 \t 6.327349 \t6.345945 \t 0.012463 \t 536.807843 s\n","648 \t 12.746741 \t 6.380958 \t6.355628 \t 0.010155 \t 537.548906 s\n","649 \t 12.818217 \t 6.419565 \t6.374779 \t 0.023873 \t 538.124699 s\n","650 \t 12.795297 \t 6.358294 \t6.409564 \t 0.027440 \t 538.685886 s\n","651 \t 12.809755 \t 6.418510 \t6.373882 \t 0.017362 \t 539.260651 s\n","652 \t 12.791667 \t 6.380055 \t6.381196 \t 0.030415 \t 539.829971 s\n","653 \t 12.809371 \t 6.417415 \t6.372785 \t 0.019171 \t 540.403408 s\n","654 \t 12.753226 \t 6.354797 \t6.371292 \t 0.027136 \t 540.976026 s\n","655 \t 12.720427 \t 6.315404 \t6.392003 \t 0.013020 \t 541.561205 s\n","656 \t 12.703949 \t 6.322646 \t6.358150 \t 0.023153 \t 542.135583 s\n","657 \t 12.720603 \t 6.342075 \t6.352348 \t 0.026180 \t 542.698422 s\n","658 \t 12.741192 \t 6.324761 \t6.394884 \t 0.021548 \t 543.430214 s\n","659 \t 12.652297 \t 6.289921 \t6.349810 \t 0.012566 \t 544.006006 s\n","660 \t 12.764353 \t 6.404468 \t6.342853 \t 0.017031 \t 544.596646 s\n","661 \t 12.680419 \t 6.306020 \t6.363949 \t 0.010450 \t 545.332207 s\n","662 \t 12.766600 \t 6.401113 \t6.334602 \t 0.030885 \t 546.032629 s\n","663 \t 12.702594 \t 6.300433 \t6.379363 \t 0.022798 \t 546.720014 s\n","664 \t 12.730340 \t 6.336946 \t6.366904 \t 0.026490 \t 547.483567 s\n","665 \t 12.672578 \t 6.265199 \t6.370510 \t 0.036868 \t 548.161990 s\n","666 \t 12.628606 \t 6.305116 \t6.306608 \t 0.016881 \t 548.729930 s\n","667 \t 12.687282 \t 6.293444 \t6.359107 \t 0.034731 \t 549.296410 s\n","668 \t 12.674308 \t 6.334478 \t6.316100 \t 0.023730 \t 550.017267 s\n","669 \t 12.781191 \t 6.407894 \t6.357901 \t 0.015395 \t 550.618260 s\n","670 \t 12.565144 \t 6.239313 \t6.314498 \t 0.011332 \t 551.179495 s\n","671 \t 12.644944 \t 6.300749 \t6.323105 \t 0.021089 \t 551.749413 s\n","672 \t 12.672123 \t 6.308181 \t6.351047 \t 0.012896 \t 552.313715 s\n","673 \t 12.695803 \t 6.369510 \t6.306539 \t 0.019753 \t 552.885747 s\n","674 \t 12.765348 \t 6.435649 \t6.313061 \t 0.016637 \t 553.457812 s\n","675 \t 12.673016 \t 6.364070 \t6.297296 \t 0.011650 \t 554.047390 s\n","676 \t 12.637513 \t 6.281201 \t6.333245 \t 0.023066 \t 554.778634 s\n","677 \t 12.594212 \t 6.271472 \t6.297765 \t 0.024975 \t 555.345199 s\n","678 \t 12.546101 \t 6.235060 \t6.280631 \t 0.030409 \t 555.925206 s\n","679 \t 12.736624 \t 6.367801 \t6.343279 \t 0.025544 \t 556.494531 s\n","680 \t 12.683331 \t 6.361166 \t6.304296 \t 0.017869 \t 557.065238 s\n","681 \t 12.743897 \t 6.373817 \t6.343192 \t 0.026888 \t 557.630042 s\n","682 \t 12.668813 \t 6.311953 \t6.326516 \t 0.030344 \t 558.327390 s\n","683 \t 12.722872 \t 6.370897 \t6.313594 \t 0.038381 \t 559.010525 s\n","684 \t 12.605540 \t 6.254612 \t6.322438 \t 0.028490 \t 559.697903 s\n","685 \t 12.498172 \t 6.196213 \t6.287531 \t 0.014428 \t 560.650789 s\n","686 \t 12.565244 \t 6.262386 \t6.284854 \t 0.018005 \t 561.319046 s\n","687 \t 12.508129 \t 6.190393 \t6.305335 \t 0.012400 \t 561.888454 s\n","688 \t 12.575291 \t 6.234560 \t6.310729 \t 0.030001 \t 562.458420 s\n","689 \t 12.671638 \t 6.316817 \t6.328671 \t 0.026150 \t 563.037817 s\n","690 \t 12.545835 \t 6.236405 \t6.284051 \t 0.025379 \t 563.600188 s\n","691 \t 12.641346 \t 6.284910 \t6.340826 \t 0.015610 \t 564.182476 s\n","692 \t 12.621312 \t 6.314323 \t6.290364 \t 0.016625 \t 564.757010 s\n","693 \t 12.724298 \t 6.391036 \t6.322376 \t 0.010886 \t 565.325114 s\n","694 \t 12.472485 \t 6.182846 \t6.270576 \t 0.019063 \t 565.889593 s\n","695 \t 12.681719 \t 6.370621 \t6.295240 \t 0.015858 \t 566.617460 s\n","696 \t 12.695454 \t 6.388363 \t6.293525 \t 0.013565 \t 567.192495 s\n","697 \t 12.684289 \t 6.389480 \t6.281386 \t 0.013423 \t 567.773190 s\n","698 \t 12.715997 \t 6.333556 \t6.297863 \t 0.084577 \t 568.346772 s\n","699 \t 12.588171 \t 6.289857 \t6.286191 \t 0.012123 \t 568.923228 s\n","700 \t 12.540310 \t 6.211806 \t6.314585 \t 0.013919 \t 569.494102 s\n","701 \t 12.491728 \t 6.167153 \t6.313477 \t 0.011098 \t 570.068142 s\n","702 \t 12.695014 \t 6.382104 \t6.296035 \t 0.016874 \t 570.651582 s\n","703 \t 12.476444 \t 6.192466 \t6.264694 \t 0.019283 \t 571.294356 s\n","704 \t 12.528344 \t 6.197920 \t6.318275 \t 0.012150 \t 571.972375 s\n","705 \t 12.548944 \t 6.226750 \t6.309033 \t 0.013160 \t 572.862408 s\n","706 \t 12.639162 \t 6.328931 \t6.291749 \t 0.018482 \t 573.608658 s\n","707 \t 12.534644 \t 6.272062 \t6.246217 \t 0.016365 \t 574.285658 s\n","708 \t 12.583564 \t 6.283370 \t6.284498 \t 0.015696 \t 574.869467 s\n","709 \t 12.581663 \t 6.281968 \t6.279375 \t 0.020320 \t 575.442058 s\n","710 \t 12.645238 \t 6.326149 \t6.298795 \t 0.020294 \t 576.015708 s\n","711 \t 12.566617 \t 6.279622 \t6.276023 \t 0.010972 \t 576.604300 s\n","712 \t 12.646518 \t 6.371049 \t6.259851 \t 0.015617 \t 577.171426 s\n","713 \t 12.537083 \t 6.243464 \t6.275039 \t 0.018581 \t 577.759848 s\n","714 \t 12.520123 \t 6.214420 \t6.292068 \t 0.013636 \t 578.342930 s\n","715 \t 12.497667 \t 6.196944 \t6.285000 \t 0.015722 \t 579.083995 s\n","716 \t 12.558496 \t 6.262993 \t6.284187 \t 0.011316 \t 579.672119 s\n","717 \t 12.556899 \t 6.240460 \t6.300777 \t 0.015663 \t 580.242137 s\n","718 \t 12.512160 \t 6.245040 \t6.256410 \t 0.010710 \t 580.848329 s\n","719 \t 12.558303 \t 6.281546 \t6.262360 \t 0.014397 \t 581.427359 s\n","720 \t 12.588113 \t 6.338193 \t6.237643 \t 0.012277 \t 581.996747 s\n","721 \t 12.560744 \t 6.278551 \t6.265181 \t 0.017012 \t 582.569317 s\n","722 \t 12.551727 \t 6.256989 \t6.274639 \t 0.020100 \t 583.153605 s\n","723 \t 12.493403 \t 6.209459 \t6.270737 \t 0.013207 \t 583.725088 s\n","724 \t 12.591766 \t 6.319642 \t6.260207 \t 0.011918 \t 584.533611 s\n","725 \t 12.524536 \t 6.262324 \t6.249290 \t 0.012923 \t 585.232107 s\n","726 \t 12.543996 \t 6.269848 \t6.255569 \t 0.018578 \t 585.922991 s\n","727 \t 12.558221 \t 6.239578 \t6.310211 \t 0.008431 \t 586.631760 s\n","728 \t 12.588469 \t 6.313157 \t6.255697 \t 0.019615 \t 587.331697 s\n","729 \t 12.512669 \t 6.226441 \t6.270086 \t 0.016141 \t 587.900005 s\n","730 \t 12.530002 \t 6.229186 \t6.283662 \t 0.017154 \t 588.504796 s\n","731 \t 12.541451 \t 6.256629 \t6.271576 \t 0.013247 \t 589.077737 s\n","732 \t 12.518927 \t 6.263086 \t6.234784 \t 0.021057 \t 589.807429 s\n","733 \t 12.571748 \t 6.297164 \t6.259376 \t 0.015207 \t 590.377097 s\n","734 \t 12.554883 \t 6.268666 \t6.273824 \t 0.012393 \t 590.958294 s\n","735 \t 12.514151 \t 6.197304 \t6.300694 \t 0.016153 \t 591.548848 s\n","736 \t 12.589074 \t 6.323306 \t6.251187 \t 0.014581 \t 592.124313 s\n","737 \t 12.440244 \t 6.173713 \t6.257610 \t 0.008920 \t 592.710585 s\n","738 \t 12.481565 \t 6.179154 \t6.286738 \t 0.015673 \t 593.291907 s\n","739 \t 12.562939 \t 6.326256 \t6.221462 \t 0.015221 \t 593.856905 s\n","740 \t 12.500731 \t 6.208201 \t6.273408 \t 0.019121 \t 594.434824 s\n","741 \t 12.494180 \t 6.223411 \t6.257002 \t 0.013766 \t 595.004959 s\n","742 \t 12.450436 \t 6.198654 \t6.237217 \t 0.014565 \t 595.727362 s\n","743 \t 12.581239 \t 6.298925 \t6.264729 \t 0.017584 \t 596.302239 s\n","744 \t 12.486375 \t 6.210052 \t6.260098 \t 0.016225 \t 596.873165 s\n","745 \t 12.484964 \t 6.225015 \t6.243878 \t 0.016072 \t 597.524252 s\n","746 \t 12.511834 \t 6.217035 \t6.273890 \t 0.020909 \t 598.202702 s\n","747 \t 12.512444 \t 6.246406 \t6.254828 \t 0.011210 \t 598.896272 s\n","748 \t 12.447630 \t 6.172277 \t6.250359 \t 0.024994 \t 599.623006 s\n","749 \t 12.490920 \t 6.215102 \t6.265642 \t 0.010176 \t 600.348042 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:29<00:00,  5.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4359\n","Hit@10: 0.6233\n","Hit@3: 0.4692\n","Hit@1: 0.3459\n","Link Prediction on Validation Set (All)\n","MRR: 0.4408\n","Hit@10: 0.6452\n","Hit@3: 0.4917\n","Hit@1: 0.3350\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4101\n","Hit@10: 0.6296\n","Hit@3: 0.5556\n","Hit@1: 0.2407\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4963\n","Hit@10: 0.6786\n","Hit@3: 0.5882\n","Hit@1: 0.3739\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2347\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2347\n","750 \t 12.406488 \t 6.152986 \t6.242603 \t 0.010900 \t 631.440644 s\n","751 \t 12.443480 \t 6.214563 \t6.217917 \t 0.011000 \t 632.021375 s\n","752 \t 12.576444 \t 6.338778 \t6.229704 \t 0.007962 \t 632.765151 s\n","753 \t 12.408473 \t 6.128846 \t6.251495 \t 0.028132 \t 633.345892 s\n","754 \t 12.450855 \t 6.169405 \t6.261016 \t 0.020435 \t 633.915483 s\n","755 \t 12.480016 \t 6.244800 \t6.213686 \t 0.021530 \t 634.498721 s\n","756 \t 12.452938 \t 6.213234 \t6.222922 \t 0.016782 \t 635.082903 s\n","757 \t 12.373422 \t 6.137959 \t6.221373 \t 0.014090 \t 635.658392 s\n","758 \t 12.504062 \t 6.222219 \t6.268412 \t 0.013431 \t 636.276125 s\n","759 \t 12.459758 \t 6.197248 \t6.244169 \t 0.018342 \t 637.461010 s\n","760 \t 12.580733 \t 6.298027 \t6.268392 \t 0.014314 \t 638.641340 s\n","761 \t 12.467258 \t 6.211468 \t6.241796 \t 0.013994 \t 639.363219 s\n","762 \t 12.512989 \t 6.289493 \t6.211054 \t 0.012442 \t 640.315825 s\n","763 \t 12.430768 \t 6.200395 \t6.213209 \t 0.017164 \t 640.891813 s\n","764 \t 12.508104 \t 6.216317 \t6.275473 \t 0.016315 \t 641.502226 s\n","765 \t 12.472941 \t 6.167591 \t6.289225 \t 0.016125 \t 642.072756 s\n","766 \t 12.356360 \t 6.132645 \t6.207880 \t 0.015836 \t 642.644460 s\n","767 \t 12.471681 \t 6.189375 \t6.272662 \t 0.009644 \t 643.222829 s\n","768 \t 12.495006 \t 6.271644 \t6.209269 \t 0.014093 \t 643.818471 s\n","769 \t 12.384280 \t 6.135297 \t6.235583 \t 0.013401 \t 644.383252 s\n","770 \t 12.462677 \t 6.242684 \t6.208080 \t 0.011913 \t 644.965184 s\n","771 \t 12.383514 \t 6.153740 \t6.218089 \t 0.011686 \t 645.558053 s\n","772 \t 12.425869 \t 6.158501 \t6.252909 \t 0.014458 \t 646.282881 s\n","773 \t 12.439395 \t 6.212921 \t6.214273 \t 0.012202 \t 646.864704 s\n","774 \t 12.544075 \t 6.265005 \t6.265945 \t 0.013125 \t 647.432960 s\n","775 \t 12.354184 \t 6.114557 \t6.226165 \t 0.013462 \t 648.018345 s\n","776 \t 12.353888 \t 6.129079 \t6.211099 \t 0.013709 \t 648.602580 s\n","777 \t 12.404353 \t 6.174461 \t6.217434 \t 0.012458 \t 649.171688 s\n","778 \t 12.435537 \t 6.197983 \t6.228720 \t 0.008835 \t 649.764210 s\n","779 \t 12.379991 \t 6.160818 \t6.206610 \t 0.012563 \t 650.386914 s\n","780 \t 12.459026 \t 6.243411 \t6.205863 \t 0.009753 \t 651.102850 s\n","781 \t 12.367146 \t 6.136336 \t6.220278 \t 0.010532 \t 651.996216 s\n","782 \t 12.450680 \t 6.234428 \t6.203200 \t 0.013052 \t 652.725965 s\n","783 \t 12.448715 \t 6.189783 \t6.248646 \t 0.010287 \t 653.488185 s\n","784 \t 12.461733 \t 6.230487 \t6.220366 \t 0.010880 \t 654.075890 s\n","785 \t 12.403274 \t 6.189405 \t6.203769 \t 0.010100 \t 654.648757 s\n","786 \t 12.454900 \t 6.248679 \t6.187028 \t 0.019193 \t 655.244359 s\n","787 \t 12.450481 \t 6.251971 \t6.188055 \t 0.010456 \t 655.820878 s\n","788 \t 12.389646 \t 6.158692 \t6.221196 \t 0.009758 \t 656.409090 s\n","789 \t 12.370421 \t 6.149050 \t6.209945 \t 0.011426 \t 657.158798 s\n","790 \t 12.306588 \t 6.096142 \t6.202113 \t 0.008332 \t 657.731183 s\n","791 \t 12.411265 \t 6.185415 \t6.210152 \t 0.015699 \t 658.308064 s\n","792 \t 12.364534 \t 6.173313 \t6.172291 \t 0.018929 \t 658.881603 s\n","793 \t 12.419363 \t 6.193682 \t6.214025 \t 0.011657 \t 659.475391 s\n","794 \t 12.361065 \t 6.142304 \t6.207052 \t 0.011709 \t 660.056118 s\n","795 \t 12.302976 \t 6.116570 \t6.173999 \t 0.012407 \t 660.638015 s\n","796 \t 12.481248 \t 6.266658 \t6.203145 \t 0.011446 \t 661.227869 s\n","797 \t 12.284079 \t 6.065666 \t6.204759 \t 0.013653 \t 661.801291 s\n","798 \t 12.386766 \t 6.177058 \t6.200005 \t 0.009702 \t 662.378294 s\n","799 \t 12.329324 \t 6.129057 \t6.185673 \t 0.014594 \t 663.113138 s\n","800 \t 12.418900 \t 6.217532 \t6.191779 \t 0.009589 \t 663.779473 s\n","801 \t 12.328296 \t 6.126987 \t6.190709 \t 0.010601 \t 664.500528 s\n","802 \t 12.282148 \t 6.080462 \t6.186816 \t 0.014871 \t 665.179101 s\n","803 \t 12.429700 \t 6.201803 \t6.209354 \t 0.018543 \t 665.922117 s\n","804 \t 12.393429 \t 6.178332 \t6.205761 \t 0.009336 \t 666.658714 s\n","805 \t 12.361873 \t 6.149725 \t6.202228 \t 0.009920 \t 667.231203 s\n","806 \t 12.361416 \t 6.168551 \t6.181858 \t 0.011008 \t 667.805295 s\n","807 \t 12.379554 \t 6.152557 \t6.216023 \t 0.010974 \t 668.374212 s\n","808 \t 12.349927 \t 6.127339 \t6.213871 \t 0.008717 \t 668.959952 s\n","809 \t 12.335540 \t 6.106447 \t6.217650 \t 0.011442 \t 669.698700 s\n","810 \t 12.428663 \t 6.205680 \t6.214573 \t 0.008409 \t 670.273195 s\n","811 \t 12.416046 \t 6.225783 \t6.183097 \t 0.007166 \t 670.845015 s\n","812 \t 12.234104 \t 6.043332 \t6.177356 \t 0.013417 \t 671.425913 s\n","813 \t 12.426177 \t 6.239504 \t6.174266 \t 0.012407 \t 671.998097 s\n","814 \t 12.364914 \t 6.167524 \t6.184027 \t 0.013364 \t 672.561817 s\n","815 \t 12.385224 \t 6.188950 \t6.184680 \t 0.011595 \t 673.146053 s\n","816 \t 12.416776 \t 6.197639 \t6.208212 \t 0.010924 \t 673.721139 s\n","817 \t 12.380924 \t 6.195193 \t6.174170 \t 0.011562 \t 674.289825 s\n","818 \t 12.301649 \t 6.098192 \t6.189268 \t 0.014189 \t 674.878469 s\n","819 \t 12.391816 \t 6.157433 \t6.223387 \t 0.010996 \t 675.603184 s\n","820 \t 12.272600 \t 6.037689 \t6.218509 \t 0.016401 \t 676.195717 s\n","821 \t 12.407846 \t 6.206393 \t6.187912 \t 0.013542 \t 676.862849 s\n","822 \t 12.306422 \t 6.110394 \t6.185622 \t 0.010406 \t 677.561983 s\n","823 \t 12.307888 \t 6.121176 \t6.176415 \t 0.010297 \t 678.246090 s\n","824 \t 12.397154 \t 6.206287 \t6.181140 \t 0.009727 \t 678.975998 s\n","825 \t 12.374228 \t 6.145591 \t6.219833 \t 0.008804 \t 679.731081 s\n","826 \t 12.305522 \t 6.091436 \t6.198630 \t 0.015456 \t 680.307416 s\n","827 \t 12.449229 \t 6.253399 \t6.185086 \t 0.010745 \t 680.875060 s\n","828 \t 12.371835 \t 6.148712 \t6.213253 \t 0.009870 \t 681.597000 s\n","829 \t 12.331951 \t 6.120939 \t6.202815 \t 0.008196 \t 682.171646 s\n","830 \t 12.270191 \t 6.066817 \t6.194675 \t 0.008699 \t 682.743963 s\n","831 \t 12.304832 \t 6.104741 \t6.186580 \t 0.013512 \t 683.315788 s\n","832 \t 12.341178 \t 6.137337 \t6.196738 \t 0.007103 \t 683.879927 s\n","833 \t 12.369790 \t 6.197835 \t6.161538 \t 0.010417 \t 684.453472 s\n","834 \t 12.283298 \t 6.101880 \t6.173983 \t 0.007436 \t 685.022600 s\n","835 \t 12.296885 \t 6.120455 \t6.168042 \t 0.008389 \t 685.598609 s\n","836 \t 12.362828 \t 6.186834 \t6.168406 \t 0.007587 \t 686.336125 s\n","837 \t 12.335878 \t 6.126062 \t6.197448 \t 0.012368 \t 686.912481 s\n","838 \t 12.303629 \t 6.134807 \t6.159442 \t 0.009380 \t 687.483171 s\n","839 \t 12.310408 \t 6.101824 \t6.194143 \t 0.014441 \t 688.052922 s\n","840 \t 12.269968 \t 6.102424 \t6.159870 \t 0.007673 \t 688.626846 s\n","841 \t 12.366042 \t 6.154471 \t6.205179 \t 0.006393 \t 689.210654 s\n","842 \t 12.301680 \t 6.102791 \t6.188048 \t 0.010841 \t 689.835596 s\n","843 \t 12.234473 \t 6.035401 \t6.188615 \t 0.010457 \t 690.564280 s\n","844 \t 12.370951 \t 6.214354 \t6.135993 \t 0.020605 \t 691.254349 s\n","845 \t 12.340516 \t 6.194003 \t6.136201 \t 0.010312 \t 691.956308 s\n","846 \t 12.318719 \t 6.141615 \t6.167947 \t 0.009156 \t 692.898600 s\n","847 \t 12.254340 \t 6.067170 \t6.179819 \t 0.007352 \t 693.465150 s\n","848 \t 12.336151 \t 6.102753 \t6.221555 \t 0.011844 \t 694.055977 s\n","849 \t 12.302641 \t 6.118975 \t6.173042 \t 0.010624 \t 694.628839 s\n","850 \t 12.355997 \t 6.172275 \t6.173196 \t 0.010525 \t 695.211512 s\n","851 \t 12.388387 \t 6.163650 \t6.218607 \t 0.006130 \t 695.785679 s\n","852 \t 12.271323 \t 6.102679 \t6.157795 \t 0.010849 \t 696.346537 s\n","853 \t 12.338649 \t 6.141204 \t6.189110 \t 0.008336 \t 696.921047 s\n","854 \t 12.358776 \t 6.196530 \t6.154783 \t 0.007463 \t 697.493408 s\n","855 \t 12.432644 \t 6.230970 \t6.193760 \t 0.007915 \t 698.077256 s\n","856 \t 12.340835 \t 6.155178 \t6.175570 \t 0.010086 \t 698.816125 s\n","857 \t 12.355267 \t 6.145908 \t6.201976 \t 0.007383 \t 699.391996 s\n","858 \t 12.244246 \t 6.080190 \t6.156184 \t 0.007873 \t 699.990045 s\n","859 \t 12.349415 \t 6.148190 \t6.189979 \t 0.011245 \t 700.565956 s\n","860 \t 12.243904 \t 6.063343 \t6.171600 \t 0.008961 \t 701.145874 s\n","861 \t 12.294192 \t 6.095425 \t6.188156 \t 0.010610 \t 701.747228 s\n","862 \t 12.326550 \t 6.144228 \t6.175068 \t 0.007252 \t 702.329616 s\n","863 \t 12.192387 \t 6.007127 \t6.175013 \t 0.010248 \t 702.919727 s\n","864 \t 12.445740 \t 6.244569 \t6.191411 \t 0.009760 \t 703.827357 s\n","865 \t 12.243334 \t 6.033731 \t6.200646 \t 0.008956 \t 704.531357 s\n","866 \t 12.204036 \t 6.035091 \t6.160100 \t 0.008845 \t 705.241223 s\n","867 \t 12.337136 \t 6.152406 \t6.167330 \t 0.017399 \t 706.006315 s\n","868 \t 12.303333 \t 6.137377 \t6.156779 \t 0.009177 \t 706.591576 s\n","869 \t 12.286874 \t 6.089967 \t6.186987 \t 0.009920 \t 707.157941 s\n","870 \t 12.231033 \t 6.073710 \t6.152536 \t 0.004788 \t 707.742166 s\n","871 \t 12.381818 \t 6.221802 \t6.150082 \t 0.009934 \t 708.316157 s\n","872 \t 12.309714 \t 6.135508 \t6.166378 \t 0.007829 \t 708.892023 s\n","873 \t 12.211605 \t 6.034467 \t6.168060 \t 0.009078 \t 709.627930 s\n","874 \t 12.265548 \t 6.071439 \t6.186051 \t 0.008058 \t 710.206049 s\n","875 \t 12.240706 \t 6.053772 \t6.180694 \t 0.006240 \t 710.797419 s\n","876 \t 12.235358 \t 6.081680 \t6.147390 \t 0.006287 \t 711.378299 s\n","877 \t 12.310200 \t 6.140190 \t6.162697 \t 0.007313 \t 711.960024 s\n","878 \t 12.216527 \t 6.040841 \t6.167533 \t 0.008153 \t 712.547930 s\n","879 \t 12.232391 \t 6.054960 \t6.170268 \t 0.007164 \t 713.126105 s\n","880 \t 12.316747 \t 6.161074 \t6.148577 \t 0.007096 \t 713.706986 s\n","881 \t 12.294049 \t 6.104549 \t6.183219 \t 0.006280 \t 714.284132 s\n","882 \t 12.203153 \t 6.034740 \t6.158390 \t 0.010023 \t 714.877292 s\n","883 \t 12.227634 \t 6.076902 \t6.137949 \t 0.012784 \t 715.625943 s\n","884 \t 12.364516 \t 6.176474 \t6.177703 \t 0.010339 \t 716.239345 s\n","885 \t 12.180636 \t 6.008464 \t6.163914 \t 0.008258 \t 716.947396 s\n","886 \t 12.348969 \t 6.169058 \t6.170670 \t 0.009241 \t 717.669523 s\n","887 \t 12.329842 \t 6.177498 \t6.141195 \t 0.011149 \t 718.374466 s\n","888 \t 12.267418 \t 6.090749 \t6.160611 \t 0.016058 \t 719.153573 s\n","889 \t 12.279976 \t 6.138809 \t6.130625 \t 0.010541 \t 719.724402 s\n","890 \t 12.231907 \t 6.099426 \t6.125358 \t 0.007123 \t 720.316443 s\n","891 \t 12.287514 \t 6.123881 \t6.154467 \t 0.009166 \t 720.906341 s\n","892 \t 12.245477 \t 6.081839 \t6.157657 \t 0.005982 \t 721.495771 s\n","893 \t 12.284773 \t 6.108659 \t6.169074 \t 0.007040 \t 722.238842 s\n","894 \t 12.223980 \t 6.063045 \t6.153617 \t 0.007318 \t 722.818300 s\n","895 \t 12.322686 \t 6.151294 \t6.161475 \t 0.009917 \t 723.417079 s\n","896 \t 12.364849 \t 6.226404 \t6.130240 \t 0.008204 \t 724.004210 s\n","897 \t 12.221187 \t 6.059906 \t6.155314 \t 0.005966 \t 724.594343 s\n","898 \t 12.279142 \t 6.116826 \t6.153380 \t 0.008936 \t 725.179873 s\n","899 \t 12.263539 \t 6.090247 \t6.164655 \t 0.008637 \t 725.751296 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:30<00:00,  5.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4444\n","Hit@10: 0.6164\n","Hit@3: 0.4897\n","Hit@1: 0.3630\n","Link Prediction on Validation Set (All)\n","MRR: 0.4420\n","Hit@10: 0.6221\n","Hit@3: 0.4884\n","Hit@1: 0.3498\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3746\n","Hit@10: 0.5864\n","Hit@3: 0.5000\n","Hit@1: 0.2160\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4839\n","Hit@10: 0.6702\n","Hit@3: 0.5609\n","Hit@1: 0.3676\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2091\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2091\n","900 \t 12.271827 \t 6.115189 \t6.147736 \t 0.008903 \t 757.851338 s\n","901 \t 12.336692 \t 6.163190 \t6.167820 \t 0.005682 \t 758.585369 s\n","902 \t 12.283425 \t 6.122813 \t6.149439 \t 0.011173 \t 759.276717 s\n","903 \t 12.290145 \t 6.155339 \t6.127814 \t 0.006992 \t 759.853083 s\n","904 \t 12.310359 \t 6.144226 \t6.158809 \t 0.007324 \t 760.428742 s\n","905 \t 12.270947 \t 6.137519 \t6.123015 \t 0.010414 \t 761.017247 s\n","906 \t 12.254388 \t 6.109563 \t6.136824 \t 0.008002 \t 761.597415 s\n","907 \t 12.298015 \t 6.129166 \t6.156335 \t 0.012514 \t 762.267436 s\n","908 \t 12.260561 \t 6.112306 \t6.141405 \t 0.006849 \t 763.247280 s\n","909 \t 12.204918 \t 6.041454 \t6.156663 \t 0.006802 \t 764.488440 s\n","910 \t 12.289222 \t 6.124114 \t6.159087 \t 0.006022 \t 765.060533 s\n","911 \t 12.268882 \t 6.114113 \t6.144968 \t 0.009801 \t 765.623523 s\n","912 \t 12.197644 \t 6.035861 \t6.156420 \t 0.005363 \t 766.210749 s\n","913 \t 12.313900 \t 6.146972 \t6.158853 \t 0.008075 \t 766.782670 s\n","914 \t 12.293989 \t 6.140781 \t6.145450 \t 0.007759 \t 767.373248 s\n","915 \t 12.290752 \t 6.121816 \t6.161605 \t 0.007331 \t 767.961167 s\n","916 \t 12.236926 \t 6.078127 \t6.151707 \t 0.007091 \t 768.541947 s\n","917 \t 12.293163 \t 6.149927 \t6.135511 \t 0.007724 \t 769.118927 s\n","918 \t 12.311485 \t 6.152628 \t6.152518 \t 0.006340 \t 769.836468 s\n","919 \t 12.257440 \t 6.088659 \t6.162225 \t 0.006557 \t 770.732541 s\n","920 \t 12.295540 \t 6.170041 \t6.117446 \t 0.008053 \t 771.434967 s\n","921 \t 12.194126 \t 6.033207 \t6.151875 \t 0.009044 \t 772.218045 s\n","922 \t 12.256571 \t 6.094427 \t6.150743 \t 0.011401 \t 772.838093 s\n","923 \t 12.341002 \t 6.187890 \t6.144517 \t 0.008594 \t 773.417140 s\n","924 \t 12.274829 \t 6.105061 \t6.154472 \t 0.015297 \t 773.987987 s\n","925 \t 12.352203 \t 6.172489 \t6.171505 \t 0.008209 \t 774.567919 s\n","926 \t 12.262926 \t 6.117919 \t6.136087 \t 0.008920 \t 775.131899 s\n","927 \t 12.203394 \t 6.060722 \t6.137371 \t 0.005302 \t 775.720641 s\n","928 \t 12.242110 \t 6.097940 \t6.136536 \t 0.007634 \t 776.296587 s\n","929 \t 12.185530 \t 6.020784 \t6.158638 \t 0.006108 \t 777.011530 s\n","930 \t 12.254025 \t 6.106969 \t6.140535 \t 0.006521 \t 777.593978 s\n","931 \t 12.270136 \t 6.109696 \t6.156548 \t 0.003893 \t 778.166362 s\n","932 \t 12.308428 \t 6.159939 \t6.140050 \t 0.008438 \t 778.759940 s\n","933 \t 12.212908 \t 6.071523 \t6.134882 \t 0.006503 \t 779.337028 s\n","934 \t 12.227612 \t 6.068611 \t6.154878 \t 0.004123 \t 779.903854 s\n","935 \t 12.158343 \t 6.007970 \t6.142080 \t 0.008293 \t 780.475976 s\n","936 \t 12.253535 \t 6.091994 \t6.154169 \t 0.007372 \t 781.047755 s\n","937 \t 12.179971 \t 6.023741 \t6.149967 \t 0.006263 \t 781.769358 s\n","938 \t 12.256871 \t 6.120069 \t6.127998 \t 0.008804 \t 782.337708 s\n","939 \t 12.208941 \t 6.070960 \t6.132528 \t 0.005454 \t 783.065795 s\n","940 \t 12.325703 \t 6.151287 \t6.159296 \t 0.015120 \t 783.769961 s\n","941 \t 12.251843 \t 6.095160 \t6.149980 \t 0.006704 \t 784.476798 s\n","942 \t 12.203772 \t 6.059639 \t6.137729 \t 0.006403 \t 785.230710 s\n","943 \t 12.220853 \t 6.043655 \t6.170723 \t 0.006475 \t 785.920481 s\n","944 \t 12.309587 \t 6.172621 \t6.127878 \t 0.009089 \t 786.486788 s\n","945 \t 12.268982 \t 6.116397 \t6.148395 \t 0.004189 \t 787.065858 s\n","946 \t 12.308313 \t 6.161573 \t6.138566 \t 0.008174 \t 787.801375 s\n","947 \t 12.260416 \t 6.134338 \t6.122297 \t 0.003781 \t 788.370844 s\n","948 \t 12.214624 \t 6.070135 \t6.138002 \t 0.006487 \t 789.008131 s\n","949 \t 12.160511 \t 6.010124 \t6.140818 \t 0.009568 \t 789.581729 s\n","950 \t 12.323225 \t 6.182670 \t6.134651 \t 0.005903 \t 790.157824 s\n","951 \t 12.270453 \t 6.134138 \t6.126943 \t 0.009373 \t 790.730773 s\n","952 \t 12.190185 \t 6.031430 \t6.145598 \t 0.013156 \t 791.314401 s\n","953 \t 12.264529 \t 6.109965 \t6.145850 \t 0.008714 \t 791.903342 s\n","954 \t 12.228990 \t 6.071494 \t6.152373 \t 0.005124 \t 792.486574 s\n","955 \t 12.193705 \t 6.051804 \t6.136666 \t 0.005235 \t 793.079888 s\n","956 \t 12.244061 \t 6.098310 \t6.140317 \t 0.005434 \t 793.808925 s\n","957 \t 12.191661 \t 6.049559 \t6.135710 \t 0.006392 \t 794.388532 s\n","958 \t 12.122036 \t 5.994289 \t6.120872 \t 0.006874 \t 794.967344 s\n","959 \t 12.115075 \t 5.970761 \t6.138927 \t 0.005386 \t 795.542739 s\n","960 \t 12.152599 \t 5.992597 \t6.154138 \t 0.005864 \t 796.273712 s\n","961 \t 12.246034 \t 6.102833 \t6.135369 \t 0.007833 \t 796.951552 s\n","962 \t 12.239791 \t 6.117846 \t6.117522 \t 0.004423 \t 797.674227 s\n","963 \t 12.205166 \t 6.066700 \t6.133815 \t 0.004652 \t 798.419839 s\n","964 \t 12.185157 \t 6.036143 \t6.140785 \t 0.008229 \t 799.102135 s\n","965 \t 12.241101 \t 6.098162 \t6.135763 \t 0.007176 \t 799.678542 s\n","966 \t 12.243466 \t 6.093767 \t6.142957 \t 0.006743 \t 800.404191 s\n","967 \t 12.223802 \t 6.067465 \t6.149522 \t 0.006815 \t 800.977326 s\n","968 \t 12.344537 \t 6.203215 \t6.130754 \t 0.010568 \t 801.555989 s\n","969 \t 12.102890 \t 5.968695 \t6.125783 \t 0.008413 \t 802.126004 s\n","970 \t 12.306533 \t 6.174531 \t6.127635 \t 0.004367 \t 802.695248 s\n","971 \t 12.264647 \t 6.117649 \t6.139675 \t 0.007322 \t 803.262383 s\n","972 \t 12.229659 \t 6.060572 \t6.164499 \t 0.004588 \t 803.848217 s\n","973 \t 12.187624 \t 6.036456 \t6.144802 \t 0.006366 \t 804.421238 s\n","974 \t 12.269214 \t 6.140255 \t6.123760 \t 0.005199 \t 805.006828 s\n","975 \t 12.169913 \t 6.036840 \t6.128374 \t 0.004699 \t 805.587476 s\n","976 \t 12.214059 \t 6.066702 \t6.141976 \t 0.005381 \t 806.303416 s\n","977 \t 12.250552 \t 6.107016 \t6.138860 \t 0.004676 \t 806.877101 s\n","978 \t 12.257425 \t 6.112251 \t6.139716 \t 0.005458 \t 807.443223 s\n","979 \t 12.231069 \t 6.080530 \t6.142416 \t 0.008123 \t 808.033242 s\n","980 \t 12.270721 \t 6.133234 \t6.127220 \t 0.010268 \t 808.616598 s\n","981 \t 12.260151 \t 6.117180 \t6.135761 \t 0.007211 \t 809.285064 s\n","982 \t 12.167820 \t 6.047316 \t6.114405 \t 0.006099 \t 809.966156 s\n","983 \t 12.180552 \t 6.062825 \t6.112258 \t 0.005468 \t 810.646446 s\n","984 \t 12.161002 \t 5.999207 \t6.152540 \t 0.009254 \t 811.364476 s\n","985 \t 12.248957 \t 6.097541 \t6.144826 \t 0.006589 \t 812.255813 s\n","986 \t 12.268301 \t 6.118323 \t6.140106 \t 0.009872 \t 812.841306 s\n","987 \t 12.283772 \t 6.154053 \t6.125192 \t 0.004527 \t 813.404074 s\n","988 \t 12.260780 \t 6.092584 \t6.161409 \t 0.006787 \t 813.991215 s\n","989 \t 12.258591 \t 6.091891 \t6.146613 \t 0.020088 \t 814.560523 s\n","990 \t 12.291695 \t 6.156827 \t6.127963 \t 0.006905 \t 815.131075 s\n","991 \t 12.177055 \t 6.028270 \t6.142049 \t 0.006736 \t 815.699580 s\n","992 \t 12.235466 \t 6.112125 \t6.117837 \t 0.005503 \t 816.268089 s\n","993 \t 12.156439 \t 6.022103 \t6.129179 \t 0.005158 \t 816.998486 s\n","994 \t 12.208485 \t 6.065814 \t6.137063 \t 0.005608 \t 817.566739 s\n","995 \t 12.286494 \t 6.143585 \t6.135719 \t 0.007190 \t 818.158739 s\n","996 \t 12.231426 \t 6.107156 \t6.118697 \t 0.005573 \t 818.727819 s\n","997 \t 12.212340 \t 6.070362 \t6.137064 \t 0.004914 \t 819.294252 s\n","998 \t 12.195911 \t 6.077859 \t6.112529 \t 0.005523 \t 819.866212 s\n","999 \t 12.208793 \t 6.064189 \t6.139885 \t 0.004718 \t 820.443354 s\n","1000 \t 12.245958 \t 6.129359 \t6.107255 \t 0.009344 \t 821.016208 s\n","1001 \t 12.262880 \t 6.109200 \t6.149263 \t 0.004417 \t 821.605397 s\n","1002 \t 12.126764 \t 5.978972 \t6.140838 \t 0.006953 \t 822.296145 s\n","1003 \t 12.233941 \t 6.088110 \t6.139295 \t 0.006536 \t 823.163616 s\n","1004 \t 12.237746 \t 6.110687 \t6.121317 \t 0.005741 \t 823.875457 s\n","1005 \t 12.200408 \t 6.063961 \t6.128845 \t 0.007601 \t 824.588490 s\n","1006 \t 12.173611 \t 6.050767 \t6.117622 \t 0.005222 \t 825.296680 s\n","1007 \t 12.257156 \t 6.116678 \t6.135039 \t 0.005440 \t 825.881255 s\n","1008 \t 12.292169 \t 6.131566 \t6.155550 \t 0.005052 \t 826.454800 s\n","1009 \t 12.270601 \t 6.134387 \t6.130539 \t 0.005675 \t 827.028211 s\n","1010 \t 12.171748 \t 6.055157 \t6.110217 \t 0.006374 \t 827.623512 s\n","1011 \t 12.282512 \t 6.114407 \t6.162004 \t 0.006100 \t 828.218945 s\n","1012 \t 12.216463 \t 6.067063 \t6.144713 \t 0.004687 \t 828.789772 s\n","1013 \t 12.227594 \t 6.080953 \t6.140754 \t 0.005887 \t 829.510398 s\n","1014 \t 12.238910 \t 6.092015 \t6.143308 \t 0.003587 \t 830.084275 s\n","1015 \t 12.313677 \t 6.163149 \t6.141804 \t 0.008724 \t 830.662614 s\n","1016 \t 12.219624 \t 6.061055 \t6.149510 \t 0.009059 \t 831.228153 s\n","1017 \t 12.150333 \t 6.003537 \t6.139885 \t 0.006910 \t 831.810145 s\n","1018 \t 12.146536 \t 6.008004 \t6.133065 \t 0.005468 \t 832.377338 s\n","1019 \t 12.277747 \t 6.129786 \t6.142514 \t 0.005446 \t 832.955455 s\n","1020 \t 12.249064 \t 6.101277 \t6.140092 \t 0.007694 \t 833.526944 s\n","1021 \t 12.280197 \t 6.142232 \t6.132743 \t 0.005221 \t 834.118999 s\n","1022 \t 12.261466 \t 6.138856 \t6.112267 \t 0.010343 \t 834.690431 s\n","1023 \t 12.206100 \t 6.059770 \t6.142040 \t 0.004289 \t 835.482462 s\n","1024 \t 12.320937 \t 6.175557 \t6.140459 \t 0.004920 \t 836.181829 s\n","1025 \t 12.184031 \t 6.067700 \t6.109708 \t 0.006623 \t 836.864193 s\n","1026 \t 12.325509 \t 6.202013 \t6.117361 \t 0.006134 \t 837.567345 s\n","1027 \t 12.173694 \t 6.020574 \t6.146825 \t 0.006295 \t 838.322114 s\n","1028 \t 12.173728 \t 6.047537 \t6.119614 \t 0.006577 \t 838.897106 s\n","1029 \t 12.289952 \t 6.154138 \t6.132000 \t 0.003814 \t 839.473310 s\n","1030 \t 12.184033 \t 6.045978 \t6.130764 \t 0.007291 \t 840.049141 s\n","1031 \t 12.186136 \t 6.038402 \t6.142082 \t 0.005652 \t 840.605169 s\n","1032 \t 12.139292 \t 6.016073 \t6.115767 \t 0.007451 \t 841.320973 s\n","1033 \t 12.285023 \t 6.123413 \t6.153574 \t 0.008036 \t 841.889635 s\n","1034 \t 12.203814 \t 6.082338 \t6.114795 \t 0.006681 \t 842.456369 s\n","1035 \t 12.240099 \t 6.098300 \t6.133796 \t 0.008002 \t 843.018890 s\n","1036 \t 12.223355 \t 6.067504 \t6.149892 \t 0.005959 \t 843.610765 s\n","1037 \t 12.271190 \t 6.144064 \t6.122491 \t 0.004635 \t 844.193441 s\n","1038 \t 12.196865 \t 6.044298 \t6.146618 \t 0.005948 \t 844.785382 s\n","1039 \t 12.227906 \t 6.094512 \t6.128947 \t 0.004447 \t 845.363456 s\n","1040 \t 12.209352 \t 6.069754 \t6.135001 \t 0.004598 \t 846.097932 s\n","1041 \t 12.241220 \t 6.121055 \t6.111079 \t 0.009087 \t 846.676579 s\n","1042 \t 12.284235 \t 6.140550 \t6.138922 \t 0.004762 \t 847.247729 s\n","1043 \t 12.135153 \t 5.974957 \t6.153308 \t 0.006889 \t 847.817993 s\n","1044 \t 12.252490 \t 6.101779 \t6.145926 \t 0.004784 \t 848.405512 s\n","1045 \t 12.318755 \t 6.202364 \t6.108421 \t 0.007970 \t 849.129115 s\n","1046 \t 12.164064 \t 6.038146 \t6.119337 \t 0.006581 \t 849.825333 s\n","1047 \t 12.281972 \t 6.136227 \t6.139055 \t 0.006690 \t 850.544622 s\n","1048 \t 12.218594 \t 6.065113 \t6.145769 \t 0.007712 \t 851.304362 s\n","1049 \t 12.229017 \t 6.106677 \t6.115466 \t 0.006875 \t 851.942869 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:29<00:00,  5.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4479\n","Hit@10: 0.6233\n","Hit@3: 0.4863\n","Hit@1: 0.3596\n","Link Prediction on Validation Set (All)\n","MRR: 0.4489\n","Hit@10: 0.6287\n","Hit@3: 0.4901\n","Hit@1: 0.3581\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3886\n","Hit@10: 0.6049\n","Hit@3: 0.5062\n","Hit@1: 0.2407\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4947\n","Hit@10: 0.6744\n","Hit@3: 0.5756\n","Hit@1: 0.3824\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2138\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2138\n"]}]},{"cell_type":"markdown","source":["# Test.py\n"],"metadata":{"id":"-BsiHiMQoArV"}},{"cell_type":"code","source":["KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix.cuda(), # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix.cuda(), # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix.cuda(), # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix.cuda(), # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask.cuda(), # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask.cuda(), # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","def load_id_mapping(file_path):\n","    id2name = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip() == \"\" or line.startswith(\"#\"):  # 주석 또는 공백 무시\n","                continue\n","            parts = line.strip().split('\\t')\n","            if len(parts) != 2:\n","                continue\n","            name, idx = parts\n","            id2name[int(idx)] = name\n","    return id2name\n","\n","id2ent = load_id_mapping(\"entity2id.txt\")\n","id2rel = load_id_mapping(\"relation2id.txt\")\n","\n","def convert_triplet_ids_to_names(triplet, id2ent, id2rel, num_ent, num_rel):\n","    triplet_named = []\n","    for idx, val in enumerate(triplet):\n","        if idx % 2 == 0:  # entity or numeric value\n","            if val < num_ent:\n","                triplet_named.append(id2ent.get(val, f\"[ENT:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[NUM:{val - num_ent}]\")\n","        else:  # relation\n","            if val < num_rel:\n","                triplet_named.append(id2rel.get(val, f\"[REL:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[MASK_REL]\")\n","    return triplet_named\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTHNKG-OA_NT/checkpoint/Reproduce/VTHNOA_NT_maximg==3/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","                else: # qualifier\n","                    sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n"],"metadata":{"id":"ChVIC_5BHELi","executionInfo":{"status":"aborted","timestamp":1748302850730,"user_tz":-540,"elapsed":2,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":null,"outputs":[]}]}