{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748100990803,"user_tz":-540,"elapsed":1431,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"f4f64ebb-c0fa-4891-c257-030be4930b3b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1748100990805,"user_tz":-540,"elapsed":5,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1748100990805,"user_tz":-540,"elapsed":3,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0: # numeric triplet 처리\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","        else:\n","            self.proj_ent_vis = nn.Identity()\n","            self.proj_rel_vis = nn.Identity()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1748100990845,"user_tz":-540,"elapsed":42,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1748100990866,"user_tz":-540,"elapsed":23,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-OA_NT/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, self.dir+ \"visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile(self.dir + 'visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load(self.dir + 'visual_features_ent_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile(self.dir + 'visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load(self.dir + 'visual_features_rel_sorted.pt')\n","        elif os.path.isfile(self.dir + 'visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load(self.dir + 'textual_features_ent.pt')\n","        self.rel2txt = torch.load(self.dir + 'textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-OA_NT/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1748100990867,"user_tz":-540,"elapsed":5,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"e13651d2-1f44-4925-c3dd-2ca7587957f1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-OA_NT\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNOA_NT_maximg==3\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()"],"metadata":{"id":"HmgS2m1upzp1","executionInfo":{"status":"ok","timestamp":1748100990867,"user_tz":-540,"elapsed":2,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748101881521,"user_tz":-540,"elapsed":890654,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"11a6d56a-01a8-4238-9ce3-820933f5d3d5"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 50.645278 \t 11.374642 \t11.526589 \t 27.744046 \t 2.140488 s\n","1 \t 31.283751 \t 10.891421 \t11.647701 \t 8.744628 \t 2.680443 s\n","2 \t 22.663210 \t 10.544717 \t11.064764 \t 1.053730 \t 3.226141 s\n","3 \t 21.469259 \t 10.421921 \t10.512493 \t 0.534845 \t 3.744148 s\n","4 \t 21.287278 \t 10.301317 \t10.338272 \t 0.647689 \t 4.351036 s\n","5 \t 20.690597 \t 10.139756 \t10.240909 \t 0.309932 \t 5.163809 s\n","6 \t 20.426011 \t 10.105886 \t9.983785 \t 0.336341 \t 5.819314 s\n","7 \t 20.341168 \t 10.014142 \t9.982244 \t 0.344783 \t 6.503206 s\n","8 \t 20.137476 \t 9.922321 \t9.975168 \t 0.239987 \t 7.163796 s\n","9 \t 19.827422 \t 9.780276 \t9.799653 \t 0.247492 \t 7.703404 s\n","10 \t 19.882655 \t 9.875149 \t9.622625 \t 0.384883 \t 8.233643 s\n","11 \t 19.841756 \t 9.945889 \t9.611157 \t 0.284711 \t 8.774584 s\n","12 \t 19.819093 \t 9.857706 \t9.618767 \t 0.342618 \t 9.314621 s\n","13 \t 19.937278 \t 9.863760 \t9.698166 \t 0.375352 \t 9.993062 s\n","14 \t 19.660441 \t 9.821615 \t9.535309 \t 0.303517 \t 10.545210 s\n","15 \t 19.611742 \t 9.801488 \t9.584702 \t 0.225552 \t 11.070120 s\n","16 \t 19.778648 \t 9.858853 \t9.563324 \t 0.356470 \t 11.608254 s\n","17 \t 19.448186 \t 9.762613 \t9.419969 \t 0.265605 \t 12.140866 s\n","18 \t 19.617831 \t 9.721333 \t9.497878 \t 0.398620 \t 12.672587 s\n","19 \t 19.657301 \t 9.822868 \t9.497247 \t 0.337186 \t 13.220903 s\n","20 \t 19.670355 \t 9.827047 \t9.528347 \t 0.314961 \t 13.752977 s\n","21 \t 19.371957 \t 9.745230 \t9.350081 \t 0.276646 \t 14.280075 s\n","22 \t 19.667169 \t 9.825526 \t9.566754 \t 0.274888 \t 14.807817 s\n","23 \t 19.412882 \t 9.675012 \t9.383389 \t 0.354481 \t 15.469376 s\n","24 \t 19.363949 \t 9.566701 \t9.323500 \t 0.473748 \t 16.015944 s\n","25 \t 19.205928 \t 9.580526 \t9.326450 \t 0.298952 \t 16.545988 s\n","26 \t 19.471513 \t 9.826914 \t9.357068 \t 0.287531 \t 17.110662 s\n","27 \t 19.372316 \t 9.738273 \t9.293264 \t 0.340780 \t 17.791723 s\n","28 \t 19.525097 \t 9.673331 \t9.318138 \t 0.533628 \t 18.439611 s\n","29 \t 19.325459 \t 9.680551 \t9.369006 \t 0.275902 \t 19.087417 s\n","30 \t 19.187002 \t 9.535980 \t9.479026 \t 0.171996 \t 19.808086 s\n","31 \t 19.076777 \t 9.577713 \t9.291726 \t 0.207338 \t 20.432738 s\n","32 \t 19.290380 \t 9.688436 \t9.403860 \t 0.198084 \t 21.091238 s\n","33 \t 18.997098 \t 9.541337 \t9.236762 \t 0.218998 \t 21.624473 s\n","34 \t 19.252684 \t 9.603069 \t9.415515 \t 0.234098 \t 22.154047 s\n","35 \t 18.927206 \t 9.498675 \t9.187414 \t 0.241116 \t 22.683983 s\n","36 \t 18.958719 \t 9.561564 \t9.217470 \t 0.179685 \t 23.224359 s\n","37 \t 18.708448 \t 9.482172 \t9.061590 \t 0.164686 \t 23.764009 s\n","38 \t 18.762177 \t 9.449670 \t9.114086 \t 0.198420 \t 24.298621 s\n","39 \t 18.638107 \t 9.399304 \t9.058656 \t 0.180147 \t 24.827170 s\n","40 \t 18.786580 \t 9.349224 \t9.162931 \t 0.274425 \t 25.376544 s\n","41 \t 18.743406 \t 9.457045 \t9.135518 \t 0.150843 \t 26.040181 s\n","42 \t 18.379748 \t 9.220577 \t8.976667 \t 0.182504 \t 26.577621 s\n","43 \t 18.452506 \t 9.357374 \t8.930674 \t 0.164457 \t 27.115388 s\n","44 \t 18.330665 \t 9.056586 \t9.162833 \t 0.111247 \t 27.643224 s\n","45 \t 18.399746 \t 9.158693 \t9.140590 \t 0.100464 \t 28.192953 s\n","46 \t 18.370461 \t 9.225196 \t8.867735 \t 0.277530 \t 28.724728 s\n","47 \t 18.684642 \t 9.442893 \t9.032034 \t 0.209715 \t 29.260032 s\n","48 \t 18.459551 \t 9.371067 \t8.889905 \t 0.198580 \t 29.785198 s\n","49 \t 18.663600 \t 9.410067 \t8.970847 \t 0.282686 \t 30.539981 s\n","50 \t 18.482123 \t 9.276585 \t8.956886 \t 0.248651 \t 31.195709 s\n","51 \t 18.166807 \t 9.141351 \t8.892709 \t 0.132747 \t 31.828051 s\n","52 \t 17.880546 \t 9.011350 \t8.736097 \t 0.133099 \t 32.499607 s\n","53 \t 18.207091 \t 9.207064 \t8.866158 \t 0.133868 \t 33.160650 s\n","54 \t 18.205136 \t 9.171356 \t8.879562 \t 0.154219 \t 33.707153 s\n","55 \t 18.361750 \t 9.225911 \t8.940184 \t 0.195656 \t 34.237261 s\n","56 \t 17.738130 \t 9.052409 \t8.618900 \t 0.066821 \t 34.785436 s\n","57 \t 17.930368 \t 9.161639 \t8.626040 \t 0.142689 \t 35.318731 s\n","58 \t 17.931535 \t 9.045119 \t8.734522 \t 0.151894 \t 35.855753 s\n","59 \t 18.009814 \t 9.080546 \t8.717542 \t 0.211725 \t 36.535418 s\n","60 \t 18.158999 \t 8.964634 \t8.877370 \t 0.316994 \t 37.061167 s\n","61 \t 17.931845 \t 9.094967 \t8.700511 \t 0.136366 \t 37.596938 s\n","62 \t 17.828861 \t 9.050160 \t8.609073 \t 0.169628 \t 38.129956 s\n","63 \t 17.725163 \t 8.945452 \t8.668491 \t 0.111220 \t 38.667739 s\n","64 \t 17.869562 \t 9.064855 \t8.676173 \t 0.128536 \t 39.210590 s\n","65 \t 17.932947 \t 9.144294 \t8.637604 \t 0.151049 \t 39.747214 s\n","66 \t 17.836836 \t 9.124665 \t8.639853 \t 0.072317 \t 40.277643 s\n","67 \t 17.554989 \t 8.986852 \t8.510124 \t 0.058013 \t 40.811517 s\n","68 \t 17.537061 \t 8.896946 \t8.545586 \t 0.094528 \t 41.351478 s\n","69 \t 17.511367 \t 8.921543 \t8.464965 \t 0.124860 \t 42.024433 s\n","70 \t 17.584553 \t 8.890874 \t8.584863 \t 0.108814 \t 42.562563 s\n","71 \t 17.680072 \t 8.962447 \t8.611948 \t 0.105677 \t 43.122542 s\n","72 \t 17.771923 \t 8.982151 \t8.713237 \t 0.076535 \t 43.808971 s\n","73 \t 17.283143 \t 8.860159 \t8.306642 \t 0.116342 \t 44.443479 s\n","74 \t 17.583148 \t 8.999031 \t8.474160 \t 0.109957 \t 45.082748 s\n","75 \t 17.379193 \t 8.843086 \t8.463285 \t 0.072822 \t 45.806540 s\n","76 \t 17.375248 \t 8.851355 \t8.443474 \t 0.080418 \t 46.434083 s\n","77 \t 17.472467 \t 8.880630 \t8.505964 \t 0.085873 \t 46.977815 s\n","78 \t 17.436693 \t 8.938115 \t8.397424 \t 0.101154 \t 47.642830 s\n","79 \t 17.303950 \t 8.738478 \t8.459770 \t 0.105702 \t 48.180616 s\n","80 \t 17.308917 \t 8.783130 \t8.443244 \t 0.082543 \t 48.713539 s\n","81 \t 17.546170 \t 8.987093 \t8.489190 \t 0.069887 \t 49.256017 s\n","82 \t 17.170025 \t 8.742666 \t8.355812 \t 0.071546 \t 49.792322 s\n","83 \t 17.334247 \t 8.707723 \t8.561080 \t 0.065444 \t 50.343177 s\n","84 \t 17.429656 \t 8.917149 \t8.438432 \t 0.074075 \t 50.878693 s\n","85 \t 17.344598 \t 8.902331 \t8.336834 \t 0.105431 \t 51.452086 s\n","86 \t 17.402920 \t 8.893732 \t8.425223 \t 0.083964 \t 51.993013 s\n","87 \t 17.097492 \t 8.764284 \t8.287116 \t 0.046092 \t 52.656357 s\n","88 \t 17.048188 \t 8.749083 \t8.237089 \t 0.062018 \t 53.196960 s\n","89 \t 17.311474 \t 8.781097 \t8.478851 \t 0.051525 \t 53.733481 s\n","90 \t 17.276505 \t 8.801586 \t8.398351 \t 0.076569 \t 54.286533 s\n","91 \t 17.090158 \t 8.775579 \t8.263923 \t 0.050656 \t 54.822381 s\n","92 \t 17.202640 \t 8.817603 \t8.326383 \t 0.058654 \t 55.365001 s\n","93 \t 16.985353 \t 8.710360 \t8.199276 \t 0.075717 \t 55.906150 s\n","94 \t 16.972550 \t 8.593194 \t8.328186 \t 0.051171 \t 56.518499 s\n","95 \t 17.244261 \t 8.808208 \t8.382400 \t 0.053653 \t 57.150617 s\n","96 \t 17.260485 \t 8.805527 \t8.387705 \t 0.067253 \t 57.976221 s\n","97 \t 16.988358 \t 8.612317 \t8.318133 \t 0.057907 \t 58.658128 s\n","98 \t 17.146241 \t 8.789443 \t8.303857 \t 0.052941 \t 59.308566 s\n","99 \t 16.908285 \t 8.623495 \t8.246973 \t 0.037819 \t 59.847069 s\n","100 \t 16.896748 \t 8.695476 \t8.150410 \t 0.050861 \t 60.396022 s\n","101 \t 16.800541 \t 8.610520 \t8.100915 \t 0.089104 \t 60.931149 s\n","102 \t 16.801291 \t 8.588253 \t8.165071 \t 0.047967 \t 61.472706 s\n","103 \t 16.884700 \t 8.652216 \t8.189947 \t 0.042537 \t 62.001330 s\n","104 \t 17.046617 \t 8.620990 \t8.320412 \t 0.105213 \t 62.548868 s\n","105 \t 16.878246 \t 8.626928 \t8.200438 \t 0.050880 \t 63.080540 s\n","106 \t 16.900838 \t 8.716800 \t8.133152 \t 0.050885 \t 63.751982 s\n","107 \t 16.993385 \t 8.677893 \t8.253593 \t 0.061900 \t 64.287231 s\n","108 \t 17.179237 \t 8.779105 \t8.327495 \t 0.072637 \t 64.827402 s\n","109 \t 16.936708 \t 8.670371 \t8.235020 \t 0.031317 \t 65.363843 s\n","110 \t 16.751961 \t 8.661418 \t8.050639 \t 0.039905 \t 65.907079 s\n","111 \t 16.777355 \t 8.619975 \t8.078596 \t 0.078785 \t 66.441944 s\n","112 \t 16.868203 \t 8.622705 \t8.216396 \t 0.029103 \t 67.000427 s\n","113 \t 16.917393 \t 8.716154 \t8.154798 \t 0.046440 \t 67.541718 s\n","114 \t 16.838298 \t 8.625232 \t8.155186 \t 0.057881 \t 68.095253 s\n","115 \t 16.927979 \t 8.578997 \t8.294962 \t 0.054020 \t 68.638910 s\n","116 \t 17.064055 \t 8.764278 \t8.263133 \t 0.036644 \t 69.370968 s\n","117 \t 16.795569 \t 8.603773 \t8.160145 \t 0.031651 \t 70.028643 s\n","118 \t 17.042292 \t 8.787211 \t8.204234 \t 0.050845 \t 70.666110 s\n","119 \t 16.955506 \t 8.569358 \t8.343905 \t 0.042243 \t 71.378960 s\n","120 \t 16.969271 \t 8.735772 \t8.193048 \t 0.040451 \t 72.088077 s\n","121 \t 16.813777 \t 8.633491 \t8.151745 \t 0.028541 \t 72.620582 s\n","122 \t 16.764731 \t 8.531350 \t8.185243 \t 0.048139 \t 73.159834 s\n","123 \t 16.789424 \t 8.601842 \t8.141861 \t 0.045721 \t 73.698053 s\n","124 \t 16.720285 \t 8.723868 \t7.955572 \t 0.040847 \t 74.247400 s\n","125 \t 16.807778 \t 8.646009 \t8.102786 \t 0.058983 \t 74.899894 s\n","126 \t 16.624313 \t 8.490333 \t8.098358 \t 0.035623 \t 75.455890 s\n","127 \t 16.535357 \t 8.462943 \t8.043982 \t 0.028433 \t 76.008911 s\n","128 \t 16.778044 \t 8.584528 \t8.160568 \t 0.032947 \t 76.544070 s\n","129 \t 16.864382 \t 8.679159 \t8.150421 \t 0.034802 \t 77.090605 s\n","130 \t 16.911936 \t 8.545943 \t8.338221 \t 0.027773 \t 77.633287 s\n","131 \t 16.803178 \t 8.514057 \t8.252292 \t 0.036831 \t 78.201383 s\n","132 \t 16.628677 \t 8.584406 \t7.992390 \t 0.051881 \t 78.751156 s\n","133 \t 16.553219 \t 8.591644 \t7.936218 \t 0.025357 \t 79.297925 s\n","134 \t 16.778933 \t 8.630115 \t8.120948 \t 0.027870 \t 79.976242 s\n","135 \t 16.519785 \t 8.518277 \t7.971742 \t 0.029766 \t 80.534627 s\n","136 \t 16.688067 \t 8.554857 \t8.110631 \t 0.022579 \t 81.065053 s\n","137 \t 16.762897 \t 8.559188 \t8.172483 \t 0.031227 \t 81.657036 s\n","138 \t 16.511854 \t 8.457765 \t8.018340 \t 0.035749 \t 82.254659 s\n","139 \t 16.732704 \t 8.473288 \t8.231595 \t 0.027821 \t 82.907186 s\n","140 \t 16.514802 \t 8.481006 \t8.001617 \t 0.032180 \t 83.554929 s\n","141 \t 16.716255 \t 8.604824 \t8.066381 \t 0.045051 \t 84.192363 s\n","142 \t 16.634866 \t 8.645689 \t7.957095 \t 0.032082 \t 84.934284 s\n","143 \t 16.535472 \t 8.471073 \t8.030254 \t 0.034145 \t 85.664297 s\n","144 \t 16.546561 \t 8.588308 \t7.925483 \t 0.032770 \t 86.200744 s\n","145 \t 16.538354 \t 8.482108 \t8.018898 \t 0.037347 \t 86.742335 s\n","146 \t 16.553581 \t 8.496543 \t8.028758 \t 0.028279 \t 87.280502 s\n","147 \t 16.522346 \t 8.514516 \t7.967994 \t 0.039836 \t 87.823660 s\n","148 \t 16.565343 \t 8.498557 \t8.023777 \t 0.043009 \t 88.363248 s\n","149 \t 16.580780 \t 8.528727 \t8.018711 \t 0.033342 \t 88.921681 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/162 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 162/162 [00:26<00:00,  6.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3733\n","Hit@10: 0.5308\n","Hit@3: 0.3870\n","Hit@1: 0.2979\n","Link Prediction on Validation Set (All)\n","MRR: 0.3766\n","Hit@10: 0.6122\n","Hit@3: 0.4307\n","Hit@1: 0.2591\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4265\n","Hit@10: 0.6790\n","Hit@3: 0.5370\n","Hit@1: 0.2654\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4841\n","Hit@10: 0.7290\n","Hit@3: 0.5735\n","Hit@1: 0.3403\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2744\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2744\n","150 \t 16.838959 \t 8.672431 \t8.109982 \t 0.056546 \t 120.785053 s\n","151 \t 16.739394 \t 8.541755 \t8.135008 \t 0.062631 \t 121.387650 s\n","152 \t 17.134570 \t 8.738973 \t8.294423 \t 0.101174 \t 122.196876 s\n","153 \t 17.131441 \t 8.803185 \t8.263702 \t 0.064554 \t 122.845047 s\n","154 \t 17.151453 \t 8.820824 \t8.180388 \t 0.150242 \t 123.519314 s\n","155 \t 17.128823 \t 8.739297 \t8.333619 \t 0.055907 \t 124.199665 s\n","156 \t 17.236336 \t 8.711451 \t8.437348 \t 0.087537 \t 124.735718 s\n","157 \t 16.896586 \t 8.734273 \t8.088478 \t 0.073834 \t 125.282040 s\n","158 \t 16.941136 \t 8.633212 \t8.221777 \t 0.086147 \t 125.818367 s\n","159 \t 17.012777 \t 8.669105 \t8.230563 \t 0.113110 \t 126.374049 s\n","160 \t 16.998924 \t 8.753104 \t8.149323 \t 0.096497 \t 127.000306 s\n","161 \t 17.119003 \t 8.701734 \t8.290645 \t 0.126624 \t 127.637604 s\n","162 \t 17.002063 \t 8.611807 \t8.211512 \t 0.178744 \t 128.476228 s\n","163 \t 16.960779 \t 8.599554 \t8.228692 \t 0.132534 \t 129.142159 s\n","164 \t 16.674583 \t 8.535378 \t8.082059 \t 0.057146 \t 129.789939 s\n","165 \t 16.893156 \t 8.678948 \t8.127249 \t 0.086959 \t 130.435174 s\n","166 \t 16.801705 \t 8.563366 \t8.153689 \t 0.084649 \t 131.077700 s\n","167 \t 16.493331 \t 8.446674 \t8.004532 \t 0.042125 \t 131.727363 s\n","168 \t 16.697130 \t 8.454229 \t8.124688 \t 0.118214 \t 132.386243 s\n","169 \t 16.574640 \t 8.506181 \t8.006185 \t 0.062276 \t 133.015997 s\n","170 \t 16.874028 \t 8.771168 \t8.034338 \t 0.068523 \t 133.664354 s\n","171 \t 16.967142 \t 8.606723 \t8.236263 \t 0.124155 \t 134.632288 s\n","172 \t 16.769754 \t 8.567203 \t8.084671 \t 0.117881 \t 135.351819 s\n","173 \t 16.835874 \t 8.534569 \t8.203678 \t 0.097627 \t 136.065294 s\n","174 \t 16.653591 \t 8.576503 \t7.985914 \t 0.091174 \t 136.789208 s\n","175 \t 16.608647 \t 8.471196 \t8.031783 \t 0.105668 \t 137.633706 s\n","176 \t 16.659178 \t 8.570316 \t7.983995 \t 0.104868 \t 138.241864 s\n","177 \t 16.320167 \t 8.375539 \t7.909255 \t 0.035373 \t 138.837716 s\n","178 \t 16.710051 \t 8.471544 \t8.067729 \t 0.170778 \t 139.427156 s\n","179 \t 16.653280 \t 8.305293 \t8.223137 \t 0.124851 \t 140.177368 s\n","180 \t 16.665215 \t 8.464390 \t8.147854 \t 0.052971 \t 140.797349 s\n","181 \t 16.440526 \t 8.346286 \t7.999016 \t 0.095223 \t 141.397559 s\n","182 \t 16.528356 \t 8.372527 \t8.028142 \t 0.127687 \t 142.021854 s\n","183 \t 16.568982 \t 8.494886 \t7.968707 \t 0.105389 \t 142.558821 s\n","184 \t 16.482264 \t 8.375815 \t8.040171 \t 0.066278 \t 143.111281 s\n","185 \t 16.373675 \t 8.311867 \t7.995119 \t 0.066690 \t 143.656621 s\n","186 \t 16.443358 \t 8.476939 \t7.871965 \t 0.094454 \t 144.220230 s\n","187 \t 16.222408 \t 8.287122 \t7.892932 \t 0.042355 \t 144.777320 s\n","188 \t 16.233478 \t 8.281977 \t7.925356 \t 0.026145 \t 145.318226 s\n","189 \t 16.485756 \t 8.351545 \t8.057429 \t 0.076781 \t 146.013909 s\n","190 \t 16.333458 \t 8.298561 \t7.990726 \t 0.044170 \t 146.549650 s\n","191 \t 16.459059 \t 8.366917 \t8.033809 \t 0.058333 \t 147.091232 s\n","192 \t 16.491739 \t 8.374300 \t8.038488 \t 0.078952 \t 147.642861 s\n","193 \t 16.585278 \t 8.475292 \t7.947219 \t 0.162766 \t 148.321249 s\n","194 \t 16.566758 \t 8.401626 \t8.089793 \t 0.075339 \t 148.974484 s\n","195 \t 16.472258 \t 8.462219 \t7.921534 \t 0.088504 \t 149.630068 s\n","196 \t 16.280876 \t 8.333480 \t7.852593 \t 0.094803 \t 150.333432 s\n","197 \t 16.302155 \t 8.290847 \t7.946343 \t 0.064965 \t 150.991225 s\n","198 \t 16.404259 \t 8.387967 \t7.924054 \t 0.092239 \t 151.553919 s\n","199 \t 16.188043 \t 8.239278 \t7.916085 \t 0.032680 \t 152.240698 s\n","200 \t 16.137332 \t 8.194780 \t7.895152 \t 0.047400 \t 152.787147 s\n","201 \t 16.092597 \t 8.256938 \t7.790150 \t 0.045507 \t 153.338859 s\n","202 \t 16.040436 \t 8.233789 \t7.742627 \t 0.064020 \t 153.888138 s\n","203 \t 16.085257 \t 8.265519 \t7.741323 \t 0.078415 \t 154.435153 s\n","204 \t 15.851069 \t 8.103662 \t7.673528 \t 0.073879 \t 154.984038 s\n","205 \t 16.048596 \t 8.220455 \t7.748848 \t 0.079294 \t 155.526821 s\n","206 \t 16.093992 \t 8.314222 \t7.737689 \t 0.042081 \t 156.201834 s\n","207 \t 15.915501 \t 8.113539 \t7.737396 \t 0.064565 \t 156.760862 s\n","208 \t 16.262484 \t 8.271208 \t7.933328 \t 0.057947 \t 157.309639 s\n","209 \t 16.223387 \t 8.233469 \t7.872094 \t 0.117823 \t 157.850158 s\n","210 \t 15.813667 \t 8.090220 \t7.624077 \t 0.099370 \t 158.398857 s\n","211 \t 15.950260 \t 8.044441 \t7.858811 \t 0.047008 \t 158.952279 s\n","212 \t 16.157777 \t 8.207935 \t7.797225 \t 0.152617 \t 159.510709 s\n","213 \t 15.870686 \t 8.057573 \t7.728575 \t 0.084538 \t 160.054361 s\n","214 \t 15.737877 \t 7.995411 \t7.696122 \t 0.046343 \t 160.623444 s\n","215 \t 15.862203 \t 8.081414 \t7.716656 \t 0.064133 \t 161.298079 s\n","216 \t 15.864831 \t 8.035503 \t7.761158 \t 0.068169 \t 162.121054 s\n","217 \t 15.934904 \t 8.152724 \t7.687479 \t 0.094701 \t 162.773058 s\n","218 \t 15.734879 \t 7.920459 \t7.757558 \t 0.056863 \t 163.494107 s\n","219 \t 15.781024 \t 8.066581 \t7.649215 \t 0.065227 \t 164.105223 s\n","220 \t 15.646889 \t 8.001302 \t7.586685 \t 0.058902 \t 164.659042 s\n","221 \t 15.797743 \t 8.053765 \t7.703874 \t 0.040104 \t 165.200275 s\n","222 \t 15.769873 \t 7.950383 \t7.791888 \t 0.027602 \t 165.765400 s\n","223 \t 15.773716 \t 8.091940 \t7.639258 \t 0.042518 \t 166.322379 s\n","224 \t 15.834771 \t 8.177719 \t7.611840 \t 0.045211 \t 166.863535 s\n","225 \t 15.732726 \t 8.094556 \t7.613080 \t 0.025090 \t 167.540269 s\n","226 \t 15.719935 \t 7.944601 \t7.718439 \t 0.056895 \t 168.079618 s\n","227 \t 15.664272 \t 7.915039 \t7.705401 \t 0.043833 \t 168.625437 s\n","228 \t 15.592999 \t 7.977039 \t7.580117 \t 0.035843 \t 169.176621 s\n","229 \t 15.715812 \t 8.000532 \t7.600147 \t 0.115133 \t 169.722250 s\n","230 \t 15.746465 \t 7.976830 \t7.730311 \t 0.039324 \t 170.278242 s\n","231 \t 15.947819 \t 8.029648 \t7.861113 \t 0.057057 \t 170.819300 s\n","232 \t 15.567288 \t 7.847212 \t7.640795 \t 0.079282 \t 171.358286 s\n","233 \t 15.622277 \t 7.933124 \t7.643162 \t 0.045991 \t 172.060872 s\n","234 \t 15.468989 \t 7.906012 \t7.457989 \t 0.104987 \t 172.612969 s\n","235 \t 15.688681 \t 7.966671 \t7.654975 \t 0.067034 \t 173.151843 s\n","236 \t 15.698908 \t 8.012645 \t7.588049 \t 0.098213 \t 173.703193 s\n","237 \t 15.485547 \t 7.849115 \t7.554362 \t 0.082069 \t 174.381948 s\n","238 \t 15.382208 \t 7.926265 \t7.405729 \t 0.050215 \t 175.037240 s\n","239 \t 15.345001 \t 7.774155 \t7.491091 \t 0.079754 \t 175.682235 s\n","240 \t 15.189778 \t 7.763106 \t7.383519 \t 0.043154 \t 176.386522 s\n","241 \t 15.440590 \t 7.884332 \t7.494458 \t 0.061801 \t 177.039892 s\n","242 \t 15.219564 \t 7.800647 \t7.373731 \t 0.045187 \t 177.750120 s\n","243 \t 15.295245 \t 7.818860 \t7.427233 \t 0.049151 \t 178.302684 s\n","244 \t 15.187193 \t 7.676846 \t7.456550 \t 0.053798 \t 178.840465 s\n","245 \t 15.427467 \t 7.899087 \t7.480664 \t 0.047716 \t 179.381701 s\n","246 \t 15.477171 \t 7.880135 \t7.514513 \t 0.082523 \t 179.936405 s\n","247 \t 15.308386 \t 7.825282 \t7.440691 \t 0.042414 \t 180.478972 s\n","248 \t 15.299257 \t 7.729306 \t7.471404 \t 0.098547 \t 181.017777 s\n","249 \t 15.129119 \t 7.729236 \t7.332588 \t 0.067296 \t 181.558350 s\n","250 \t 15.331336 \t 7.837136 \t7.409775 \t 0.084424 \t 182.094944 s\n","251 \t 15.109921 \t 7.656570 \t7.419813 \t 0.033539 \t 182.638092 s\n","252 \t 15.229658 \t 7.741842 \t7.432853 \t 0.054963 \t 183.324510 s\n","253 \t 15.258430 \t 7.737228 \t7.477631 \t 0.043570 \t 183.856347 s\n","254 \t 15.020828 \t 7.709675 \t7.251471 \t 0.059682 \t 184.398447 s\n","255 \t 15.143894 \t 7.715750 \t7.378792 \t 0.049352 \t 184.939352 s\n","256 \t 15.077541 \t 7.608118 \t7.426298 \t 0.043126 \t 185.485998 s\n","257 \t 15.016934 \t 7.607795 \t7.377243 \t 0.031896 \t 186.023707 s\n","258 \t 14.815773 \t 7.591098 \t7.190753 \t 0.033923 \t 186.568476 s\n","259 \t 14.851525 \t 7.577494 \t7.238820 \t 0.035210 \t 187.202846 s\n","260 \t 15.000078 \t 7.642354 \t7.336462 \t 0.021261 \t 187.852284 s\n","261 \t 15.133402 \t 7.719937 \t7.366390 \t 0.047076 \t 188.500372 s\n","262 \t 14.889059 \t 7.561381 \t7.265070 \t 0.062608 \t 189.367650 s\n","263 \t 14.851222 \t 7.525858 \t7.294724 \t 0.030639 \t 190.043735 s\n","264 \t 14.969890 \t 7.635459 \t7.285496 \t 0.048936 \t 190.590046 s\n","265 \t 14.946227 \t 7.660910 \t7.256541 \t 0.028776 \t 191.155128 s\n","266 \t 14.798606 \t 7.620629 \t7.159414 \t 0.018565 \t 191.709124 s\n","267 \t 14.839913 \t 7.448317 \t7.360991 \t 0.030605 \t 192.250219 s\n","268 \t 14.673610 \t 7.345316 \t7.292628 \t 0.035666 \t 192.785771 s\n","269 \t 14.756857 \t 7.504356 \t7.217737 \t 0.034764 \t 193.320291 s\n","270 \t 14.913067 \t 7.449532 \t7.426017 \t 0.037519 \t 193.862745 s\n","271 \t 14.700376 \t 7.418823 \t7.252888 \t 0.028665 \t 194.535300 s\n","272 \t 14.709203 \t 7.485529 \t7.197135 \t 0.026539 \t 195.071939 s\n","273 \t 14.721040 \t 7.544781 \t7.156102 \t 0.020157 \t 195.620232 s\n","274 \t 14.789361 \t 7.553109 \t7.205999 \t 0.030253 \t 196.168994 s\n","275 \t 14.567013 \t 7.335625 \t7.211016 \t 0.020372 \t 196.710211 s\n","276 \t 14.949018 \t 7.616248 \t7.309438 \t 0.023332 \t 197.261548 s\n","277 \t 14.668435 \t 7.463322 \t7.179194 \t 0.025919 \t 197.795142 s\n","278 \t 14.639102 \t 7.455004 \t7.140559 \t 0.043540 \t 198.353655 s\n","279 \t 14.771776 \t 7.533711 \t7.194795 \t 0.043271 \t 199.039737 s\n","280 \t 14.620409 \t 7.402255 \t7.198836 \t 0.019318 \t 199.577598 s\n","281 \t 14.804176 \t 7.598676 \t7.165923 \t 0.039577 \t 200.222984 s\n","282 \t 14.632115 \t 7.521303 \t7.063833 \t 0.046979 \t 200.861612 s\n","283 \t 14.512631 \t 7.327080 \t7.161642 \t 0.023909 \t 201.527509 s\n","284 \t 14.426336 \t 7.400419 \t6.994571 \t 0.031345 \t 202.232259 s\n","285 \t 14.385981 \t 7.350303 \t7.001422 \t 0.034256 \t 202.900168 s\n","286 \t 14.484299 \t 7.392389 \t7.068810 \t 0.023099 \t 203.436078 s\n","287 \t 14.614861 \t 7.390537 \t7.198285 \t 0.026039 \t 203.981248 s\n","288 \t 14.552341 \t 7.470406 \t7.055119 \t 0.026817 \t 204.537722 s\n","289 \t 14.300538 \t 7.328424 \t6.943252 \t 0.028862 \t 205.210830 s\n","290 \t 14.465937 \t 7.400040 \t7.042035 \t 0.023863 \t 205.749959 s\n","291 \t 14.366714 \t 7.374125 \t6.965894 \t 0.026696 \t 206.290971 s\n","292 \t 14.363531 \t 7.259490 \t7.076148 \t 0.027893 \t 206.822251 s\n","293 \t 14.534765 \t 7.467019 \t7.042057 \t 0.025690 \t 207.375385 s\n","294 \t 14.391599 \t 7.297751 \t7.060125 \t 0.033722 \t 207.918146 s\n","295 \t 14.334229 \t 7.213674 \t7.079653 \t 0.040903 \t 208.458282 s\n","296 \t 14.260882 \t 7.190815 \t7.049306 \t 0.020761 \t 208.998431 s\n","297 \t 14.342991 \t 7.296746 \t7.027637 \t 0.018607 \t 209.531321 s\n","298 \t 14.383059 \t 7.334495 \t7.020691 \t 0.027874 \t 210.207076 s\n","299 \t 14.332606 \t 7.261953 \t7.041654 \t 0.028999 \t 210.766993 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:26<00:00,  6.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4353\n","Hit@10: 0.6130\n","Hit@3: 0.4726\n","Hit@1: 0.3390\n","Link Prediction on Validation Set (All)\n","MRR: 0.4420\n","Hit@10: 0.6914\n","Hit@3: 0.5165\n","Hit@1: 0.3086\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4102\n","Hit@10: 0.7160\n","Hit@3: 0.5432\n","Hit@1: 0.2284\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5093\n","Hit@10: 0.7752\n","Hit@3: 0.6050\n","Hit@1: 0.3592\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2697\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2697\n","300 \t 14.252851 \t 7.221153 \t7.009451 \t 0.022247 \t 243.063284 s\n","301 \t 14.100346 \t 7.151906 \t6.933074 \t 0.015366 \t 243.610059 s\n","302 \t 14.058608 \t 7.078920 \t6.958270 \t 0.021417 \t 244.152144 s\n","303 \t 14.201110 \t 7.136843 \t7.040827 \t 0.023440 \t 244.697924 s\n","304 \t 14.232612 \t 7.232737 \t6.968656 \t 0.031219 \t 245.238639 s\n","305 \t 14.217206 \t 7.250033 \t6.941838 \t 0.025335 \t 245.782251 s\n","306 \t 14.176770 \t 7.209166 \t6.933627 \t 0.033978 \t 246.319371 s\n","307 \t 14.249504 \t 7.197121 \t7.018454 \t 0.033929 \t 247.000776 s\n","308 \t 14.285627 \t 7.313883 \t6.940966 \t 0.030778 \t 247.531709 s\n","309 \t 14.099953 \t 7.226306 \t6.850405 \t 0.023241 \t 248.070133 s\n","310 \t 14.194311 \t 7.168481 \t6.993679 \t 0.032151 \t 248.655636 s\n","311 \t 13.999254 \t 7.114460 \t6.865631 \t 0.019163 \t 249.338054 s\n","312 \t 14.053454 \t 7.094128 \t6.920168 \t 0.039159 \t 250.009492 s\n","313 \t 14.002615 \t 7.123992 \t6.856458 \t 0.022165 \t 250.642972 s\n","314 \t 14.052322 \t 7.179913 \t6.853370 \t 0.019039 \t 251.339070 s\n","315 \t 13.927422 \t 7.078490 \t6.812464 \t 0.036468 \t 252.817195 s\n","316 \t 14.127814 \t 7.224070 \t6.877225 \t 0.026519 \t 253.946569 s\n","317 \t 14.096339 \t 7.257666 \t6.810453 \t 0.028220 \t 255.119671 s\n","318 \t 13.922680 \t 7.077203 \t6.816832 \t 0.028646 \t 255.759638 s\n","319 \t 14.139668 \t 7.183089 \t6.914965 \t 0.041614 \t 256.419620 s\n","320 \t 13.907549 \t 7.126441 \t6.753695 \t 0.027413 \t 257.058284 s\n","321 \t 13.930514 \t 7.134781 \t6.756343 \t 0.039390 \t 257.737834 s\n","322 \t 13.831314 \t 7.075427 \t6.735736 \t 0.020151 \t 258.296858 s\n","323 \t 13.947692 \t 7.120292 \t6.789039 \t 0.038360 \t 258.868124 s\n","324 \t 13.785627 \t 6.961875 \t6.803590 \t 0.020162 \t 259.475202 s\n","325 \t 13.837490 \t 7.035126 \t6.760294 \t 0.042069 \t 260.225288 s\n","326 \t 13.953544 \t 7.169429 \t6.757043 \t 0.027072 \t 260.861657 s\n","327 \t 13.658309 \t 6.886675 \t6.745404 \t 0.026231 \t 261.488419 s\n","328 \t 13.814470 \t 6.995296 \t6.798744 \t 0.020430 \t 262.078366 s\n","329 \t 13.845562 \t 7.064896 \t6.762177 \t 0.018489 \t 262.718459 s\n","330 \t 13.787862 \t 6.904262 \t6.864387 \t 0.019214 \t 263.319802 s\n","331 \t 13.759114 \t 7.006964 \t6.733942 \t 0.018208 \t 263.928237 s\n","332 \t 13.710752 \t 6.911025 \t6.768492 \t 0.031236 \t 264.554041 s\n","333 \t 13.721124 \t 6.984984 \t6.712945 \t 0.023195 \t 265.174102 s\n","334 \t 13.819413 \t 7.070127 \t6.714388 \t 0.034898 \t 265.885563 s\n","335 \t 13.651608 \t 6.952121 \t6.662435 \t 0.037053 \t 266.709263 s\n","336 \t 13.775331 \t 7.124942 \t6.632566 \t 0.017823 \t 267.419500 s\n","337 \t 13.749986 \t 6.979979 \t6.749990 \t 0.020016 \t 268.141005 s\n","338 \t 13.726647 \t 6.979500 \t6.718957 \t 0.028190 \t 268.692847 s\n","339 \t 13.681924 \t 6.978316 \t6.684494 \t 0.019114 \t 269.239357 s\n","340 \t 13.681222 \t 6.912665 \t6.747886 \t 0.020672 \t 269.776706 s\n","341 \t 13.518207 \t 6.883959 \t6.603294 \t 0.030954 \t 270.309836 s\n","342 \t 13.661119 \t 6.966552 \t6.668758 \t 0.025808 \t 270.855098 s\n","343 \t 13.648263 \t 6.989514 \t6.639236 \t 0.019513 \t 271.396359 s\n","344 \t 13.679859 \t 7.017848 \t6.642066 \t 0.019945 \t 271.944447 s\n","345 \t 13.532567 \t 6.913187 \t6.601685 \t 0.017694 \t 272.621256 s\n","346 \t 13.485635 \t 6.817234 \t6.644719 \t 0.023682 \t 273.163060 s\n","347 \t 13.601050 \t 6.927079 \t6.658158 \t 0.015813 \t 273.699636 s\n","348 \t 13.448258 \t 6.893514 \t6.539449 \t 0.015295 \t 274.254973 s\n","349 \t 13.628193 \t 6.889231 \t6.717462 \t 0.021501 \t 274.795514 s\n","350 \t 13.696144 \t 6.979015 \t6.686276 \t 0.030853 \t 275.359456 s\n","351 \t 13.495668 \t 6.846898 \t6.618221 \t 0.030549 \t 275.909793 s\n","352 \t 13.722249 \t 6.965084 \t6.742741 \t 0.014423 \t 276.453916 s\n","353 \t 13.518286 \t 6.929206 \t6.575156 \t 0.013923 \t 277.014160 s\n","354 \t 13.544549 \t 6.934587 \t6.584483 \t 0.025478 \t 277.684089 s\n","355 \t 13.706051 \t 7.043124 \t6.643917 \t 0.019011 \t 278.268340 s\n","356 \t 13.438128 \t 6.856548 \t6.559499 \t 0.022082 \t 278.920061 s\n","357 \t 13.412994 \t 6.846408 \t6.551037 \t 0.015549 \t 279.590270 s\n","358 \t 13.442019 \t 6.880070 \t6.542756 \t 0.019193 \t 280.259616 s\n","359 \t 13.593590 \t 6.901387 \t6.668014 \t 0.024189 \t 280.974347 s\n","360 \t 13.380028 \t 6.852960 \t6.512154 \t 0.014914 \t 281.571994 s\n","361 \t 13.476607 \t 6.817752 \t6.639444 \t 0.019412 \t 282.109328 s\n","362 \t 13.323937 \t 6.746178 \t6.558416 \t 0.019343 \t 282.795260 s\n","363 \t 13.482718 \t 6.912072 \t6.554736 \t 0.015910 \t 283.337987 s\n","364 \t 13.381031 \t 6.794644 \t6.570876 \t 0.015511 \t 283.875354 s\n","365 \t 13.324234 \t 6.724583 \t6.579564 \t 0.020088 \t 284.418705 s\n","366 \t 13.342949 \t 6.818548 \t6.506758 \t 0.017642 \t 284.958459 s\n","367 \t 13.395006 \t 6.841511 \t6.538409 \t 0.015086 \t 285.499361 s\n","368 \t 13.444163 \t 6.840219 \t6.582525 \t 0.021419 \t 286.034195 s\n","369 \t 13.345015 \t 6.805225 \t6.522700 \t 0.017090 \t 286.576753 s\n","370 \t 13.414649 \t 6.924919 \t6.474966 \t 0.014765 \t 287.118042 s\n","371 \t 13.440001 \t 6.832222 \t6.595992 \t 0.011788 \t 287.661572 s\n","372 \t 13.355899 \t 6.805610 \t6.535814 \t 0.014476 \t 288.346766 s\n","373 \t 13.397716 \t 6.830806 \t6.547832 \t 0.019077 \t 288.899487 s\n","374 \t 13.377639 \t 6.802767 \t6.542328 \t 0.032545 \t 289.453227 s\n","375 \t 13.516614 \t 6.873122 \t6.628046 \t 0.015446 \t 289.994052 s\n","376 \t 13.354918 \t 6.814951 \t6.523758 \t 0.016208 \t 290.554567 s\n","377 \t 13.391906 \t 6.843944 \t6.536948 \t 0.011014 \t 291.098926 s\n","378 \t 13.401381 \t 6.902164 \t6.486786 \t 0.012431 \t 291.790966 s\n","379 \t 13.330866 \t 6.818496 \t6.494162 \t 0.018208 \t 292.436360 s\n","380 \t 13.281879 \t 6.802862 \t6.464133 \t 0.014885 \t 293.102997 s\n","381 \t 13.527687 \t 6.969708 \t6.524316 \t 0.033663 \t 293.791443 s\n","382 \t 13.322348 \t 6.810265 \t6.494128 \t 0.017955 \t 294.596032 s\n","383 \t 13.204583 \t 6.676915 \t6.508544 \t 0.019124 \t 295.143806 s\n","384 \t 13.221573 \t 6.707369 \t6.495467 \t 0.018737 \t 295.682547 s\n","385 \t 13.343230 \t 6.784422 \t6.540310 \t 0.018498 \t 296.236879 s\n","386 \t 13.309204 \t 6.797651 \t6.496464 \t 0.015090 \t 296.779905 s\n","387 \t 13.232657 \t 6.784278 \t6.436084 \t 0.012295 \t 297.315564 s\n","388 \t 13.346687 \t 6.896351 \t6.438685 \t 0.011651 \t 297.859046 s\n","389 \t 13.216087 \t 6.733508 \t6.471178 \t 0.011401 \t 298.393026 s\n","390 \t 13.210641 \t 6.724523 \t6.464895 \t 0.021222 \t 298.940993 s\n","391 \t 13.306811 \t 6.795218 \t6.490944 \t 0.020650 \t 299.484365 s\n","392 \t 13.378033 \t 6.900106 \t6.465317 \t 0.012609 \t 300.159279 s\n","393 \t 13.380672 \t 6.859211 \t6.504097 \t 0.017364 \t 300.697459 s\n","394 \t 13.166478 \t 6.695321 \t6.449972 \t 0.021184 \t 301.250920 s\n","395 \t 13.154113 \t 6.701857 \t6.434072 \t 0.018184 \t 301.798419 s\n","396 \t 13.224847 \t 6.720842 \t6.486484 \t 0.017520 \t 302.350158 s\n","397 \t 13.281641 \t 6.784988 \t6.485041 \t 0.011612 \t 302.889831 s\n","398 \t 13.348186 \t 6.837940 \t6.493239 \t 0.017008 \t 303.447875 s\n","399 \t 13.201964 \t 6.742954 \t6.442866 \t 0.016145 \t 303.998551 s\n","400 \t 13.316665 \t 6.807484 \t6.490571 \t 0.018609 \t 304.602010 s\n","401 \t 13.330976 \t 6.862787 \t6.452684 \t 0.015505 \t 305.399420 s\n","402 \t 13.237989 \t 6.733457 \t6.483073 \t 0.021460 \t 306.067633 s\n","403 \t 13.108449 \t 6.711696 \t6.382935 \t 0.013818 \t 306.751151 s\n","404 \t 13.181813 \t 6.712847 \t6.446910 \t 0.022055 \t 307.396856 s\n","405 \t 13.183131 \t 6.706306 \t6.463404 \t 0.013420 \t 307.933890 s\n","406 \t 13.167010 \t 6.666220 \t6.475248 \t 0.025543 \t 308.473701 s\n","407 \t 13.091439 \t 6.703485 \t6.368303 \t 0.019652 \t 309.007344 s\n","408 \t 13.180894 \t 6.758889 \t6.403698 \t 0.018307 \t 309.577231 s\n","409 \t 13.224139 \t 6.802597 \t6.405113 \t 0.016429 \t 310.252001 s\n","410 \t 13.378277 \t 6.880172 \t6.482389 \t 0.015716 \t 310.785812 s\n","411 \t 13.214381 \t 6.711125 \t6.489630 \t 0.013626 \t 311.334002 s\n","412 \t 13.169450 \t 6.678980 \t6.475225 \t 0.015245 \t 311.878673 s\n","413 \t 13.230095 \t 6.777295 \t6.439932 \t 0.012868 \t 312.446653 s\n","414 \t 13.220672 \t 6.703335 \t6.499177 \t 0.018160 \t 312.992925 s\n","415 \t 13.296271 \t 6.845127 \t6.429060 \t 0.022084 \t 313.543958 s\n","416 \t 13.214450 \t 6.698907 \t6.497189 \t 0.018354 \t 314.090058 s\n","417 \t 13.115232 \t 6.627954 \t6.473497 \t 0.013781 \t 314.636413 s\n","418 \t 13.242824 \t 6.806238 \t6.426427 \t 0.010160 \t 315.176691 s\n","419 \t 13.238768 \t 6.772431 \t6.447722 \t 0.018614 \t 315.857014 s\n","420 \t 13.311052 \t 6.814465 \t6.482418 \t 0.014169 \t 316.394639 s\n","421 \t 13.141265 \t 6.703315 \t6.416129 \t 0.021821 \t 316.948588 s\n","422 \t 13.249258 \t 6.783261 \t6.448500 \t 0.017497 \t 317.565737 s\n","423 \t 13.236162 \t 6.786164 \t6.433735 \t 0.016264 \t 318.202844 s\n","424 \t 13.063183 \t 6.648698 \t6.401343 \t 0.013141 \t 318.851215 s\n","425 \t 13.183141 \t 6.737235 \t6.426837 \t 0.019069 \t 319.523296 s\n","426 \t 13.167183 \t 6.698754 \t6.449928 \t 0.018501 \t 320.210197 s\n","427 \t 13.194069 \t 6.781296 \t6.397045 \t 0.015728 \t 320.762500 s\n","428 \t 13.027172 \t 6.671429 \t6.338231 \t 0.017512 \t 321.299488 s\n","429 \t 13.097641 \t 6.682231 \t6.397517 \t 0.017893 \t 321.970568 s\n","430 \t 13.183148 \t 6.761428 \t6.405283 \t 0.016437 \t 322.535666 s\n","431 \t 13.152774 \t 6.704671 \t6.435250 \t 0.012854 \t 323.076970 s\n","432 \t 13.245474 \t 6.813450 \t6.417980 \t 0.014044 \t 323.618392 s\n","433 \t 13.137667 \t 6.698860 \t6.420051 \t 0.018756 \t 324.183252 s\n","434 \t 13.128160 \t 6.677248 \t6.434953 \t 0.015958 \t 324.732330 s\n","435 \t 13.150558 \t 6.701631 \t6.431864 \t 0.017063 \t 325.293150 s\n","436 \t 13.246396 \t 6.744334 \t6.489365 \t 0.012696 \t 325.843206 s\n","437 \t 13.083138 \t 6.666923 \t6.401052 \t 0.015163 \t 326.517085 s\n","438 \t 13.170853 \t 6.747317 \t6.404380 \t 0.019156 \t 327.057387 s\n","439 \t 13.161955 \t 6.752962 \t6.395860 \t 0.013132 \t 327.592764 s\n","440 \t 13.253730 \t 6.768834 \t6.469474 \t 0.015423 \t 328.137769 s\n","441 \t 13.240355 \t 6.760438 \t6.465071 \t 0.014845 \t 328.675188 s\n","442 \t 13.321502 \t 6.791090 \t6.512184 \t 0.018228 \t 329.247416 s\n","443 \t 13.264596 \t 6.801722 \t6.440338 \t 0.022536 \t 329.802992 s\n","444 \t 13.224034 \t 6.739308 \t6.467625 \t 0.017102 \t 330.428682 s\n","445 \t 13.100183 \t 6.684717 \t6.403246 \t 0.012219 \t 331.101152 s\n","446 \t 13.282464 \t 6.842327 \t6.415395 \t 0.024742 \t 331.905842 s\n","447 \t 13.137659 \t 6.722008 \t6.398665 \t 0.016987 \t 332.602414 s\n","448 \t 13.225028 \t 6.783876 \t6.428092 \t 0.013060 \t 333.263149 s\n","449 \t 13.152665 \t 6.742464 \t6.397628 \t 0.012573 \t 333.797984 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:26<00:00,  6.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4463\n","Hit@10: 0.5993\n","Hit@3: 0.4863\n","Hit@1: 0.3562\n","Link Prediction on Validation Set (All)\n","MRR: 0.4588\n","Hit@10: 0.6749\n","Hit@3: 0.5182\n","Hit@1: 0.3465\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4163\n","Hit@10: 0.6975\n","Hit@3: 0.5123\n","Hit@1: 0.2531\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5135\n","Hit@10: 0.7458\n","Hit@3: 0.5840\n","Hit@1: 0.3866\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2189\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2189\n","450 \t 13.139558 \t 6.697617 \t6.418347 \t 0.023595 \t 366.013039 s\n","451 \t 13.498550 \t 6.840177 \t6.604565 \t 0.053808 \t 366.557964 s\n","452 \t 13.707085 \t 6.964131 \t6.698887 \t 0.044066 \t 367.088151 s\n","453 \t 13.601456 \t 6.842013 \t6.732116 \t 0.027326 \t 367.641023 s\n","454 \t 13.777820 \t 7.041707 \t6.711485 \t 0.024628 \t 368.181358 s\n","455 \t 13.666137 \t 6.983603 \t6.656013 \t 0.026521 \t 368.729009 s\n","456 \t 13.713984 \t 6.990965 \t6.699535 \t 0.023484 \t 369.464691 s\n","457 \t 13.679575 \t 6.946475 \t6.689263 \t 0.043837 \t 370.115314 s\n","458 \t 13.610249 \t 6.887681 \t6.684322 \t 0.038246 \t 370.775230 s\n","459 \t 13.859256 \t 7.089708 \t6.729092 \t 0.040456 \t 371.598193 s\n","460 \t 13.730466 \t 7.010231 \t6.658727 \t 0.061508 \t 372.602007 s\n","461 \t 13.676260 \t 6.946602 \t6.694518 \t 0.035140 \t 373.282434 s\n","462 \t 13.514253 \t 6.843355 \t6.636963 \t 0.033935 \t 373.953686 s\n","463 \t 13.415107 \t 6.816343 \t6.568267 \t 0.030497 \t 374.599645 s\n","464 \t 13.545072 \t 6.846349 \t6.655411 \t 0.043313 \t 375.258574 s\n","465 \t 13.684284 \t 6.926831 \t6.723143 \t 0.034310 \t 376.082000 s\n","466 \t 13.572735 \t 6.927179 \t6.603252 \t 0.042304 \t 376.732345 s\n","467 \t 13.415311 \t 6.784808 \t6.580522 \t 0.049981 \t 377.378924 s\n","468 \t 13.698533 \t 6.944977 \t6.700847 \t 0.052709 \t 378.040154 s\n","469 \t 13.449637 \t 6.790593 \t6.615451 \t 0.043592 \t 378.676497 s\n","470 \t 13.440964 \t 6.870189 \t6.533726 \t 0.037049 \t 379.339449 s\n","471 \t 13.547110 \t 6.927720 \t6.588470 \t 0.030919 \t 379.951702 s\n","472 \t 13.450274 \t 6.796175 \t6.618301 \t 0.035797 \t 380.577146 s\n","473 \t 13.367451 \t 6.847015 \t6.491625 \t 0.028811 \t 381.188907 s\n","474 \t 13.485012 \t 6.870429 \t6.581703 \t 0.032881 \t 381.794991 s\n","475 \t 13.436935 \t 6.799341 \t6.599842 \t 0.037752 \t 382.614048 s\n","476 \t 13.424248 \t 6.822713 \t6.556944 \t 0.044590 \t 383.416769 s\n","477 \t 13.440490 \t 6.838805 \t6.563666 \t 0.038019 \t 384.150331 s\n","478 \t 13.465832 \t 6.858373 \t6.555588 \t 0.051871 \t 384.914980 s\n","479 \t 13.445335 \t 6.863581 \t6.551581 \t 0.030174 \t 385.768552 s\n","480 \t 13.371701 \t 6.790463 \t6.534945 \t 0.046293 \t 386.381219 s\n","481 \t 13.469709 \t 6.832388 \t6.598892 \t 0.038429 \t 386.944429 s\n","482 \t 13.473022 \t 6.824794 \t6.601504 \t 0.046724 \t 387.487365 s\n","483 \t 13.398213 \t 6.830897 \t6.496992 \t 0.070325 \t 388.044985 s\n","484 \t 13.555918 \t 6.920839 \t6.590934 \t 0.044145 \t 388.726282 s\n","485 \t 13.513795 \t 6.876866 \t6.593147 \t 0.043783 \t 389.262216 s\n","486 \t 13.446631 \t 6.848241 \t6.539790 \t 0.058601 \t 389.803045 s\n","487 \t 13.334033 \t 6.780566 \t6.515693 \t 0.037774 \t 390.344345 s\n","488 \t 13.348102 \t 6.842403 \t6.473660 \t 0.032038 \t 390.881963 s\n","489 \t 13.442749 \t 6.825513 \t6.558986 \t 0.058250 \t 391.417592 s\n","490 \t 13.251183 \t 6.736832 \t6.484030 \t 0.030321 \t 391.960334 s\n","491 \t 13.404862 \t 6.904753 \t6.472446 \t 0.027663 \t 392.498929 s\n","492 \t 13.446267 \t 6.880701 \t6.534252 \t 0.031315 \t 393.179401 s\n","493 \t 13.263062 \t 6.747067 \t6.490120 \t 0.025875 \t 393.725537 s\n","494 \t 13.331284 \t 6.870395 \t6.445755 \t 0.015134 \t 394.264876 s\n","495 \t 13.410424 \t 6.873715 \t6.518437 \t 0.018272 \t 394.807386 s\n","496 \t 13.239826 \t 6.700024 \t6.502011 \t 0.037791 \t 395.347979 s\n","497 \t 13.433304 \t 6.849958 \t6.548407 \t 0.034939 \t 395.937208 s\n","498 \t 13.407343 \t 6.830936 \t6.548505 \t 0.027902 \t 396.596728 s\n","499 \t 13.366717 \t 6.825465 \t6.510499 \t 0.030754 \t 397.249811 s\n","500 \t 13.281580 \t 6.708650 \t6.535484 \t 0.037446 \t 397.912098 s\n","501 \t 13.307294 \t 6.791548 \t6.485121 \t 0.030626 \t 398.639025 s\n","502 \t 13.202606 \t 6.716576 \t6.438641 \t 0.047389 \t 399.370892 s\n","503 \t 13.296478 \t 6.755321 \t6.505488 \t 0.035668 \t 399.909831 s\n","504 \t 13.341784 \t 6.788383 \t6.509833 \t 0.043568 \t 400.462934 s\n","505 \t 13.488121 \t 6.871468 \t6.556080 \t 0.060573 \t 401.008920 s\n","506 \t 13.259939 \t 6.711177 \t6.516159 \t 0.032603 \t 401.547915 s\n","507 \t 13.294852 \t 6.711815 \t6.530931 \t 0.052106 \t 402.088962 s\n","508 \t 13.226025 \t 6.786146 \t6.405722 \t 0.034157 \t 402.625694 s\n","509 \t 13.329499 \t 6.788320 \t6.506052 \t 0.035126 \t 403.181899 s\n","510 \t 13.325787 \t 6.836645 \t6.436473 \t 0.052668 \t 403.724505 s\n","511 \t 13.257800 \t 6.749915 \t6.473106 \t 0.034779 \t 404.270229 s\n","512 \t 13.293332 \t 6.808583 \t6.438409 \t 0.046340 \t 404.940848 s\n","513 \t 13.110231 \t 6.660939 \t6.427705 \t 0.021587 \t 405.481784 s\n","514 \t 13.294072 \t 6.763826 \t6.451921 \t 0.078325 \t 406.021833 s\n","515 \t 13.137076 \t 6.694773 \t6.404749 \t 0.037555 \t 406.578183 s\n","516 \t 13.105083 \t 6.687247 \t6.388011 \t 0.029824 \t 407.126355 s\n","517 \t 13.412734 \t 6.865973 \t6.491830 \t 0.054931 \t 407.695382 s\n","518 \t 13.289107 \t 6.796541 \t6.468147 \t 0.024419 \t 408.248395 s\n","519 \t 13.362326 \t 6.867476 \t6.446184 \t 0.048666 \t 408.795718 s\n","520 \t 13.140280 \t 6.694026 \t6.424638 \t 0.021617 \t 409.493215 s\n","521 \t 13.023221 \t 6.583191 \t6.386716 \t 0.053314 \t 410.147151 s\n","522 \t 13.060501 \t 6.683492 \t6.326960 \t 0.050049 \t 411.004417 s\n","523 \t 13.179352 \t 6.647656 \t6.493070 \t 0.038626 \t 411.758161 s\n","524 \t 13.205336 \t 6.742295 \t6.412324 \t 0.050717 \t 412.295946 s\n","525 \t 13.215019 \t 6.833276 \t6.333455 \t 0.048288 \t 412.844970 s\n","526 \t 13.179896 \t 6.760745 \t6.390725 \t 0.028426 \t 413.401681 s\n","527 \t 13.264020 \t 6.790133 \t6.414731 \t 0.059156 \t 413.955273 s\n","528 \t 13.166780 \t 6.663827 \t6.473127 \t 0.029825 \t 414.497641 s\n","529 \t 13.090459 \t 6.637707 \t6.411667 \t 0.041086 \t 415.050534 s\n","530 \t 13.119144 \t 6.707742 \t6.378880 \t 0.032522 \t 415.585335 s\n","531 \t 13.139125 \t 6.775814 \t6.329324 \t 0.033988 \t 416.270402 s\n","532 \t 13.070804 \t 6.696076 \t6.353576 \t 0.021151 \t 416.813015 s\n","533 \t 13.059770 \t 6.686256 \t6.350371 \t 0.023143 \t 417.358127 s\n","534 \t 13.007452 \t 6.642261 \t6.329398 \t 0.035792 \t 417.900660 s\n","535 \t 12.999959 \t 6.630203 \t6.315100 \t 0.054657 \t 418.434804 s\n","536 \t 12.984805 \t 6.606681 \t6.348725 \t 0.029399 \t 418.976624 s\n","537 \t 13.018607 \t 6.587710 \t6.357893 \t 0.073004 \t 419.515757 s\n","538 \t 12.989160 \t 6.595662 \t6.363697 \t 0.029802 \t 420.053955 s\n","539 \t 12.989608 \t 6.530378 \t6.404524 \t 0.054705 \t 420.721494 s\n","540 \t 13.093448 \t 6.629552 \t6.428195 \t 0.035700 \t 421.268943 s\n","541 \t 13.082845 \t 6.674393 \t6.381848 \t 0.026604 \t 421.831429 s\n","542 \t 12.961344 \t 6.601886 \t6.333185 \t 0.026273 \t 422.502364 s\n","543 \t 12.930681 \t 6.552691 \t6.361197 \t 0.016793 \t 423.146292 s\n","544 \t 12.896718 \t 6.536364 \t6.334920 \t 0.025435 \t 423.799399 s\n","545 \t 13.031927 \t 6.677033 \t6.336231 \t 0.018662 \t 424.527004 s\n","546 \t 13.064466 \t 6.653612 \t6.374808 \t 0.036046 \t 425.175344 s\n","547 \t 12.958355 \t 6.578319 \t6.350222 \t 0.029813 \t 425.711220 s\n","548 \t 13.017837 \t 6.650730 \t6.337423 \t 0.029684 \t 426.255882 s\n","549 \t 13.003230 \t 6.588649 \t6.383213 \t 0.031368 \t 426.928666 s\n","550 \t 13.009317 \t 6.660626 \t6.313158 \t 0.035533 \t 427.474425 s\n","551 \t 13.014891 \t 6.623124 \t6.347291 \t 0.044476 \t 428.020248 s\n","552 \t 12.993539 \t 6.635951 \t6.319918 \t 0.037671 \t 428.573985 s\n","553 \t 13.013356 \t 6.606413 \t6.379487 \t 0.027455 \t 429.113500 s\n","554 \t 13.163606 \t 6.734984 \t6.389865 \t 0.038757 \t 429.666737 s\n","555 \t 13.030857 \t 6.676140 \t6.312774 \t 0.041942 \t 430.221113 s\n","556 \t 12.815177 \t 6.505013 \t6.273106 \t 0.037059 \t 430.767795 s\n","557 \t 12.962541 \t 6.623945 \t6.304471 \t 0.034125 \t 431.309970 s\n","558 \t 12.900307 \t 6.552693 \t6.307161 \t 0.040453 \t 431.848356 s\n","559 \t 12.800291 \t 6.488647 \t6.286514 \t 0.025131 \t 432.530403 s\n","560 \t 12.768940 \t 6.522926 \t6.231000 \t 0.015015 \t 433.063174 s\n","561 \t 12.879248 \t 6.620446 \t6.242093 \t 0.016709 \t 433.604655 s\n","562 \t 12.970936 \t 6.646802 \t6.279817 \t 0.044317 \t 434.139547 s\n","563 \t 12.958389 \t 6.624538 \t6.304764 \t 0.029086 \t 434.687126 s\n","564 \t 12.868053 \t 6.585614 \t6.250713 \t 0.031727 \t 435.361083 s\n","565 \t 12.903823 \t 6.585463 \t6.294807 \t 0.023552 \t 436.027301 s\n","566 \t 12.890993 \t 6.596874 \t6.269948 \t 0.024171 \t 436.702907 s\n","567 \t 12.823799 \t 6.555580 \t6.238807 \t 0.029411 \t 437.388864 s\n","568 \t 12.827825 \t 6.566364 \t6.238780 \t 0.022682 \t 438.052153 s\n","569 \t 12.901108 \t 6.583679 \t6.278006 \t 0.039424 \t 438.722319 s\n","570 \t 12.826032 \t 6.588448 \t6.205383 \t 0.032201 \t 439.256067 s\n","571 \t 13.020115 \t 6.628870 \t6.348406 \t 0.042840 \t 439.786700 s\n","572 \t 12.940302 \t 6.672451 \t6.221719 \t 0.046132 \t 440.318798 s\n","573 \t 12.952486 \t 6.631063 \t6.278475 \t 0.042948 \t 440.856458 s\n","574 \t 12.797996 \t 6.526197 \t6.251224 \t 0.020575 \t 441.392232 s\n","575 \t 12.938514 \t 6.610095 \t6.301487 \t 0.026932 \t 441.931267 s\n","576 \t 12.738186 \t 6.483869 \t6.222987 \t 0.031331 \t 442.466428 s\n","577 \t 12.814702 \t 6.559840 \t6.226435 \t 0.028426 \t 443.168620 s\n","578 \t 12.792500 \t 6.541439 \t6.216600 \t 0.034462 \t 443.717169 s\n","579 \t 12.693990 \t 6.453038 \t6.213838 \t 0.027113 \t 444.262383 s\n","580 \t 12.901757 \t 6.682116 \t6.190693 \t 0.028948 \t 444.807281 s\n","581 \t 12.798466 \t 6.523243 \t6.245958 \t 0.029265 \t 445.353673 s\n","582 \t 12.800112 \t 6.549870 \t6.210351 \t 0.039890 \t 445.905531 s\n","583 \t 12.768641 \t 6.543009 \t6.193805 \t 0.031827 \t 446.445089 s\n","584 \t 12.706164 \t 6.503568 \t6.176443 \t 0.026153 \t 446.996655 s\n","585 \t 12.724669 \t 6.520697 \t6.186904 \t 0.017068 \t 447.537059 s\n","586 \t 12.809378 \t 6.596332 \t6.185957 \t 0.027088 \t 448.312885 s\n","587 \t 12.799352 \t 6.540526 \t6.222781 \t 0.036045 \t 448.981186 s\n","588 \t 12.850177 \t 6.547785 \t6.259634 \t 0.042757 \t 449.631096 s\n","589 \t 12.755240 \t 6.523926 \t6.200594 \t 0.030721 \t 450.302787 s\n","590 \t 12.826220 \t 6.529251 \t6.267959 \t 0.029009 \t 450.952180 s\n","591 \t 12.848464 \t 6.600339 \t6.226459 \t 0.021665 \t 451.504150 s\n","592 \t 12.721595 \t 6.466210 \t6.221848 \t 0.033538 \t 452.041612 s\n","593 \t 12.771088 \t 6.537392 \t6.188783 \t 0.044912 \t 452.586921 s\n","594 \t 12.743312 \t 6.538832 \t6.181978 \t 0.022502 \t 453.129030 s\n","595 \t 12.727985 \t 6.524889 \t6.175154 \t 0.027942 \t 453.664085 s\n","596 \t 12.812012 \t 6.584152 \t6.194628 \t 0.033231 \t 454.340465 s\n","597 \t 12.680582 \t 6.418756 \t6.216566 \t 0.045260 \t 454.869195 s\n","598 \t 12.747732 \t 6.479748 \t6.240174 \t 0.027811 \t 455.413602 s\n","599 \t 12.722704 \t 6.506051 \t6.180132 \t 0.036521 \t 455.956244 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:26<00:00,  6.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4272\n","Hit@10: 0.5925\n","Hit@3: 0.4760\n","Hit@1: 0.3356\n","Link Prediction on Validation Set (All)\n","MRR: 0.4412\n","Hit@10: 0.6485\n","Hit@3: 0.4917\n","Hit@1: 0.3350\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4051\n","Hit@10: 0.6790\n","Hit@3: 0.5000\n","Hit@1: 0.2469\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4998\n","Hit@10: 0.7248\n","Hit@3: 0.5651\n","Hit@1: 0.3761\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2649\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2649\n","600 \t 12.687960 \t 6.481465 \t6.191544 \t 0.014951 \t 489.991622 s\n","601 \t 12.706855 \t 6.451851 \t6.221981 \t 0.033023 \t 490.539215 s\n","602 \t 12.596920 \t 6.416222 \t6.162548 \t 0.018151 \t 491.069833 s\n","603 \t 12.674832 \t 6.435354 \t6.225532 \t 0.013947 \t 491.619406 s\n","604 \t 12.615052 \t 6.406845 \t6.177208 \t 0.030998 \t 492.160895 s\n","605 \t 12.711409 \t 6.477330 \t6.200625 \t 0.033454 \t 492.836808 s\n","606 \t 12.707888 \t 6.484706 \t6.182867 \t 0.040315 \t 493.372509 s\n","607 \t 12.733753 \t 6.526130 \t6.172086 \t 0.035537 \t 493.924751 s\n","608 \t 12.739975 \t 6.466248 \t6.218273 \t 0.055455 \t 494.465228 s\n","609 \t 12.593844 \t 6.404854 \t6.158466 \t 0.030525 \t 495.019608 s\n","610 \t 12.705730 \t 6.451690 \t6.189262 \t 0.064778 \t 495.590658 s\n","611 \t 12.658192 \t 6.411515 \t6.190062 \t 0.056614 \t 496.126256 s\n","612 \t 12.557216 \t 6.416194 \t6.100665 \t 0.040357 \t 496.675288 s\n","613 \t 12.618521 \t 6.452755 \t6.145680 \t 0.020087 \t 497.224365 s\n","614 \t 12.691169 \t 6.467173 \t6.193939 \t 0.030057 \t 497.905513 s\n","615 \t 12.525095 \t 6.345275 \t6.153347 \t 0.026474 \t 498.443783 s\n","616 \t 12.583693 \t 6.392218 \t6.164922 \t 0.026552 \t 498.988110 s\n","617 \t 12.601530 \t 6.462462 \t6.119432 \t 0.019635 \t 499.598220 s\n","618 \t 12.680172 \t 6.480950 \t6.180356 \t 0.018867 \t 500.506576 s\n","619 \t 12.487068 \t 6.335633 \t6.122695 \t 0.028740 \t 501.619204 s\n","620 \t 12.556233 \t 6.384786 \t6.118666 \t 0.052781 \t 502.871331 s\n","621 \t 12.767747 \t 6.534233 \t6.207569 \t 0.025944 \t 503.613543 s\n","622 \t 12.592871 \t 6.446464 \t6.128524 \t 0.017882 \t 504.276958 s\n","623 \t 12.492619 \t 6.371448 \t6.095332 \t 0.025838 \t 505.114079 s\n","624 \t 12.576062 \t 6.425933 \t6.137150 \t 0.012979 \t 505.777631 s\n","625 \t 12.612414 \t 6.452979 \t6.141377 \t 0.018058 \t 506.429291 s\n","626 \t 12.600080 \t 6.467345 \t6.112121 \t 0.020614 \t 507.086599 s\n","627 \t 12.577554 \t 6.388611 \t6.158262 \t 0.030681 \t 507.748239 s\n","628 \t 12.658436 \t 6.535224 \t6.105255 \t 0.017956 \t 508.407222 s\n","629 \t 12.592452 \t 6.388448 \t6.159818 \t 0.044185 \t 509.058805 s\n","630 \t 12.593380 \t 6.443222 \t6.126421 \t 0.023737 \t 509.673269 s\n","631 \t 12.630385 \t 6.438892 \t6.158356 \t 0.033137 \t 510.259000 s\n","632 \t 12.570774 \t 6.425674 \t6.124714 \t 0.020386 \t 510.987080 s\n","633 \t 12.635132 \t 6.479554 \t6.130244 \t 0.025333 \t 511.583771 s\n","634 \t 12.600540 \t 6.423255 \t6.145018 \t 0.032267 \t 512.149082 s\n","635 \t 12.631583 \t 6.478884 \t6.127305 \t 0.025394 \t 512.740735 s\n","636 \t 12.512589 \t 6.397350 \t6.093941 \t 0.021297 \t 513.444893 s\n","637 \t 12.604892 \t 6.434278 \t6.148075 \t 0.022539 \t 514.154604 s\n","638 \t 12.564743 \t 6.374382 \t6.178765 \t 0.011597 \t 514.877170 s\n","639 \t 12.452156 \t 6.314314 \t6.117719 \t 0.020122 \t 515.633374 s\n","640 \t 12.547718 \t 6.370647 \t6.151657 \t 0.025414 \t 516.353709 s\n","641 \t 12.580617 \t 6.448980 \t6.104836 \t 0.026801 \t 516.951910 s\n","642 \t 12.484011 \t 6.374265 \t6.095803 \t 0.013943 \t 517.638202 s\n","643 \t 12.467790 \t 6.310959 \t6.139422 \t 0.017409 \t 518.176861 s\n","644 \t 12.555792 \t 6.404984 \t6.129257 \t 0.021550 \t 518.733573 s\n","645 \t 12.462502 \t 6.355062 \t6.080669 \t 0.026772 \t 519.270308 s\n","646 \t 12.531927 \t 6.369722 \t6.140218 \t 0.021987 \t 519.812505 s\n","647 \t 12.464196 \t 6.356582 \t6.089455 \t 0.018159 \t 520.361051 s\n","648 \t 12.512660 \t 6.398833 \t6.093645 \t 0.020182 \t 520.907855 s\n","649 \t 12.451442 \t 6.348123 \t6.081572 \t 0.021746 \t 521.446349 s\n","650 \t 12.497863 \t 6.360199 \t6.117665 \t 0.019998 \t 521.993701 s\n","651 \t 12.425092 \t 6.346810 \t6.062309 \t 0.015972 \t 522.540343 s\n","652 \t 12.578299 \t 6.415954 \t6.123314 \t 0.039031 \t 523.220712 s\n","653 \t 12.561583 \t 6.433329 \t6.112840 \t 0.015413 \t 523.767188 s\n","654 \t 12.510666 \t 6.382939 \t6.111642 \t 0.016086 \t 524.303112 s\n","655 \t 12.579591 \t 6.456224 \t6.101259 \t 0.022109 \t 524.845399 s\n","656 \t 12.357604 \t 6.262962 \t6.068657 \t 0.025986 \t 525.382724 s\n","657 \t 12.507142 \t 6.419307 \t6.063341 \t 0.024495 \t 525.940621 s\n","658 \t 12.439278 \t 6.339223 \t6.083876 \t 0.016179 \t 526.541480 s\n","659 \t 12.537537 \t 6.419113 \t6.093693 \t 0.024731 \t 527.198181 s\n","660 \t 12.375761 \t 6.260715 \t6.094056 \t 0.020989 \t 527.834356 s\n","661 \t 12.352161 \t 6.283164 \t6.047415 \t 0.021583 \t 528.530654 s\n","662 \t 12.429906 \t 6.326702 \t6.090217 \t 0.012988 \t 529.339900 s\n","663 \t 12.456121 \t 6.362415 \t6.078309 \t 0.015397 \t 529.873198 s\n","664 \t 12.444734 \t 6.375616 \t6.055224 \t 0.013894 \t 530.429502 s\n","665 \t 12.375607 \t 6.290647 \t6.071596 \t 0.013363 \t 530.977539 s\n","666 \t 12.379251 \t 6.312409 \t6.053182 \t 0.013661 \t 531.539918 s\n","667 \t 12.432869 \t 6.356371 \t6.060556 \t 0.015942 \t 532.081363 s\n","668 \t 12.443478 \t 6.351708 \t6.076008 \t 0.015761 \t 532.618759 s\n","669 \t 12.430767 \t 6.356476 \t6.056197 \t 0.018094 \t 533.166305 s\n","670 \t 12.435386 \t 6.313114 \t6.111261 \t 0.011011 \t 533.717802 s\n","671 \t 12.324081 \t 6.301583 \t6.008203 \t 0.014295 \t 534.401161 s\n","672 \t 12.382879 \t 6.325927 \t6.044183 \t 0.012768 \t 534.950342 s\n","673 \t 12.456098 \t 6.349639 \t6.090781 \t 0.015678 \t 535.494911 s\n","674 \t 12.444172 \t 6.374231 \t6.053097 \t 0.016844 \t 536.041765 s\n","675 \t 12.343766 \t 6.286024 \t6.046174 \t 0.011567 \t 536.588391 s\n","676 \t 12.436180 \t 6.338660 \t6.079335 \t 0.018186 \t 537.121226 s\n","677 \t 12.430099 \t 6.344146 \t6.072768 \t 0.013185 \t 537.674035 s\n","678 \t 12.463286 \t 6.366951 \t6.073762 \t 0.022573 \t 538.215160 s\n","679 \t 12.398210 \t 6.307148 \t6.070549 \t 0.020512 \t 538.900244 s\n","680 \t 12.446486 \t 6.334651 \t6.097672 \t 0.014163 \t 539.520144 s\n","681 \t 12.532736 \t 6.430325 \t6.076722 \t 0.025689 \t 540.155438 s\n","682 \t 12.434303 \t 6.380475 \t6.033025 \t 0.020804 \t 540.793686 s\n","683 \t 12.414079 \t 6.308666 \t6.072153 \t 0.033260 \t 541.479126 s\n","684 \t 12.405918 \t 6.343996 \t6.045575 \t 0.016346 \t 542.177629 s\n","685 \t 12.464449 \t 6.391760 \t6.045948 \t 0.026741 \t 542.722092 s\n","686 \t 12.372365 \t 6.286471 \t6.048311 \t 0.037584 \t 543.258577 s\n","687 \t 12.363102 \t 6.316995 \t6.033239 \t 0.012868 \t 543.799583 s\n","688 \t 12.387182 \t 6.334613 \t6.040014 \t 0.012556 \t 544.337022 s\n","689 \t 12.381329 \t 6.334574 \t6.026069 \t 0.020685 \t 545.016610 s\n","690 \t 12.397491 \t 6.339452 \t6.043423 \t 0.014616 \t 545.552385 s\n","691 \t 12.373888 \t 6.338850 \t6.021606 \t 0.013432 \t 546.102798 s\n","692 \t 12.301127 \t 6.229435 \t6.049421 \t 0.022271 \t 546.652420 s\n","693 \t 12.459673 \t 6.402865 \t6.029153 \t 0.027655 \t 547.193558 s\n","694 \t 12.340025 \t 6.242496 \t6.080424 \t 0.017105 \t 547.741377 s\n","695 \t 12.299863 \t 6.241684 \t6.043969 \t 0.014210 \t 548.275793 s\n","696 \t 12.353662 \t 6.312231 \t6.019815 \t 0.021615 \t 548.815309 s\n","697 \t 12.319868 \t 6.268814 \t6.033511 \t 0.017542 \t 549.348812 s\n","698 \t 12.296491 \t 6.261008 \t6.020089 \t 0.015394 \t 549.895757 s\n","699 \t 12.237853 \t 6.223872 \t6.000995 \t 0.012986 \t 550.560985 s\n","700 \t 12.223428 \t 6.189202 \t6.007822 \t 0.026405 \t 551.093475 s\n","701 \t 12.332989 \t 6.258976 \t6.060173 \t 0.013840 \t 551.626915 s\n","702 \t 12.366352 \t 6.325243 \t6.026331 \t 0.014778 \t 552.184089 s\n","703 \t 12.359632 \t 6.278942 \t6.051071 \t 0.029620 \t 552.854625 s\n","704 \t 12.300337 \t 6.277763 \t6.003299 \t 0.019276 \t 553.489861 s\n","705 \t 12.323071 \t 6.291627 \t6.014648 \t 0.016797 \t 554.133038 s\n","706 \t 12.308449 \t 6.270337 \t6.013238 \t 0.024875 \t 554.831589 s\n","707 \t 12.329099 \t 6.305531 \t6.008064 \t 0.015504 \t 555.621610 s\n","708 \t 12.231290 \t 6.222850 \t5.996901 \t 0.011540 \t 556.164775 s\n","709 \t 12.274581 \t 6.261508 \t6.000492 \t 0.012581 \t 556.699649 s\n","710 \t 12.276194 \t 6.270274 \t5.990695 \t 0.015225 \t 557.248243 s\n","711 \t 12.356882 \t 6.346471 \t5.990253 \t 0.020157 \t 557.784197 s\n","712 \t 12.244126 \t 6.231004 \t5.995010 \t 0.018112 \t 558.341235 s\n","713 \t 12.276628 \t 6.209611 \t6.046233 \t 0.020783 \t 558.879795 s\n","714 \t 12.214523 \t 6.186469 \t6.008031 \t 0.020023 \t 559.416495 s\n","715 \t 12.224154 \t 6.217621 \t5.984984 \t 0.021549 \t 559.950955 s\n","716 \t 12.392338 \t 6.392438 \t5.985458 \t 0.014441 \t 560.624341 s\n","717 \t 12.270204 \t 6.272195 \t5.969887 \t 0.028121 \t 561.151312 s\n","718 \t 12.268894 \t 6.242192 \t6.011994 \t 0.014707 \t 561.697988 s\n","719 \t 12.213122 \t 6.204967 \t5.994739 \t 0.013415 \t 562.244010 s\n","720 \t 12.322001 \t 6.303721 \t6.006944 \t 0.011337 \t 562.777126 s\n","721 \t 12.207023 \t 6.167353 \t6.026225 \t 0.013445 \t 563.314820 s\n","722 \t 12.280127 \t 6.264394 \t6.003245 \t 0.012487 \t 563.874675 s\n","723 \t 12.283160 \t 6.256926 \t6.001435 \t 0.024799 \t 564.431764 s\n","724 \t 12.397033 \t 6.363023 \t6.016453 \t 0.017557 \t 564.978984 s\n","725 \t 12.335463 \t 6.318685 \t5.998082 \t 0.018696 \t 565.602772 s\n","726 \t 12.211993 \t 6.241389 \t5.953170 \t 0.017435 \t 566.393435 s\n","727 \t 12.198053 \t 6.194654 \t5.985628 \t 0.017771 \t 567.049152 s\n","728 \t 12.156836 \t 6.155261 \t5.981304 \t 0.020271 \t 567.751593 s\n","729 \t 12.180906 \t 6.155789 \t6.009387 \t 0.015730 \t 568.398306 s\n","730 \t 12.221606 \t 6.200827 \t5.994712 \t 0.026067 \t 568.949696 s\n","731 \t 12.268447 \t 6.266080 \t5.990040 \t 0.012327 \t 569.482494 s\n","732 \t 12.200486 \t 6.170574 \t6.013282 \t 0.016629 \t 570.039502 s\n","733 \t 12.193320 \t 6.196033 \t5.983437 \t 0.013850 \t 570.581277 s\n","734 \t 12.382188 \t 6.350628 \t6.017459 \t 0.014101 \t 571.129299 s\n","735 \t 12.234537 \t 6.215848 \t6.004698 \t 0.013991 \t 571.682413 s\n","736 \t 12.294698 \t 6.313984 \t5.969629 \t 0.011085 \t 572.353069 s\n","737 \t 12.247021 \t 6.234725 \t6.000842 \t 0.011454 \t 572.886187 s\n","738 \t 12.226165 \t 6.198602 \t6.013131 \t 0.014432 \t 573.422038 s\n","739 \t 12.357334 \t 6.370832 \t5.966592 \t 0.019911 \t 573.961920 s\n","740 \t 12.177928 \t 6.173816 \t5.993329 \t 0.010783 \t 574.493977 s\n","741 \t 12.191148 \t 6.216879 \t5.960105 \t 0.014164 \t 575.039855 s\n","742 \t 12.291831 \t 6.299644 \t5.974610 \t 0.017578 \t 575.573411 s\n","743 \t 12.249242 \t 6.224132 \t6.012799 \t 0.012311 \t 576.109126 s\n","744 \t 12.335177 \t 6.354650 \t5.969325 \t 0.011203 \t 576.647428 s\n","745 \t 12.274235 \t 6.268458 \t5.980892 \t 0.024885 \t 577.197772 s\n","746 \t 12.201794 \t 6.199135 \t5.990079 \t 0.012580 \t 577.866072 s\n","747 \t 12.154427 \t 6.187632 \t5.951495 \t 0.015300 \t 578.466775 s\n","748 \t 12.107537 \t 6.159298 \t5.934548 \t 0.013691 \t 579.138791 s\n","749 \t 12.190436 \t 6.227385 \t5.952474 \t 0.010577 \t 579.779078 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:26<00:00,  6.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4191\n","Hit@10: 0.5753\n","Hit@3: 0.4589\n","Hit@1: 0.3288\n","Link Prediction on Validation Set (All)\n","MRR: 0.4217\n","Hit@10: 0.6139\n","Hit@3: 0.4719\n","Hit@1: 0.3168\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3939\n","Hit@10: 0.6358\n","Hit@3: 0.5000\n","Hit@1: 0.2407\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4881\n","Hit@10: 0.6933\n","Hit@3: 0.5630\n","Hit@1: 0.3676\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2191\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2191\n","750 \t 12.211056 \t 6.242424 \t5.956533 \t 0.012099 \t 613.016796 s\n","751 \t 12.091002 \t 6.153812 \t5.925998 \t 0.011192 \t 613.564615 s\n","752 \t 12.145257 \t 6.166226 \t5.970974 \t 0.008058 \t 614.096706 s\n","753 \t 12.102931 \t 6.154452 \t5.937905 \t 0.010575 \t 614.642168 s\n","754 \t 12.122746 \t 6.143595 \t5.968153 \t 0.010998 \t 615.183649 s\n","755 \t 12.111050 \t 6.151955 \t5.947093 \t 0.012002 \t 615.867665 s\n","756 \t 12.183693 \t 6.219814 \t5.951449 \t 0.012431 \t 616.409593 s\n","757 \t 12.308748 \t 6.326620 \t5.969377 \t 0.012752 \t 616.957241 s\n","758 \t 12.251826 \t 6.276194 \t5.964759 \t 0.010874 \t 617.631253 s\n","759 \t 12.144260 \t 6.198465 \t5.933830 \t 0.011966 \t 618.280833 s\n","760 \t 12.253187 \t 6.285315 \t5.949093 \t 0.018778 \t 618.949719 s\n","761 \t 12.183625 \t 6.218287 \t5.951060 \t 0.014278 \t 619.689124 s\n","762 \t 12.182611 \t 6.208226 \t5.963021 \t 0.011364 \t 620.316010 s\n","763 \t 12.237869 \t 6.285059 \t5.940080 \t 0.012731 \t 620.996186 s\n","764 \t 12.177249 \t 6.229780 \t5.937276 \t 0.010194 \t 621.529368 s\n","765 \t 12.124401 \t 6.164675 \t5.951656 \t 0.008070 \t 622.084292 s\n","766 \t 12.151720 \t 6.205656 \t5.937940 \t 0.008124 \t 622.622319 s\n","767 \t 12.235129 \t 6.262233 \t5.956358 \t 0.016538 \t 623.160241 s\n","768 \t 12.151311 \t 6.211893 \t5.929677 \t 0.009741 \t 623.805752 s\n","769 \t 12.212022 \t 6.216481 \t5.984029 \t 0.011511 \t 624.445756 s\n","770 \t 12.058920 \t 6.111713 \t5.937011 \t 0.010196 \t 625.116956 s\n","771 \t 12.219885 \t 6.216596 \t5.991508 \t 0.011781 \t 625.758122 s\n","772 \t 12.200463 \t 6.216286 \t5.973631 \t 0.010545 \t 626.394146 s\n","773 \t 12.106512 \t 6.154967 \t5.933124 \t 0.018420 \t 627.218590 s\n","774 \t 12.087899 \t 6.104442 \t5.964685 \t 0.018771 \t 627.884815 s\n","775 \t 12.127986 \t 6.155378 \t5.961545 \t 0.011063 \t 628.524074 s\n","776 \t 12.030594 \t 6.094527 \t5.927275 \t 0.008793 \t 629.162076 s\n","777 \t 12.117918 \t 6.152061 \t5.957256 \t 0.008600 \t 629.812600 s\n","778 \t 12.102846 \t 6.121395 \t5.964110 \t 0.017342 \t 630.946622 s\n","779 \t 12.095451 \t 6.150299 \t5.935127 \t 0.010025 \t 632.126257 s\n","780 \t 11.982520 \t 6.073955 \t5.899779 \t 0.008785 \t 632.990838 s\n","781 \t 12.163857 \t 6.232938 \t5.920249 \t 0.010670 \t 633.708235 s\n","782 \t 12.042890 \t 6.118334 \t5.914401 \t 0.010155 \t 634.324578 s\n","783 \t 12.114930 \t 6.170503 \t5.927515 \t 0.016912 \t 635.061537 s\n","784 \t 12.089668 \t 6.151429 \t5.928823 \t 0.009416 \t 635.658824 s\n","785 \t 12.099863 \t 6.163809 \t5.923745 \t 0.012310 \t 636.257962 s\n","786 \t 12.106343 \t 6.147900 \t5.947133 \t 0.011311 \t 636.884840 s\n","787 \t 12.134749 \t 6.188336 \t5.934705 \t 0.011709 \t 637.491353 s\n","788 \t 12.184255 \t 6.246799 \t5.927803 \t 0.009653 \t 638.103201 s\n","789 \t 12.207642 \t 6.270635 \t5.924314 \t 0.012693 \t 638.682392 s\n","790 \t 12.094859 \t 6.171661 \t5.910902 \t 0.012297 \t 639.264732 s\n","791 \t 12.111837 \t 6.179941 \t5.917672 \t 0.014223 \t 639.936840 s\n","792 \t 12.086352 \t 6.138689 \t5.934908 \t 0.012755 \t 640.478906 s\n","793 \t 12.075326 \t 6.145614 \t5.911264 \t 0.018448 \t 641.043840 s\n","794 \t 12.091656 \t 6.182560 \t5.900076 \t 0.009020 \t 641.590178 s\n","795 \t 12.127888 \t 6.234215 \t5.884465 \t 0.009208 \t 642.131698 s\n","796 \t 12.024949 \t 6.092110 \t5.924479 \t 0.008359 \t 642.671580 s\n","797 \t 12.108965 \t 6.163622 \t5.935747 \t 0.009596 \t 643.218011 s\n","798 \t 12.120349 \t 6.170943 \t5.937967 \t 0.011439 \t 643.847777 s\n","799 \t 12.078049 \t 6.148335 \t5.916327 \t 0.013387 \t 644.669812 s\n","800 \t 12.138570 \t 6.211731 \t5.912583 \t 0.014256 \t 645.318443 s\n","801 \t 12.078159 \t 6.153881 \t5.911225 \t 0.013053 \t 645.995930 s\n","802 \t 12.047357 \t 6.154530 \t5.885143 \t 0.007684 \t 646.647911 s\n","803 \t 12.160397 \t 6.228613 \t5.915145 \t 0.016638 \t 647.195171 s\n","804 \t 12.103214 \t 6.179792 \t5.909903 \t 0.013519 \t 647.734938 s\n","805 \t 12.003934 \t 6.077536 \t5.917679 \t 0.008720 \t 648.292292 s\n","806 \t 12.178835 \t 6.214508 \t5.947200 \t 0.017127 \t 648.833632 s\n","807 \t 12.146024 \t 6.215137 \t5.914673 \t 0.016214 \t 649.378193 s\n","808 \t 12.083455 \t 6.171203 \t5.897115 \t 0.015137 \t 649.935217 s\n","809 \t 12.097672 \t 6.196023 \t5.890142 \t 0.011507 \t 650.606747 s\n","810 \t 12.130385 \t 6.190901 \t5.931297 \t 0.008188 \t 651.148627 s\n","811 \t 12.005082 \t 6.088226 \t5.904000 \t 0.012857 \t 651.681388 s\n","812 \t 12.060560 \t 6.141949 \t5.908473 \t 0.010138 \t 652.225641 s\n","813 \t 12.160425 \t 6.208928 \t5.935541 \t 0.015956 \t 652.761004 s\n","814 \t 12.043200 \t 6.152729 \t5.880962 \t 0.009509 \t 653.302830 s\n","815 \t 12.105819 \t 6.194550 \t5.903536 \t 0.007732 \t 653.835096 s\n","816 \t 11.997575 \t 6.088920 \t5.898432 \t 0.010224 \t 654.418849 s\n","817 \t 12.099345 \t 6.194221 \t5.896060 \t 0.009064 \t 654.958287 s\n","818 \t 12.058541 \t 6.159737 \t5.887952 \t 0.010852 \t 655.518878 s\n","819 \t 12.157608 \t 6.228208 \t5.910064 \t 0.019336 \t 656.192608 s\n","820 \t 12.012482 \t 6.101739 \t5.900470 \t 0.010273 \t 656.796954 s\n","821 \t 12.024732 \t 6.108441 \t5.906923 \t 0.009369 \t 657.452665 s\n","822 \t 12.063672 \t 6.124490 \t5.927985 \t 0.011197 \t 658.091662 s\n","823 \t 12.109124 \t 6.199519 \t5.894648 \t 0.014958 \t 658.775547 s\n","824 \t 12.081284 \t 6.147453 \t5.921878 \t 0.011953 \t 659.499893 s\n","825 \t 12.101069 \t 6.161036 \t5.933673 \t 0.006361 \t 660.046293 s\n","826 \t 12.070807 \t 6.143850 \t5.912141 \t 0.014816 \t 660.729233 s\n","827 \t 12.054869 \t 6.127326 \t5.916042 \t 0.011501 \t 661.269716 s\n","828 \t 12.071480 \t 6.180874 \t5.872971 \t 0.017635 \t 661.819197 s\n","829 \t 12.032284 \t 6.138175 \t5.885728 \t 0.008382 \t 662.359551 s\n","830 \t 12.033271 \t 6.106855 \t5.916245 \t 0.010171 \t 662.914168 s\n","831 \t 12.058853 \t 6.118054 \t5.933262 \t 0.007538 \t 663.461284 s\n","832 \t 12.150175 \t 6.221679 \t5.919755 \t 0.008740 \t 663.995629 s\n","833 \t 12.026843 \t 6.126755 \t5.893901 \t 0.006187 \t 664.540333 s\n","834 \t 12.068678 \t 6.156328 \t5.899786 \t 0.012565 \t 665.077081 s\n","835 \t 12.070431 \t 6.189391 \t5.873516 \t 0.007523 \t 665.749222 s\n","836 \t 12.117118 \t 6.212107 \t5.896203 \t 0.008808 \t 666.285025 s\n","837 \t 12.040089 \t 6.139162 \t5.892542 \t 0.008385 \t 666.822199 s\n","838 \t 12.017575 \t 6.132154 \t5.877923 \t 0.007499 \t 667.360255 s\n","839 \t 12.012993 \t 6.112230 \t5.893755 \t 0.007008 \t 667.897308 s\n","840 \t 12.081205 \t 6.183097 \t5.886087 \t 0.012022 \t 668.431153 s\n","841 \t 12.080551 \t 6.168822 \t5.901053 \t 0.010676 \t 668.976118 s\n","842 \t 12.164067 \t 6.262916 \t5.894636 \t 0.006515 \t 669.540812 s\n","843 \t 11.950517 \t 6.052603 \t5.888481 \t 0.009433 \t 670.228755 s\n","844 \t 11.980917 \t 6.074455 \t5.895568 \t 0.010895 \t 670.876005 s\n","845 \t 12.088378 \t 6.201301 \t5.873282 \t 0.013795 \t 671.693932 s\n","846 \t 12.087282 \t 6.195737 \t5.879086 \t 0.012459 \t 672.421669 s\n","847 \t 11.960742 \t 6.064488 \t5.886795 \t 0.009459 \t 672.994556 s\n","848 \t 12.113584 \t 6.210528 \t5.894772 \t 0.008284 \t 673.544778 s\n","849 \t 12.053961 \t 6.144420 \t5.901060 \t 0.008482 \t 674.079893 s\n","850 \t 12.092803 \t 6.200267 \t5.887357 \t 0.005178 \t 674.618936 s\n","851 \t 12.044263 \t 6.118565 \t5.912682 \t 0.013016 \t 675.165141 s\n","852 \t 11.980671 \t 6.080360 \t5.888619 \t 0.011691 \t 675.696341 s\n","853 \t 11.969787 \t 6.082855 \t5.880016 \t 0.006915 \t 676.241017 s\n","854 \t 11.980782 \t 6.081022 \t5.891080 \t 0.008680 \t 676.782146 s\n","855 \t 11.933373 \t 6.066070 \t5.861382 \t 0.005922 \t 677.459348 s\n","856 \t 12.054999 \t 6.161551 \t5.884279 \t 0.009169 \t 678.004107 s\n","857 \t 11.916272 \t 6.003851 \t5.885652 \t 0.026769 \t 678.540093 s\n","858 \t 11.953246 \t 6.076067 \t5.868316 \t 0.008863 \t 679.079335 s\n","859 \t 12.007702 \t 6.109983 \t5.884175 \t 0.013545 \t 679.611307 s\n","860 \t 11.994101 \t 6.103254 \t5.880964 \t 0.009882 \t 680.159481 s\n","861 \t 12.091023 \t 6.252646 \t5.830772 \t 0.007604 \t 680.699601 s\n","862 \t 11.893064 \t 6.023107 \t5.862695 \t 0.007262 \t 681.249947 s\n","863 \t 12.039775 \t 6.149769 \t5.879793 \t 0.010214 \t 681.783633 s\n","864 \t 11.959292 \t 6.084168 \t5.868795 \t 0.006330 \t 682.466785 s\n","865 \t 12.004803 \t 6.124604 \t5.875377 \t 0.004822 \t 683.147882 s\n","866 \t 11.990052 \t 6.113599 \t5.869025 \t 0.007428 \t 683.790177 s\n","867 \t 11.976020 \t 6.079137 \t5.883631 \t 0.013252 \t 684.475969 s\n","868 \t 12.002037 \t 6.124057 \t5.866341 \t 0.011639 \t 685.163453 s\n","869 \t 12.052485 \t 6.177756 \t5.868865 \t 0.005863 \t 685.819432 s\n","870 \t 12.040644 \t 6.162067 \t5.869663 \t 0.008914 \t 686.359905 s\n","871 \t 11.990077 \t 6.129877 \t5.851238 \t 0.008963 \t 686.902098 s\n","872 \t 12.053302 \t 6.140800 \t5.906389 \t 0.006112 \t 687.582370 s\n","873 \t 12.074241 \t 6.173043 \t5.888973 \t 0.012225 \t 688.117712 s\n","874 \t 11.969903 \t 6.085990 \t5.876739 \t 0.007174 \t 688.660817 s\n","875 \t 12.006311 \t 6.125906 \t5.869190 \t 0.011214 \t 689.204330 s\n","876 \t 12.073831 \t 6.192145 \t5.870485 \t 0.011200 \t 689.742147 s\n","877 \t 12.034502 \t 6.122329 \t5.904586 \t 0.007588 \t 690.275801 s\n","878 \t 12.009325 \t 6.114607 \t5.884517 \t 0.010201 \t 690.813571 s\n","879 \t 11.951947 \t 6.093809 \t5.849050 \t 0.009089 \t 691.350678 s\n","880 \t 11.966684 \t 6.117363 \t5.840318 \t 0.009003 \t 691.893293 s\n","881 \t 12.005096 \t 6.114638 \t5.880493 \t 0.009965 \t 692.433112 s\n","882 \t 11.913569 \t 6.023514 \t5.882228 \t 0.007827 \t 693.128601 s\n","883 \t 11.974369 \t 6.091945 \t5.872319 \t 0.010104 \t 693.676626 s\n","884 \t 12.062747 \t 6.175329 \t5.879694 \t 0.007724 \t 694.221691 s\n","885 \t 12.036072 \t 6.163248 \t5.862409 \t 0.010416 \t 694.772288 s\n","886 \t 11.983253 \t 6.104924 \t5.870906 \t 0.007423 \t 695.309661 s\n","887 \t 11.962459 \t 6.099125 \t5.852519 \t 0.010815 \t 695.912422 s\n","888 \t 12.054336 \t 6.158143 \t5.890455 \t 0.005738 \t 696.553293 s\n","889 \t 11.933556 \t 6.032816 \t5.892931 \t 0.007809 \t 697.197677 s\n","890 \t 11.975177 \t 6.096344 \t5.868767 \t 0.010066 \t 697.871202 s\n","891 \t 12.014694 \t 6.141160 \t5.863055 \t 0.010478 \t 698.582462 s\n","892 \t 11.937508 \t 6.049475 \t5.879456 \t 0.008578 \t 699.262148 s\n","893 \t 11.946219 \t 6.052641 \t5.885998 \t 0.007580 \t 699.791740 s\n","894 \t 12.046866 \t 6.141447 \t5.895824 \t 0.009594 \t 700.348048 s\n","895 \t 12.125850 \t 6.246077 \t5.870284 \t 0.009490 \t 700.903732 s\n","896 \t 11.971657 \t 6.081951 \t5.882994 \t 0.006712 \t 701.443943 s\n","897 \t 11.995615 \t 6.119910 \t5.869952 \t 0.005753 \t 701.993699 s\n","898 \t 12.055912 \t 6.178511 \t5.869578 \t 0.007823 \t 702.529830 s\n","899 \t 12.010270 \t 6.132540 \t5.869851 \t 0.007877 \t 703.081660 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:26<00:00,  6.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4272\n","Hit@10: 0.5753\n","Hit@3: 0.4692\n","Hit@1: 0.3390\n","Link Prediction on Validation Set (All)\n","MRR: 0.4254\n","Hit@10: 0.6106\n","Hit@3: 0.4818\n","Hit@1: 0.3201\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.4016\n","Hit@10: 0.6235\n","Hit@3: 0.5185\n","Hit@1: 0.2469\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4905\n","Hit@10: 0.6975\n","Hit@3: 0.5651\n","Hit@1: 0.3676\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2256\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2256\n","900 \t 11.948470 \t 6.067851 \t5.871763 \t 0.008856 \t 736.519274 s\n","901 \t 11.978091 \t 6.088269 \t5.882826 \t 0.006996 \t 737.407683 s\n","902 \t 11.942913 \t 6.073990 \t5.864137 \t 0.004785 \t 737.944285 s\n","903 \t 11.918736 \t 6.041111 \t5.871042 \t 0.006583 \t 738.498379 s\n","904 \t 11.957982 \t 6.112212 \t5.841186 \t 0.004584 \t 739.030494 s\n","905 \t 11.978994 \t 6.094985 \t5.876796 \t 0.007213 \t 739.570070 s\n","906 \t 11.973573 \t 6.089457 \t5.877971 \t 0.006145 \t 740.114891 s\n","907 \t 11.914001 \t 6.041291 \t5.866454 \t 0.006256 \t 740.654261 s\n","908 \t 12.021806 \t 6.161047 \t5.852301 \t 0.008458 \t 741.196327 s\n","909 \t 11.958545 \t 6.068375 \t5.878535 \t 0.011635 \t 741.751883 s\n","910 \t 11.903936 \t 6.029093 \t5.867728 \t 0.007116 \t 742.420947 s\n","911 \t 11.942709 \t 6.077562 \t5.858735 \t 0.006413 \t 742.972639 s\n","912 \t 12.027832 \t 6.164690 \t5.855884 \t 0.007259 \t 743.530241 s\n","913 \t 11.879382 \t 5.998768 \t5.874337 \t 0.006277 \t 744.070816 s\n","914 \t 11.925100 \t 6.045648 \t5.873680 \t 0.005772 \t 744.643989 s\n","915 \t 11.900926 \t 6.052186 \t5.840614 \t 0.008126 \t 745.191427 s\n","916 \t 11.972159 \t 6.125886 \t5.839473 \t 0.006800 \t 745.736339 s\n","917 \t 11.991806 \t 6.133281 \t5.851533 \t 0.006991 \t 746.271981 s\n","918 \t 11.876698 \t 6.012968 \t5.855502 \t 0.008228 \t 746.939489 s\n","919 \t 11.953574 \t 6.075062 \t5.872220 \t 0.006293 \t 747.543588 s\n","920 \t 11.980253 \t 6.106872 \t5.866409 \t 0.006972 \t 748.471292 s\n","921 \t 11.899382 \t 6.052463 \t5.838461 \t 0.008458 \t 749.733210 s\n","922 \t 11.993969 \t 6.148221 \t5.838654 \t 0.007093 \t 750.864110 s\n","923 \t 11.964867 \t 6.105561 \t5.853400 \t 0.005906 \t 751.506289 s\n","924 \t 11.921329 \t 6.035257 \t5.880704 \t 0.005368 \t 752.161751 s\n","925 \t 11.893178 \t 6.001546 \t5.880207 \t 0.011425 \t 752.797092 s\n","926 \t 11.979359 \t 6.112394 \t5.858028 \t 0.008936 \t 753.448483 s\n","927 \t 11.983720 \t 6.124088 \t5.852387 \t 0.007245 \t 754.122164 s\n","928 \t 11.983855 \t 6.112362 \t5.868775 \t 0.002717 \t 754.949960 s\n","929 \t 11.985808 \t 6.118040 \t5.861240 \t 0.006528 \t 755.629328 s\n","930 \t 11.858316 \t 6.017642 \t5.832988 \t 0.007686 \t 756.291272 s\n","931 \t 11.917296 \t 6.054789 \t5.856721 \t 0.005786 \t 756.941736 s\n","932 \t 11.887347 \t 6.034241 \t5.845983 \t 0.007122 \t 757.513403 s\n","933 \t 12.045438 \t 6.193213 \t5.845650 \t 0.006576 \t 758.108622 s\n","934 \t 11.863954 \t 5.992665 \t5.862434 \t 0.008854 \t 758.712758 s\n","935 \t 11.929812 \t 6.061942 \t5.860544 \t 0.007326 \t 759.280383 s\n","936 \t 11.992404 \t 6.129552 \t5.857635 \t 0.005216 \t 759.876007 s\n","937 \t 11.942747 \t 6.075826 \t5.852593 \t 0.014328 \t 760.479919 s\n","938 \t 11.989963 \t 6.127249 \t5.855651 \t 0.007062 \t 761.320358 s\n","939 \t 11.931721 \t 6.069567 \t5.856123 \t 0.006031 \t 762.038335 s\n","940 \t 11.952801 \t 6.091259 \t5.854702 \t 0.006839 \t 762.750238 s\n","941 \t 11.940834 \t 6.060957 \t5.873130 \t 0.006747 \t 763.500758 s\n","942 \t 11.966689 \t 6.107639 \t5.847620 \t 0.011429 \t 764.150006 s\n","943 \t 12.028596 \t 6.181948 \t5.840796 \t 0.005853 \t 764.692179 s\n","944 \t 11.983781 \t 6.111922 \t5.866997 \t 0.004863 \t 765.235069 s\n","945 \t 11.974927 \t 6.110378 \t5.855277 \t 0.009272 \t 765.782106 s\n","946 \t 11.996662 \t 6.135892 \t5.855478 \t 0.005292 \t 766.323265 s\n","947 \t 11.948791 \t 6.066861 \t5.877516 \t 0.004414 \t 766.869066 s\n","948 \t 11.957763 \t 6.108911 \t5.840055 \t 0.008797 \t 767.555327 s\n","949 \t 12.115660 \t 6.256174 \t5.850185 \t 0.009301 \t 768.103092 s\n","950 \t 11.992404 \t 6.111232 \t5.872437 \t 0.008734 \t 768.652999 s\n","951 \t 11.879711 \t 6.046680 \t5.826333 \t 0.006697 \t 769.199435 s\n","952 \t 11.900953 \t 6.036628 \t5.857819 \t 0.006507 \t 769.760937 s\n","953 \t 11.844846 \t 5.981255 \t5.856973 \t 0.006618 \t 770.296210 s\n","954 \t 11.873119 \t 6.007040 \t5.858644 \t 0.007435 \t 770.840011 s\n","955 \t 12.005137 \t 6.141889 \t5.855052 \t 0.008197 \t 771.381183 s\n","956 \t 11.838094 \t 5.978443 \t5.854125 \t 0.005526 \t 771.925166 s\n","957 \t 11.905008 \t 6.062037 \t5.835940 \t 0.007031 \t 772.603294 s\n","958 \t 12.006017 \t 6.132479 \t5.867755 \t 0.005783 \t 773.153783 s\n","959 \t 11.876857 \t 5.997771 \t5.874050 \t 0.005037 \t 773.688942 s\n","960 \t 12.006219 \t 6.152266 \t5.845984 \t 0.007969 \t 774.317283 s\n","961 \t 11.929334 \t 6.067908 \t5.855402 \t 0.006024 \t 775.007465 s\n","962 \t 12.009822 \t 6.135347 \t5.867988 \t 0.006488 \t 775.653104 s\n","963 \t 11.903486 \t 6.051852 \t5.846315 \t 0.005319 \t 776.338441 s\n","964 \t 11.973944 \t 6.106378 \t5.860482 \t 0.007084 \t 777.048001 s\n","965 \t 11.958945 \t 6.088525 \t5.863925 \t 0.006495 \t 777.724860 s\n","966 \t 11.994452 \t 6.141460 \t5.846221 \t 0.006770 \t 778.268663 s\n","967 \t 12.004279 \t 6.166222 \t5.832674 \t 0.005383 \t 778.803004 s\n","968 \t 11.952258 \t 6.104067 \t5.843267 \t 0.004923 \t 779.350148 s\n","969 \t 11.952851 \t 6.075325 \t5.870758 \t 0.006768 \t 779.894242 s\n","970 \t 11.932986 \t 6.076399 \t5.850372 \t 0.006216 \t 780.444842 s\n","971 \t 11.980007 \t 6.120926 \t5.852925 \t 0.006156 \t 780.989461 s\n","972 \t 11.902905 \t 6.048506 \t5.848303 \t 0.006096 \t 781.544242 s\n","973 \t 11.970664 \t 6.131229 \t5.834394 \t 0.005041 \t 782.100497 s\n","974 \t 12.008608 \t 6.143711 \t5.858305 \t 0.006592 \t 782.656433 s\n","975 \t 11.973410 \t 6.133054 \t5.836393 \t 0.003963 \t 783.339960 s\n","976 \t 11.862877 \t 5.982631 \t5.876415 \t 0.003832 \t 783.874395 s\n","977 \t 11.799521 \t 5.949613 \t5.842037 \t 0.007871 \t 784.420860 s\n","978 \t 11.897868 \t 6.069348 \t5.823139 \t 0.005382 \t 784.960207 s\n","979 \t 11.976633 \t 6.110807 \t5.860191 \t 0.005634 \t 785.498081 s\n","980 \t 11.952038 \t 6.072975 \t5.872005 \t 0.007059 \t 786.032226 s\n","981 \t 11.964290 \t 6.113733 \t5.843548 \t 0.007009 \t 786.570195 s\n","982 \t 11.956429 \t 6.078232 \t5.869665 \t 0.008532 \t 787.163354 s\n","983 \t 11.954417 \t 6.092735 \t5.856410 \t 0.005272 \t 787.847089 s\n","984 \t 11.932869 \t 6.082629 \t5.844249 \t 0.005990 \t 788.485337 s\n","985 \t 11.925392 \t 6.041125 \t5.877005 \t 0.007262 \t 789.315664 s\n","986 \t 11.957017 \t 6.073187 \t5.874560 \t 0.009270 \t 790.025625 s\n","987 \t 11.925375 \t 6.090184 \t5.827377 \t 0.007814 \t 790.568583 s\n","988 \t 11.935650 \t 6.077322 \t5.854202 \t 0.004127 \t 791.105082 s\n","989 \t 11.876945 \t 6.033194 \t5.835876 \t 0.007875 \t 791.657467 s\n","990 \t 11.952587 \t 6.109625 \t5.837872 \t 0.005090 \t 792.200846 s\n","991 \t 12.010102 \t 6.151957 \t5.852120 \t 0.006025 \t 792.758296 s\n","992 \t 11.896104 \t 6.040289 \t5.851155 \t 0.004660 \t 793.292170 s\n","993 \t 11.910888 \t 6.064797 \t5.839370 \t 0.006721 \t 793.830803 s\n","994 \t 11.955281 \t 6.111933 \t5.839305 \t 0.004043 \t 794.368820 s\n","995 \t 11.892020 \t 6.053720 \t5.832803 \t 0.005498 \t 795.057550 s\n","996 \t 11.859233 \t 6.022353 \t5.832656 \t 0.004224 \t 795.601492 s\n","997 \t 11.905872 \t 6.069380 \t5.829934 \t 0.006557 \t 796.165250 s\n","998 \t 11.964616 \t 6.121450 \t5.834317 \t 0.008849 \t 796.715912 s\n","999 \t 11.971433 \t 6.116858 \t5.850168 \t 0.004407 \t 797.249902 s\n","1000 \t 11.886997 \t 6.026779 \t5.852569 \t 0.007650 \t 797.790832 s\n","1001 \t 11.823209 \t 5.948607 \t5.868552 \t 0.006050 \t 798.326409 s\n","1002 \t 11.948181 \t 6.088788 \t5.854688 \t 0.004704 \t 798.863455 s\n","1003 \t 11.952739 \t 6.086705 \t5.857867 \t 0.008168 \t 799.391846 s\n","1004 \t 12.038757 \t 6.185327 \t5.848385 \t 0.005045 \t 800.080491 s\n","1005 \t 11.927353 \t 6.058259 \t5.861534 \t 0.007562 \t 800.748265 s\n","1006 \t 11.958926 \t 6.114765 \t5.838392 \t 0.005768 \t 801.402660 s\n","1007 \t 11.924480 \t 6.054097 \t5.864589 \t 0.005793 \t 802.052855 s\n","1008 \t 11.981173 \t 6.126808 \t5.847049 \t 0.007315 \t 802.774847 s\n","1009 \t 11.943000 \t 6.082562 \t5.855378 \t 0.005060 \t 803.419831 s\n","1010 \t 11.981642 \t 6.142896 \t5.833648 \t 0.005098 \t 803.955992 s\n","1011 \t 11.916570 \t 6.066654 \t5.845640 \t 0.004277 \t 804.535159 s\n","1012 \t 11.899520 \t 6.044596 \t5.843430 \t 0.011494 \t 805.240609 s\n","1013 \t 11.987606 \t 6.134291 \t5.846592 \t 0.006723 \t 805.771632 s\n","1014 \t 11.850618 \t 6.011551 \t5.832701 \t 0.006367 \t 806.321662 s\n","1015 \t 11.857463 \t 5.996294 \t5.852818 \t 0.008351 \t 806.862215 s\n","1016 \t 11.956276 \t 6.106975 \t5.841547 \t 0.007753 \t 807.399060 s\n","1017 \t 11.933270 \t 6.093283 \t5.835115 \t 0.004871 \t 807.940331 s\n","1018 \t 11.936886 \t 6.086559 \t5.839197 \t 0.011130 \t 808.484859 s\n","1019 \t 11.929486 \t 6.093404 \t5.829887 \t 0.006195 \t 809.024211 s\n","1020 \t 11.941419 \t 6.088934 \t5.847358 \t 0.005127 \t 809.568487 s\n","1021 \t 11.986503 \t 6.125029 \t5.852572 \t 0.008902 \t 810.105483 s\n","1022 \t 11.855783 \t 6.003646 \t5.844900 \t 0.007237 \t 810.786911 s\n","1023 \t 11.966487 \t 6.111818 \t5.849609 \t 0.005060 \t 811.326024 s\n","1024 \t 11.982955 \t 6.130399 \t5.848932 \t 0.003624 \t 811.863478 s\n","1025 \t 11.943208 \t 6.080895 \t5.857636 \t 0.004678 \t 812.413445 s\n","1026 \t 11.951851 \t 6.091300 \t5.854593 \t 0.005958 \t 812.950176 s\n","1027 \t 11.938435 \t 6.096267 \t5.835420 \t 0.006748 \t 813.636405 s\n","1028 \t 11.888308 \t 6.035666 \t5.847145 \t 0.005497 \t 814.276944 s\n","1029 \t 11.928793 \t 6.084360 \t5.836530 \t 0.007903 \t 814.945732 s\n","1030 \t 11.972332 \t 6.098344 \t5.868293 \t 0.005694 \t 815.656731 s\n","1031 \t 11.914124 \t 6.045935 \t5.862328 \t 0.005861 \t 816.296072 s\n","1032 \t 11.907022 \t 6.065266 \t5.836497 \t 0.005259 \t 816.984795 s\n","1033 \t 11.927515 \t 6.067107 \t5.853304 \t 0.007103 \t 817.524500 s\n","1034 \t 11.909088 \t 6.062027 \t5.840765 \t 0.006296 \t 818.070724 s\n","1035 \t 11.862035 \t 6.010661 \t5.845827 \t 0.005547 \t 818.608060 s\n","1036 \t 11.996530 \t 6.171259 \t5.819717 \t 0.005555 \t 819.147104 s\n","1037 \t 11.989271 \t 6.136131 \t5.849284 \t 0.003856 \t 819.700843 s\n","1038 \t 11.947713 \t 6.102672 \t5.834471 \t 0.010570 \t 820.252377 s\n","1039 \t 11.854878 \t 5.986467 \t5.862512 \t 0.005899 \t 820.799091 s\n","1040 \t 11.967493 \t 6.098444 \t5.863655 \t 0.005393 \t 821.468404 s\n","1041 \t 11.854382 \t 6.010696 \t5.837158 \t 0.006528 \t 822.013140 s\n","1042 \t 11.873058 \t 6.026154 \t5.841655 \t 0.005250 \t 822.552847 s\n","1043 \t 11.896476 \t 6.034815 \t5.855577 \t 0.006084 \t 823.096919 s\n","1044 \t 11.878806 \t 6.015789 \t5.858371 \t 0.004646 \t 823.653002 s\n","1045 \t 12.090325 \t 6.240858 \t5.845694 \t 0.003773 \t 824.213702 s\n","1046 \t 11.975687 \t 6.137754 \t5.832809 \t 0.005124 \t 824.751781 s\n","1047 \t 11.903135 \t 6.036803 \t5.856501 \t 0.009832 \t 825.301886 s\n","1048 \t 11.934150 \t 6.095072 \t5.833321 \t 0.005757 \t 825.838442 s\n","1049 \t 11.911619 \t 6.042231 \t5.862737 \t 0.006651 \t 826.627096 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 162/162 [00:26<00:00,  6.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4289\n","Hit@10: 0.5719\n","Hit@3: 0.4658\n","Hit@1: 0.3493\n","Link Prediction on Validation Set (All)\n","MRR: 0.4225\n","Hit@10: 0.6122\n","Hit@3: 0.4835\n","Hit@1: 0.3185\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3973\n","Hit@10: 0.6173\n","Hit@3: 0.4938\n","Hit@1: 0.2593\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4855\n","Hit@10: 0.6870\n","Hit@3: 0.5546\n","Hit@1: 0.3718\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.2277\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.2277\n"]}]},{"cell_type":"markdown","source":["# Test.py\n"],"metadata":{"id":"-BsiHiMQoArV"}},{"cell_type":"code","source":["KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix.cuda(), # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix.cuda(), # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix.cuda(), # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix.cuda(), # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask.cuda(), # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask.cuda(), # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","def load_id_mapping(file_path):\n","    id2name = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip() == \"\" or line.startswith(\"#\"):  # 주석 또는 공백 무시\n","                continue\n","            parts = line.strip().split('\\t')\n","            if len(parts) != 2:\n","                continue\n","            name, idx = parts\n","            id2name[int(idx)] = name\n","    return id2name\n","\n","id2ent = load_id_mapping(\"entity2id.txt\")\n","id2rel = load_id_mapping(\"relation2id.txt\")\n","\n","def convert_triplet_ids_to_names(triplet, id2ent, id2rel, num_ent, num_rel):\n","    triplet_named = []\n","    for idx, val in enumerate(triplet):\n","        if idx % 2 == 0:  # entity or numeric value\n","            if val < num_ent:\n","                triplet_named.append(id2ent.get(val, f\"[ENT:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[NUM:{val - num_ent}]\")\n","        else:  # relation\n","            if val < num_rel:\n","                triplet_named.append(id2rel.get(val, f\"[REL:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[MASK_REL]\")\n","    return triplet_named\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTHNKG-OA_NT/checkpoint/Reproduce/VTHNOA_NT_maximg==3/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","                else: # qualifier\n","                    sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n"],"metadata":{"id":"ChVIC_5BHELi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748101926070,"user_tz":-540,"elapsed":44534,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"7e961034-f18e-41f7-ff49-f731ff3d6c7e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":[" 81%|████████  | 134/165 [00:26<00:03,  8.32it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0339, Pred: 0.0083, SE: 0.000655\n","[Triplet Num] GT: 0.0000, Pred: 0.1254, SE: 0.015719\n","[Triplet Num] GT: 0.0015, Pred: -0.0052, SE: 0.000045\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▎ | 138/165 [00:26<00:02, 11.25it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0678, Pred: 0.0057, SE: 0.003861\n","[Triplet Num] GT: 0.0169, Pred: -0.0084, SE: 0.000643\n","[Triplet Num] GT: 0.0000, Pred: -0.0116, SE: 0.000134\n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▍ | 140/165 [00:26<00:02, 12.18it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: 0.0013, SE: 0.000002\n","[Triplet Num] GT: 0.0095, Pred: 0.0419, SE: 0.001051\n","[Triplet Num] GT: 0.3719, Pred: 0.0897, SE: 0.079618\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 144/165 [00:27<00:01, 13.13it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: 0.0006, SE: 0.000000\n","[Triplet Num] GT: 0.0000, Pred: 0.0390, SE: 0.001523\n","[Triplet Num] GT: 0.0034, Pred: 0.0222, SE: 0.000353\n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 146/165 [00:27<00:01, 13.66it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0268, SE: 0.000721\n","[Triplet Num] GT: 0.0000, Pred: -0.0040, SE: 0.000016\n","[Triplet Num] GT: 0.2632, Pred: 0.0935, SE: 0.028799\n"]},{"output_type":"stream","name":"stderr","text":[" 91%|█████████ | 150/165 [00:27<00:01, 14.11it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0339, Pred: -0.0083, SE: 0.001777\n","[Triplet Num] GT: 0.0000, Pred: -0.0105, SE: 0.000110\n","[Triplet Num] GT: 0.0000, Pred: 0.0551, SE: 0.003034\n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 152/165 [00:27<00:00, 14.36it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0005, Pred: 0.0256, SE: 0.000628\n","[Triplet Num] GT: 0.0145, Pred: 0.0333, SE: 0.000354\n","[Triplet Num] GT: 0.0704, Pred: 0.0235, SE: 0.002191\n"]},{"output_type":"stream","name":"stderr","text":[" 95%|█████████▍| 156/165 [00:27<00:00, 14.62it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0000, Pred: -0.0078, SE: 0.000061\n","[Triplet Num] GT: 0.0068, Pred: 0.0059, SE: 0.000001\n","[Triplet Num] GT: 0.0005, Pred: 0.0092, SE: 0.000076\n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 158/165 [00:28<00:00, 14.55it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0201, Pred: 0.1300, SE: 0.012085\n","[Triplet Num] GT: 0.1695, Pred: 0.2281, SE: 0.003436\n","[Triplet Num] GT: 0.0000, Pred: 0.0332, SE: 0.001100\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 162/165 [00:28<00:00, 14.54it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.0034, Pred: -0.0029, SE: 0.000039\n","[Triplet Num] GT: 0.1346, Pred: -0.0193, SE: 0.023680\n","[Triplet Num] GT: 0.2632, Pred: 0.1948, SE: 0.004671\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 165/165 [00:28<00:00,  5.79it/s]"]},{"output_type":"stream","name":"stdout","text":["[Triplet Num] GT: 0.2965, Pred: 0.1176, SE: 0.031988\n","[Triplet Num] GT: 0.0000, Pred: -0.0127, SE: 0.000162\n","[Triplet Num] GT: 0.0169, Pred: -0.0051, SE: 0.000485\n","Link Prediction on Validation Set (Tri)\n","MRR: 0.4663\n","Hit@10: 0.5960\n","Hit@3: 0.4949\n","Hit@1: 0.3906\n","Link Prediction on Validation Set (All)\n","MRR: 0.4929\n","Hit@10: 0.6565\n","Hit@3: 0.5415\n","Hit@1: 0.3962\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3131\n","Hit@10: 0.5333\n","Hit@3: 0.4061\n","Hit@1: 0.1758\n","Relation Prediction on Validation Set (All)\n","MRR: 0.4808\n","Hit@10: 0.6883\n","Hit@3: 0.5709\n","Hit@1: 0.3563\n","Numeric Value Prediction on Validation Set (Tri)\n","RMSE: 0.0815\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0815\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}