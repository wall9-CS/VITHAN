{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"15ZyBUPxReo2Og3zVmylZnwhZI7jNLkVw","authorship_tag":"ABX9TyPi/9UVtp+AmrmeH/HB6buN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tMncOeX6pDmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747974125701,"user_tz":-540,"elapsed":26387,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"b38c38ee-d86b-4e40-8425-cf38c68c116d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# import\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import numpy as np\n","import copy\n","import argparse\n","import datetime\n","import time\n","import os\n","import math\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"xWGfSBgsm1r2","executionInfo":{"status":"ok","timestamp":1747974130765,"user_tz":-540,"elapsed":3687,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# util.py"],"metadata":{"id":"rhEFWjoInTFU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_rank(score, target, filter_list):\n","\tscore_target = score[target]\n","\tscore[filter_list] = score_target - 1\n","\trank = np.sum(score > score_target) + np.sum(score == score_target) // 2 + 1\n","\treturn rank\n","\n","def metrics(rank):\n","    mrr = np.mean(1 / rank)\n","    hit10 = np.sum(rank < 11) / len(rank)\n","    hit3 = np.sum(rank < 4) / len(rank)\n","    hit1 = np.sum(rank < 2) / len(rank)\n","    return mrr, hit10, hit3, hit1"],"metadata":{"id":"YjFx5ALxnShV","executionInfo":{"status":"ok","timestamp":1747974130780,"user_tz":-540,"elapsed":2,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Model.py"],"metadata":{"id":"uu_H9jBNmDRJ"}},{"cell_type":"code","source":["class VTHN(nn.Module):\n","    def __init__(self, num_ent, num_rel, ent_vis, rel_vis, dim_vis, ent_txt, rel_txt, dim_txt, ent_vis_mask, rel_vis_mask,\n","                 dim_str, num_head, dim_hid, num_layer_enc_ent, num_layer_enc_rel, num_layer_prediction, num_layer_context,\n","                 dropout=0.1, emb_dropout=0.6, vis_dropout=0.1, txt_dropout=0.1, emb_as_proj=False):\n","        super(VTHN, self).__init__()\n","        self.dim_str = dim_str\n","        self.num_head = num_head\n","        self.dim_hid = dim_hid\n","        self.num_ent = num_ent\n","        self.num_rel = num_rel\n","        self.mask_token_id = num_ent + num_rel  # 마스킹 인덱스 정의\n","\n","        self.ent_vis = ent_vis\n","        self.rel_vis = rel_vis\n","        self.ent_txt = ent_txt.unsqueeze(dim=1)\n","        self.rel_txt = rel_txt.unsqueeze(dim=1)\n","\n","        false_ents = torch.full((self.num_ent, 1), False).cuda()\n","        self.ent_mask = torch.cat([false_ents, false_ents, ent_vis_mask, false_ents], dim=1)\n","        false_rels = torch.full((self.num_rel, 1), False).cuda()\n","        self.rel_mask = torch.cat([false_rels, false_rels, rel_vis_mask, false_rels], dim=1)\n","\n","        self.ent_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.nv_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_rel_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.q_v_token = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.ent_embeddings = nn.Parameter(torch.Tensor(num_ent, 1, dim_str))\n","        self.rel_embeddings = nn.Parameter(torch.Tensor(num_rel, 1, dim_str))\n","\n","        self.lp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.rp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","        self.nvp_token = nn.Parameter(torch.Tensor(1, dim_str))\n","\n","        self.ent_dec = nn.Linear(dim_str, num_ent)\n","        self.rel_dec = nn.Linear(dim_str, num_rel)\n","        self.num_dec = nn.Linear(dim_str, num_rel)\n","\n","        self.num_mask = nn.Parameter(torch.tensor(0.5))\n","\n","        self.str_ent_ln = nn.LayerNorm(dim_str)\n","        self.str_rel_ln = nn.LayerNorm(dim_str)\n","        self.str_nv_ln = nn.LayerNorm(dim_str)\n","        self.vis_ln = nn.LayerNorm(dim_str)\n","        self.txt_ln = nn.LayerNorm(dim_str)\n","\n","        self.embdr = nn.Dropout(p=emb_dropout)\n","        self.visdr = nn.Dropout(p=vis_dropout)\n","        self.txtdr = nn.Dropout(p=txt_dropout)\n","\n","        self.pos_str_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_ent = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_str_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_vis_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_txt_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_head = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_rel = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_tail = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_q = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_v = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        self.pos_triplet = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","        self.pos_qualifier = nn.Parameter(torch.Tensor(1, 1, dim_str))\n","\n","        if dim_vis > 0: # numeric triplet 처리\n","            self.proj_ent_vis = nn.Linear(dim_vis, dim_str)\n","            self.proj_rel_vis = nn.Linear(3 * dim_vis, dim_str)\n","        else:\n","            self.proj_ent_vis = nn.Identity()\n","            self.proj_rel_vis = nn.Identity()\n","        self.proj_txt = nn.Linear(dim_txt, dim_str)\n","\n","        self.pri_enc = nn.Linear(self.dim_str * 3, self.dim_str)\n","        self.qv_enc = nn.Linear(self.dim_str * 2, self.dim_str)\n","\n","\n","        ent_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.ent_encoder = nn.TransformerEncoder(ent_encoder_layer, num_layer_enc_ent)\n","        rel_encoder_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.rel_encoder = nn.TransformerEncoder(rel_encoder_layer, num_layer_enc_rel)\n","        context_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.context_transformer = nn.TransformerEncoder(context_transformer_layer, num_layer_context)\n","        prediction_transformer_layer = nn.TransformerEncoderLayer(dim_str, num_head, dim_hid, dropout, batch_first=True)\n","        self.prediction_transformer = nn.TransformerEncoder(prediction_transformer_layer, num_layer_prediction)\n","\n","        nn.init.xavier_uniform_(self.ent_embeddings)\n","        nn.init.xavier_uniform_(self.rel_embeddings)\n","        nn.init.xavier_uniform_(self.proj_ent_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_rel_vis.weight)\n","        nn.init.xavier_uniform_(self.proj_txt.weight)\n","\n","        nn.init.xavier_uniform_(self.ent_token)\n","        nn.init.xavier_uniform_(self.rel_token)\n","        nn.init.xavier_uniform_(self.nv_token)\n","\n","        nn.init.xavier_uniform_(self.lp_token)\n","        nn.init.xavier_uniform_(self.rp_token)\n","        nn.init.xavier_uniform_(self.nvp_token)\n","\n","        nn.init.xavier_uniform_(self.pos_str_ent)\n","        nn.init.xavier_uniform_(self.pos_vis_ent)\n","        nn.init.xavier_uniform_(self.pos_txt_ent)\n","        nn.init.xavier_uniform_(self.pos_str_rel)\n","        nn.init.xavier_uniform_(self.pos_vis_rel)\n","        nn.init.xavier_uniform_(self.pos_txt_rel)\n","        nn.init.xavier_uniform_(self.pos_head)\n","        nn.init.xavier_uniform_(self.pos_rel)\n","        nn.init.xavier_uniform_(self.pos_tail)\n","        nn.init.xavier_uniform_(self.pos_q)\n","        nn.init.xavier_uniform_(self.pos_v)\n","        nn.init.xavier_uniform_(self.pos_triplet)\n","        nn.init.xavier_uniform_(self.pos_qualifier)\n","\n","        nn.init.xavier_uniform_(self.ent_dec.weight)\n","        nn.init.xavier_uniform_(self.rel_dec.weight)\n","        nn.init.xavier_uniform_(self.num_dec.weight)\n","\n","        self.proj_ent_vis.bias.data.zero_()\n","        self.proj_rel_vis.bias.data.zero_()\n","        self.proj_txt.bias.data.zero_()\n","\n","        self.emb_as_proj = emb_as_proj\n","\n","    def forward(self, src, num_values, src_key_padding_mask, mask_locs):\n","        batch_size = len(src)\n","        num_val = torch.where(num_values != -1, num_values, self.num_mask)\n","\n","        # entity & relation embedding\n","        ent_tkn = self.ent_token.tile(self.num_ent, 1, 1)\n","        rep_ent_str = self.embdr(self.str_ent_ln(self.ent_embeddings)) + self.pos_str_ent\n","        rep_ent_vis = self.visdr(self.vis_ln(self.proj_ent_vis(self.ent_vis))) + self.pos_vis_ent\n","        rep_ent_txt = self.txtdr(self.txt_ln(self.proj_txt(self.ent_txt))) + self.pos_txt_ent\n","        ent_seq = torch.cat([ent_tkn, rep_ent_str, rep_ent_vis, rep_ent_txt], dim=1)\n","        ent_embs = self.ent_encoder(ent_seq, src_key_padding_mask=self.ent_mask)[:, 0]\n","\n","        rel_tkn = self.rel_token.tile(self.num_rel, 1, 1)\n","        rep_rel_str = self.embdr(self.str_rel_ln(self.rel_embeddings)) + self.pos_str_rel\n","        rep_rel_vis = self.visdr(self.vis_ln(self.proj_rel_vis(self.rel_vis))) + self.pos_vis_rel\n","        rep_rel_txt = self.txtdr(self.txt_ln(self.proj_txt(self.rel_txt))) + self.pos_txt_rel\n","        rel_seq = torch.cat([rel_tkn, rep_rel_str, rep_rel_vis, rep_rel_txt], dim=1)\n","        rel_embs = self.rel_encoder(rel_seq, src_key_padding_mask=self.rel_mask)[:, 0]\n","\n","        # masking된 인덱스가 범위를 벗어나지 않도록 방어 처리\n","        h_idx = src[..., 0].clamp(0, self.num_ent - 1)\n","        r_idx = src[..., 1].clamp(0, self.num_rel - 1)\n","        t_idx = src[..., 2].clamp(0, self.num_ent - 1)\n","        q_idx = src[..., 3::2].flatten().clamp(0, self.num_rel - 1)\n","        v_idx = src[..., 4::2].flatten().clamp(0, self.num_ent - 1)\n","\n","        h_seq = ent_embs[h_idx].view(batch_size, 1, self.dim_str)\n","        r_seq = rel_embs[r_idx].view(batch_size, 1, self.dim_str)\n","        t_seq = (ent_embs[t_idx] * num_val[..., 0:1]).view(batch_size, 1, self.dim_str)\n","        q_seq = rel_embs[q_idx].view(batch_size, -1, self.dim_str)\n","        v_seq = (ent_embs[v_idx] * num_val[..., 1:].flatten().unsqueeze(-1)).view(batch_size, -1, self.dim_str)\n","\n","        tri_seq = self.pri_enc(torch.cat([h_seq, r_seq, t_seq], dim=-1)) + self.pos_triplet\n","        qv_seqs = self.qv_enc(torch.cat([q_seq, v_seq], dim=-1)) + self.pos_qualifier\n","\n","        enc_in_seq = torch.cat([tri_seq, qv_seqs], dim=1)\n","        enc_out_seq = self.context_transformer(enc_in_seq, src_key_padding_mask=src_key_padding_mask)\n","\n","        dec_in_rep = enc_out_seq[mask_locs].view(batch_size, 1, self.dim_str)\n","        triplet = torch.stack([h_seq + self.pos_head, r_seq + self.pos_rel, t_seq + self.pos_tail], dim=2)\n","        qv = torch.stack([q_seq + self.pos_q, v_seq + self.pos_v, torch.zeros_like(v_seq)], dim=2)\n","        dec_in_part = torch.cat([triplet, qv], dim=1)[mask_locs]\n","\n","        dec_in_seq = torch.cat([dec_in_rep, dec_in_part], dim=1)\n","        dec_in_mask = torch.full((batch_size, 4), False, device=src.device)\n","        dec_in_mask[torch.nonzero(mask_locs == 1)[:, 1] != 0, 3] = True\n","        dec_out_seq = self.prediction_transformer(dec_in_seq, src_key_padding_mask=dec_in_mask)\n","\n","        return self.ent_dec(dec_out_seq), self.rel_dec(dec_out_seq), self.num_dec(dec_out_seq)"],"metadata":{"id":"2CgXgeAXmg-C","executionInfo":{"status":"ok","timestamp":1747974131325,"user_tz":-540,"elapsed":24,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset.py"],"metadata":{"id":"cQiHkCXOmfb6"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"mTMmNF8Cl5it","executionInfo":{"status":"ok","timestamp":1747974135599,"user_tz":-540,"elapsed":56,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"outputs":[],"source":["class VTHNKG(Dataset):\n","    def __init__(self, data, max_vis_len = -1, test = False):\n","        # entity, relation data 로드\n","        self.data = data\n","        # self.dir = \"{}\".format(self.data)\n","        self.dir = \"/content/drive/MyDrive/code/VTHNKG-OA_CQI/\" ################# Change dataset here!! ####################\n","        self.ent2id = {}\n","        self.id2ent = {}\n","        self.rel2id = {}\n","        self.id2rel = {}\n","        with open(self.dir+\"entity2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_ent = int(lines[0].strip())\n","            for line in lines[1:]:\n","                ent, idx = line.strip().split(\"\\t\")\n","                self.ent2id[ent] = int(idx)\n","                self.id2ent[int(idx)] = ent\n","\n","        with open(self.dir+\"relation2id.txt\") as f:\n","            lines = f.readlines()\n","            self.num_rel = int(lines[0].strip())\n","            for line in lines[1:]:\n","                rel, idx = line.strip().split(\"\\t\")\n","                self.rel2id[rel] = int(idx)\n","                self.id2rel[int(idx)] = rel\n","\n","        # train data 로드\n","        self.train = []\n","        self.train_pad = []\n","        self.train_num = []\n","        self.train_len = []\n","        self.max_len = 0\n","        with open(self.dir+\"train.txt\") as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = line.strip().split(\"\\t\")\n","                h,r,t = hp_triplet[:3]\n","                num_qual = (len(hp_triplet)-3)//2\n","                self.train_len.append(len(hp_triplet))\n","                try:\n","                    self.train_num.append([float(t)])\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.num_ent+self.rel2id[r]])\n","                except:\n","                    self.train.append([self.ent2id[h],self.rel2id[r],self.ent2id[t]])\n","                    self.train_num.append([1])\n","                self.train_pad.append([False])\n","                for i in range(num_qual):\n","                    q = hp_triplet[3+2*i]\n","                    v = hp_triplet[4+2*i]\n","                    self.train[-1].append(self.rel2id[q])\n","                    try:\n","                        self.train_num[-1].append(float(v))\n","                        self.train[-1].append(self.num_ent+self.rel2id[q])\n","                    except:\n","                        self.train_num[-1].append(1)\n","                        self.train[-1].append(self.ent2id[v])\n","                    self.train_pad[-1].append(False)\n","                tri_len = num_qual*2+3\n","                if tri_len > self.max_len:\n","                    self.max_len = tri_len\n","        self.num_train = len(self.train)\n","        for i in range(self.num_train):\n","            curr_len = len(self.train[i])\n","            for j in range((self.max_len-curr_len)//2):\n","                self.train[i].append(0)\n","                self.train[i].append(0)\n","                self.train_pad[i].append(True)\n","                self.train_num[i].append(1)\n","\n","        # test data 로드\n","        self.test = []\n","        self.test_pad = []\n","        self.test_num = []\n","        self.test_len = []\n","        if test:\n","            test_dir = self.dir + \"test.txt\"\n","        else:\n","            test_dir = self.dir + \"valid.txt\"\n","        with open(test_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.test_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.test.append(hp_triplet)\n","                self.test_pad.append(hp_pad)\n","                self.test_num.append(hp_num)\n","        self.num_test = len(self.test)\n","\n","        # validation data 로드\n","        self.valid = []\n","        self.valid_pad = []\n","        self.valid_num = []\n","        self.valid_len = []\n","        if test:\n","            valid_dir = self.dir + \"valid.txt\"\n","        else:\n","            valid_dir = self.dir + \"test.txt\"\n","        with open(valid_dir) as f:\n","            for line in f.readlines()[1:]:\n","                hp_triplet = []\n","                hp_pad = []\n","                hp_num = []\n","                for i, anything in enumerate(line.strip().split(\"\\t\")):\n","                    if i % 2 == 0 and i != 0:\n","                        try:\n","                            hp_num.append(float(anything))\n","                            hp_triplet.append(self.num_ent + hp_triplet[-1])\n","                        except:\n","                            hp_triplet.append(self.ent2id[anything])\n","                            hp_num.append(1)\n","                    elif i == 0:\n","                        hp_triplet.append(self.ent2id[anything])\n","                    else:\n","                        hp_triplet.append(self.rel2id[anything])\n","                        hp_pad.append(False)\n","                flag = 0\n","                self.valid_len.append(len(hp_triplet))\n","                while len(hp_triplet) < self.max_len:\n","                    hp_triplet.append(0)\n","                    flag += 1\n","                    if flag % 2:\n","                        hp_num.append(1)\n","                        hp_pad.append(True)\n","                self.valid.append(hp_triplet)\n","                self.valid_pad.append(hp_pad)\n","                self.valid_num.append(hp_num)\n","        self.num_valid = len(self.valid)\n","\n","        # 예측을 위한 filter dictionary 생성\n","        self.filter_dict = self.construct_filter_dict()\n","        self.train = torch.tensor(self.train)\n","        self.train_pad = torch.tensor(self.train_pad)\n","        self.train_num = torch.tensor(self.train_num)\n","        self.train_len = torch.tensor(self.train_len)\n","\n","        # Visual Textual data 로드\n","        self.max_vis_len_ent = max_vis_len\n","        self.max_vis_len_rel = max_vis_len\n","        self.gather_vis_feature()\n","        self.gather_txt_feature()\n","\n","    # VISTA dataset.py 인용\n","    def sort_vis_features(self, item = 'entity'):\n","        if item == 'entity':\n","            vis_feats = torch.load(self.dir + 'visual_features_ent.pt')\n","        elif item == 'relation':\n","            vis_feats = torch.load(self.dir + 'visual_features_rel.pt')\n","        else:\n","            raise NotImplementedError\n","\n","        sorted_vis_feats = {}\n","        for obj in tqdm(vis_feats):\n","            if item == 'entity' and obj not in self.ent2id:\n","                continue\n","            if item == 'relation' and obj not in self.rel2id:\n","                continue\n","            num_feats = len(vis_feats[obj])\n","            sim_val = torch.zeros(num_feats).cuda()\n","            iterate = tqdm(range(num_feats)) if num_feats > 1000 else range(num_feats)\n","            cudaed_feats = vis_feats[obj].cuda()\n","            for i in iterate:\n","                sims = torch.inner(cudaed_feats[i], cudaed_feats[i:])\n","                sim_val[i:] += sims\n","                sim_val[i] += sims.sum()-torch.inner(cudaed_feats[i], cudaed_feats[i])\n","            sorted_vis_feats[obj] = vis_feats[obj][torch.argsort(sim_val, descending = True)]\n","\n","        if item == 'entity':\n","            torch.save(sorted_vis_feats, \"/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt\")\n","        else:\n","            torch.save(sorted_vis_feats, \"/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt\")\n","\n","        return sorted_vis_feats\n","\n","    # VISTA dataset.py 인용\n","    def gather_vis_feature(self):\n","        if os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt'):\n","            # self.logger.info(\"Found sorted entity visual features!\")\n","            self.ent2vis = torch.load('/content/drive/MyDrive/code/VTKG-I/visual_features_ent_sorted.pt')\n","        elif os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_ent.pt'):\n","            # self.logger.info(\"Entity visual features are not sorted! sorting...\")\n","            self.ent2vis = self.sort_vis_features(item = 'entity')\n","        else:\n","            # self.logger.info(\"Entity visual features are not found!\")\n","            self.ent2vis = {}\n","\n","        if os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt'):\n","            # self.logger.info(\"Found sorted relation visual features!\")\n","            self.rel2vis = torch.load('/content/drive/MyDrive/code/VTKG-I/visual_features_rel_sorted.pt')\n","        elif os.path.isfile('/content/drive/MyDrive/code/VTKG-I/visual_features_rel.pt'):\n","            # self.logger.info(\"Relation visual feature are not sorted! sorting...\")\n","            self.rel2vis = self.sort_vis_features(item = 'relation')\n","        else:\n","            # self.logger.info(\"Relation visual features are not found!\")\n","            self.rel2vis = {}\n","\n","        self.vis_feat_size = len(self.ent2vis[list(self.ent2vis.keys())[0]][0])\n","\n","        total_num = 0\n","        if self.max_vis_len_ent != -1:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                self.ent2vis[ent_name] = self.ent2vis[ent_name][:self.max_vis_len_ent]\n","            for rel_name in self.rel2vis:\n","                self.rel2vis[rel_name] = self.rel2vis[rel_name][:self.max_vis_len_rel]\n","        else:\n","            for ent_name in self.ent2vis:\n","                num_feats = len(self.ent2vis[ent_name])\n","                total_num += num_feats\n","                if self.max_vis_len_ent < len(self.ent2vis[ent_name]):\n","                    self.max_vis_len_ent = len(self.ent2vis[ent_name])\n","            self.max_vis_len_ent = max(self.max_vis_len_ent, 0)\n","            for rel_name in self.rel2vis:\n","                if self.max_vis_len_rel < len(self.rel2vis[rel_name]):\n","                    self.max_vis_len_rel = len(self.rel2vis[rel_name])\n","            self.max_vis_len_rel = max(self.max_vis_len_rel, 0)\n","        self.ent_vis_mask = torch.full((self.num_ent, self.max_vis_len_ent), True).cuda()\n","        self.ent_vis_matrix = torch.zeros((self.num_ent, self.max_vis_len_ent, self.vis_feat_size)).cuda()\n","        self.rel_vis_mask = torch.full((self.num_rel, self.max_vis_len_rel), True).cuda()\n","        self.rel_vis_matrix = torch.zeros((self.num_rel, self.max_vis_len_rel, 3*self.vis_feat_size)).cuda()\n","\n","\n","        for ent_name in self.ent2vis:\n","            ent_id = self.ent2id[ent_name]\n","            num_feats = len(self.ent2vis[ent_name])\n","            self.ent_vis_mask[ent_id, :num_feats] = False\n","            self.ent_vis_matrix[ent_id, :num_feats] = self.ent2vis[ent_name]\n","\n","        for rel_name in self.rel2vis:\n","            rel_id = self.rel2id[rel_name]\n","            num_feats = len(self.rel2vis[rel_name])\n","            self.rel_vis_mask[rel_id, :num_feats] = False\n","            self.rel_vis_matrix[rel_id, :num_feats] = self.rel2vis[rel_name]\n","\n","    # VISTA dataset.py 인용\n","    def gather_txt_feature(self):\n","\n","        self.ent2txt = torch.load('/content/drive/MyDrive/code/VTHNKG-NT/textual_features_ent.pt')\n","        self.rel2txt = torch.load('/content/drive/MyDrive/code/VTHNKG-NT/textual_features_rel.pt')\n","        self.txt_feat_size = len(self.ent2txt[self.id2ent[0]])\n","\n","        self.ent_txt_matrix = torch.zeros((self.num_ent, self.txt_feat_size)).cuda()\n","        self.rel_txt_matrix = torch.zeros((self.num_rel, self.txt_feat_size)).cuda()\n","\n","        for ent_name in self.ent2id:\n","            self.ent_txt_matrix[self.ent2id[ent_name]] = self.ent2txt[ent_name]\n","\n","        for rel_name in self.rel2id:\n","            self.rel_txt_matrix[self.rel2id[rel_name]] = self.rel2txt[rel_name]\n","\n","\n","    def __len__(self):\n","        return self.num_train\n","\n","    def __getitem__(self, idx):\n","        masked = self.train[idx].clone()\n","        masked_num = self.train_num[idx].clone()\n","        mask_idx = np.random.randint(self.train_len[idx])\n","\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] < self.num_ent:\n","                masked[mask_idx] = self.num_ent+self.num_rel\n","        else:\n","            masked[mask_idx] = self.num_rel\n","            if masked[mask_idx+1] >= self.num_ent:\n","                masked[mask_idx+1] = self.num_ent+self.num_rel\n","        answer = self.train[idx, mask_idx]\n","\n","        mask_locs = torch.full(((self.max_len-3)//2+1,), False)\n","        if mask_idx < 3:\n","            mask_locs[0] = True\n","        else:\n","            mask_locs[(mask_idx-3)//2+1] = True\n","\n","        mask_idx_mask = torch.full((4,), False)\n","        if mask_idx < 3:\n","            mask_idx_mask[mask_idx+1] = True\n","        else:\n","            mask_idx_mask[2-mask_idx%2] = True\n","\n","        num_idx_mask = torch.full((self.num_rel,),False)\n","        if mask_idx % 2 == 0:\n","            if self.train[idx, mask_idx] >= self.num_ent:\n","                num_idx_mask[self.train[idx,mask_idx]-self.num_ent] = True\n","                answer = self.train_num[idx, (mask_idx-1)//2]\n","                masked_num[mask_idx//2-1] = -1\n","                ent_mask = [0]\n","                num_mask = [1]\n","            else:\n","                num_mask = [0]\n","                ent_mask = [1]\n","            rel_mask = [0]\n","        else:\n","            num_mask = [0]\n","            ent_mask = [0]\n","            rel_mask = [1]\n","\n","        return masked, self.train_pad[idx], mask_locs, answer, mask_idx_mask, masked_num, torch.tensor(ent_mask), torch.tensor(rel_mask), torch.tensor(num_mask), num_idx_mask, self.train_len[idx]\n","\n","    def max_len(self):\n","        return self.max_len\n","\n","    def construct_filter_dict(self):\n","        res = {}\n","        for data, data_len, data_num in [[self.train, self.train_len, self.train_num],[self.valid, self.valid_len, self.valid_num],[self.test, self.test_len, self.test_num]]:\n","            for triplet, triplet_len, triplet_num in zip(data, data_len, data_num):\n","                real_triplet = copy.deepcopy(triplet[:triplet_len])\n","                if real_triplet[2] < self.num_ent:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[2])]\n","                else:\n","                    re_pair = [(real_triplet[0], real_triplet[1], real_triplet[1]*2 + triplet_num[0])]\n","                for idx, (q,v) in enumerate(zip(real_triplet[3::2], real_triplet[4::2])):\n","                    if v <self.num_ent:\n","                        re_pair.append((q, v))\n","                    else:\n","                        re_pair.append((q, q*2 + triplet_num[idx + 1]))\n","                for i, pair in enumerate(re_pair):\n","                    for j, anything in enumerate(pair):\n","                        filtered_filter = copy.deepcopy(re_pair)\n","                        new_pair = copy.deepcopy(list(pair))\n","                        new_pair[j] = 2*(self.num_ent+self.num_rel)\n","                        filtered_filter[i] = tuple(new_pair)\n","                        filtered_filter.sort()\n","                        try:\n","                            res[tuple(filtered_filter)].append(pair[j])\n","                        except:\n","                            res[tuple(filtered_filter)] = [pair[j]]\n","        for key in res:\n","            res[key] = np.array(res[key])\n","\n","        return res\n"]},{"cell_type":"markdown","source":["# Train.py"],"metadata":{"id":"jAAtyrlFmKaq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fRYvXkTNmgw0"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/code/VTHNKG-OA_CQI/\"\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PfJz9pIhed","executionInfo":{"status":"ok","timestamp":1747974142495,"user_tz":-540,"elapsed":1327,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"0dad7d24-57cc-4a02-ed95-be8214f38f28"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/VTHNKG-OA_CQI\n"," checkpoint\t\t\t     relation2textlong.txt\n"," entities.txt\t\t\t     relation2text.txt\n"," entity2id.txt\t\t\t     relations.txt\n"," entity2textlong.txt\t\t     result\n"," entity2text.txt\t\t     test.txt\n"," heads_with_numerics_nomalized.txt   train.txt\n","'Model based on VTHNKG-OA_CQI'\t     triplets_discrete.txt\n","'process VTHNKG-OA_CQI'\t\t     triplets.txt\n"," relation2id.txt\t\t     valid.txt\n"]}]},{"cell_type":"code","source":["# import 및 초기 세팅 (코어, 랜덤 시드, logger)\n","\n","# HyNT와 동일\n","OMP_NUM_THREADS=8\n","torch.backends.cudnn.benchmark = True\n","torch.set_num_threads(8)\n","torch.cuda.empty_cache()\n","\n","torch.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","# argument 정의\n","\"\"\"\n","data 종류\n","learning rate\n","dimension of embedding\n","number of epoch\n","validation period (epoch)\n","number of layer for entity encoder\n","number of layer for relation encoder\n","number of layer for context encoder\n","number of layer for prediction decoder\n","head number\n","hidden dimension for feedforward\n","dropout rate\n","smoothing rate\n","batch size\n","step size\n","\"\"\"\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-OA_CQI_seed42\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()\n","\n","# 모델 불러오기 및 데이터 로딩 (model.py 와 dataset.py)\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = False)\n","\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\"\"\"\n","num_ent\n","num_rel\n","num_nv\n","num_qual\n","ent_vis\n","rel_vis\n","dim_vis\n","ent_txt\n","rel_txt\n","dim_txt\n","ent_vis_mask\n","rel_vis_mask\n","dim_str\n","num_head\n","dim_hid\n","num_layer_enc_ent\n","num_layer_enc_rel\n","num_layer_prediction\n","num_layer_context\n","dropout = 0.1\n","emb_dropout = 0.6\n","vis_dropout = 0.1\n","txt_dropout = 0.1\n","max_qual = 5\n","emb_as_proj = False\n","\"\"\"\n","model = VTHN(\n","    num_ent = KG.num_ent, # 엔티티 개수\n","    num_rel = KG.num_rel, # relation 개수\n","    ## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","    ## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","    ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","    rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","    dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","    ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","    rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","    dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","    ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","    rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","    dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","    num_head = args.num_head, # multihead 개수\n","    dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","    num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","    num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","    num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","    num_layer_context = args.num_layer_context, # context transformer layer 개수\n","    dropout = args.dropout, # transformer layer의 dropout\n","    emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","    vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","    txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","    ## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","    emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","# loss function, optimizer, scheduler, logging, savepoint 정의\n","criterion = nn.CrossEntropyLoss(label_smoothing = args.smoothing)\n","mse_criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.step_size, T_mult = 2)\n","\n","file_format = f\"{args.exp}/{args.data}/lr_{args.lr}_dim_{args.dim}_\"\n","\n","\"\"\" 이 부분은 나중에 수정 필요\n","if args.emb_as_proj:\n","    file_format += \"_embproj\"\n","\"\"\"\n","os.makedirs(f\"./result/{args.exp}/{args.data}/\", exist_ok=True)\n","os.makedirs(f\"./checkpoint/{args.exp}/{args.data}/\", exist_ok=True)\n","with open(f\"./result/{file_format}.txt\", \"w\") as f:\n","    f.write(f\"{datetime.datetime.now()}\\n\")\n","\n","\n","# 학습 시작\n","\n","# epoch 반복\n","## batch마다 연산 (dataset.py에서 batch 등의 parameter 불러오는 방식 확인 필요)\n","### batch 처리 후 entity, relation, number score 계산\n","### 정답 비교 후 loss 계산\n","### loss 기반으로 backward pass, 학습\n","\n","## 특정 epoch마다 validation\n","### 모든 엔티티 (discrete, numeric)에 대해 score 및 rank 계산\n","### 모든 관계에 대해 score 및 rank 계산\n","## validation logging\n","\n","start = time.time() # 스탑워치 시작\n","print(\"EPOCH \\t TOTAL LOSS \\t ENTITY LOSS \\t RELATION LOSS \\t NUMERIC LOSS \\t TOTAL TIME\")\n","for epoch in range(args.num_epoch):\n","  total_loss = 0.0\n","  total_ent_loss = 0.0\n","  total_rel_loss = 0.0\n","  total_num_loss = 0.0\n","  for batch, batch_pad, batch_mask_locs, answers, mask_idx, batch_num, ent_mask, rel_mask, num_mask, num_idx_mask, batch_real_len in KG_DataLoader:\n","    batch_len = max(batch_real_len)\n","    batch = batch[:,:batch_len]\n","    batch_pad = batch_pad[:,:batch_len//2] ## 이렇게 할거면 max_qual이 필요 없음.\n","    batch_mask_locs = batch_mask_locs[:,:batch_len//2]\n","    batch_num = batch_num[:,:batch_len//2]\n","\n","    # 예측\n","    ent_score, rel_score, num_score = model(batch.cuda(), batch_num.cuda(), batch_pad.cuda(), batch_mask_locs.cuda())\n","    real_ent_mask = (ent_mask.cuda()!=0).squeeze()\n","    real_rel_mask = (rel_mask.cuda()!=0).squeeze()\n","    real_num_mask = (num_mask.cuda()!=0).squeeze()\n","    answer = answers.cuda()\n","    mask_idx = mask_idx.cuda()\n","\n","    # loss 계산\n","    loss = 0\n","    if torch.any(ent_mask):\n","        real_ent_mask = real_ent_mask.cuda()\n","        ent_loss = criterion(ent_score[mask_idx][real_ent_mask], answer[real_ent_mask].long())\n","        loss += ent_loss\n","        total_ent_loss += ent_loss.item()\n","\n","    if torch.any(rel_mask):\n","        real_rel_mask = real_rel_mask.cuda()\n","        rel_loss = criterion(rel_score[mask_idx][real_rel_mask], answer[real_rel_mask].long())\n","        loss += rel_loss\n","        total_rel_loss += rel_loss.item()\n","\n","    if torch.any(num_mask):\n","        real_num_mask = real_num_mask.cuda()\n","        num_loss = mse_criterion(num_score[mask_idx][num_idx_mask], answer[real_num_mask])\n","        loss += num_loss\n","        total_num_loss += num_loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","    optimizer.step()\n","    total_loss += loss.item()\n","\n","  scheduler.step()\n","  print(f\"{epoch} \\t {total_loss:.6f} \\t {total_ent_loss:.6f} \\t\" + \\\n","        f\"{total_rel_loss:.6f} \\t {total_num_loss:.6f} \\t {time.time() - start:.6f} s\")\n","\n","  # validation 진행\n","  if (epoch + 1) % args.valid_epoch == 0:\n","    model.eval()\n","\n","    lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","    lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","    rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","    rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","    nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","    nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","    nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","    nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","    with torch.no_grad():\n","        for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","            tri_len = len(tri)\n","            pad_idx = 0\n","            for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","                # 패딩 확인\n","                if tri_pad[pad_idx]:\n","                    break\n","                if ent_idx != 0:\n","                    pad_idx += 1\n","\n","                # 테스트 트리플렛\n","                test_triplet = torch.tensor([tri])\n","\n","                # 마스킹 위치 설정\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                if ent_idx < 2:\n","                    mask_locs[0,0] = True\n","                else:\n","                    mask_locs[0,ent_idx-1] = True\n","                if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                    assert ent_idx != 0\n","                    test_num = torch.tensor([tri_num])\n","                    test_num[0,ent_idx-1] = -1\n","                    # 숫자 마스킹 후 예측\n","                    _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_num = score_num.detach().cpu().numpy()\n","                    if ent_idx == 1: # triplet의 숫자\n","                        sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                        nvp_tri_se += sq_error\n","                        nvp_tri_se_num += 1\n","                    else: # qualifier\n","                        sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    nvp_all_se += sq_error\n","                    nvp_all_se_num += 1\n","                else: # 엔티티 예측\n","                    test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                    filt_tri = copy.deepcopy(tri)\n","                    filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                    if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                    else:\n","                        re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                    for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                        if tri_pad[qual_idx+1]:\n","                            break\n","                        if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                            re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                        else:\n","                            re_pair.append((q,v))\n","                    re_pair.sort()\n","                    filt = KG.filter_dict[tuple(re_pair)]\n","                    score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                    score_ent = score_ent.detach().cpu().numpy()\n","                    if ent_idx < 2:\n","                        rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                        lp_tri_list_rank.append(rank)\n","                    else:\n","                        rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    lp_all_list_rank.append(rank)\n","            for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","                if tri_pad[rel_idx]:\n","                    break\n","                mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","                mask_locs[0,rel_idx] = True\n","                test_triplet = torch.tensor([tri])\n","                orig_rels = tri[1::2]\n","                test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","                if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                    test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","                filt_tri = copy.deepcopy(tri)\n","                # 필터링 및 scoring (entity와 동일)\n","                filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","                if filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if v >= KG.num_ent:\n","                        re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_rel = score_rel.detach().cpu().numpy()\n","                if rel_idx == 0:\n","                    rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                    rp_tri_list_rank.append(rank)\n","                else:\n","                    rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                rp_all_list_rank.append(rank)\n","\n","    lp_tri_list_rank = np.array(lp_tri_list_rank)\n","    lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","    print(\"Link Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {lp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","    lp_all_list_rank = np.array(lp_all_list_rank)\n","    lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","    print(\"Link Prediction on Validation Set (All)\")\n","    print(f\"MRR: {lp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","    rp_tri_list_rank = np.array(rp_tri_list_rank)\n","    rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","    print(\"Relation Prediction on Validation Set (Tri)\")\n","    print(f\"MRR: {rp_tri_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","    rp_all_list_rank = np.array(rp_all_list_rank)\n","    rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","    print(\"Relation Prediction on Validation Set (All)\")\n","    print(f\"MRR: {rp_all_mrr:.4f}\")\n","    print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","    print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","    print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","    if nvp_tri_se_num > 0:\n","        nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","        print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","    if nvp_all_se_num > 0:\n","        nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","        print(\"Numeric Value Prediction on Validation Set (All)\")\n","        print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","\n","    with open(f\"./result/{file_format}.txt\", 'a') as f:\n","        f.write(f\"Epoch: {epoch+1}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (Tri): {lp_tri_mrr:.4f} {lp_tri_hit10:.4f} {lp_tri_hit3:.4f} {lp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Link Prediction on Validation Set (All): {lp_all_mrr:.4f} {lp_all_hit10:.4f} {lp_all_hit3:.4f} {lp_all_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (Tri): {rp_tri_mrr:.4f} {rp_tri_hit10:.4f} {rp_tri_hit3:.4f} {rp_tri_hit1:.4f}\\n\")\n","        f.write(f\"Relation Prediction on Validation Set (All): {rp_all_mrr:.4f} {rp_all_hit10:.4f} {rp_all_hit3:.4f} {rp_all_hit1:.4f}\\n\")\n","        if nvp_tri_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (Tri): {nvp_tri_rmse:.4f}\\n\")\n","        if nvp_all_se_num > 0:\n","            f.write(f\"Numeric Value Prediction on Validation Set (All): {nvp_all_rmse:.4f}\\n\")\n","\n","\n","    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()},\n","                f\"./checkpoint/{file_format}_{epoch+1}.ckpt\")\n","\n","    model.train()\n"],"metadata":{"id":"1bX-xxnbmPYo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747894777793,"user_tz":-540,"elapsed":974090,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"a616ef73-931f-442f-edeb-2c89e16f9101","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH \t TOTAL LOSS \t ENTITY LOSS \t RELATION LOSS \t NUMERIC LOSS \t TOTAL TIME\n","0 \t 41.222980 \t 11.551749 \t10.817339 \t 18.853893 \t 2.052648 s\n","1 \t 26.322887 \t 11.022244 \t10.421609 \t 4.879034 \t 2.593946 s\n","2 \t 23.735950 \t 10.866050 \t9.759164 \t 3.110737 \t 3.119219 s\n","3 \t 21.060219 \t 10.069355 \t9.982398 \t 1.008465 \t 3.639099 s\n","4 \t 20.339911 \t 9.752483 \t9.715204 \t 0.872224 \t 4.156296 s\n","5 \t 20.922766 \t 10.723554 \t9.499174 \t 0.700039 \t 4.677640 s\n","6 \t 20.520827 \t 10.446392 \t9.391305 \t 0.683131 \t 5.334985 s\n","7 \t 19.539578 \t 9.572998 \t9.741616 \t 0.224966 \t 5.962856 s\n","8 \t 19.438196 \t 10.183897 \t8.617369 \t 0.636931 \t 6.609913 s\n","9 \t 18.726180 \t 9.855284 \t8.640955 \t 0.229941 \t 7.449911 s\n","10 \t 19.416615 \t 10.102803 \t8.929983 \t 0.383829 \t 8.085769 s\n","11 \t 18.873573 \t 9.757402 \t8.981733 \t 0.134438 \t 8.610039 s\n","12 \t 19.345367 \t 9.877310 \t8.915667 \t 0.552391 \t 9.122297 s\n","13 \t 20.254802 \t 10.289768 \t9.294361 \t 0.670672 \t 9.636376 s\n","14 \t 19.370963 \t 10.288829 \t8.761775 \t 0.320359 \t 10.151045 s\n","15 \t 18.709954 \t 9.585553 \t8.802737 \t 0.321666 \t 10.676353 s\n","16 \t 19.065439 \t 10.040929 \t8.719122 \t 0.305389 \t 11.194770 s\n","17 \t 18.483597 \t 9.661778 \t8.599916 \t 0.221901 \t 11.934520 s\n","18 \t 18.755472 \t 9.778174 \t8.611629 \t 0.365669 \t 12.441406 s\n","19 \t 19.451276 \t 9.953639 \t9.158949 \t 0.338689 \t 12.952672 s\n","20 \t 19.096225 \t 9.730058 \t9.101817 \t 0.264350 \t 13.465660 s\n","21 \t 19.160596 \t 10.134876 \t8.850310 \t 0.175409 \t 13.979081 s\n","22 \t 17.938859 \t 9.339469 \t8.441919 \t 0.157470 \t 14.619287 s\n","23 \t 18.889858 \t 9.953860 \t8.704589 \t 0.231411 \t 15.135625 s\n","24 \t 18.762182 \t 9.875571 \t8.690212 \t 0.196399 \t 15.647273 s\n","25 \t 18.051808 \t 9.717146 \t8.033778 \t 0.300884 \t 16.181211 s\n","26 \t 18.629699 \t 9.861922 \t8.538489 \t 0.229287 \t 16.697105 s\n","27 \t 19.096156 \t 9.668423 \t9.224514 \t 0.203220 \t 17.220688 s\n","28 \t 18.986156 \t 10.152334 \t8.597115 \t 0.236707 \t 17.730696 s\n","29 \t 18.663728 \t 9.566158 \t8.881209 \t 0.216359 \t 18.312840 s\n","30 \t 19.279816 \t 9.969881 \t9.149058 \t 0.160876 \t 18.944489 s\n","31 \t 18.967694 \t 10.162384 \t8.469620 \t 0.335691 \t 19.729572 s\n","32 \t 19.059298 \t 9.931130 \t8.948666 \t 0.179500 \t 20.368415 s\n","33 \t 18.625240 \t 10.052023 \t8.386715 \t 0.186501 \t 20.988030 s\n","34 \t 19.038068 \t 10.076797 \t8.838836 \t 0.122434 \t 21.522580 s\n","35 \t 18.917136 \t 9.665730 \t9.091478 \t 0.159928 \t 22.032208 s\n","36 \t 18.470063 \t 9.378238 \t8.761543 \t 0.330282 \t 22.558256 s\n","37 \t 18.909584 \t 9.966208 \t8.690470 \t 0.252907 \t 23.070257 s\n","38 \t 17.986321 \t 9.453469 \t8.394722 \t 0.138131 \t 23.611372 s\n","39 \t 18.567925 \t 10.078905 \t8.221761 \t 0.267259 \t 24.131534 s\n","40 \t 18.717743 \t 9.851763 \t8.725294 \t 0.140686 \t 24.777637 s\n","41 \t 18.383393 \t 9.851343 \t8.389434 \t 0.142617 \t 25.322748 s\n","42 \t 18.904401 \t 9.921165 \t8.786084 \t 0.197152 \t 25.838719 s\n","43 \t 18.991506 \t 10.067339 \t8.639102 \t 0.285065 \t 26.356840 s\n","44 \t 18.349400 \t 9.489433 \t8.551411 \t 0.308555 \t 26.878213 s\n","45 \t 18.542162 \t 9.484448 \t8.785970 \t 0.271744 \t 27.392722 s\n","46 \t 19.007196 \t 9.860214 \t8.818233 \t 0.328749 \t 27.909622 s\n","47 \t 18.444120 \t 9.840181 \t8.495475 \t 0.108464 \t 28.432030 s\n","48 \t 18.279456 \t 9.605861 \t8.435712 \t 0.237882 \t 28.952863 s\n","49 \t 18.979355 \t 10.020428 \t8.803450 \t 0.155475 \t 29.465107 s\n","50 \t 17.295761 \t 9.323882 \t7.816607 \t 0.155272 \t 30.118067 s\n","51 \t 18.300087 \t 9.491573 \t8.675743 \t 0.132771 \t 30.634614 s\n","52 \t 17.856206 \t 9.367300 \t8.176991 \t 0.311915 \t 31.201291 s\n","53 \t 17.727981 \t 9.352760 \t8.257365 \t 0.117856 \t 31.829525 s\n","54 \t 17.900336 \t 9.360653 \t8.386952 \t 0.152729 \t 32.442457 s\n","55 \t 18.093859 \t 9.885489 \t8.083277 \t 0.125091 \t 33.082705 s\n","56 \t 18.891427 \t 9.796455 \t8.828629 \t 0.266342 \t 33.799546 s\n","57 \t 17.710865 \t 9.379131 \t8.195645 \t 0.136090 \t 34.347552 s\n","58 \t 18.040064 \t 9.106416 \t8.764231 \t 0.169417 \t 34.866243 s\n","59 \t 18.402924 \t 9.498286 \t8.729278 \t 0.175359 \t 35.383779 s\n","60 \t 18.243308 \t 9.600320 \t8.404802 \t 0.238185 \t 35.901799 s\n","61 \t 18.483472 \t 9.713486 \t8.517410 \t 0.252576 \t 36.545815 s\n","62 \t 18.338376 \t 10.081940 \t8.094404 \t 0.162031 \t 37.074907 s\n","63 \t 17.838910 \t 9.055956 \t8.491301 \t 0.291653 \t 37.592742 s\n","64 \t 18.148792 \t 9.874948 \t8.141684 \t 0.132160 \t 38.130108 s\n","65 \t 18.390584 \t 9.814258 \t8.364497 \t 0.211830 \t 38.653477 s\n","66 \t 18.498919 \t 9.726538 \t8.523797 \t 0.248584 \t 39.193685 s\n","67 \t 18.422872 \t 9.769547 \t8.460438 \t 0.192887 \t 39.701299 s\n","68 \t 18.311434 \t 9.952481 \t8.217950 \t 0.141004 \t 40.230690 s\n","69 \t 17.664356 \t 9.129695 \t8.203480 \t 0.331182 \t 40.750607 s\n","70 \t 18.840678 \t 9.982298 \t8.638326 \t 0.220055 \t 41.279522 s\n","71 \t 17.815358 \t 9.396136 \t8.247177 \t 0.172045 \t 41.799121 s\n","72 \t 18.237216 \t 9.765006 \t8.330640 \t 0.141571 \t 42.326285 s\n","73 \t 18.427892 \t 10.098902 \t8.187250 \t 0.141740 \t 42.983375 s\n","74 \t 17.642242 \t 9.209802 \t8.219463 \t 0.212976 \t 43.504473 s\n","75 \t 18.273476 \t 9.837681 \t8.328613 \t 0.107181 \t 44.091001 s\n","76 \t 19.011820 \t 9.751875 \t9.046561 \t 0.213385 \t 44.715028 s\n","77 \t 17.945963 \t 9.325085 \t8.517485 \t 0.103393 \t 45.354834 s\n","78 \t 18.392951 \t 9.733455 \t8.518020 \t 0.141476 \t 45.965053 s\n","79 \t 18.352086 \t 9.983771 \t8.274866 \t 0.093449 \t 46.648848 s\n","80 \t 17.793140 \t 9.581701 \t8.122572 \t 0.088867 \t 47.266237 s\n","81 \t 17.916521 \t 9.220551 \t8.592803 \t 0.103169 \t 47.783474 s\n","82 \t 18.174330 \t 9.676574 \t8.343042 \t 0.154714 \t 48.308213 s\n","83 \t 18.100583 \t 9.429635 \t8.524982 \t 0.145967 \t 48.825831 s\n","84 \t 16.833109 \t 8.344972 \t8.417122 \t 0.071016 \t 49.351269 s\n","85 \t 17.757723 \t 9.508498 \t8.106819 \t 0.142406 \t 50.019428 s\n","86 \t 18.167652 \t 9.201292 \t8.858448 \t 0.107913 \t 50.543266 s\n","87 \t 18.269302 \t 9.579492 \t8.577281 \t 0.112529 \t 51.058472 s\n","88 \t 18.288830 \t 9.836808 \t8.187309 \t 0.264712 \t 51.586332 s\n","89 \t 17.677887 \t 9.599649 \t7.935932 \t 0.142305 \t 52.101794 s\n","90 \t 18.720041 \t 9.732002 \t8.612757 \t 0.375282 \t 52.628167 s\n","91 \t 17.602969 \t 8.983246 \t8.495470 \t 0.124254 \t 53.149675 s\n","92 \t 17.975446 \t 9.724708 \t8.158062 \t 0.092677 \t 53.698470 s\n","93 \t 17.647730 \t 9.191578 \t8.360584 \t 0.095567 \t 54.229009 s\n","94 \t 17.785706 \t 9.312080 \t8.366879 \t 0.106746 \t 54.766137 s\n","95 \t 17.866516 \t 9.446411 \t8.301930 \t 0.118175 \t 55.292122 s\n","96 \t 17.109357 \t 8.890374 \t8.131201 \t 0.087781 \t 55.831847 s\n","97 \t 18.138309 \t 9.299776 \t8.743812 \t 0.094723 \t 56.346750 s\n","98 \t 18.171506 \t 9.550580 \t8.541429 \t 0.079496 \t 57.052935 s\n","99 \t 17.147587 \t 9.083350 \t7.974072 \t 0.090166 \t 57.697808 s\n","100 \t 17.417489 \t 8.921074 \t8.390963 \t 0.105452 \t 58.316130 s\n","101 \t 18.278254 \t 9.869498 \t8.311893 \t 0.096863 \t 58.932798 s\n","102 \t 18.192235 \t 9.800855 \t8.265458 \t 0.125922 \t 59.626225 s\n","103 \t 17.830054 \t 9.762610 \t7.916533 \t 0.150911 \t 60.256433 s\n","104 \t 17.817924 \t 9.147217 \t8.455926 \t 0.214780 \t 60.786352 s\n","105 \t 18.174434 \t 9.593675 \t8.467265 \t 0.113494 \t 61.320417 s\n","106 \t 17.562096 \t 9.083001 \t8.387313 \t 0.091782 \t 61.975814 s\n","107 \t 17.616749 \t 9.637766 \t7.858100 \t 0.120883 \t 62.505318 s\n","108 \t 17.384963 \t 9.507685 \t7.735869 \t 0.141408 \t 63.029488 s\n","109 \t 17.256916 \t 9.021479 \t8.060608 \t 0.174829 \t 63.560528 s\n","110 \t 18.041203 \t 9.511857 \t8.201786 \t 0.327561 \t 64.084980 s\n","111 \t 17.942441 \t 9.631454 \t8.210527 \t 0.100459 \t 64.614568 s\n","112 \t 17.381865 \t 9.534990 \t7.583106 \t 0.263769 \t 65.142241 s\n","113 \t 17.997655 \t 9.589082 \t8.318470 \t 0.090103 \t 65.662462 s\n","114 \t 17.062878 \t 9.409600 \t7.563226 \t 0.090052 \t 66.198238 s\n","115 \t 17.782516 \t 9.174433 \t8.526359 \t 0.081723 \t 66.716214 s\n","116 \t 17.513594 \t 9.531801 \t7.877769 \t 0.104025 \t 67.379074 s\n","117 \t 18.071646 \t 9.285564 \t8.557012 \t 0.229069 \t 67.903234 s\n","118 \t 17.249529 \t 9.204578 \t7.910732 \t 0.134218 \t 68.426789 s\n","119 \t 17.318913 \t 9.063972 \t8.145640 \t 0.109301 \t 68.961844 s\n","120 \t 17.595079 \t 9.390968 \t8.109494 \t 0.094617 \t 69.759046 s\n","121 \t 17.866801 \t 9.175767 \t8.486012 \t 0.205022 \t 70.328552 s\n","122 \t 17.893399 \t 9.284593 \t8.511989 \t 0.096817 \t 70.938949 s\n","123 \t 17.992694 \t 9.683528 \t8.226735 \t 0.082432 \t 71.609617 s\n","124 \t 17.542043 \t 9.473745 \t7.971016 \t 0.097281 \t 72.270961 s\n","125 \t 17.359241 \t 9.210032 \t7.991235 \t 0.157973 \t 72.961454 s\n","126 \t 17.914742 \t 9.355939 \t8.451187 \t 0.107617 \t 73.497310 s\n","127 \t 18.012837 \t 9.828665 \t8.026914 \t 0.157259 \t 74.167445 s\n","128 \t 17.876304 \t 9.307316 \t8.478981 \t 0.090006 \t 74.712345 s\n","129 \t 17.656151 \t 9.406096 \t8.163307 \t 0.086748 \t 75.242066 s\n","130 \t 17.737745 \t 9.687397 \t7.978756 \t 0.071593 \t 75.761861 s\n","131 \t 17.467219 \t 9.187406 \t8.197888 \t 0.081926 \t 76.288333 s\n","132 \t 18.162075 \t 9.594215 \t8.467363 \t 0.100497 \t 76.804356 s\n","133 \t 17.110359 \t 8.792575 \t8.145138 \t 0.172647 \t 77.339165 s\n","134 \t 18.722727 \t 9.930352 \t8.652211 \t 0.140163 \t 77.864616 s\n","135 \t 17.756248 \t 9.420483 \t8.224096 \t 0.111670 \t 78.403606 s\n","136 \t 17.944449 \t 9.611305 \t8.163531 \t 0.169614 \t 78.928093 s\n","137 \t 18.212825 \t 9.630178 \t8.459535 \t 0.123112 \t 79.462543 s\n","138 \t 17.106734 \t 8.889939 \t8.142005 \t 0.074790 \t 80.121466 s\n","139 \t 17.556579 \t 9.647489 \t7.844378 \t 0.064712 \t 80.643718 s\n","140 \t 17.087413 \t 8.689946 \t8.272094 \t 0.125374 \t 81.176523 s\n","141 \t 17.180815 \t 9.287686 \t7.806152 \t 0.086976 \t 81.699586 s\n","142 \t 18.414026 \t 9.568519 \t8.766641 \t 0.078867 \t 82.222814 s\n","143 \t 18.021333 \t 9.288567 \t8.574872 \t 0.157894 \t 82.747609 s\n","144 \t 17.312229 \t 9.333349 \t7.870336 \t 0.108543 \t 83.309958 s\n","145 \t 17.882619 \t 9.528762 \t8.289014 \t 0.064842 \t 83.942528 s\n","146 \t 17.815608 \t 9.340194 \t8.391072 \t 0.084341 \t 84.546850 s\n","147 \t 18.026847 \t 9.402916 \t8.285356 \t 0.338576 \t 85.236792 s\n","148 \t 18.542363 \t 9.955836 \t8.530880 \t 0.055647 \t 85.934005 s\n","149 \t 17.565315 \t 9.636043 \t7.853203 \t 0.076070 \t 86.482477 s\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/130 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n","  output = torch._nested_tensor_from_mask(\n","100%|██████████| 130/130 [00:46<00:00,  2.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3473\n","Hit@10: 0.4577\n","Hit@3: 0.3500\n","Hit@1: 0.2923\n","Link Prediction on Validation Set (All)\n","MRR: 0.2592\n","Hit@10: 0.4374\n","Hit@3: 0.2705\n","Hit@1: 0.1736\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2876\n","Hit@10: 0.4692\n","Hit@3: 0.2923\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5183\n","Hit@10: 0.7808\n","Hit@3: 0.6621\n","Hit@1: 0.3605\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1978\n","150 \t 17.976387 \t 9.520330 \t8.286618 \t 0.169440 \t 134.145523 s\n","151 \t 17.625053 \t 8.994138 \t8.377780 \t 0.253136 \t 134.701745 s\n","152 \t 17.411779 \t 9.182799 \t7.951858 \t 0.277121 \t 135.232342 s\n","153 \t 18.338652 \t 9.974220 \t8.012492 \t 0.351939 \t 135.804510 s\n","154 \t 17.895898 \t 9.150344 \t8.501173 \t 0.244381 \t 136.429168 s\n","155 \t 18.130438 \t 9.663657 \t8.162728 \t 0.304052 \t 137.054721 s\n","156 \t 17.689922 \t 9.874381 \t7.629272 \t 0.186271 \t 137.726178 s\n","157 \t 17.109807 \t 9.218726 \t7.666986 \t 0.224095 \t 138.415468 s\n","158 \t 18.042686 \t 9.747452 \t8.166149 \t 0.129086 \t 139.087504 s\n","159 \t 17.938672 \t 9.384591 \t8.315346 \t 0.238735 \t 139.609846 s\n","160 \t 17.181956 \t 9.065635 \t8.016143 \t 0.100179 \t 140.220658 s\n","161 \t 18.026937 \t 9.986248 \t7.957665 \t 0.083025 \t 140.837245 s\n","162 \t 18.208029 \t 9.628233 \t8.483512 \t 0.096285 \t 141.567279 s\n","163 \t 18.404698 \t 9.724878 \t8.416968 \t 0.262850 \t 142.211971 s\n","164 \t 18.234290 \t 9.838156 \t8.293001 \t 0.103133 \t 142.894628 s\n","165 \t 18.046256 \t 9.587962 \t8.246712 \t 0.211581 \t 143.486718 s\n","166 \t 17.750465 \t 9.488624 \t8.175313 \t 0.086529 \t 144.140633 s\n","167 \t 18.242373 \t 9.784890 \t8.333757 \t 0.123726 \t 144.658478 s\n","168 \t 18.141882 \t 9.858348 \t8.007715 \t 0.275818 \t 145.188020 s\n","169 \t 17.475561 \t 9.467058 \t7.911445 \t 0.097058 \t 145.727784 s\n","170 \t 17.642638 \t 9.561817 \t7.861537 \t 0.219285 \t 146.251717 s\n","171 \t 17.931316 \t 9.580064 \t8.146554 \t 0.204698 \t 146.775366 s\n","172 \t 18.310202 \t 9.450054 \t8.642238 \t 0.217910 \t 147.301684 s\n","173 \t 17.435235 \t 9.111559 \t8.078946 \t 0.244730 \t 147.824769 s\n","174 \t 17.389256 \t 9.257466 \t8.013694 \t 0.118096 \t 148.354610 s\n","175 \t 18.328051 \t 9.518584 \t8.541455 \t 0.268012 \t 148.943161 s\n","176 \t 18.756313 \t 10.086837 \t8.538161 \t 0.131315 \t 149.728766 s\n","177 \t 16.678995 \t 8.363861 \t8.170656 \t 0.144477 \t 150.351846 s\n","178 \t 18.129450 \t 9.514153 \t8.441423 \t 0.173874 \t 150.986342 s\n","179 \t 16.945881 \t 8.588684 \t8.250166 \t 0.107030 \t 151.626555 s\n","180 \t 17.150567 \t 8.857614 \t8.099457 \t 0.193496 \t 152.149958 s\n","181 \t 18.359730 \t 9.801031 \t8.370675 \t 0.188024 \t 152.674841 s\n","182 \t 18.328420 \t 9.977950 \t8.191094 \t 0.159374 \t 153.202351 s\n","183 \t 17.326819 \t 9.516505 \t7.737169 \t 0.073146 \t 153.725854 s\n","184 \t 18.186841 \t 9.582696 \t8.491199 \t 0.112947 \t 154.253706 s\n","185 \t 17.997453 \t 9.853909 \t8.066586 \t 0.076957 \t 154.798316 s\n","186 \t 18.161917 \t 9.746960 \t8.287545 \t 0.127412 \t 155.455899 s\n","187 \t 17.668444 \t 9.152155 \t8.442780 \t 0.073509 \t 155.985297 s\n","188 \t 18.489004 \t 9.752103 \t8.650342 \t 0.086559 \t 156.521491 s\n","189 \t 17.714576 \t 9.410624 \t8.161360 \t 0.142593 \t 157.038720 s\n","190 \t 17.096584 \t 9.093849 \t7.851802 \t 0.150934 \t 157.572361 s\n","191 \t 17.754973 \t 9.460754 \t8.221187 \t 0.073032 \t 158.091552 s\n","192 \t 17.386560 \t 9.297577 \t7.893317 \t 0.195666 \t 158.627383 s\n","193 \t 17.476445 \t 9.113148 \t8.273931 \t 0.089366 \t 159.147711 s\n","194 \t 17.331190 \t 9.283258 \t7.945184 \t 0.102748 \t 159.698370 s\n","195 \t 18.229253 \t 9.396899 \t8.539551 \t 0.292803 \t 160.218273 s\n","196 \t 16.761676 \t 8.833957 \t7.820383 \t 0.107336 \t 160.750949 s\n","197 \t 17.620328 \t 9.446146 \t8.079535 \t 0.094648 \t 161.272510 s\n","198 \t 17.137953 \t 8.930748 \t8.080098 \t 0.127106 \t 162.004666 s\n","199 \t 17.652637 \t 9.504940 \t8.057799 \t 0.089900 \t 162.622147 s\n","200 \t 17.033233 \t 8.825210 \t8.124018 \t 0.084004 \t 163.255558 s\n","201 \t 17.796033 \t 9.076147 \t8.611504 \t 0.108381 \t 163.897548 s\n","202 \t 17.342391 \t 8.791104 \t8.462565 \t 0.088722 \t 164.530980 s\n","203 \t 17.278417 \t 9.335220 \t7.843194 \t 0.100003 \t 165.058902 s\n","204 \t 17.397770 \t 9.086136 \t8.193785 \t 0.117848 \t 165.588634 s\n","205 \t 17.619103 \t 9.272634 \t8.257343 \t 0.089126 \t 166.115808 s\n","206 \t 17.405726 \t 9.148623 \t8.150301 \t 0.106803 \t 166.636352 s\n","207 \t 16.801187 \t 8.790744 \t7.933921 \t 0.076522 \t 167.166323 s\n","208 \t 17.578606 \t 9.087240 \t8.394523 \t 0.096843 \t 167.687623 s\n","209 \t 17.544865 \t 9.571743 \t7.915239 \t 0.057882 \t 168.217733 s\n","210 \t 17.792861 \t 9.572067 \t8.007807 \t 0.212987 \t 168.740161 s\n","211 \t 17.607255 \t 9.555637 \t7.877288 \t 0.174329 \t 169.404968 s\n","212 \t 17.735545 \t 9.461396 \t8.036643 \t 0.237507 \t 169.917251 s\n","213 \t 17.499457 \t 9.324782 \t8.022388 \t 0.152288 \t 170.458265 s\n","214 \t 16.732873 \t 8.928594 \t7.679389 \t 0.124889 \t 170.984515 s\n","215 \t 17.399333 \t 8.875027 \t8.417797 \t 0.106509 \t 171.532732 s\n","216 \t 17.398325 \t 9.042308 \t8.184739 \t 0.171278 \t 172.068354 s\n","217 \t 18.265833 \t 9.467787 \t8.615918 \t 0.182127 \t 172.591352 s\n","218 \t 17.739069 \t 9.344977 \t8.297753 \t 0.096339 \t 173.124454 s\n","219 \t 17.654867 \t 9.314645 \t8.218654 \t 0.121568 \t 173.650460 s\n","220 \t 17.137116 \t 9.095321 \t7.831647 \t 0.210149 \t 174.304787 s\n","221 \t 16.843270 \t 8.665414 \t8.009648 \t 0.168209 \t 174.882208 s\n","222 \t 16.737084 \t 8.567163 \t7.979164 \t 0.190759 \t 175.504465 s\n","223 \t 17.856478 \t 9.674700 \t8.056095 \t 0.125683 \t 176.133920 s\n","224 \t 17.374146 \t 9.592514 \t7.670847 \t 0.110786 \t 176.778550 s\n","225 \t 18.635337 \t 9.610977 \t8.943930 \t 0.080430 \t 177.460860 s\n","226 \t 17.639583 \t 9.636518 \t7.832355 \t 0.170711 \t 177.982823 s\n","227 \t 17.670021 \t 9.724646 \t7.855515 \t 0.089861 \t 178.508090 s\n","228 \t 17.417871 \t 9.383766 \t7.953588 \t 0.080518 \t 179.031669 s\n","229 \t 17.225700 \t 9.073650 \t8.091331 \t 0.060720 \t 179.700585 s\n","230 \t 18.264878 \t 9.920250 \t8.280944 \t 0.063685 \t 180.228918 s\n","231 \t 17.783139 \t 9.547693 \t8.085602 \t 0.149844 \t 180.774608 s\n","232 \t 17.252897 \t 9.159496 \t8.031852 \t 0.061550 \t 181.302986 s\n","233 \t 17.092373 \t 8.978227 \t8.047111 \t 0.067035 \t 181.840611 s\n","234 \t 17.751328 \t 9.359133 \t8.311327 \t 0.080869 \t 182.365793 s\n","235 \t 17.215154 \t 8.981233 \t8.139683 \t 0.094239 \t 182.910989 s\n","236 \t 17.339741 \t 8.972985 \t8.296117 \t 0.070638 \t 183.436808 s\n","237 \t 17.468650 \t 9.294908 \t7.968758 \t 0.204984 \t 183.965750 s\n","238 \t 18.414357 \t 9.938588 \t8.383665 \t 0.092105 \t 184.483802 s\n","239 \t 17.304973 \t 9.262217 \t7.834793 \t 0.207963 \t 185.139741 s\n","240 \t 16.915116 \t 9.033621 \t7.797929 \t 0.083568 \t 185.669632 s\n","241 \t 17.283010 \t 9.319654 \t7.877625 \t 0.085732 \t 186.191313 s\n","242 \t 17.768844 \t 9.516150 \t8.167435 \t 0.085259 \t 186.713778 s\n","243 \t 17.988919 \t 9.672776 \t8.089380 \t 0.226764 \t 187.238268 s\n","244 \t 17.137750 \t 8.905071 \t8.148539 \t 0.084140 \t 187.813320 s\n","245 \t 17.442996 \t 8.849181 \t8.517908 \t 0.075906 \t 188.434332 s\n","246 \t 18.013956 \t 9.481663 \t8.456602 \t 0.075691 \t 189.047273 s\n","247 \t 17.843689 \t 9.291583 \t8.432054 \t 0.120053 \t 189.690105 s\n","248 \t 16.877598 \t 9.081689 \t7.726955 \t 0.068953 \t 190.401800 s\n","249 \t 17.268173 \t 9.214256 \t7.850016 \t 0.203901 \t 191.052992 s\n","250 \t 17.422263 \t 9.564052 \t7.758027 \t 0.100183 \t 191.576413 s\n","251 \t 17.343616 \t 9.058329 \t8.172461 \t 0.112825 \t 192.104647 s\n","252 \t 17.697178 \t 9.287244 \t8.353604 \t 0.056330 \t 192.627184 s\n","253 \t 16.768023 \t 8.767966 \t7.894380 \t 0.105678 \t 193.156520 s\n","254 \t 17.735831 \t 9.410277 \t8.212450 \t 0.113103 \t 193.677574 s\n","255 \t 16.486380 \t 8.862561 \t7.546765 \t 0.077054 \t 194.210076 s\n","256 \t 16.968976 \t 9.043136 \t7.861941 \t 0.063900 \t 194.734611 s\n","257 \t 17.523277 \t 9.099961 \t8.349004 \t 0.074313 \t 195.264594 s\n","258 \t 17.469584 \t 9.657825 \t7.745673 \t 0.066086 \t 195.783587 s\n","259 \t 17.351500 \t 9.200868 \t7.915344 \t 0.235289 \t 196.307383 s\n","260 \t 17.197720 \t 9.016156 \t8.101852 \t 0.079710 \t 196.832276 s\n","261 \t 17.429202 \t 9.168574 \t8.224338 \t 0.036290 \t 197.500618 s\n","262 \t 16.355370 \t 8.470963 \t7.817226 \t 0.067180 \t 198.025984 s\n","263 \t 16.590858 \t 8.691174 \t7.841207 \t 0.058477 \t 198.550536 s\n","264 \t 18.195402 \t 9.771020 \t8.347751 \t 0.076630 \t 199.083635 s\n","265 \t 17.723126 \t 9.610650 \t8.065569 \t 0.046907 \t 199.607242 s\n","266 \t 17.602349 \t 9.495470 \t8.007027 \t 0.099853 \t 200.140580 s\n","267 \t 17.118567 \t 8.944083 \t8.140489 \t 0.033995 \t 200.709792 s\n","268 \t 17.321563 \t 9.305888 \t7.948805 \t 0.066871 \t 201.348086 s\n","269 \t 16.610840 \t 8.797372 \t7.730181 \t 0.083288 \t 201.958264 s\n","270 \t 16.983867 \t 9.262710 \t7.639910 \t 0.081248 \t 202.586162 s\n","271 \t 17.296109 \t 9.342486 \t7.882843 \t 0.070778 \t 203.270923 s\n","272 \t 17.141997 \t 9.300107 \t7.765321 \t 0.076570 \t 203.863867 s\n","273 \t 17.229032 \t 9.247932 \t7.943792 \t 0.037307 \t 204.399363 s\n","274 \t 16.879928 \t 9.150350 \t7.676160 \t 0.053418 \t 205.048209 s\n","275 \t 16.656079 \t 9.201407 \t7.402273 \t 0.052398 \t 205.577001 s\n","276 \t 16.984005 \t 9.080357 \t7.862090 \t 0.041558 \t 206.124040 s\n","277 \t 17.300329 \t 9.795380 \t7.449403 \t 0.055545 \t 206.646309 s\n","278 \t 16.967058 \t 9.313127 \t7.536036 \t 0.117896 \t 207.173577 s\n","279 \t 18.087361 \t 9.736958 \t8.296628 \t 0.053775 \t 207.696563 s\n","280 \t 17.258743 \t 9.209094 \t7.969211 \t 0.080438 \t 208.224892 s\n","281 \t 17.265518 \t 9.267480 \t7.946970 \t 0.051068 \t 208.750662 s\n","282 \t 17.470016 \t 9.387703 \t8.018557 \t 0.063756 \t 209.274768 s\n","283 \t 16.611567 \t 8.913024 \t7.639041 \t 0.059502 \t 209.940253 s\n","284 \t 16.879404 \t 8.860168 \t7.956975 \t 0.062260 \t 210.464214 s\n","285 \t 17.401853 \t 9.396616 \t7.947774 \t 0.057461 \t 211.010784 s\n","286 \t 16.883485 \t 9.048406 \t7.797799 \t 0.037278 \t 211.540189 s\n","287 \t 16.977794 \t 9.285183 \t7.647474 \t 0.045136 \t 212.079975 s\n","288 \t 16.614127 \t 8.531744 \t8.024868 \t 0.057516 \t 212.618954 s\n","289 \t 17.125400 \t 9.685523 \t7.407103 \t 0.032775 \t 213.147575 s\n","290 \t 16.789832 \t 8.886921 \t7.857546 \t 0.045366 \t 213.726894 s\n","291 \t 16.344633 \t 8.758604 \t7.541412 \t 0.044617 \t 214.356773 s\n","292 \t 17.036821 \t 9.214921 \t7.777997 \t 0.043905 \t 215.138570 s\n","293 \t 16.455676 \t 9.137429 \t7.258558 \t 0.059689 \t 215.783467 s\n","294 \t 16.266363 \t 8.417485 \t7.787920 \t 0.060958 \t 216.445171 s\n","295 \t 16.682080 \t 8.738885 \t7.907402 \t 0.035793 \t 216.976509 s\n","296 \t 16.823031 \t 8.513860 \t8.223555 \t 0.085617 \t 217.512162 s\n","297 \t 17.048997 \t 8.886291 \t8.108059 \t 0.054648 \t 218.036686 s\n","298 \t 17.117546 \t 9.062503 \t7.997096 \t 0.057946 \t 218.563011 s\n","299 \t 17.869620 \t 9.647980 \t8.113161 \t 0.108480 \t 219.086297 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:46<00:00,  2.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.3718\n","Hit@10: 0.4846\n","Hit@3: 0.3615\n","Hit@1: 0.3115\n","Link Prediction on Validation Set (All)\n","MRR: 0.3203\n","Hit@10: 0.5142\n","Hit@3: 0.3489\n","Hit@1: 0.2237\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.2934\n","Hit@10: 0.5000\n","Hit@3: 0.2923\n","Hit@1: 0.2077\n","Relation Prediction on Validation Set (All)\n","MRR: 0.5940\n","Hit@10: 0.8034\n","Hit@3: 0.6870\n","Hit@1: 0.4599\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1164\n","300 \t 16.815839 \t 8.912302 \t7.798446 \t 0.105090 \t 267.652616 s\n","301 \t 16.692167 \t 8.937312 \t7.707474 \t 0.047382 \t 268.462759 s\n","302 \t 16.876396 \t 8.895175 \t7.895009 \t 0.086214 \t 269.130281 s\n","303 \t 16.844843 \t 8.697452 \t8.091608 \t 0.055782 \t 269.661552 s\n","304 \t 16.678251 \t 8.871373 \t7.727191 \t 0.079688 \t 270.185467 s\n","305 \t 15.717453 \t 8.395364 \t7.266850 \t 0.055240 \t 270.713461 s\n","306 \t 16.670712 \t 8.676876 \t7.951518 \t 0.042319 \t 271.239226 s\n","307 \t 16.222714 \t 8.700777 \t7.466594 \t 0.055343 \t 271.779600 s\n","308 \t 17.340441 \t 9.465055 \t7.835594 \t 0.039791 \t 272.301969 s\n","309 \t 16.950157 \t 9.252971 \t7.648924 \t 0.048263 \t 272.888552 s\n","310 \t 16.392264 \t 8.866688 \t7.481235 \t 0.044342 \t 273.824794 s\n","311 \t 16.482367 \t 8.795248 \t7.644081 \t 0.043038 \t 274.477247 s\n","312 \t 16.944730 \t 9.419712 \t7.422331 \t 0.102687 \t 275.120065 s\n","313 \t 17.061084 \t 9.361114 \t7.643092 \t 0.056879 \t 275.825410 s\n","314 \t 16.277499 \t 8.496609 \t7.740200 \t 0.040690 \t 276.452523 s\n","315 \t 17.272951 \t 9.359396 \t7.878439 \t 0.035116 \t 277.001197 s\n","316 \t 17.414091 \t 9.381252 \t8.007823 \t 0.025015 \t 277.532038 s\n","317 \t 16.622387 \t 8.760359 \t7.743110 \t 0.118917 \t 278.077200 s\n","318 \t 17.337853 \t 9.550223 \t7.747879 \t 0.039751 \t 278.754400 s\n","319 \t 16.404064 \t 8.664147 \t7.709435 \t 0.030481 \t 279.314129 s\n","320 \t 16.917396 \t 9.005731 \t7.863121 \t 0.048544 \t 279.960605 s\n","321 \t 16.457346 \t 8.619267 \t7.790631 \t 0.047449 \t 280.578359 s\n","322 \t 16.466245 \t 8.571890 \t7.850760 \t 0.043595 \t 281.207170 s\n","323 \t 16.073201 \t 8.696446 \t7.322977 \t 0.053778 \t 281.918614 s\n","324 \t 16.581614 \t 8.944334 \t7.573895 \t 0.063385 \t 282.500277 s\n","325 \t 16.842724 \t 9.023079 \t7.783677 \t 0.035968 \t 283.036039 s\n","326 \t 16.668206 \t 8.708665 \t7.925657 \t 0.033885 \t 283.567633 s\n","327 \t 16.129640 \t 8.550541 \t7.534601 \t 0.044498 \t 284.097360 s\n","328 \t 16.462260 \t 8.839727 \t7.580605 \t 0.041928 \t 284.622552 s\n","329 \t 16.707986 \t 9.039823 \t7.598822 \t 0.069340 \t 285.281291 s\n","330 \t 17.112885 \t 9.088454 \t7.993009 \t 0.031420 \t 285.814790 s\n","331 \t 17.877732 \t 9.310545 \t8.447883 \t 0.119305 \t 286.345387 s\n","332 \t 16.994561 \t 9.339936 \t7.622655 \t 0.031970 \t 286.878270 s\n","333 \t 16.486214 \t 8.671380 \t7.774039 \t 0.040794 \t 287.405287 s\n","334 \t 16.662653 \t 9.216023 \t7.377929 \t 0.068701 \t 287.931371 s\n","335 \t 16.651591 \t 9.004387 \t7.616491 \t 0.030713 \t 288.463543 s\n","336 \t 17.167486 \t 9.144951 \t7.989314 \t 0.033222 \t 288.988940 s\n","337 \t 16.106115 \t 8.671679 \t7.393411 \t 0.041025 \t 289.535487 s\n","338 \t 16.784080 \t 9.149515 \t7.567211 \t 0.067353 \t 290.058440 s\n","339 \t 16.064195 \t 8.593248 \t7.319925 \t 0.151022 \t 290.607369 s\n","340 \t 16.780357 \t 8.737494 \t8.001153 \t 0.041710 \t 291.136132 s\n","341 \t 15.652905 \t 7.882122 \t7.669712 \t 0.101071 \t 291.809426 s\n","342 \t 16.255977 \t 8.813693 \t7.387133 \t 0.055150 \t 292.390425 s\n","343 \t 16.976846 \t 9.042062 \t7.767185 \t 0.167600 \t 293.015562 s\n","344 \t 16.516054 \t 8.948409 \t7.535713 \t 0.031933 \t 293.639200 s\n","345 \t 16.280606 \t 9.012932 \t7.244939 \t 0.022735 \t 294.264730 s\n","346 \t 16.508295 \t 9.158475 \t7.329600 \t 0.020221 \t 294.975512 s\n","347 \t 16.889301 \t 8.843873 \t8.017010 \t 0.028418 \t 295.552525 s\n","348 \t 16.791471 \t 8.766088 \t7.997699 \t 0.027685 \t 296.092636 s\n","349 \t 15.546839 \t 8.235124 \t7.268834 \t 0.042881 \t 296.630121 s\n","350 \t 16.950358 \t 9.113674 \t7.803021 \t 0.033663 \t 297.163259 s\n","351 \t 16.488434 \t 8.603284 \t7.866455 \t 0.018694 \t 297.685443 s\n","352 \t 16.351259 \t 8.906249 \t7.422986 \t 0.022024 \t 298.212034 s\n","353 \t 15.461239 \t 8.124838 \t7.317130 \t 0.019270 \t 298.738473 s\n","354 \t 16.689380 \t 8.730401 \t7.925768 \t 0.033211 \t 299.392413 s\n","355 \t 16.345803 \t 8.557726 \t7.753217 \t 0.034862 \t 299.939053 s\n","356 \t 15.952688 \t 8.608925 \t7.291169 \t 0.052594 \t 300.465597 s\n","357 \t 16.562477 \t 8.849579 \t7.680504 \t 0.032395 \t 301.008751 s\n","358 \t 16.667930 \t 9.170460 \t7.444210 \t 0.053259 \t 301.538652 s\n","359 \t 16.111756 \t 8.507307 \t7.579097 \t 0.025352 \t 302.093197 s\n","360 \t 16.408239 \t 8.879025 \t7.489099 \t 0.040116 \t 302.628754 s\n","361 \t 17.264941 \t 9.216200 \t8.027769 \t 0.020972 \t 303.169192 s\n","362 \t 16.916658 \t 8.734190 \t8.144252 \t 0.038218 \t 303.700410 s\n","363 \t 15.771368 \t 8.558973 \t7.177720 \t 0.034675 \t 304.358222 s\n","364 \t 16.784746 \t 8.813522 \t7.916868 \t 0.054357 \t 304.885578 s\n","365 \t 16.285831 \t 8.362137 \t7.866361 \t 0.057334 \t 305.464237 s\n","366 \t 16.523564 \t 8.871238 \t7.625361 \t 0.026967 \t 306.107799 s\n","367 \t 15.375179 \t 7.842457 \t7.475041 \t 0.057680 \t 306.709441 s\n","368 \t 16.535422 \t 9.300393 \t7.206636 \t 0.028393 \t 307.361747 s\n","369 \t 16.687594 \t 8.775922 \t7.890575 \t 0.021097 \t 308.063690 s\n","370 \t 16.248948 \t 8.704949 \t7.513565 \t 0.030435 \t 308.596730 s\n","371 \t 16.441210 \t 9.122800 \t7.276669 \t 0.041741 \t 309.251939 s\n","372 \t 16.481181 \t 8.902354 \t7.550920 \t 0.027907 \t 309.773303 s\n","373 \t 16.687477 \t 9.153300 \t7.511856 \t 0.022321 \t 310.300798 s\n","374 \t 16.206586 \t 8.896451 \t7.281742 \t 0.028393 \t 310.823825 s\n","375 \t 15.795594 \t 8.409598 \t7.351347 \t 0.034649 \t 311.364786 s\n","376 \t 16.429247 \t 8.860682 \t7.543098 \t 0.025467 \t 311.880033 s\n","377 \t 15.953541 \t 8.695045 \t7.179629 \t 0.078867 \t 312.414676 s\n","378 \t 16.765232 \t 8.732897 \t7.984716 \t 0.047619 \t 312.934978 s\n","379 \t 16.747330 \t 9.092994 \t7.625787 \t 0.028549 \t 313.473152 s\n","380 \t 16.312624 \t 8.628661 \t7.497356 \t 0.186606 \t 314.001945 s\n","381 \t 16.259221 \t 8.596645 \t7.645694 \t 0.016881 \t 314.662778 s\n","382 \t 16.573977 \t 8.603092 \t7.901331 \t 0.069555 \t 315.188605 s\n","383 \t 15.538718 \t 8.275188 \t7.230902 \t 0.032628 \t 315.720378 s\n","384 \t 16.113647 \t 8.617770 \t7.442155 \t 0.053720 \t 316.269183 s\n","385 \t 15.348267 \t 8.198063 \t7.118083 \t 0.032120 \t 316.789987 s\n","386 \t 15.934626 \t 7.917661 \t7.986004 \t 0.030961 \t 317.326526 s\n","387 \t 15.420314 \t 8.013724 \t7.357292 \t 0.049297 \t 317.844006 s\n","388 \t 16.277127 \t 8.659510 \t7.594397 \t 0.023220 \t 318.431545 s\n","389 \t 16.145444 \t 8.650034 \t7.459422 \t 0.035988 \t 319.064720 s\n","390 \t 16.670911 \t 8.693376 \t7.934934 \t 0.042603 \t 319.688195 s\n","391 \t 16.239420 \t 8.317401 \t7.892202 \t 0.029818 \t 320.490930 s\n","392 \t 15.983776 \t 8.432004 \t7.516756 \t 0.035015 \t 321.156731 s\n","393 \t 16.452673 \t 8.718951 \t7.707975 \t 0.025747 \t 321.674798 s\n","394 \t 15.848913 \t 8.167720 \t7.634232 \t 0.046961 \t 322.202157 s\n","395 \t 16.048313 \t 8.529884 \t7.496570 \t 0.021859 \t 322.724093 s\n","396 \t 15.703458 \t 8.058408 \t7.618551 \t 0.026499 \t 323.245477 s\n","397 \t 16.360196 \t 8.813521 \t7.515733 \t 0.030943 \t 323.775851 s\n","398 \t 15.960209 \t 8.249603 \t7.614313 \t 0.096293 \t 324.295185 s\n","399 \t 16.504557 \t 8.681566 \t7.799562 \t 0.023429 \t 324.821897 s\n","400 \t 16.564389 \t 8.739266 \t7.738938 \t 0.086184 \t 325.343727 s\n","401 \t 15.990932 \t 7.962127 \t7.968240 \t 0.060566 \t 325.873004 s\n","402 \t 16.147449 \t 8.710865 \t7.416353 \t 0.020232 \t 326.407318 s\n","403 \t 16.192564 \t 8.592607 \t7.580291 \t 0.019666 \t 327.069131 s\n","404 \t 15.656293 \t 8.313468 \t7.310823 \t 0.032003 \t 327.604146 s\n","405 \t 15.880984 \t 8.391967 \t7.460383 \t 0.028633 \t 328.130321 s\n","406 \t 16.505929 \t 8.941992 \t7.527768 \t 0.036169 \t 328.659470 s\n","407 \t 16.090161 \t 8.829740 \t7.220052 \t 0.040369 \t 329.189859 s\n","408 \t 16.138946 \t 8.483810 \t7.608960 \t 0.046176 \t 329.727449 s\n","409 \t 16.851377 \t 8.909399 \t7.914508 \t 0.027471 \t 330.250038 s\n","410 \t 15.781672 \t 8.320878 \t7.438918 \t 0.021876 \t 330.776448 s\n","411 \t 16.119657 \t 8.560777 \t7.452178 \t 0.106701 \t 331.349444 s\n","412 \t 16.599433 \t 9.114008 \t7.451046 \t 0.034379 \t 331.981775 s\n","413 \t 16.052245 \t 8.642657 \t7.383317 \t 0.026271 \t 332.594631 s\n","414 \t 15.989830 \t 8.590687 \t7.370870 \t 0.028272 \t 333.225867 s\n","415 \t 16.621907 \t 8.856226 \t7.737145 \t 0.028536 \t 334.087167 s\n","416 \t 16.311392 \t 8.559906 \t7.722567 \t 0.028918 \t 334.623637 s\n","417 \t 16.776173 \t 8.936074 \t7.810422 \t 0.029678 \t 335.149235 s\n","418 \t 16.223896 \t 8.600498 \t7.571275 \t 0.052122 \t 335.669266 s\n","419 \t 16.007851 \t 8.235050 \t7.678077 \t 0.094724 \t 336.200080 s\n","420 \t 15.844794 \t 8.422500 \t7.398736 \t 0.023559 \t 336.730463 s\n","421 \t 15.272363 \t 8.067373 \t7.180758 \t 0.024233 \t 337.260032 s\n","422 \t 16.069703 \t 8.775488 \t7.264216 \t 0.029999 \t 337.793958 s\n","423 \t 16.664504 \t 8.714779 \t7.922590 \t 0.027135 \t 338.320933 s\n","424 \t 16.105556 \t 8.743231 \t7.336569 \t 0.025756 \t 338.849980 s\n","425 \t 15.263049 \t 8.233999 \t6.990230 \t 0.038819 \t 339.391322 s\n","426 \t 15.992213 \t 8.442514 \t7.515059 \t 0.034640 \t 339.931368 s\n","427 \t 16.731237 \t 9.095748 \t7.576525 \t 0.058964 \t 340.459379 s\n","428 \t 16.171772 \t 8.678469 \t7.425961 \t 0.067342 \t 341.106796 s\n","429 \t 15.152994 \t 8.080879 \t7.053878 \t 0.018237 \t 341.651228 s\n","430 \t 15.902671 \t 8.344334 \t7.534626 \t 0.023712 \t 342.190975 s\n","431 \t 15.919350 \t 8.679018 \t7.220197 \t 0.020135 \t 342.719040 s\n","432 \t 15.930668 \t 8.458263 \t7.448230 \t 0.024175 \t 343.251760 s\n","433 \t 16.328490 \t 8.099582 \t8.191883 \t 0.037025 \t 343.770869 s\n","434 \t 15.915926 \t 8.568271 \t7.320441 \t 0.027213 \t 344.351867 s\n","435 \t 15.568293 \t 7.776111 \t7.737958 \t 0.054223 \t 344.977417 s\n","436 \t 16.414729 \t 9.016105 \t7.378938 \t 0.019686 \t 345.601576 s\n","437 \t 15.880294 \t 8.146145 \t7.681270 \t 0.052879 \t 346.392087 s\n","438 \t 15.551413 \t 8.254428 \t7.280695 \t 0.016290 \t 347.075981 s\n","439 \t 15.778594 \t 8.171067 \t7.572537 \t 0.034990 \t 347.609990 s\n","440 \t 15.983819 \t 8.194831 \t7.770709 \t 0.018279 \t 348.134811 s\n","441 \t 15.692094 \t 8.237769 \t7.428728 \t 0.025597 \t 348.659252 s\n","442 \t 16.212514 \t 8.476261 \t7.711306 \t 0.024946 \t 349.184973 s\n","443 \t 16.316019 \t 8.613777 \t7.678844 \t 0.023398 \t 349.708408 s\n","444 \t 16.082808 \t 8.330181 \t7.728232 \t 0.024395 \t 350.231970 s\n","445 \t 16.124976 \t 8.457206 \t7.620719 \t 0.047051 \t 350.759894 s\n","446 \t 15.913649 \t 8.512227 \t7.380023 \t 0.021399 \t 351.423977 s\n","447 \t 15.513134 \t 8.355610 \t7.113222 \t 0.044303 \t 351.948713 s\n","448 \t 16.676793 \t 8.684515 \t7.916532 \t 0.075746 \t 352.475135 s\n","449 \t 16.032710 \t 8.368853 \t7.642727 \t 0.021129 \t 353.016289 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:47<00:00,  2.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4021\n","Hit@10: 0.5500\n","Hit@3: 0.4000\n","Hit@1: 0.3308\n","Link Prediction on Validation Set (All)\n","MRR: 0.3634\n","Hit@10: 0.5743\n","Hit@3: 0.4073\n","Hit@1: 0.2521\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3208\n","Hit@10: 0.5538\n","Hit@3: 0.3308\n","Hit@1: 0.2154\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6492\n","Hit@10: 0.8260\n","Hit@3: 0.7186\n","Hit@1: 0.5390\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0695\n","450 \t 16.364094 \t 8.762258 \t7.561328 \t 0.040508 \t 402.109808 s\n","451 \t 16.248279 \t 8.489513 \t7.697374 \t 0.061393 \t 402.656397 s\n","452 \t 16.802079 \t 8.739671 \t8.021290 \t 0.041119 \t 403.206423 s\n","453 \t 16.738360 \t 9.047240 \t7.617879 \t 0.073241 \t 403.746391 s\n","454 \t 17.200422 \t 9.027657 \t8.122650 \t 0.050115 \t 404.275164 s\n","455 \t 16.615125 \t 9.074502 \t7.488151 \t 0.052471 \t 404.933485 s\n","456 \t 17.054439 \t 9.068240 \t7.930176 \t 0.056023 \t 405.453749 s\n","457 \t 16.427885 \t 8.919975 \t7.470963 \t 0.036947 \t 405.980816 s\n","458 \t 16.284269 \t 8.601112 \t7.632183 \t 0.050973 \t 406.505522 s\n","459 \t 16.245458 \t 8.681836 \t7.499572 \t 0.064050 \t 407.034543 s\n","460 \t 16.851995 \t 8.835277 \t7.955053 \t 0.061665 \t 407.617080 s\n","461 \t 17.035604 \t 9.264128 \t7.704268 \t 0.067208 \t 408.283684 s\n","462 \t 16.773949 \t 9.408048 \t7.295728 \t 0.070174 \t 408.931087 s\n","463 \t 16.502960 \t 8.859390 \t7.616163 \t 0.027407 \t 409.549144 s\n","464 \t 16.239559 \t 8.939301 \t7.127095 \t 0.173162 \t 410.490982 s\n","465 \t 16.986020 \t 8.939215 \t7.988774 \t 0.058030 \t 411.297619 s\n","466 \t 16.968298 \t 9.294426 \t7.643186 \t 0.030686 \t 412.005852 s\n","467 \t 15.781111 \t 8.687059 \t7.043340 \t 0.050711 \t 412.671124 s\n","468 \t 16.548774 \t 8.989562 \t7.484072 \t 0.075141 \t 413.316096 s\n","469 \t 16.228481 \t 9.013374 \t7.131433 \t 0.083674 \t 413.849251 s\n","470 \t 16.223837 \t 8.773138 \t7.397140 \t 0.053560 \t 414.377783 s\n","471 \t 16.783592 \t 8.907306 \t7.810662 \t 0.065623 \t 414.904396 s\n","472 \t 16.075816 \t 8.618266 \t7.415996 \t 0.041555 \t 415.430827 s\n","473 \t 16.708349 \t 8.907137 \t7.615781 \t 0.185431 \t 415.952256 s\n","474 \t 16.426259 \t 8.994441 \t7.377980 \t 0.053839 \t 416.629467 s\n","475 \t 16.820181 \t 9.189062 \t7.486324 \t 0.144796 \t 417.165630 s\n","476 \t 17.007900 \t 8.831380 \t8.103192 \t 0.073328 \t 417.708016 s\n","477 \t 15.710539 \t 8.405316 \t7.215540 \t 0.089682 \t 418.236984 s\n","478 \t 16.093119 \t 8.425842 \t7.589229 \t 0.078047 \t 418.801756 s\n","479 \t 16.924131 \t 9.262693 \t7.565230 \t 0.096208 \t 419.335250 s\n","480 \t 16.013494 \t 8.310056 \t7.639019 \t 0.064418 \t 419.860411 s\n","481 \t 16.059613 \t 8.893132 \t7.092369 \t 0.074111 \t 420.399030 s\n","482 \t 16.183438 \t 8.709403 \t7.429346 \t 0.044689 \t 420.929085 s\n","483 \t 16.491593 \t 8.997528 \t7.427602 \t 0.066465 \t 421.480036 s\n","484 \t 16.214158 \t 8.471113 \t7.660367 \t 0.082677 \t 422.158382 s\n","485 \t 16.442642 \t 8.825966 \t7.528713 \t 0.087963 \t 422.678066 s\n","486 \t 16.169414 \t 8.777458 \t7.330702 \t 0.061253 \t 423.216319 s\n","487 \t 17.059403 \t 9.576889 \t7.431587 \t 0.050928 \t 423.875529 s\n","488 \t 17.372296 \t 9.021302 \t8.294864 \t 0.056130 \t 424.492439 s\n","489 \t 16.426362 \t 8.860657 \t7.401890 \t 0.163815 \t 425.132813 s\n","490 \t 16.533146 \t 8.721473 \t7.768055 \t 0.043616 \t 425.816597 s\n","491 \t 16.833685 \t 9.002533 \t7.764080 \t 0.067072 \t 426.441214 s\n","492 \t 16.135817 \t 8.466122 \t7.510262 \t 0.159433 \t 426.964022 s\n","493 \t 15.824383 \t 8.055908 \t7.677520 \t 0.090955 \t 427.495837 s\n","494 \t 16.778141 \t 8.744421 \t7.977940 \t 0.055780 \t 428.036334 s\n","495 \t 16.359646 \t 8.530359 \t7.784035 \t 0.045252 \t 428.711266 s\n","496 \t 17.051834 \t 9.165339 \t7.804542 \t 0.081954 \t 429.253057 s\n","497 \t 16.003583 \t 8.381358 \t7.571088 \t 0.051138 \t 429.781326 s\n","498 \t 16.580505 \t 8.717543 \t7.807847 \t 0.055116 \t 430.308189 s\n","499 \t 17.132250 \t 9.072504 \t8.003137 \t 0.056611 \t 430.846062 s\n","500 \t 16.208537 \t 8.349865 \t7.833071 \t 0.025602 \t 431.365729 s\n","501 \t 16.587136 \t 8.808766 \t7.745468 \t 0.032902 \t 431.924582 s\n","502 \t 16.235906 \t 8.445436 \t7.751343 \t 0.039128 \t 432.450440 s\n","503 \t 15.880934 \t 8.416358 \t7.426520 \t 0.038056 \t 432.994615 s\n","504 \t 16.260772 \t 8.620485 \t7.613268 \t 0.027019 \t 433.546178 s\n","505 \t 15.844253 \t 8.538702 \t7.264415 \t 0.041137 \t 434.072551 s\n","506 \t 15.959189 \t 8.594621 \t7.325567 \t 0.039002 \t 434.595581 s\n","507 \t 16.350844 \t 8.116141 \t8.178627 \t 0.056076 \t 435.248226 s\n","508 \t 16.061171 \t 8.686462 \t7.330393 \t 0.044316 \t 435.768726 s\n","509 \t 16.368689 \t 9.048017 \t7.268262 \t 0.052410 \t 436.342411 s\n","510 \t 16.005701 \t 8.093318 \t7.859780 \t 0.052604 \t 436.964451 s\n","511 \t 16.686474 \t 8.679552 \t7.947933 \t 0.058989 \t 437.596817 s\n","512 \t 15.876790 \t 8.532096 \t7.283988 \t 0.060706 \t 438.218483 s\n","513 \t 16.705409 \t 8.944755 \t7.708827 \t 0.051827 \t 438.902354 s\n","514 \t 16.657971 \t 8.925655 \t7.687798 \t 0.044518 \t 439.540552 s\n","515 \t 16.397473 \t 8.417825 \t7.953546 \t 0.026103 \t 440.068473 s\n","516 \t 16.168312 \t 8.692028 \t7.427671 \t 0.048613 \t 440.752083 s\n","517 \t 15.585611 \t 8.255060 \t7.299405 \t 0.031146 \t 441.285464 s\n","518 \t 16.366798 \t 9.175994 \t7.140338 \t 0.050466 \t 441.807920 s\n","519 \t 16.543982 \t 9.084080 \t7.434873 \t 0.025029 \t 442.338749 s\n","520 \t 16.021706 \t 8.216127 \t7.759031 \t 0.046548 \t 442.862118 s\n","521 \t 16.869637 \t 8.865991 \t7.968644 \t 0.035001 \t 443.391220 s\n","522 \t 16.180187 \t 8.464786 \t7.673140 \t 0.042261 \t 443.909282 s\n","523 \t 16.492815 \t 8.520181 \t7.916179 \t 0.056455 \t 444.435728 s\n","524 \t 16.739428 \t 8.752070 \t7.939145 \t 0.048211 \t 444.959350 s\n","525 \t 16.274368 \t 9.128312 \t7.107727 \t 0.038330 \t 445.614668 s\n","526 \t 16.793461 \t 9.072347 \t7.641637 \t 0.079477 \t 446.136103 s\n","527 \t 16.704309 \t 8.816157 \t7.818977 \t 0.069175 \t 446.661235 s\n","528 \t 15.956014 \t 8.819349 \t7.085069 \t 0.051596 \t 447.206358 s\n","529 \t 15.743665 \t 8.135766 \t7.576250 \t 0.031649 \t 447.726789 s\n","530 \t 16.603967 \t 9.248061 \t7.320122 \t 0.035785 \t 448.253038 s\n","531 \t 16.512535 \t 9.110855 \t7.360008 \t 0.041673 \t 448.777287 s\n","532 \t 15.899136 \t 8.404313 \t7.462366 \t 0.032457 \t 449.346086 s\n","533 \t 15.725603 \t 8.272496 \t7.393888 \t 0.059220 \t 449.981312 s\n","534 \t 16.093785 \t 8.482636 \t7.501172 \t 0.109977 \t 450.611521 s\n","535 \t 16.162490 \t 8.610821 \t7.499437 \t 0.052233 \t 451.232082 s\n","536 \t 15.913754 \t 8.470948 \t7.400406 \t 0.042400 \t 452.112553 s\n","537 \t 17.075531 \t 9.007157 \t8.031421 \t 0.036953 \t 452.638739 s\n","538 \t 16.707596 \t 9.142708 \t7.528738 \t 0.036150 \t 453.163491 s\n","539 \t 16.274035 \t 8.912719 \t7.288488 \t 0.072827 \t 453.689547 s\n","540 \t 16.167971 \t 8.552610 \t7.566008 \t 0.049353 \t 454.209555 s\n","541 \t 15.545744 \t 8.337939 \t7.158339 \t 0.049466 \t 454.742570 s\n","542 \t 16.423875 \t 9.041642 \t7.315896 \t 0.066336 \t 455.263163 s\n","543 \t 16.161744 \t 8.363766 \t7.760990 \t 0.036988 \t 455.799569 s\n","544 \t 16.056897 \t 8.893739 \t7.099555 \t 0.063603 \t 456.323909 s\n","545 \t 15.317135 \t 7.932574 \t7.339094 \t 0.045467 \t 456.848088 s\n","546 \t 16.125618 \t 8.875165 \t7.167587 \t 0.082866 \t 457.380343 s\n","547 \t 15.978651 \t 8.523792 \t7.408696 \t 0.046163 \t 457.898592 s\n","548 \t 17.015291 \t 9.021436 \t7.955112 \t 0.038742 \t 458.548975 s\n","549 \t 16.623936 \t 8.926067 \t7.634382 \t 0.063486 \t 459.063529 s\n","550 \t 16.491015 \t 8.950354 \t7.467575 \t 0.073086 \t 459.593852 s\n","551 \t 16.629215 \t 8.934402 \t7.656512 \t 0.038301 \t 460.128517 s\n","552 \t 15.079796 \t 7.705186 \t7.345555 \t 0.029056 \t 460.649653 s\n","553 \t 16.470975 \t 8.995564 \t7.444751 \t 0.030659 \t 461.174744 s\n","554 \t 16.088133 \t 8.240773 \t7.819566 \t 0.027794 \t 461.685049 s\n","555 \t 15.364323 \t 8.014805 \t7.329586 \t 0.019932 \t 462.246834 s\n","556 \t 16.722042 \t 9.175828 \t7.507255 \t 0.038959 \t 462.870451 s\n","557 \t 16.177921 \t 9.020163 \t7.129945 \t 0.027814 \t 463.510650 s\n","558 \t 16.536282 \t 9.353167 \t7.150913 \t 0.032202 \t 464.136773 s\n","559 \t 16.392436 \t 8.739654 \t7.624226 \t 0.028557 \t 464.801527 s\n","560 \t 16.335762 \t 8.191975 \t8.120607 \t 0.023180 \t 465.439443 s\n","561 \t 15.577437 \t 8.022806 \t7.534447 \t 0.020184 \t 466.088792 s\n","562 \t 15.936106 \t 8.197554 \t7.702009 \t 0.036544 \t 466.631992 s\n","563 \t 16.190223 \t 8.457864 \t7.704994 \t 0.027364 \t 467.169578 s\n","564 \t 17.045196 \t 8.802048 \t8.202759 \t 0.040389 \t 467.699503 s\n","565 \t 16.194331 \t 9.010048 \t7.136709 \t 0.047574 \t 468.234439 s\n","566 \t 16.248329 \t 8.466021 \t7.755519 \t 0.026789 \t 468.752096 s\n","567 \t 16.680660 \t 8.924320 \t7.729684 \t 0.026656 \t 469.286670 s\n","568 \t 15.485392 \t 8.233956 \t7.217840 \t 0.033596 \t 469.805050 s\n","569 \t 15.982316 \t 8.793031 \t7.159621 \t 0.029665 \t 470.339626 s\n","570 \t 16.188056 \t 8.807855 \t7.312063 \t 0.068138 \t 470.855717 s\n","571 \t 16.967967 \t 9.052058 \t7.720518 \t 0.195391 \t 471.386535 s\n","572 \t 15.562013 \t 8.104298 \t7.353654 \t 0.104062 \t 471.908074 s\n","573 \t 15.401071 \t 8.030622 \t7.330012 \t 0.040437 \t 472.434056 s\n","574 \t 15.672904 \t 8.306726 \t7.298644 \t 0.067534 \t 473.073056 s\n","575 \t 16.018749 \t 8.759254 \t7.125832 \t 0.133664 \t 473.603239 s\n","576 \t 15.402334 \t 8.396518 \t6.928623 \t 0.077193 \t 474.125760 s\n","577 \t 15.559916 \t 8.759403 \t6.750223 \t 0.050289 \t 474.649113 s\n","578 \t 15.846402 \t 8.524233 \t7.276529 \t 0.045641 \t 475.218459 s\n","579 \t 16.038151 \t 8.615595 \t7.356854 \t 0.065702 \t 475.850958 s\n","580 \t 16.085691 \t 8.402353 \t7.650293 \t 0.033045 \t 476.482112 s\n","581 \t 15.284701 \t 7.878732 \t7.370990 \t 0.034979 \t 477.140685 s\n","582 \t 15.496465 \t 8.124545 \t7.348120 \t 0.023799 \t 477.844017 s\n","583 \t 15.991214 \t 8.730830 \t7.219928 \t 0.040457 \t 478.610111 s\n","584 \t 15.771221 \t 8.323313 \t7.408323 \t 0.039585 \t 479.151535 s\n","585 \t 16.649745 \t 8.477196 \t8.140013 \t 0.032535 \t 479.674037 s\n","586 \t 16.166602 \t 8.277076 \t7.853189 \t 0.036338 \t 480.226051 s\n","587 \t 16.105462 \t 8.646553 \t7.408206 \t 0.050703 \t 480.753135 s\n","588 \t 15.405580 \t 8.173960 \t7.208733 \t 0.022887 \t 481.292935 s\n","589 \t 16.616004 \t 8.895907 \t7.689136 \t 0.030961 \t 481.824846 s\n","590 \t 16.013768 \t 8.556626 \t7.418503 \t 0.038639 \t 482.343428 s\n","591 \t 16.131715 \t 8.714355 \t7.370802 \t 0.046558 \t 482.876764 s\n","592 \t 15.750152 \t 8.537149 \t7.116457 \t 0.096546 \t 483.530333 s\n","593 \t 15.144787 \t 8.047787 \t7.068050 \t 0.028950 \t 484.050606 s\n","594 \t 15.872848 \t 8.370179 \t7.473086 \t 0.029582 \t 484.584387 s\n","595 \t 15.988413 \t 8.710854 \t7.253643 \t 0.023916 \t 485.109846 s\n","596 \t 16.321541 \t 9.014448 \t7.274698 \t 0.032395 \t 485.630574 s\n","597 \t 16.390492 \t 8.713089 \t7.565436 \t 0.111966 \t 486.150486 s\n","598 \t 16.328488 \t 8.683644 \t7.597930 \t 0.046915 \t 486.666395 s\n","599 \t 15.016557 \t 8.080726 \t6.902112 \t 0.033718 \t 487.194893 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:46<00:00,  2.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4252\n","Hit@10: 0.5923\n","Hit@3: 0.4577\n","Hit@1: 0.3385\n","Link Prediction on Validation Set (All)\n","MRR: 0.3849\n","Hit@10: 0.6110\n","Hit@3: 0.4224\n","Hit@1: 0.2738\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3337\n","Hit@10: 0.5538\n","Hit@3: 0.3692\n","Hit@1: 0.2308\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6589\n","Hit@10: 0.8215\n","Hit@3: 0.7186\n","Hit@1: 0.5627\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.1077\n","600 \t 15.950648 \t 8.578454 \t7.331690 \t 0.040503 \t 535.370496 s\n","601 \t 15.047227 \t 7.885154 \t7.129565 \t 0.032508 \t 535.902359 s\n","602 \t 15.306797 \t 8.411954 \t6.860434 \t 0.034410 \t 536.567298 s\n","603 \t 15.700533 \t 7.913896 \t7.749751 \t 0.036887 \t 537.123561 s\n","604 \t 15.799767 \t 8.626023 \t7.110302 \t 0.063442 \t 537.658105 s\n","605 \t 15.669303 \t 8.335551 \t7.292670 \t 0.041082 \t 538.193521 s\n","606 \t 15.822797 \t 8.507028 \t7.264142 \t 0.051627 \t 538.710354 s\n","607 \t 16.432007 \t 8.718420 \t7.684047 \t 0.029540 \t 539.244865 s\n","608 \t 15.812036 \t 8.465593 \t7.312027 \t 0.034416 \t 539.762571 s\n","609 \t 15.598942 \t 8.332019 \t7.246238 \t 0.020684 \t 540.296698 s\n","610 \t 15.675485 \t 8.309900 \t7.296535 \t 0.069050 \t 541.030504 s\n","611 \t 15.669022 \t 8.527840 \t7.113014 \t 0.028168 \t 541.821652 s\n","612 \t 15.857809 \t 8.729070 \t7.096794 \t 0.031945 \t 542.620332 s\n","613 \t 16.259390 \t 8.391469 \t7.836598 \t 0.031324 \t 543.436519 s\n","614 \t 16.438417 \t 8.682359 \t7.634836 \t 0.121222 \t 544.224397 s\n","615 \t 15.832868 \t 8.472458 \t7.333849 \t 0.026561 \t 544.784843 s\n","616 \t 16.069394 \t 8.120317 \t7.744054 \t 0.205024 \t 545.310766 s\n","617 \t 15.282395 \t 8.218967 \t7.036325 \t 0.027103 \t 545.838212 s\n","618 \t 16.368576 \t 8.572924 \t7.750365 \t 0.045288 \t 546.364359 s\n","619 \t 15.820405 \t 8.537883 \t7.260977 \t 0.021545 \t 546.886087 s\n","620 \t 16.036069 \t 8.272768 \t7.727741 \t 0.035560 \t 547.538322 s\n","621 \t 16.350837 \t 8.604348 \t7.712681 \t 0.033808 \t 548.074003 s\n","622 \t 15.517565 \t 7.985705 \t7.491863 \t 0.039998 \t 548.606537 s\n","623 \t 15.906319 \t 8.502006 \t7.376078 \t 0.028236 \t 549.128984 s\n","624 \t 15.458568 \t 8.289534 \t7.131938 \t 0.037097 \t 549.661980 s\n","625 \t 15.609641 \t 8.274680 \t7.304816 \t 0.030144 \t 550.185108 s\n","626 \t 15.741484 \t 8.324890 \t7.358294 \t 0.058300 \t 550.716976 s\n","627 \t 14.718887 \t 7.679158 \t7.015035 \t 0.024694 \t 551.235786 s\n","628 \t 15.384963 \t 8.182104 \t7.174825 \t 0.028034 \t 551.768043 s\n","629 \t 15.782957 \t 8.080246 \t7.676052 \t 0.026659 \t 552.284981 s\n","630 \t 15.509459 \t 8.205462 \t7.231411 \t 0.072585 \t 552.815232 s\n","631 \t 15.860131 \t 8.549870 \t7.277133 \t 0.033129 \t 553.453568 s\n","632 \t 16.063241 \t 8.470540 \t7.516859 \t 0.075842 \t 553.977065 s\n","633 \t 15.599932 \t 8.514896 \t7.058121 \t 0.026914 \t 554.577510 s\n","634 \t 15.410261 \t 8.002409 \t7.376341 \t 0.031511 \t 555.193398 s\n","635 \t 16.348475 \t 8.448462 \t7.881653 \t 0.018361 \t 555.802021 s\n","636 \t 15.261411 \t 7.950314 \t7.278774 \t 0.032322 \t 556.449303 s\n","637 \t 15.445358 \t 8.062239 \t7.354234 \t 0.028886 \t 557.128559 s\n","638 \t 15.831082 \t 8.575869 \t7.215439 \t 0.039775 \t 557.658707 s\n","639 \t 15.593478 \t 8.290468 \t7.261782 \t 0.041228 \t 558.184556 s\n","640 \t 15.721986 \t 8.188779 \t7.463728 \t 0.069479 \t 558.714025 s\n","641 \t 15.362227 \t 8.229185 \t7.087900 \t 0.045142 \t 559.251920 s\n","642 \t 15.869865 \t 8.272772 \t7.563281 \t 0.033813 \t 559.773598 s\n","643 \t 14.929934 \t 7.910008 \t6.950404 \t 0.069522 \t 560.421581 s\n","644 \t 16.071531 \t 8.249568 \t7.709754 \t 0.112209 \t 560.942683 s\n","645 \t 16.126153 \t 8.308681 \t7.757427 \t 0.060045 \t 561.467838 s\n","646 \t 16.684958 \t 8.368910 \t8.277804 \t 0.038244 \t 561.992821 s\n","647 \t 15.232656 \t 8.265359 \t6.942827 \t 0.024471 \t 562.526912 s\n","648 \t 16.918496 \t 8.854427 \t8.044209 \t 0.019861 \t 563.049974 s\n","649 \t 15.419622 \t 7.856183 \t7.528009 \t 0.035431 \t 563.591794 s\n","650 \t 15.427089 \t 8.288322 \t7.115714 \t 0.023052 \t 564.118399 s\n","651 \t 16.476516 \t 8.729935 \t7.720432 \t 0.026149 \t 564.660190 s\n","652 \t 16.001569 \t 8.529773 \t7.440096 \t 0.031701 \t 565.318257 s\n","653 \t 15.878078 \t 8.742601 \t7.098288 \t 0.037188 \t 565.835422 s\n","654 \t 15.776247 \t 8.713478 \t7.033258 \t 0.029511 \t 566.366148 s\n","655 \t 15.173754 \t 7.852081 \t7.291067 \t 0.030606 \t 566.886014 s\n","656 \t 16.630803 \t 8.396473 \t8.191646 \t 0.042685 \t 567.477327 s\n","657 \t 15.608906 \t 8.220109 \t7.365997 \t 0.022801 \t 568.097538 s\n","658 \t 15.494758 \t 8.104759 \t7.357928 \t 0.032071 \t 568.732920 s\n","659 \t 15.770987 \t 8.586068 \t7.158647 \t 0.026273 \t 569.370492 s\n","660 \t 15.655388 \t 8.310646 \t7.210555 \t 0.134188 \t 570.045319 s\n","661 \t 15.665175 \t 8.096841 \t7.445388 \t 0.122946 \t 570.716061 s\n","662 \t 15.165724 \t 8.147851 \t6.962685 \t 0.055189 \t 571.233273 s\n","663 \t 15.503296 \t 8.100997 \t7.367344 \t 0.034955 \t 571.756941 s\n","664 \t 15.404461 \t 8.173567 \t7.178763 \t 0.052131 \t 572.285900 s\n","665 \t 15.008290 \t 8.025372 \t6.947577 \t 0.035341 \t 572.809259 s\n","666 \t 16.082236 \t 8.578772 \t7.476402 \t 0.027062 \t 573.328213 s\n","667 \t 15.317924 \t 8.049973 \t7.242288 \t 0.025663 \t 573.853405 s\n","668 \t 16.309373 \t 8.490643 \t7.654311 \t 0.164420 \t 574.375970 s\n","669 \t 15.442647 \t 8.762772 \t6.647124 \t 0.032752 \t 574.890165 s\n","670 \t 15.784752 \t 8.180025 \t7.528091 \t 0.076637 \t 575.414575 s\n","671 \t 15.673877 \t 8.176789 \t7.464357 \t 0.032731 \t 576.066444 s\n","672 \t 16.388304 \t 9.134048 \t7.219682 \t 0.034574 \t 576.593699 s\n","673 \t 15.769032 \t 8.372972 \t7.374544 \t 0.021518 \t 577.133698 s\n","674 \t 16.065370 \t 8.444294 \t7.582582 \t 0.038493 \t 577.657691 s\n","675 \t 15.931519 \t 8.155245 \t7.758517 \t 0.017756 \t 578.200676 s\n","676 \t 15.155883 \t 7.915164 \t7.217461 \t 0.023258 \t 578.723423 s\n","677 \t 15.767361 \t 8.580937 \t7.160713 \t 0.025711 \t 579.264564 s\n","678 \t 15.636072 \t 8.112190 \t7.489211 \t 0.034671 \t 579.800187 s\n","679 \t 15.409810 \t 8.194317 \t7.197524 \t 0.017969 \t 580.376936 s\n","680 \t 15.362480 \t 8.261203 \t7.082262 \t 0.019015 \t 581.160548 s\n","681 \t 15.151518 \t 7.789546 \t7.342286 \t 0.019687 \t 581.782637 s\n","682 \t 15.575444 \t 7.760872 \t7.729179 \t 0.085392 \t 582.431062 s\n","683 \t 15.122260 \t 8.264524 \t6.791992 \t 0.065745 \t 583.070967 s\n","684 \t 15.224932 \t 8.207901 \t6.992863 \t 0.024169 \t 583.594749 s\n","685 \t 15.533556 \t 8.203565 \t7.295299 \t 0.034693 \t 584.130867 s\n","686 \t 15.413184 \t 8.507571 \t6.877521 \t 0.028092 \t 584.661607 s\n","687 \t 15.716375 \t 8.292942 \t7.390767 \t 0.032667 \t 585.193804 s\n","688 \t 16.142572 \t 8.635876 \t7.482682 \t 0.024015 \t 585.712680 s\n","689 \t 16.397703 \t 8.551527 \t7.786429 \t 0.059748 \t 586.241261 s\n","690 \t 15.178136 \t 8.225488 \t6.863052 \t 0.089596 \t 586.765318 s\n","691 \t 15.150944 \t 7.636871 \t7.487249 \t 0.026823 \t 587.415276 s\n","692 \t 15.486993 \t 8.435425 \t7.029601 \t 0.021967 \t 587.934350 s\n","693 \t 15.760943 \t 8.317195 \t7.420709 \t 0.023040 \t 588.460185 s\n","694 \t 15.797135 \t 8.476486 \t7.293958 \t 0.026691 \t 588.983109 s\n","695 \t 15.552633 \t 8.174885 \t7.343499 \t 0.034249 \t 589.512358 s\n","696 \t 15.867336 \t 8.396528 \t7.428958 \t 0.041849 \t 590.032334 s\n","697 \t 15.095374 \t 8.219479 \t6.847235 \t 0.028661 \t 590.566183 s\n","698 \t 15.462914 \t 8.001779 \t7.428895 \t 0.032240 \t 591.090418 s\n","699 \t 15.083525 \t 7.797466 \t7.258420 \t 0.027639 \t 591.633212 s\n","700 \t 16.172374 \t 8.389044 \t7.734108 \t 0.049221 \t 592.165152 s\n","701 \t 15.473495 \t 8.120504 \t7.327865 \t 0.025125 \t 592.704274 s\n","702 \t 15.038130 \t 8.051178 \t6.956560 \t 0.030393 \t 593.273780 s\n","703 \t 15.344511 \t 8.100053 \t7.219923 \t 0.024534 \t 594.055862 s\n","704 \t 16.072834 \t 8.480649 \t7.566126 \t 0.026059 \t 594.675958 s\n","705 \t 15.861178 \t 8.258933 \t7.579632 \t 0.022614 \t 595.313619 s\n","706 \t 14.962362 \t 8.126820 \t6.782125 \t 0.053418 \t 595.976047 s\n","707 \t 15.832281 \t 8.633932 \t7.173547 \t 0.024801 \t 596.506695 s\n","708 \t 15.702740 \t 8.247547 \t7.425559 \t 0.029634 \t 597.041214 s\n","709 \t 15.782922 \t 8.291742 \t7.471318 \t 0.019862 \t 597.578949 s\n","710 \t 16.358958 \t 8.804087 \t7.528803 \t 0.026068 \t 598.098009 s\n","711 \t 16.190928 \t 8.357728 \t7.785903 \t 0.047297 \t 598.635560 s\n","712 \t 15.238837 \t 7.909952 \t7.312593 \t 0.016291 \t 599.167206 s\n","713 \t 15.222849 \t 7.904974 \t7.278942 \t 0.038933 \t 599.700652 s\n","714 \t 16.041456 \t 8.280596 \t7.735985 \t 0.024875 \t 600.227571 s\n","715 \t 15.048316 \t 7.960996 \t7.065237 \t 0.022083 \t 600.762281 s\n","716 \t 16.152904 \t 8.342290 \t7.793107 \t 0.017506 \t 601.416351 s\n","717 \t 14.459342 \t 7.465307 \t6.963968 \t 0.030066 \t 601.937956 s\n","718 \t 15.453350 \t 8.211542 \t7.203328 \t 0.038480 \t 602.459702 s\n","719 \t 16.284129 \t 8.323095 \t7.947757 \t 0.013276 \t 602.983872 s\n","720 \t 14.683952 \t 7.757040 \t6.901362 \t 0.025550 \t 603.508101 s\n","721 \t 15.320742 \t 7.863554 \t7.423613 \t 0.033576 \t 604.034137 s\n","722 \t 14.854424 \t 7.895204 \t6.935834 \t 0.023387 \t 604.559787 s\n","723 \t 15.412344 \t 7.987080 \t7.406515 \t 0.018749 \t 605.080609 s\n","724 \t 14.947194 \t 7.714834 \t7.206995 \t 0.025365 \t 605.609156 s\n","725 \t 15.119748 \t 7.944422 \t7.156367 \t 0.018959 \t 606.184092 s\n","726 \t 15.708521 \t 8.067903 \t7.616549 \t 0.024069 \t 606.800788 s\n","727 \t 15.407063 \t 8.220329 \t7.148794 \t 0.037939 \t 607.424032 s\n","728 \t 15.033234 \t 7.752470 \t7.257185 \t 0.023580 \t 608.040699 s\n","729 \t 16.167935 \t 8.270338 \t7.867353 \t 0.030243 \t 608.882063 s\n","730 \t 15.078985 \t 7.918897 \t7.137270 \t 0.022817 \t 609.425312 s\n","731 \t 15.608068 \t 8.510064 \t7.000937 \t 0.097068 \t 609.947665 s\n","732 \t 14.885620 \t 7.804518 \t7.059545 \t 0.021557 \t 610.493022 s\n","733 \t 15.170105 \t 7.799848 \t7.347140 \t 0.023117 \t 611.018251 s\n","734 \t 15.609964 \t 8.215314 \t7.374574 \t 0.020076 \t 611.559973 s\n","735 \t 15.614045 \t 7.696673 \t7.899246 \t 0.018125 \t 612.079371 s\n","736 \t 14.858795 \t 7.684732 \t7.152138 \t 0.021926 \t 612.598658 s\n","737 \t 15.057075 \t 8.145639 \t6.896671 \t 0.014765 \t 613.256554 s\n","738 \t 15.144184 \t 8.232967 \t6.894111 \t 0.017106 \t 613.774711 s\n","739 \t 15.981219 \t 8.119181 \t7.842395 \t 0.019643 \t 614.301766 s\n","740 \t 14.644578 \t 7.842001 \t6.774476 \t 0.028100 \t 614.827730 s\n","741 \t 15.905895 \t 8.567294 \t7.323142 \t 0.015460 \t 615.351503 s\n","742 \t 15.154503 \t 8.016534 \t7.103358 \t 0.034612 \t 615.874480 s\n","743 \t 15.063511 \t 7.526934 \t7.524447 \t 0.012131 \t 616.405272 s\n","744 \t 15.448212 \t 8.375062 \t7.063000 \t 0.010149 \t 616.926264 s\n","745 \t 15.074647 \t 8.047375 \t6.984075 \t 0.043198 \t 617.447005 s\n","746 \t 15.704562 \t 8.076943 \t7.607248 \t 0.020370 \t 618.092974 s\n","747 \t 15.020107 \t 7.576656 \t7.421343 \t 0.022109 \t 618.620047 s\n","748 \t 14.783087 \t 7.801675 \t6.967231 \t 0.014181 \t 619.197982 s\n","749 \t 15.621738 \t 8.032044 \t7.532701 \t 0.056993 \t 619.835845 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:46<00:00,  2.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4450\n","Hit@10: 0.6462\n","Hit@3: 0.4654\n","Hit@1: 0.3577\n","Link Prediction on Validation Set (All)\n","MRR: 0.3984\n","Hit@10: 0.6528\n","Hit@3: 0.4457\n","Hit@1: 0.2788\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3288\n","Hit@10: 0.5923\n","Hit@3: 0.3462\n","Hit@1: 0.2154\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6789\n","Hit@10: 0.8418\n","Hit@3: 0.7379\n","Hit@1: 0.5864\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0788\n","750 \t 16.032995 \t 8.829672 \t7.177613 \t 0.025710 \t 668.217155 s\n","751 \t 14.545323 \t 7.616731 \t6.903359 \t 0.025233 \t 668.741873 s\n","752 \t 15.489732 \t 8.058725 \t7.414953 \t 0.016054 \t 669.268438 s\n","753 \t 16.172945 \t 8.659456 \t7.487027 \t 0.026462 \t 669.804100 s\n","754 \t 15.425699 \t 8.161829 \t7.241117 \t 0.022753 \t 670.333269 s\n","755 \t 14.883170 \t 7.418955 \t7.436402 \t 0.027813 \t 670.866089 s\n","756 \t 15.486907 \t 8.141516 \t7.326271 \t 0.019120 \t 671.416476 s\n","757 \t 15.227130 \t 8.140355 \t7.071772 \t 0.015003 \t 672.230583 s\n","758 \t 15.907473 \t 8.549824 \t7.338580 \t 0.019069 \t 672.857400 s\n","759 \t 15.818832 \t 8.334926 \t7.451325 \t 0.032581 \t 673.494267 s\n","760 \t 15.715480 \t 8.257593 \t7.439676 \t 0.018212 \t 674.400723 s\n","761 \t 15.576470 \t 8.081988 \t7.472173 \t 0.022308 \t 675.110152 s\n","762 \t 15.088465 \t 7.851717 \t7.223446 \t 0.013302 \t 675.754922 s\n","763 \t 14.417002 \t 7.499248 \t6.897943 \t 0.019811 \t 676.449007 s\n","764 \t 14.447850 \t 7.472067 \t6.940517 \t 0.035266 \t 676.993963 s\n","765 \t 14.870782 \t 7.868520 \t6.986111 \t 0.016152 \t 677.658509 s\n","766 \t 15.067932 \t 7.919449 \t7.132494 \t 0.015988 \t 678.189080 s\n","767 \t 15.491271 \t 8.088713 \t7.382702 \t 0.019856 \t 678.720689 s\n","768 \t 15.227480 \t 7.850128 \t7.201940 \t 0.175412 \t 679.256455 s\n","769 \t 15.374428 \t 7.726367 \t7.622966 \t 0.025095 \t 679.784138 s\n","770 \t 15.392212 \t 8.053979 \t7.319300 \t 0.018933 \t 680.308100 s\n","771 \t 14.812223 \t 8.131113 \t6.648514 \t 0.032597 \t 680.832903 s\n","772 \t 15.019222 \t 7.659269 \t7.344270 \t 0.015683 \t 681.364897 s\n","773 \t 14.797390 \t 7.994811 \t6.777668 \t 0.024910 \t 681.889254 s\n","774 \t 15.150939 \t 7.585962 \t7.527788 \t 0.037189 \t 682.553816 s\n","775 \t 15.467316 \t 8.329195 \t7.117199 \t 0.020922 \t 683.084251 s\n","776 \t 15.442254 \t 8.067086 \t7.359902 \t 0.015266 \t 683.619579 s\n","777 \t 15.081718 \t 8.034167 \t7.024385 \t 0.023167 \t 684.161826 s\n","778 \t 14.987298 \t 8.229620 \t6.741138 \t 0.016539 \t 684.727903 s\n","779 \t 15.615101 \t 8.159551 \t7.407393 \t 0.048158 \t 685.362813 s\n","780 \t 14.998117 \t 7.920817 \t7.048346 \t 0.028954 \t 685.972114 s\n","781 \t 14.861183 \t 7.739408 \t7.105160 \t 0.016614 \t 686.613242 s\n","782 \t 14.845320 \t 7.674561 \t7.149493 \t 0.021267 \t 687.311181 s\n","783 \t 15.152007 \t 8.181051 \t6.936277 \t 0.034678 \t 687.886451 s\n","784 \t 16.050788 \t 8.836390 \t7.194674 \t 0.019723 \t 688.554422 s\n","785 \t 14.894102 \t 7.659426 \t7.210907 \t 0.023769 \t 689.075201 s\n","786 \t 15.420437 \t 8.041958 \t7.339703 \t 0.038776 \t 689.603142 s\n","787 \t 14.616221 \t 7.672701 \t6.926761 \t 0.016759 \t 690.129633 s\n","788 \t 14.956734 \t 7.631598 \t7.311062 \t 0.014074 \t 690.657988 s\n","789 \t 15.126644 \t 8.063848 \t7.041940 \t 0.020856 \t 691.192336 s\n","790 \t 15.200080 \t 8.080845 \t7.100484 \t 0.018751 \t 691.724798 s\n","791 \t 14.563190 \t 7.274450 \t7.276695 \t 0.012045 \t 692.248493 s\n","792 \t 15.344011 \t 8.291376 \t7.015882 \t 0.036753 \t 692.786572 s\n","793 \t 15.104255 \t 7.605031 \t7.478752 \t 0.020472 \t 693.453022 s\n","794 \t 15.059290 \t 7.980770 \t7.066537 \t 0.011984 \t 693.994949 s\n","795 \t 14.892569 \t 7.752057 \t7.125970 \t 0.014541 \t 694.530259 s\n","796 \t 14.980246 \t 7.918124 \t7.033981 \t 0.028141 \t 695.080896 s\n","797 \t 15.231544 \t 7.720432 \t7.491077 \t 0.020036 \t 695.620769 s\n","798 \t 15.189348 \t 7.892652 \t7.282777 \t 0.013919 \t 696.142207 s\n","799 \t 14.654439 \t 7.275160 \t7.360063 \t 0.019216 \t 696.677361 s\n","800 \t 14.967451 \t 7.814631 \t7.090333 \t 0.062486 \t 697.200819 s\n","801 \t 15.233731 \t 8.448122 \t6.773697 \t 0.011912 \t 697.794841 s\n","802 \t 15.394537 \t 8.268003 \t7.110948 \t 0.015586 \t 698.420944 s\n","803 \t 14.730349 \t 7.600027 \t7.109212 \t 0.021110 \t 699.228125 s\n","804 \t 14.650617 \t 7.495796 \t7.138161 \t 0.016659 \t 699.874415 s\n","805 \t 14.734726 \t 7.806880 \t6.911430 \t 0.016415 \t 700.513949 s\n","806 \t 15.378415 \t 8.109163 \t7.219021 \t 0.050231 \t 701.046246 s\n","807 \t 14.961818 \t 7.863188 \t7.080731 \t 0.017899 \t 701.579375 s\n","808 \t 15.281018 \t 7.982133 \t7.274356 \t 0.024528 \t 702.111903 s\n","809 \t 14.680226 \t 7.409851 \t7.232708 \t 0.037668 \t 702.638934 s\n","810 \t 13.891435 \t 7.147590 \t6.720076 \t 0.023768 \t 703.164993 s\n","811 \t 15.350249 \t 7.879379 \t7.443814 \t 0.027056 \t 703.681089 s\n","812 \t 14.849626 \t 7.769162 \t7.067835 \t 0.012629 \t 704.210470 s\n","813 \t 14.568432 \t 7.724108 \t6.813949 \t 0.030376 \t 704.871844 s\n","814 \t 14.995437 \t 7.937675 \t7.046182 \t 0.011580 \t 705.415524 s\n","815 \t 14.872747 \t 7.530081 \t7.327399 \t 0.015267 \t 705.933660 s\n","816 \t 14.660774 \t 7.812224 \t6.836253 \t 0.012297 \t 706.465466 s\n","817 \t 15.455067 \t 8.161672 \t7.282465 \t 0.010931 \t 706.986387 s\n","818 \t 14.836726 \t 7.785341 \t7.028600 \t 0.022786 \t 707.539475 s\n","819 \t 15.348634 \t 7.814625 \t7.510179 \t 0.023830 \t 708.070057 s\n","820 \t 13.978627 \t 6.801303 \t7.158801 \t 0.018523 \t 708.605343 s\n","821 \t 15.620730 \t 8.427423 \t7.168644 \t 0.024662 \t 709.150231 s\n","822 \t 14.923690 \t 8.017846 \t6.888236 \t 0.017608 \t 709.681045 s\n","823 \t 15.063663 \t 7.904619 \t7.145947 \t 0.013098 \t 710.333795 s\n","824 \t 14.367721 \t 7.579467 \t6.773530 \t 0.014723 \t 710.964298 s\n","825 \t 14.495777 \t 7.651322 \t6.821502 \t 0.022953 \t 711.586978 s\n","826 \t 15.212912 \t 7.666012 \t7.514450 \t 0.032450 \t 712.210783 s\n","827 \t 14.818180 \t 7.568295 \t7.238936 \t 0.010948 \t 712.865135 s\n","828 \t 15.224217 \t 7.951502 \t7.256874 \t 0.015841 \t 713.499069 s\n","829 \t 15.017901 \t 7.785405 \t7.210885 \t 0.021611 \t 714.020457 s\n","830 \t 14.878320 \t 7.643761 \t7.218666 \t 0.015893 \t 714.548661 s\n","831 \t 15.598468 \t 8.167789 \t7.376276 \t 0.054404 \t 715.069569 s\n","832 \t 14.973834 \t 7.830803 \t7.119730 \t 0.023301 \t 715.598424 s\n","833 \t 14.931986 \t 8.114241 \t6.797432 \t 0.020314 \t 716.126572 s\n","834 \t 14.097313 \t 7.189800 \t6.881204 \t 0.026309 \t 716.787815 s\n","835 \t 14.930369 \t 7.872933 \t7.042510 \t 0.014926 \t 717.312004 s\n","836 \t 14.951518 \t 8.130968 \t6.808483 \t 0.012066 \t 717.879044 s\n","837 \t 14.700254 \t 7.681480 \t6.992173 \t 0.026601 \t 718.404782 s\n","838 \t 14.186215 \t 7.461195 \t6.711532 \t 0.013489 \t 718.935486 s\n","839 \t 14.479301 \t 7.607332 \t6.831085 \t 0.040884 \t 719.450239 s\n","840 \t 15.301795 \t 8.031908 \t7.245757 \t 0.024129 \t 720.002614 s\n","841 \t 14.526473 \t 7.553636 \t6.942669 \t 0.030168 \t 720.555430 s\n","842 \t 15.139330 \t 8.085405 \t7.028327 \t 0.025598 \t 721.083798 s\n","843 \t 15.114758 \t 7.876536 \t7.134416 \t 0.103806 \t 721.637193 s\n","844 \t 14.513779 \t 7.511319 \t6.981469 \t 0.020991 \t 722.166452 s\n","845 \t 14.852577 \t 7.945997 \t6.880320 \t 0.026260 \t 722.700747 s\n","846 \t 14.677495 \t 7.386270 \t7.277890 \t 0.013336 \t 723.354863 s\n","847 \t 15.172651 \t 7.918952 \t7.218003 \t 0.035697 \t 724.004224 s\n","848 \t 14.228184 \t 7.499852 \t6.706506 \t 0.021826 \t 724.625898 s\n","849 \t 15.182467 \t 7.684596 \t7.486617 \t 0.011255 \t 725.258992 s\n","850 \t 15.598096 \t 8.080532 \t7.501455 \t 0.016109 \t 725.891402 s\n","851 \t 14.404758 \t 7.433368 \t6.955438 \t 0.015952 \t 726.520344 s\n","852 \t 14.977952 \t 7.784454 \t7.180679 \t 0.012818 \t 727.048014 s\n","853 \t 15.022738 \t 7.720484 \t7.283970 \t 0.018285 \t 727.574673 s\n","854 \t 15.042674 \t 7.437283 \t7.592229 \t 0.013162 \t 728.106595 s\n","855 \t 14.304240 \t 7.303890 \t6.976337 \t 0.024013 \t 728.627256 s\n","856 \t 15.173527 \t 7.820836 \t7.340760 \t 0.011932 \t 729.149442 s\n","857 \t 15.428037 \t 7.927254 \t7.475373 \t 0.025409 \t 729.669945 s\n","858 \t 15.134862 \t 7.698871 \t7.420573 \t 0.015418 \t 730.189313 s\n","859 \t 14.229892 \t 7.427197 \t6.789343 \t 0.013353 \t 730.843740 s\n","860 \t 14.911047 \t 7.556338 \t7.339132 \t 0.015577 \t 731.388913 s\n","861 \t 15.875031 \t 8.430188 \t7.430770 \t 0.014072 \t 731.911827 s\n","862 \t 14.946026 \t 7.655061 \t7.282234 \t 0.008731 \t 732.444555 s\n","863 \t 14.403543 \t 7.643867 \t6.748592 \t 0.011083 \t 732.965342 s\n","864 \t 14.569992 \t 7.495938 \t7.041113 \t 0.032941 \t 733.512776 s\n","865 \t 14.669421 \t 7.657174 \t6.988531 \t 0.023717 \t 734.047822 s\n","866 \t 14.746370 \t 7.702657 \t7.030153 \t 0.013561 \t 734.565295 s\n","867 \t 14.773177 \t 7.711662 \t7.044731 \t 0.016784 \t 735.095964 s\n","868 \t 14.439060 \t 7.394351 \t7.009870 \t 0.034839 \t 735.756239 s\n","869 \t 15.081361 \t 7.955616 \t7.105722 \t 0.020024 \t 736.281357 s\n","870 \t 15.278602 \t 7.659072 \t7.603621 \t 0.015909 \t 736.903026 s\n","871 \t 14.838854 \t 7.891753 \t6.933058 \t 0.014042 \t 737.531648 s\n","872 \t 14.149049 \t 7.344753 \t6.792832 \t 0.011465 \t 738.149603 s\n","873 \t 15.090765 \t 7.681601 \t7.392258 \t 0.016906 \t 738.822180 s\n","874 \t 15.027288 \t 7.963032 \t7.048147 \t 0.016110 \t 739.465360 s\n","875 \t 15.254915 \t 7.942143 \t7.286189 \t 0.026582 \t 739.976504 s\n","876 \t 15.106896 \t 7.486113 \t7.602803 \t 0.017980 \t 740.505011 s\n","877 \t 15.044848 \t 7.879756 \t7.142850 \t 0.022242 \t 741.167786 s\n","878 \t 14.849657 \t 7.575798 \t7.251482 \t 0.022377 \t 741.691682 s\n","879 \t 14.351650 \t 7.478478 \t6.858781 \t 0.014392 \t 742.215251 s\n","880 \t 14.600211 \t 7.556794 \t7.030536 \t 0.012881 \t 742.738688 s\n","881 \t 14.651213 \t 7.760992 \t6.873215 \t 0.017006 \t 743.263415 s\n","882 \t 13.893116 \t 7.167965 \t6.711586 \t 0.013566 \t 743.787436 s\n","883 \t 15.303967 \t 7.777513 \t7.513879 \t 0.012575 \t 744.302174 s\n","884 \t 14.622395 \t 7.366131 \t7.241722 \t 0.014542 \t 744.829383 s\n","885 \t 14.705887 \t 7.286457 \t7.406102 \t 0.013328 \t 745.351068 s\n","886 \t 14.353144 \t 7.610572 \t6.720978 \t 0.021594 \t 745.887187 s\n","887 \t 14.271758 \t 7.197924 \t7.061451 \t 0.012383 \t 746.549447 s\n","888 \t 14.604207 \t 7.500491 \t7.087430 \t 0.016285 \t 747.065378 s\n","889 \t 14.473156 \t 7.424242 \t7.030897 \t 0.018017 \t 747.598285 s\n","890 \t 14.618714 \t 7.714026 \t6.890999 \t 0.013688 \t 748.121030 s\n","891 \t 14.056819 \t 7.362643 \t6.666462 \t 0.027713 \t 748.650315 s\n","892 \t 14.735020 \t 7.732846 \t6.986266 \t 0.015909 \t 749.170570 s\n","893 \t 14.166540 \t 7.595129 \t6.560311 \t 0.011100 \t 749.759441 s\n","894 \t 15.617246 \t 8.093520 \t7.478070 \t 0.045657 \t 750.375781 s\n","895 \t 14.692361 \t 7.644894 \t7.028193 \t 0.019273 \t 750.989954 s\n","896 \t 14.806007 \t 7.662631 \t7.121531 \t 0.021846 \t 751.637135 s\n","897 \t 14.404517 \t 7.466527 \t6.919188 \t 0.018802 \t 752.460431 s\n","898 \t 14.196954 \t 7.361369 \t6.816456 \t 0.019130 \t 752.984219 s\n","899 \t 14.864677 \t 7.782080 \t7.065302 \t 0.017295 \t 753.510414 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:46<00:00,  2.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4700\n","Hit@10: 0.6654\n","Hit@3: 0.5000\n","Hit@1: 0.3731\n","Link Prediction on Validation Set (All)\n","MRR: 0.4407\n","Hit@10: 0.6962\n","Hit@3: 0.4992\n","Hit@1: 0.3139\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3466\n","Hit@10: 0.5846\n","Hit@3: 0.3846\n","Hit@1: 0.2308\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6932\n","Hit@10: 0.8520\n","Hit@3: 0.7480\n","Hit@1: 0.6034\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0648\n","900 \t 14.399998 \t 7.447032 \t6.936704 \t 0.016262 \t 801.676170 s\n","901 \t 14.367241 \t 7.316173 \t7.021646 \t 0.029423 \t 802.279049 s\n","902 \t 15.280507 \t 7.922012 \t7.349074 \t 0.009422 \t 802.913672 s\n","903 \t 14.755895 \t 7.494268 \t7.248904 \t 0.012722 \t 803.521369 s\n","904 \t 14.334918 \t 7.172337 \t7.129187 \t 0.033395 \t 804.161066 s\n","905 \t 14.250218 \t 7.375894 \t6.859961 \t 0.014363 \t 804.838953 s\n","906 \t 14.610761 \t 7.753670 \t6.843994 \t 0.013097 \t 805.372404 s\n","907 \t 14.330608 \t 7.503731 \t6.803487 \t 0.023390 \t 806.038277 s\n","908 \t 14.019814 \t 7.249919 \t6.747698 \t 0.022197 \t 806.566741 s\n","909 \t 15.020778 \t 8.131602 \t6.878146 \t 0.011031 \t 807.099607 s\n","910 \t 15.276585 \t 7.642814 \t7.621607 \t 0.012164 \t 807.683646 s\n","911 \t 14.496460 \t 7.450553 \t7.014631 \t 0.031276 \t 808.466325 s\n","912 \t 14.359242 \t 7.539143 \t6.805600 \t 0.014499 \t 809.292727 s\n","913 \t 15.343713 \t 7.687359 \t7.636541 \t 0.019812 \t 810.107787 s\n","914 \t 14.744803 \t 7.836133 \t6.897522 \t 0.011147 \t 810.658023 s\n","915 \t 14.813343 \t 7.522079 \t7.271682 \t 0.019582 \t 811.184133 s\n","916 \t 14.411176 \t 7.457397 \t6.935374 \t 0.018404 \t 811.742151 s\n","917 \t 14.872061 \t 8.021786 \t6.834351 \t 0.015923 \t 812.416657 s\n","918 \t 15.159894 \t 7.749448 \t7.399542 \t 0.010905 \t 812.937683 s\n","919 \t 14.866132 \t 7.372554 \t7.476686 \t 0.016892 \t 813.464336 s\n","920 \t 14.271974 \t 7.626271 \t6.635319 \t 0.010385 \t 813.986869 s\n","921 \t 14.883743 \t 7.668768 \t7.189892 \t 0.025082 \t 814.521988 s\n","922 \t 13.951392 \t 7.337675 \t6.604636 \t 0.009081 \t 815.091033 s\n","923 \t 14.677979 \t 7.431899 \t7.230895 \t 0.015186 \t 815.727298 s\n","924 \t 14.423426 \t 7.304358 \t7.097512 \t 0.021556 \t 816.337829 s\n","925 \t 14.840386 \t 7.672263 \t7.156337 \t 0.011786 \t 816.988938 s\n","926 \t 14.710111 \t 7.941395 \t6.754326 \t 0.014389 \t 817.697410 s\n","927 \t 14.249680 \t 7.350756 \t6.881355 \t 0.017568 \t 818.267276 s\n","928 \t 14.714523 \t 7.903461 \t6.798501 \t 0.012562 \t 818.792388 s\n","929 \t 14.062635 \t 7.404125 \t6.599341 \t 0.059168 \t 819.467753 s\n","930 \t 14.406983 \t 7.681119 \t6.714667 \t 0.011196 \t 820.004957 s\n","931 \t 14.644835 \t 7.619699 \t7.011787 \t 0.013349 \t 820.549585 s\n","932 \t 14.076920 \t 7.496559 \t6.561872 \t 0.018490 \t 821.097263 s\n","933 \t 14.715529 \t 7.689776 \t7.012766 \t 0.012987 \t 821.626749 s\n","934 \t 14.713727 \t 7.623194 \t7.076892 \t 0.013641 \t 822.168830 s\n","935 \t 14.483004 \t 7.596396 \t6.875306 \t 0.011302 \t 822.689703 s\n","936 \t 14.677506 \t 7.554889 \t7.106066 \t 0.016551 \t 823.226920 s\n","937 \t 14.426671 \t 7.401979 \t7.006588 \t 0.018105 \t 823.764959 s\n","938 \t 15.020819 \t 7.698278 \t7.310974 \t 0.011567 \t 824.300439 s\n","939 \t 14.626073 \t 7.724481 \t6.893788 \t 0.007804 \t 824.843205 s\n","940 \t 14.172399 \t 7.189241 \t6.971182 \t 0.011975 \t 825.378602 s\n","941 \t 14.185355 \t 7.457500 \t6.713246 \t 0.014609 \t 825.917758 s\n","942 \t 15.716424 \t 8.599121 \t7.105351 \t 0.011952 \t 826.586939 s\n","943 \t 14.871957 \t 7.847241 \t7.011550 \t 0.013166 \t 827.122402 s\n","944 \t 14.791371 \t 7.697701 \t7.077782 \t 0.015888 \t 827.643210 s\n","945 \t 14.661880 \t 7.342550 \t7.303551 \t 0.015779 \t 828.237714 s\n","946 \t 14.845857 \t 7.551820 \t7.281909 \t 0.012127 \t 828.857225 s\n","947 \t 15.128001 \t 7.635190 \t7.478219 \t 0.014591 \t 829.505672 s\n","948 \t 15.058414 \t 7.762716 \t7.284186 \t 0.011512 \t 830.164089 s\n","949 \t 14.135674 \t 7.591467 \t6.531614 \t 0.012593 \t 830.870542 s\n","950 \t 14.327625 \t 7.552602 \t6.759303 \t 0.015720 \t 831.421973 s\n","951 \t 14.067714 \t 7.012369 \t7.043063 \t 0.012283 \t 831.962247 s\n","952 \t 14.084946 \t 7.206935 \t6.866502 \t 0.011509 \t 832.511164 s\n","953 \t 15.266488 \t 7.403357 \t7.844612 \t 0.018519 \t 833.036210 s\n","954 \t 14.669914 \t 7.782269 \t6.866280 \t 0.021365 \t 833.577828 s\n","955 \t 14.871565 \t 7.800946 \t7.037280 \t 0.033339 \t 834.253691 s\n","956 \t 15.053290 \t 7.389828 \t7.651444 \t 0.012018 \t 834.788410 s\n","957 \t 14.726306 \t 7.578328 \t7.131323 \t 0.016656 \t 835.325031 s\n","958 \t 13.887585 \t 7.019435 \t6.854701 \t 0.013450 \t 835.859977 s\n","959 \t 15.432270 \t 7.802337 \t7.616173 \t 0.013759 \t 836.394570 s\n","960 \t 14.853836 \t 7.182960 \t7.657940 \t 0.012937 \t 836.933313 s\n","961 \t 14.631505 \t 7.469460 \t7.146446 \t 0.015599 \t 837.457263 s\n","962 \t 14.952322 \t 7.829685 \t7.112797 \t 0.009840 \t 837.985528 s\n","963 \t 14.260794 \t 7.478291 \t6.604905 \t 0.177598 \t 838.644935 s\n","964 \t 14.810193 \t 7.750184 \t7.043689 \t 0.016320 \t 839.171206 s\n","965 \t 14.355482 \t 7.307799 \t7.037529 \t 0.010154 \t 839.697001 s\n","966 \t 14.199367 \t 7.248546 \t6.923315 \t 0.027505 \t 840.227665 s\n","967 \t 14.610214 \t 7.305923 \t7.284428 \t 0.019864 \t 840.767645 s\n","968 \t 14.669164 \t 7.395318 \t7.257572 \t 0.016273 \t 841.373398 s\n","969 \t 13.917037 \t 6.987191 \t6.906030 \t 0.023815 \t 841.997092 s\n","970 \t 14.530377 \t 7.351146 \t7.155669 \t 0.023563 \t 842.620752 s\n","971 \t 14.257497 \t 7.285140 \t6.960406 \t 0.011952 \t 843.256000 s\n","972 \t 14.584226 \t 7.450291 \t7.091147 \t 0.042788 \t 843.924577 s\n","973 \t 14.647201 \t 7.655523 \t6.978891 \t 0.012787 \t 844.585572 s\n","974 \t 14.435791 \t 7.215811 \t7.208055 \t 0.011923 \t 845.112349 s\n","975 \t 14.454343 \t 7.627401 \t6.812637 \t 0.014305 \t 845.641950 s\n","976 \t 14.472413 \t 7.308218 \t7.132001 \t 0.032195 \t 846.166553 s\n","977 \t 14.209375 \t 7.639080 \t6.559103 \t 0.011192 \t 846.692464 s\n","978 \t 14.206544 \t 7.120965 \t7.073826 \t 0.011752 \t 847.217997 s\n","979 \t 15.053065 \t 7.752659 \t7.288940 \t 0.011465 \t 847.750076 s\n","980 \t 14.081556 \t 7.125128 \t6.930347 \t 0.026081 \t 848.282484 s\n","981 \t 15.024117 \t 7.817559 \t7.188805 \t 0.017753 \t 848.814373 s\n","982 \t 14.097824 \t 7.174218 \t6.908200 \t 0.015406 \t 849.334405 s\n","983 \t 14.575836 \t 7.292326 \t7.273282 \t 0.010227 \t 849.866708 s\n","984 \t 14.906894 \t 7.862339 \t7.034120 \t 0.010434 \t 850.525451 s\n","985 \t 14.472389 \t 7.613708 \t6.847132 \t 0.011548 \t 851.051721 s\n","986 \t 13.968595 \t 7.149660 \t6.803654 \t 0.015281 \t 851.576416 s\n","987 \t 13.907400 \t 7.147330 \t6.744617 \t 0.015453 \t 852.105605 s\n","988 \t 15.002417 \t 7.830864 \t7.159088 \t 0.012465 \t 852.627719 s\n","989 \t 14.391921 \t 7.573521 \t6.806205 \t 0.012194 \t 853.163945 s\n","990 \t 15.007062 \t 7.835343 \t7.155459 \t 0.016260 \t 853.686516 s\n","991 \t 14.758809 \t 7.721066 \t7.019645 \t 0.018098 \t 854.271450 s\n","992 \t 14.857905 \t 7.611594 \t7.235279 \t 0.011032 \t 854.894599 s\n","993 \t 14.542815 \t 7.584500 \t6.945098 \t 0.013217 \t 855.542430 s\n","994 \t 14.076037 \t 7.063125 \t6.999315 \t 0.013598 \t 856.186675 s\n","995 \t 13.881517 \t 7.323466 \t6.535274 \t 0.022778 \t 856.997782 s\n","996 \t 13.901502 \t 7.142982 \t6.745025 \t 0.013495 \t 857.539775 s\n","997 \t 15.120751 \t 7.818532 \t7.276325 \t 0.025894 \t 858.062020 s\n","998 \t 14.404749 \t 7.687312 \t6.706818 \t 0.010620 \t 858.605700 s\n","999 \t 14.547711 \t 7.650528 \t6.872766 \t 0.024416 \t 859.132996 s\n","1000 \t 13.762441 \t 6.869159 \t6.883944 \t 0.009337 \t 859.671249 s\n","1001 \t 14.145040 \t 7.214742 \t6.914580 \t 0.015719 \t 860.217064 s\n","1002 \t 14.560751 \t 7.608745 \t6.942759 \t 0.009247 \t 860.731997 s\n","1003 \t 14.645594 \t 7.430384 \t7.202269 \t 0.012940 \t 861.264781 s\n","1004 \t 15.356665 \t 8.278678 \t7.068208 \t 0.009778 \t 861.779030 s\n","1005 \t 14.464613 \t 7.450766 \t7.002469 \t 0.011377 \t 862.318579 s\n","1006 \t 14.445653 \t 7.406988 \t7.023682 \t 0.014983 \t 862.993639 s\n","1007 \t 13.991638 \t 7.339535 \t6.637869 \t 0.014235 \t 863.525424 s\n","1008 \t 14.000347 \t 7.234824 \t6.750218 \t 0.015305 \t 864.042616 s\n","1009 \t 14.442565 \t 7.502226 \t6.908221 \t 0.032118 \t 864.567013 s\n","1010 \t 14.233358 \t 7.390082 \t6.834979 \t 0.008298 \t 865.089130 s\n","1011 \t 14.598793 \t 7.538472 \t7.047716 \t 0.012604 \t 865.620436 s\n","1012 \t 14.109206 \t 7.186893 \t6.910607 \t 0.011706 \t 866.148057 s\n","1013 \t 14.073615 \t 7.246001 \t6.813895 \t 0.013719 \t 866.671241 s\n","1014 \t 14.686234 \t 7.835872 \t6.833537 \t 0.016824 \t 867.253075 s\n","1015 \t 14.386086 \t 7.397141 \t6.975881 \t 0.013064 \t 867.871902 s\n","1016 \t 15.079903 \t 7.723183 \t7.345524 \t 0.011196 \t 868.520778 s\n","1017 \t 14.718042 \t 7.256522 \t7.443537 \t 0.017982 \t 869.164112 s\n","1018 \t 14.437765 \t 7.216232 \t7.203858 \t 0.017675 \t 869.852033 s\n","1019 \t 14.958316 \t 7.706058 \t7.232307 \t 0.019951 \t 870.538044 s\n","1020 \t 14.889472 \t 7.798872 \t7.016141 \t 0.074459 \t 871.072143 s\n","1021 \t 15.065094 \t 7.586405 \t7.463484 \t 0.015205 \t 871.608694 s\n","1022 \t 13.733705 \t 6.840268 \t6.880967 \t 0.012469 \t 872.156340 s\n","1023 \t 14.419307 \t 7.131779 \t7.276187 \t 0.011341 \t 872.682518 s\n","1024 \t 14.047394 \t 7.020144 \t7.017892 \t 0.009358 \t 873.213588 s\n","1025 \t 14.420576 \t 7.425596 \t6.976290 \t 0.018690 \t 873.739923 s\n","1026 \t 14.752514 \t 7.311982 \t7.430190 \t 0.010343 \t 874.259525 s\n","1027 \t 14.353518 \t 7.414191 \t6.925579 \t 0.013747 \t 874.794578 s\n","1028 \t 14.436806 \t 7.476593 \t6.947170 \t 0.013042 \t 875.452386 s\n","1029 \t 14.671493 \t 7.421941 \t7.230656 \t 0.018895 \t 875.976381 s\n","1030 \t 14.547636 \t 7.404033 \t7.132141 \t 0.011462 \t 876.500740 s\n","1031 \t 14.641805 \t 7.646705 \t6.980585 \t 0.014515 \t 877.031737 s\n","1032 \t 13.991464 \t 6.907311 \t7.075865 \t 0.008287 \t 877.556365 s\n","1033 \t 14.534085 \t 7.243300 \t7.281021 \t 0.009764 \t 878.082513 s\n","1034 \t 14.551265 \t 7.578902 \t6.962113 \t 0.010250 \t 878.604357 s\n","1035 \t 14.915959 \t 8.149638 \t6.750473 \t 0.015849 \t 879.133529 s\n","1036 \t 14.399216 \t 7.535599 \t6.854099 \t 0.009518 \t 879.657396 s\n","1037 \t 14.493002 \t 7.328578 \t7.145937 \t 0.018487 \t 880.385362 s\n","1038 \t 14.101526 \t 7.750144 \t6.335384 \t 0.015999 \t 881.010091 s\n","1039 \t 14.165926 \t 7.160038 \t6.994371 \t 0.011517 \t 881.623346 s\n","1040 \t 14.297298 \t 7.136903 \t7.149170 \t 0.011226 \t 882.272480 s\n","1041 \t 14.451830 \t 7.417906 \t7.013628 \t 0.020296 \t 882.934177 s\n","1042 \t 14.206941 \t 7.130057 \t7.059946 \t 0.016937 \t 883.476034 s\n","1043 \t 14.098600 \t 7.339159 \t6.752070 \t 0.007371 \t 883.998347 s\n","1044 \t 14.617093 \t 7.537343 \t7.039291 \t 0.040459 \t 884.541185 s\n","1045 \t 14.159758 \t 7.014842 \t7.122862 \t 0.022053 \t 885.065921 s\n","1046 \t 14.598943 \t 7.650822 \t6.894836 \t 0.053285 \t 885.733578 s\n","1047 \t 14.890042 \t 7.637793 \t7.237813 \t 0.014436 \t 886.263196 s\n","1048 \t 14.531138 \t 7.662045 \t6.855720 \t 0.013374 \t 886.781459 s\n","1049 \t 14.550206 \t 7.407932 \t7.126914 \t 0.015359 \t 887.317485 s\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 130/130 [00:47<00:00,  2.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.4814\n","Hit@10: 0.6846\n","Hit@3: 0.5192\n","Hit@1: 0.3846\n","Link Prediction on Validation Set (All)\n","MRR: 0.4538\n","Hit@10: 0.7078\n","Hit@3: 0.5092\n","Hit@1: 0.3322\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3592\n","Hit@10: 0.5923\n","Hit@3: 0.3846\n","Hit@1: 0.2462\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6970\n","Hit@10: 0.8542\n","Hit@3: 0.7514\n","Hit@1: 0.6068\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0630\n"]}]},{"cell_type":"markdown","source":["# Test.py"],"metadata":{"id":"fWMLVrKVXoQ6"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--exp', default='Reproduce') # 실험 이름\n","parser.add_argument('--data', default = \"VTHNKG-OA_CQI_seed42\", type = str)\n","parser.add_argument('--lr', default=4e-4, type=float)\n","parser.add_argument('--dim', default=256, type=int)\n","parser.add_argument('--num_epoch', default=1050, type=int)        # Tuning 필요\n","parser.add_argument('--valid_epoch', default=150, type=int)\n","parser.add_argument('--num_layer_enc_ent', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_enc_rel', default=4, type=int)   # Tuning 필요\n","#parser.add_argument('--num_layer_enc_nv', default=4, type=int)  < numeric value는 visual-textual feagture이 없으므로 transformer로 학습할 필요 X\n","parser.add_argument('--num_layer_prediction', default=4, type=int)   # Tuning 필요\n","parser.add_argument('--num_layer_context', default=4, type=int)  # Tuning 필요\n","parser.add_argument('--num_head', default=8, type=int)            # Tuning 필요?\n","parser.add_argument('--hidden_dim', default = 2048, type = int)   # Tuning 필요?\n","parser.add_argument('--dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--emb_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--vis_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--txt_dropout', default = 0.15, type = float)    # Tuning 필요\n","parser.add_argument('--smoothing', default = 0.4, type = float)   # Tuning 필요\n","parser.add_argument('--max_img_num', default = 3, type = int)\n","parser.add_argument('--batch_size', default = 1024, type = int)\n","parser.add_argument('--step_size', default = 150, type = int)     # Tuning 필요?\n","# exp, no_Write, emb_as_proj는 단순화 제외되었음.\n","args, unknown = parser.parse_known_args()"],"metadata":{"id":"yh_SZDs-XoGH","executionInfo":{"status":"ok","timestamp":1747974196323,"user_tz":-540,"elapsed":44,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def load_id_mapping(file_path):\n","    id2name = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            if line.strip() == \"\" or line.startswith(\"#\"):  # 주석 또는 공백 무시\n","                continue\n","            parts = line.strip().split('\\t')\n","            if len(parts) != 2:\n","                continue\n","            name, idx = parts\n","            id2name[int(idx)] = name\n","    return id2name"],"metadata":{"id":"BjS3Mu8kXr9D","executionInfo":{"status":"ok","timestamp":1747974147682,"user_tz":-540,"elapsed":6,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["id2ent = load_id_mapping(\"entity2id.txt\")\n","id2rel = load_id_mapping(\"relation2id.txt\")"],"metadata":{"id":"XjjcwxsxXuGq","executionInfo":{"status":"ok","timestamp":1747974152513,"user_tz":-540,"elapsed":1968,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def convert_triplet_ids_to_names(triplet, id2ent, id2rel, num_ent, num_rel):\n","    triplet_named = []\n","    for idx, val in enumerate(triplet):\n","        if idx % 2 == 0:  # entity or numeric value\n","            if val < num_ent:\n","                triplet_named.append(id2ent.get(val, f\"[ENT:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[NUM:{val - num_ent}]\")\n","        else:  # relation\n","            if val < num_rel:\n","                triplet_named.append(id2rel.get(val, f\"[REL:{val}]\"))\n","            else:\n","                triplet_named.append(f\"[MASK_REL]\")\n","    return triplet_named"],"metadata":{"id":"eQxfi98KXvyM","executionInfo":{"status":"ok","timestamp":1747974152527,"user_tz":-540,"elapsed":11,"user":{"displayName":"URP","userId":"16515248769931109428"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","KG = VTHNKG(args.data, max_vis_len = args.max_img_num, test = True)\n","\n","KG_DataLoader = torch.utils.data.DataLoader(KG, batch_size = args.batch_size ,shuffle = True)\n","\n","model = VTHN(\n","num_ent = KG.num_ent, # 엔티티 개수\n","num_rel = KG.num_rel, # relation 개수\n","## num_nv = KG.num_nv, # numeric value 개수 -> 필요 없음\n","## num_qual = KG.num_qual, # qualifier 개수 -> 필요 없음\n","ent_vis = KG.ent_vis_matrix, # entity에 대한 visual feature\n","rel_vis = KG.rel_vis_matrix, # relation에 대한 visual feature\n","dim_vis = KG.vis_feat_size, # visual feature의 dimension\n","ent_txt = KG.ent_txt_matrix, # entity의 textual feature\n","rel_txt = KG.rel_txt_matrix, # relation의 textual feature\n","dim_txt = KG.txt_feat_size, # textual feature의 dimension\n","ent_vis_mask = KG.ent_vis_mask, # entity의 visual feature의 유무 판정 마스크\n","rel_vis_mask = KG.rel_vis_mask, # relation의 visual feature의 유무 판정 마스크\n","dim_str = args.dim, # structual dimension(기본이 되는 차원)\n","num_head = args.num_head, # multihead 개수\n","dim_hid = args.hidden_dim, # ff layer hidden layer dimension\n","num_layer_enc_ent = args.num_layer_enc_ent, # entity encoder layer 개수\n","num_layer_enc_rel = args.num_layer_enc_rel, # relation encoder layer 개수\n","num_layer_prediction = args.num_layer_prediction, # prediction transformer layer 개수\n","num_layer_context = args.num_layer_context, # context transformer layer 개수\n","dropout = args.dropout, # transformer layer의 dropout\n","emb_dropout = args.emb_dropout, # structural embedding 생성에서의 dropout (structural 정보를 얼마나 버릴지 결정)\n","vis_dropout = args.vis_dropout, # visual embedding 생성에서의 dropout (visual 정보를 얼마나 버릴지 결정)\n","txt_dropout = args.txt_dropout, # textual embedding 생성에서의 dropout (textual 정보를 얼마나 버릴지 결정)\n","## max_qual = 5, # qualfier 최대 개수 (padding 때문에 필요) -> 이후의 batch_pad 계산 방식으로 인해 필요 없음.\n","emb_as_proj = False # 학습 효율성을 위한 조정\n",")\n","\n","model = model.cuda()\n","\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/code/VTHNKG-OA_CQI/checkpoint/Reproduce/VTHNKG-OA_CQI_seed42/lr_0.0004_dim_256__1050.ckpt\")[\"model_state_dict\"])\n","\n","model.eval()\n","\n","lp_tri_list_rank = []  # 기본 triplet 링크 예측 순위 저장\n","lp_all_list_rank = []  # 모든 링크 예측(기본+확장) 순위 저장\n","rp_tri_list_rank = []  # 기본 triplet 관계 예측 순위 저장\n","rp_all_list_rank = []  # 모든 관계 예측 순위 저장\n","nvp_tri_se = 0         # 기본 triplet 숫자값 예측 제곱 오차 합\n","nvp_tri_se_num = 0     # 기본 triplet 숫자값 예측 횟수\n","nvp_all_se = 0         # 모든 숫자값 예측 제곱 오차 합\n","nvp_all_se_num = 0     # 모든 숫자값 예측 횟수\n","with torch.no_grad():\n","    entity_pred_log = []\n","    relation_pred_log = []\n","    numeric_pred_log = []\n","    for tri, tri_pad, tri_num in tqdm(zip(KG.test, KG.test_pad, KG.test_num), total = len(KG.test)):\n","        tri_len = len(tri)\n","        pad_idx = 0\n","        for ent_idx in range((tri_len+1)//2): # 총 엔티티 개수만큼큼\n","            # 패딩 확인\n","            if tri_pad[pad_idx]:\n","                break\n","            if ent_idx != 0:\n","                pad_idx += 1\n","\n","            # 테스트 트리플렛\n","            test_triplet = torch.tensor([tri])\n","\n","            # 마스킹 위치 설정\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            if ent_idx < 2:\n","                mask_locs[0,0] = True\n","            else:\n","                mask_locs[0,ent_idx-1] = True\n","            if tri[ent_idx*2] >= KG.num_ent: # 숫자 예측 경우\n","                assert ent_idx != 0\n","                test_num = torch.tensor([tri_num])\n","                test_num[0,ent_idx-1] = -1\n","                # 숫자 마스킹 후 예측\n","                _,_,score_num = model(test_triplet.cuda(), test_num.cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_num = score_num.detach().cpu().numpy()\n","                if ent_idx == 1: # triplet의 숫자\n","                    # sq_error = (score_num[0,3,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                    # nvp_tri_se += sq_error\n","                    # nvp_tri_se_num += 1\n","                    pred = score_num[0, 3, tri[ent_idx*2] - KG.num_ent]\n","                    gt = tri_num[ent_idx - 1]\n","                    sq_error = (pred - gt) ** 2\n","                    nvp_tri_se += sq_error\n","                    nvp_tri_se_num += 1\n","                    # ⭐️ 예측값 출력\n","                    print(f\"[Triplet Num] GT: {gt:.4f}, Pred: {pred:.4f}, SE: {sq_error:.6f}\")\n","\n","                else: # qualifier\n","                  pred = score_num[0, 2, tri[ent_idx*2] - KG.num_ent]\n","                  gt = tri_num[ent_idx - 1]\n","                  sq_error = (pred - gt) ** 2\n","                  named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                  numeric_pred_log.append({\n","                      \"triplet_id\": str(tri),\n","                      \"triplet_named\": \":\".join(named_triplet),\n","                      \"position\": ent_idx,\n","                      \"type\": \"qualifier\",\n","                      \"gt\": float(gt),\n","                      \"pred\": float(pred),\n","                      \"se\": float(sq_error)\n","                  })\n","                    # sq_error = (score_num[0,2,tri[ent_idx*2]-KG.num_ent] - tri_num[ent_idx-1])**2\n","                nvp_all_se += sq_error\n","                nvp_all_se_num += 1\n","            else: # 엔티티 예측\n","                test_triplet[0,2*ent_idx] = KG.num_ent+KG.num_rel # 사용되는 특수 마스크 토큰 (다른 엔티티와 겹치지 않음)\n","                filt_tri = copy.deepcopy(tri)\n","                filt_tri[ent_idx*2] = 2*(KG.num_ent+KG.num_rel)\n","                if ent_idx != 1 and filt_tri[2] >= KG.num_ent:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[1] * 2 + tri_num[0])] # 숫자자\n","                else:\n","                    re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","                for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])): # qualifier에 대해 반복복\n","                    if tri_pad[qual_idx+1]:\n","                        break\n","                    if ent_idx != qual_idx + 2 and v >= KG.num_ent:\n","                        re_pair.append((q, q*2 + tri_num[qual_idx + 1]))\n","                    else:\n","                        re_pair.append((q,v))\n","                re_pair.sort()\n","                filt = KG.filter_dict[tuple(re_pair)]\n","                score_ent, _, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","                score_ent = score_ent.detach().cpu().numpy()\n","                if ent_idx < 2:\n","                    rank = calculate_rank(score_ent[0,1+2*ent_idx],tri[ent_idx*2], filt)\n","                    lp_tri_list_rank.append(rank)\n","                    topk = np.argsort(-score_ent[0,1+2*ent_idx])[:5]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                else:\n","                    rank = calculate_rank(score_ent[0,2], tri[ent_idx*2], filt)\n","                    try:\n","                      topk = np.argsort(-score_ent[0,2])[:5]\n","                    except:\n","                      topk = np.argsort(-score_ent[0,2])[:]\n","                    named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                    entity_pred_log.append({\n","                        \"triplet_id\": str(tri),\n","                        \"triplet_named\": \":\".join(named_triplet),\n","                        \"position\": ent_idx,\n","                        \"type\": \"head\" if ent_idx == 0 else \"tail\" if ent_idx == 1 else \"value\",\n","                        \"gt\": named_triplet[ent_idx*2],\n","                        \"top1\": id2ent.get(topk[0]),\n","                        \"top5\": [id2ent.get(i) for i in topk.tolist()],\n","                        \"rank\": int(rank)\n","                    })\n","                lp_all_list_rank.append(rank)\n","        for rel_idx in range(tri_len//2): # 관계에 대한 예측\n","            if tri_pad[rel_idx]:\n","                break\n","            mask_locs = torch.full((1,(KG.max_len-3)//2+1), False)\n","            mask_locs[0,rel_idx] = True\n","            test_triplet = torch.tensor([tri])\n","            orig_rels = tri[1::2]\n","            test_triplet[0, rel_idx*2 + 1] = KG.num_rel\n","            if test_triplet[0, rel_idx*2+2] >= KG.num_ent: # 숫자값의 경우 특수 마스크 토큰큰\n","                test_triplet[0, rel_idx*2 + 2] = KG.num_ent + KG.num_rel\n","            filt_tri = copy.deepcopy(tri)\n","            # 필터링 및 scoring (entity와 동일)\n","            filt_tri[rel_idx*2+1] = 2*(KG.num_ent+KG.num_rel)\n","            if filt_tri[2] >= KG.num_ent:\n","                re_pair = [(filt_tri[0], filt_tri[1], orig_rels[0]*2 + tri_num[0])]\n","            else:\n","                re_pair = [(filt_tri[0], filt_tri[1], filt_tri[2])]\n","            for qual_idx,(q,v) in enumerate(zip(filt_tri[3::2], filt_tri[4::2])):\n","                if tri_pad[qual_idx+1]:\n","                    break\n","                if v >= KG.num_ent:\n","                    re_pair.append((q, orig_rels[qual_idx + 1]*2 + tri_num[qual_idx + 1]))\n","                else:\n","                    re_pair.append((q,v))\n","            re_pair.sort()\n","            filt = KG.filter_dict[tuple(re_pair)]\n","            _,score_rel, _ = model(test_triplet.cuda(), torch.tensor([tri_num]).cuda(), torch.tensor([tri_pad]).cuda(), mask_locs)\n","            score_rel = score_rel.detach().cpu().numpy()\n","            if rel_idx == 0:\n","                rank = calculate_rank(score_rel[0,2], tri[rel_idx*2+1], filt)\n","                rp_tri_list_rank.append(rank)\n","                topk = np.argsort(-score_rel[0,2])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"relation\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            else:\n","                rank = calculate_rank(score_rel[0,1], tri[rel_idx*2+1], filt)\n","                topk = np.argsort(-score_rel[0,1])[:5]\n","                named_triplet = convert_triplet_ids_to_names(tri, id2ent, id2rel, KG.num_ent, KG.num_rel)\n","                relation_pred_log.append({\n","                    \"triplet_id\": str(tri),\n","                    \"triplet_named\": \":\".join(named_triplet),\n","                    \"position\": rel_idx,\n","                    \"type\": \"qualifier\",\n","                    \"gt\": named_triplet[rel_idx*2+1],\n","                    \"top1\": id2rel.get(topk[0]),\n","                    \"top5\": [id2rel.get(i) for i in topk.tolist()],\n","                    \"rank\": int(rank)\n","                })\n","            rp_all_list_rank.append(rank)\n","\n","lp_tri_list_rank = np.array(lp_tri_list_rank)\n","lp_tri_mrr, lp_tri_hit10, lp_tri_hit3, lp_tri_hit1 = metrics(lp_tri_list_rank)\n","print(\"Link Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {lp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {lp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {lp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {lp_tri_hit1:.4f}\")\n","\n","lp_all_list_rank = np.array(lp_all_list_rank)\n","lp_all_mrr, lp_all_hit10, lp_all_hit3, lp_all_hit1 = metrics(lp_all_list_rank)\n","print(\"Link Prediction on Validation Set (All)\")\n","print(f\"MRR: {lp_all_mrr:.4f}\")\n","print(f\"Hit@10: {lp_all_hit10:.4f}\")\n","print(f\"Hit@3: {lp_all_hit3:.4f}\")\n","print(f\"Hit@1: {lp_all_hit1:.4f}\")\n","\n","rp_tri_list_rank = np.array(rp_tri_list_rank)\n","rp_tri_mrr, rp_tri_hit10, rp_tri_hit3, rp_tri_hit1 = metrics(rp_tri_list_rank)\n","print(\"Relation Prediction on Validation Set (Tri)\")\n","print(f\"MRR: {rp_tri_mrr:.4f}\")\n","print(f\"Hit@10: {rp_tri_hit10:.4f}\")\n","print(f\"Hit@3: {rp_tri_hit3:.4f}\")\n","print(f\"Hit@1: {rp_tri_hit1:.4f}\")\n","\n","rp_all_list_rank = np.array(rp_all_list_rank)\n","rp_all_mrr, rp_all_hit10, rp_all_hit3, rp_all_hit1 = metrics(rp_all_list_rank)\n","print(\"Relation Prediction on Validation Set (All)\")\n","print(f\"MRR: {rp_all_mrr:.4f}\")\n","print(f\"Hit@10: {rp_all_hit10:.4f}\")\n","print(f\"Hit@3: {rp_all_hit3:.4f}\")\n","print(f\"Hit@1: {rp_all_hit1:.4f}\")\n","\n","if nvp_tri_se_num > 0:\n","    nvp_tri_rmse = math.sqrt(nvp_tri_se/nvp_tri_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (Tri)\")\n","    print(f\"RMSE: {nvp_tri_rmse:.4f}\")\n","\n","if nvp_all_se_num > 0:\n","    nvp_all_rmse = math.sqrt(nvp_all_se/nvp_all_se_num)\n","    print(\"Numeric Value Prediction on Validation Set (All)\")\n","    print(f\"RMSE: {nvp_all_rmse:.4f}\")\n","\n","pd.DataFrame(entity_pred_log).to_csv(\"entity_predictions.csv\", index=False)\n","pd.DataFrame(relation_pred_log).to_csv(\"relation_predictions.csv\", index=False)\n","pd.DataFrame(numeric_pred_log).to_csv(\"numeric_predictions.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtLTF6iXXw4I","executionInfo":{"status":"ok","timestamp":1747974359585,"user_tz":-540,"elapsed":51601,"user":{"displayName":"URP","userId":"16515248769931109428"}},"outputId":"e87cfd90-43a9-42d2-ea62-b4086ab1d79c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 132/132 [00:46<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Link Prediction on Validation Set (Tri)\n","MRR: 0.5209\n","Hit@10: 0.6818\n","Hit@3: 0.5606\n","Hit@1: 0.4394\n","Link Prediction on Validation Set (All)\n","MRR: 0.4723\n","Hit@10: 0.7070\n","Hit@3: 0.5400\n","Hit@1: 0.3578\n","Relation Prediction on Validation Set (Tri)\n","MRR: 0.3032\n","Hit@10: 0.5606\n","Hit@3: 0.3485\n","Hit@1: 0.1742\n","Relation Prediction on Validation Set (All)\n","MRR: 0.6966\n","Hit@10: 0.8545\n","Hit@3: 0.7534\n","Hit@1: 0.6057\n","Numeric Value Prediction on Validation Set (All)\n","RMSE: 0.0579\n"]}]}]}